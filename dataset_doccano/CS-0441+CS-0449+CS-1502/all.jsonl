{"id":7,"segment": ["test_set", "labeled"],  "course": "cs0449", "lec": "lec17", "text":"17\n\nFiles and\nDirectories\n\nIntroduction to\nSystems Software\n\nwilkie\n\nSpring 2019\/2020\n\n\fThe Nature of Data\nThe Lord of the Files\n\nSpring 2019\/2020\n\n2\n\n\fFiles and Data\n• What is a file? What is “data”? What is a binary file vs. a text file?\n\nfiles.c\nSpring 2019\/2020\n\nfiles.o\n\ncat.gif\n3\n\n\fFormats\n• What is a file “format”? Why are there so many different image types??\n▪ All files are flat binary blobs of information. How can we tell them apart?\n\n• Well, remember ELF? Our executable format?\n▪ And the MAGIC NUMBER inside the box?\n\n• This is one way we differentiate different files.\n▪ Archivists and librarians keep track of different\nfile formats when they digitize, store, and retrieve\ndata. They maintain the PRONOM database of formats.\n\n• Ok. How do we read files?\nSpring 2019\/2020\n\n4\n\n\fC Programming: Manipulating Files with stdio\nC(\n\n)\n\n• Here is a simple C program that creates a\nfile called “\n” and writes a string\nto it, then opens it again to print it out.\n\nOpen for\nwriting\n\n•\n\n:\n\nOpen read-only\n\nOpens a file with the given path. The\nstring that follows is the access mode.\n“ ” opens for writing; overwrites file.\n“ ” opens read-only.\n\n•\n\n:\n\nReads to the provided buffer the given\nnumber of bytes. (The “ ” is the number\nof elements to read if reading an array.)\n\nSeek 0 from end\nSeek to byte 0 •\nPolitely close the file\n\nSpring 2019\/2020\n\n:\n\nWrites from the provided buffer the given\nnumber of bytes. (Similar to\n)\n\n•\n\n:\n\nMoves the current file position.\n\n•\n\n:\n\nReturns the current file position.\n\n5\n\n\fC Programming: Manipulating Files with syscalls\nC(\n\n)\n\nOpen for\nwriting\n\n• Here is a simple C program that creates a\nfile called “\n” and writes a string\nto it, then opens it again to print it out.\n•\n\n:\n\nOpens a file with the given path. The\nvalue that follows is the access mode.\nopens for writing.\ncreates the file, if needed.\nremoves the data in the file.\nopens read-only.\n\n•\n\n:\n\nReads to the provided buffer the given\nnumber of bytes.\n\nOpen read-only\nSeek to end\nSeek to byte 0\nPolitely close the file\n\nSpring 2019\/2020\n\n•\n\n:\n\nWrites from the provided buffer the given\nnumber of bytes. (Similar to\n)\n\n•\n\n:\n\nMoves the current file position. Returns\nthe new position.\n\n6\n\n\fEverything is a “file”\n• UNIX makes lots of things “files” in a\nnon-traditional sense.\n• Sockets, named pipes, all kinds of exotic\nthings. Directories are files in the\ntraditional sense (stored on disk\nproperly)\n• You can use the\n,\n, and\nsystem calls with any of these diverse\nset of data streams.\nSpring 2019\/2020\n\nThe Internet\n7\n\n\fHow data is stored…\n• Data in RAM is generally volatile memory.\n▪ It disappears after you shut off your computer.\n\n• So, you want some kind of persistent memory.\n▪ Storing data on disk involves creating a physical\nrepresentation of that binary data.\n\n• Fun fact: there are non-volatile (persistent)\nmain memories in development. (NVRAM)\n▪ They are really neat! (and slow!)\n▪ But wow they really complicate things!!\n• Consider the implications.\n\n• Let’s dig in…\nSpring 2019\/2020\n\n8\n\n\fDisks\nYou throw them and dogs chase them. Wait.. no… don’t do that.\n\nSpring 2019\/2020\n\n9\n\n\fFloppy Disks\n• Data is stored in analog on magnetic material.\n▪ I love them.\n• Buy me a random box for my birthday, please.\n• When is my birthday? It is everyday.\n\n• Termed “floppy” due to the soft, flexible\nnature of the magnetic material.\n• Wait. Magnets?\n• How do you store data… with magnets?\n▪ If you’re thinking “they have two poles… so they\nare binary natured,” then you are on to something.\nSpring 2019\/2020\n\n10\n\n\fRepresenting continuous data…\n• If you have some (continuous) data, represented by a waveform…\n• How to transmit\/store that wave?\n• Amplitude modulation…\n▪ Send pulses of data sampling the wave.\n▪ Data encoded in the amplitude of pulse.\n\n• Frequency modulation…\n\nDifferent waveform streams\n\n▪ Data encoded in variation of frequency\nof pulse. (Yes, like FM radio)\n▪ Disks actually store data using a form of this encoding!\nSpring 2019\/2020\n\n11\n\n\fData and magnets… how do they work\n• We can use magnets to represent\n\nand\n\n(discrete binary)\n\n▪ The drive’s read head contains a sensor that detects the “magnetic flux”\n▪ It can sense a change in magnetism over time.\n• This shows the ideal world, without any modulation:\n\nN\n\nS\n\nS\n\nN\n\nN\n\nN\n\nS\n\nS\n\nS\n\nS\n\nN\n\nN\n\nS\n\nS\n\nS\n\nN\n\nN\n\nN\n\nSpring 2019\/2020\n\nNorth and South poles indicate stored values\n\n12\n\n\fThe peril of nature…\n• The magnetic drives read the change in magnetism.\n▪ It is difficult to tell the difference between two consecutive magnets…\n▪ This is also because co-aligned magnets HATE being next to each other.\n• Opposites attract, n’at. (They repel and affect each other’s signal)\n\nN\n\nS\n\nS\n\nN\n\nN\n\nN\n\nS\n\nS\n\nS\n\nS\n\nN\n\nN\n\nS\n\nS\n\nS\n\nN\n\nN\n\nN\n\nSpring 2019\/2020\n\n13\n\n\fThe peril of nature…\n• So, we can give up almost half of our data to add synchronization.\n▪ When we read, every other “sense” affects the next read.\n▪ If we read a “0” and then sense a change, the next bit stays the same.\n• If we sense a delay (long frequency), it is a “1” and we continue. (Modified-FM Encoding)\n\nSenses delay\n\nSwaps bit\n\nS\n\nN\n\nS\n\nN\n\nS\n\nS\n\nN\n\nS\n\nN\n\nN\n\nS\n\nN\n\nS\n\nN\n\nN\n\nS\n\nN\n\nS\n\nSpring 2019\/2020\n\nBounds the amount of “0”s physically stored.\n\n14\n\n\fDisk Drives\n• Also known as a “hard disk” due to the\ninflexible nature of its magnetic material.\n• Data is also stored digitally using a physical\nmedium, such as, again, magnets.\n▪ Uses a similar yet stronger encoding scheme.\n\nPlatter\n\nHead\n\n• Mechanical parts.\n▪ Can read random access, but it is slower than\nreading data sequentially (in physical order).\n\n• Bits are hard… let’s start abstracting…\nSpring 2019\/2020\n\n15\n\n\fThe platter matters:\n• Magnetic disk is represented by a set of\nstacked platters with magnetic bits.\n• A cylinder is a subdivision of platters (a\ntrack is such a subdivision on a single\nplatter.)\n• A sector is a subdivision of a\ncylinder\/track.\n▪ You typically read information from a\ndisk in units of sectors.\n▪ Files are, generally, a set of sectors.\nSpring 2019\/2020\n\nSector\nCylinder\n\n16\n\n\fMaking heads turn (actually, they don’t turn at all)\n• Magnetic disk is represented by a set of\nstacked platters with magnetic bits.\n▪ There may be several platters.\n▪ Each read by at least one head.\n▪ Access time is how long it takes to read a sector.\n\n• As a head moves, it goes to a different cylinder.\n▪ As the platter spins, the head reads\na different sector.\n\n• You can potentially read multiple sectors in\nparallel.\n▪ So how should we layout data on disk to take\nadvantage of this?\nSpring 2019\/2020\n\n17\n\n\fMaking best use of sequential access\n• Seek time is the time it takes for the head to get into\nposition. Latency: the time for the platter to spin.\n▪ Data is located at a two-dimensional coordinate\non a spinning surface.\n• so the math is not trivial.\n\n• Seek time is relative to the current\nposition of the head.\n▪ The closer the next bit of data you need…\n• The sooner it will get there.\n\n• So… to reduce the seek time to nil…\n▪ We position adjacent data in the same\ncylinder and respective sector.\n▪ Next set goes into subsequent sector. Heads don’t move; the platters spin.\nSpring 2019\/2020\n\n18\n\n\fAin’t no platter like a hard disk platter ‘cause a hard disk platter don’t stop\n\n• Seek time is the time it takes for the head to get into\nposition. Latency: the time for the platter to spin.\n▪ Data is located at a two-dimensional coordinate\non a spinning surface.\n• so the math is not trivial.\n\n• Seek time is relative to the current\nposition of the head.\n▪ The closer the next bit of data you need…\n• The sooner it will get there.\n\n• Here, the head does not have to move at\nall and blocks 0 and 1 are read easily.\n▪ Yet, to read block 2, we have to wait for the platter\nto completely spin back around!! Seek time is zero, but maximum latency loss.\nSpring 2019\/2020\n\n19\n\n\fAin’t no platter like a hard disk platter ‘cause a hard disk platter don’t stop\n\n• Seek time is the time it takes for the head to get into\nposition. Latency: the time for the platter to spin.\n▪ Data is located at a two-dimensional coordinate\non a spinning surface.\n• so the math is not trivial.\n\n• Seek time is relative to the current\nposition of the head.\n▪ The closer the next bit of data you need…\n• The sooner it will get there.\n\n• Yikes! Blocks are in different cylinders\nand subsequent blocks are behind the head.\n▪ Worst case! Latency and seek time really suffer.\n▪ Need to keep data in order! How do we organize data on disk?\nSpring 2019\/2020\n\n20\n\n\fFile Systems\nYet another abstraction… moving toward applications.\n\nSpring 2019\/2020\n\n21\n\n\fFile Systems\n• There are many ways of representing files on the disks themselves.\n• As you know, you are familiar with:\n▪ Files having names!\n▪ Directories\/folders for organization\n▪ Perhaps special files such as symbolic-links\/shortcuts\n\n• A file system entails describing how we represent:\n▪ File data (of course)\n▪ The location of the file (a file path)\n▪ Meta data about the file (what kind of file?)\n▪ Access control (who can access the file)\nSpring 2019\/2020\n\n22\n\n\fFile Metadata\n• There is a long list of possible\nmetadata associated with files:\n▪ The file size.\n▪ The file name.\n▪ When it was last accessed.\n▪ Who created it and when.\n\n• And access control:\n▪ Who can read it.\n▪ Who can write it.\n▪ Who can run it.\n\nSpring 2019\/2020\n\n23\n\n\fLinux\/UNIX\n\nSpring 2019\/2020\n\nmetadata:\n\n24\n\n\fOperating Systems and Files\n• The\nfunction returns a file descriptor, an integer that identifies\nthe open file in the process.\n▪ Every process can have open files, but none are shared across processes.\n\n• On Linux\/UNIX, some file descriptors are established automatically\nfor every process by the shell:\n▪\n▪\n▪\n\n– the output file (can be a file on disk! Recall terminal redirection.)\n– a file for error output.\n– the input file (could be a file on disk… or user input in the terminal.)\n\n• The OS maintains a table of open files per process. When it sees a\nsyscall such as\nor\n, it uses that table to determine the file.\nSpring 2019\/2020\n\n25\n\n\fProcesses and Files\n• The OS maintains a table of open files per process. When it sees a\nsyscall such as\nor\n, it uses that table to determine the file.\n• The table contains a set of\nopen files indexed by the file\ndescriptor.\n\nProcess\n\nstack\n.bss\n.data\n.text\nCPU State A:\nRegisters\n,\n\nPID: 4356\nPage Table A\nSpring 2019\/2020\n\nFile Table\n\n• Several files are generally\nopened for you by the shell.\n• Each open file maintains its\nown current position.\n▪\n\n\/\n\nmanipulate it.\n26\n\n\fI nodes, you nodes, we all nodes for inodes\n• Files are a set of disk blocks.\n▪ Hopefully laid out in a nice order!\n\n• How do we organize these?\n▪ Similar to virtual memory!\n\nmain.c\n\ninode\n\n• We use a disk block that holds\naddresses to other blocks.\n▪ It is a simple table. The blocks that\nmake up the file are in the order\nreflected by the table.\n\n• An index node is this main block.\n▪ Often seen shortened to “inode”\nSpring 2019\/2020\n\n27\n\n\fFile systems are about organizing the disk\nmain.c\n\ninode\n\nSpring 2019\/2020\n\n28\n\n\fCheap Versioning: WAFL\nHere is WAFL performing “snapshot” backups of files:\nWe can keep around snapshots and back them up\nto remote systems at our leisure.\n\nsnapshot\n\ninode\n\ninode\n\nThe prior\nstate\nremains.\n\nSpring 2019\/2020\n\nSmall changes overwrite\nonly parts of the file.\nOnce we back them up, we can\noverwrite the snapshot inode with the current inode.\n\n29\n\n\fHierarchies\n• Directories maintain strict hierarchical structure for files in the system.\n▪ For instance, your home folder is often something like\n\n.\n\n• An absolute path is a fully-qualified name for a file that indicates\nexactly where in the hierarchy it is located.\n▪ Often organized by a human being in some logical way:\n•\n\n▪ There are many special paths. OS data structures go in:\n•\n\n▪ Devices go in:\n•\n\n▪ Use the Linux\n\nSpring 2019\/2020\n\ncommand to find out where your system binaries go!\n30\n\n\fDirectories (Folders)\n• A directory is a file that contains a\nset of named links to other files.\n\nroot\n\n▪ The earliest file systems did not even\nhave directories… just a bunch of files.\n\nhome\n\n• Groups a set of files together under\na single name.\n\nsys\n\n▪ Strictly hierarchical…\n▪ Generally, a file can only be within one\ndirectory.\n▪ Although, a directory can also be within\na directory… creating a cascade.\nhw1.doc\n\nhw2.doc\n\nmain.c\n\n\/root\/home\/main.c\nSpring 2019\/2020\n\nmain.h\n\n31\n\n\fImplementing Directories\n• Directories can simply be text files, if you want!\n▪ Every line contains a name and then a block address on disk for the inode.\n\n• Obviously, there are a variety of ways to do this.\n▪ Do you keep a sorted order to make searching directories easier?\n▪ Can a directory refer to a file that is part of a different disk?\n• A file from a completely different machine??\n\n• If a directory is the only thing linking to a file, and removes that link,\nwhat happens?\n▪ How do you access a file if you cannot look up where it is?\n▪ Deleting a file is really as simple as removing it from the directory.\n▪ (And marking its blocks on disk free)\nSpring 2019\/2020\n\n32\n\n\fA directory\nName Block Address\n\nType\n\n• A directory is a simple table,\nimplemented as a file, that maps\nnames to inodes.\n▪ Each file system (NTFS, FAT, HFS,\nEXT) will implement it slightly\ndifferently.\n\n• It may also contain metadata\nabout the file for each entry.\n▪ Creation date, author, file size.\n\n• How does a directory know its\nparent?\nSpring 2019\/2020\n\n▪ Special entry! “ ” points to parent.\n▪“\n” is quite literal.\n33\n\n\fHeretic of the Day: Alternatives to Hierarchies\n• Margo Seltzer: Hierarchical File-Systems are\nDead (HotOS ‘09)\n▪ In this paper, she re-orients file systems around\nhuman beings and our own needs.\n\n• She asks the simple task: “Group these.”\n\nProfessor Margo Seltzer\nUniversity of British Columbia\n\nSpring 2019\/2020\n\n34\n\n\fAww… Human nature at work…\n• How did you group them?\n\nSpring 2019\/2020\n\n35\n\n\fGreat Expectations\n• How can we expect anybody to use hierarchies?\n▪ It does not seem to be how we actually think.\n\n• Organize by description instead.\n▪ No more placing files in directories.\n▪ Tag files based on what they are.\n▪ Search for files based on tag \/ keyword.\n\n• “I want to see all images that are red.”\n▪ “I want all images of squares…,” etc.\n▪ “I want to list all music that is 135bpm.”\n\n• Draws inspiration from the web: we often search by keywords.\nSpring 2019\/2020\n\n36\n\n\fFiles with tags…\n• We can attach tags to data files.\n• Then, when I’m feeling down and\nout, I can ask my computer:\n“Hey, show me all the pictures\nthat are cute.”\n\ncute\n\n▪ No longer looking into random\ndirectories to find what I need.\n\ncat\n\nfood\nImage by\nDimitri\nHoutteman\nSpring 2019\/2020\n\ncat.gif\n37\n\n\fPOSIX: Contradicting the definition of “path”\n• One piece of trouble with being heretical is that everybody asks you\n“how will you implement this and get people to switch????”\n• You don’t want to completely change C functions… so… let’s make use\nof traditional ideas to implement tags.\n▪\n▪\n▪\n\ncan still list all the files with both tags!\ncan open a particular file.\nis, naturally but oddly, the same file.\n\n• And you can also fit traditional POSIX paths to tags. Files in\n“\n” can simply be files tagged with\n“\n”\n▪ We don’t do this. Why… don’t we just do this? … … Well, change is hard.\nSpring 2019\/2020\n\n38\n\n\fC File I\/O Summary\n• C Standard IO\n▪\n▪\nReads to the given buffer the given number of bytes from the file indicated by the file descriptor. Returns the\nnumber of bytes read or 0 if the file reached its end. Negative on error.\n\n▪\nSimilarly writes to the given buffer to the file indicated by the file descriptor.\n\n▪\nModifies and returns the file position for the given file descriptor to the given offset from the reference point.\n\n▪\nCloses the file for this process. No subsequent action can be taken on this file. Returns negative on error.\n\n▪\nOpens the given file at the provided path with access depending on the provided flags. If the flags consist of a\nstring with “r” in it, it will be read-only. If it contains a “w” it will be writable. If it contains a “b” it will not be\ninterpreted as a text file, but as “binary” instead. If it is “w+”, it will completely overwrite the file. If it has an\n“a”, it will automatically write to the end of the existing file.\nSpring 2019\/2020\n\n39\n\n\fC File I\/O Summary\n• UNIX System Calls\n▪\n▪\nReads to the given buffer the given number of bytes from the file indicated by the file descriptor. Returns the\nnumber of bytes read or 0 if the file reached its end. Negative on error.\n\n▪\nSimilarly writes to the given buffer to the file indicated by the file descriptor.\n\n▪\nModifies and returns the file position for the given file descriptor to the given offset from the reference point.\n\n▪\nCloses the file for this process. No subsequent action can be taken on this file. Returns negative on error.\n\n▪\n▪\nOpens the given file at the provided path with access depending on the provided flags. If the O_CREAT flag is\ngiven, the file is created if it does not exist. If the O_TRUNC is passed and the file is writable, it will remove all\nthe data in the file after it opens. If O_RDONLY is specified, no writes can occur.\nSpring 2019\/2020\n\n40\n\n\fSummary\n• Files are just binary blobs of information.\n▪ Disambiguating that data requires a specification and consistency.\n\n• Disks are physical and rely on nature, which is chaotic.\n▪ We have strategies for encoding digital data on analog (magnetic) media.\n▪ Physical addressing requires care in where blocks of data are stored.\n\n• File systems are an opinionated space related to how humans organize\ndata on disk (and share\/discover that data)\n▪ Files are generally organized in trees, much like our virtual memory!\n▪ Hierarchical file systems still dominate: directory structures.\n▪ Other file systems are possible: relational searches and tags.\nSpring 2019\/2020\n\n41\n\n\fDistributed Filesystems and Storage\n• Now… what happens when we have file systems that span machines?\n▪ The power of networks and storage combined!\n\n• What are some unique issues to files that span multiple systems?\n• How can we create better methods of transmitting data between\nmachines?\n▪ What if we break down the “client-server” model.\n▪ What if files and data need not be in one particular place?\n▪ How do we find information, then?\n\n• Stay tuned!\nSpring 2019\/2020\n\n42\n\n\f","label":[[2216,2220,"Concept"],[2571,2586,"Concept"],[2667,2684,"Concept"],[3652,3672,"Concept"],[3760,3780,"Concept"],[5720,5728,"Concept"],[5761,5766,"Concept"],[5815,5821,"Concept"],[5844,5852,"Concept"],[5853,5858,"Concept"],[5900,5904,"Concept"],[5917,5924,"Concept"],[5986,5992,"Concept"],[5993,6001,"Concept"],[6072,6076,"Concept"],[6112,6120,"Concept"],[6164,6172,"Concept"],[6202,6206,"Concept"],[6253,6259,"Concept"],[6269,6273,"Concept"],[6304,6312,"Concept"],[6323,6330,"Concept"],[6342,6346,"Concept"],[6365,6371,"Concept"],[6410,6417,"Concept"],[6465,6469,"Concept"],[6559,6568,"Concept"],[6625,6632,"Concept"],[6774,6783,"Concept"],[6827,6831,"Concept"],[6988,6996,"Concept"],[7012,7018,"Concept"],[7052,7058,"Concept"],[7082,7090,"Concept"],[7129,7136,"Concept"],[7154,7161,"Concept"],[7181,7188,"Concept"],[7203,7212,"Concept"],[7242,7246,"Concept"],[7269,7276,"Concept"],[7418,7427,"Concept"],[7471,7475,"Concept"],[7566,7570,"Concept"],[7684,7691,"Concept"],[7756,7763,"Concept"],[7802,7809,"Concept"],[7827,7834,"Concept"],[7854,7861,"Concept"],[7876,7885,"Concept"],[7915,7919,"Concept"],[7942,7949,"Concept"],[7968,7975,"Concept"],[8091,8100,"Concept"],[8144,8148,"Concept"],[8260,8269,"Concept"],[8307,8311,"Concept"],[8327,8334,"Concept"],[8339,8348,"Concept"],[8450,8462,"Concept"],[8540,8552,"Concept"],[8783,8794,"Concept"],[8891,8895,"Concept"],[9385,9400,"Concept"],[9450,9457,"Concept"],[9467,9474,"Concept"],[9523,9532,"Concept"],[9557,9573,"Concept"],[9614,9621,"Concept"],[9863,9870,"Concept"],[9973,9982,"Concept"],[10038,10045,"Concept"],[10181,10196,"Concept"],[10199,10206,"Concept"],[10802,10812,"Concept"],[11488,11501,"Concept"],[11831,11842,"Concept"],[11857,11866,"Concept"],[11950,11962,"Concept"]],"Comments":[]}
{"id":8,"segment": ["test_set", "labeled"],  "course": "cs0449", "lec": "lec08","text":"8\n\nInvestigating\nthe Code\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fGoing with the Flow\nTracing the footsteps\n\n2\n\n\fBringing back our alphabet soup: The C ABI\n• The C Application Binary Interface (ABI) are assembly conventions\n• Like MIPS, certain registers are typically used for returns values, args, etc\n• It is not defined by the language, but rather the OS.\n• Windows and Linux (UNIX\/System V) have a different C ABI \n\n• In our x86-64 Linux C ABI, registers are used to pass arguments:\n•\n,\n,\n,\n,\n,\n(First, second, etc) (Like MIPS\n–\n)\n• Remaining arguments go on the stack.\n• Callee must preserve\n,\n,\n,\n,\n,\n(Like MIPS\n–\n• Return value:\n(overflows into\nfor 128-bits) (MIPS\n–\n)\n• Lots of other small things not worth going over.\n\n)\n\n• For reference: https:\/\/github.com\/hjl-tools\/x86-psABI\/wiki\/x86-64-psABI-1.0.pdf\n3\n\n\fFunction, function… what’s your… function\n• The activation frame contains\ntemporary data needed by the function.\n•\n•\n•\nC\n\nis the return value\nis the current stack address\nis the address of this frame\n\nWhat goes here?\n\nx86-64 (gas \/ AT&T syntax,\n\n)\n\n4\n\n\fOh, that’s your function\n• First: it fills the activation frame\nwith initial variable values.\n• It may not allocate them in\nany strict order. Here, it\nallocates x first and further away.\nx86-64 (gas \/ AT&T syntax,\n\n–\n)\n\n–\nPreserves\n(caller activation frame)\nAllocates “ ” on stack (\nfrom top)\nAllocates “ ” on stack (\nfrom top)\n(it does not have to be in order)\n\nResets caller activation frame\nReturns (return value is in\n)\n\n5\n\n\fThese are actual sandwiches (no hot dogs or w\/e)\n• When identifying\nfunctions, you are\nlooking for that\ntell-tale sandwich\npattern.\n•A\nis a good sign\nof the beginning of a\nfunction\n• And the\nwill\nhappen before the\nat the end.\n• Everything between is\nthe sweet, sweet jam\nthat makes it unique.\n\n6\n\n\fWho controls the\n• Control flow is a\n•\n•\n\nwill set\nwill set\n\n•\n\ngroup set\n\nor\n\ncontrols the flow\nfollowed by\n\nbased on the difference (subtraction) between values\nbased on bitwise AND of both values (faster, but less useful)\n\n(program counter) to an address based on\n\n• Often it is much more useful to just interpret the\nC\n\nx86-64 (gas \/ AT&T syntax,\n\n(\n\nis\n\n)\n)\n\n7\n\n\fWho controls the\n•\n\ncontrols the flow\n\nhas bits that are set based on the ALU (CPU math logic) result\n– most significant bit of result\n– set if overflow occurred\n\n• Each jump looks at different\n– Jumps when\nC\n\nWorks because of 2’s\ncomplement math.\n(thus, instead of its strict definition,\nbetter to think about it abstractly)\n\n– set if result is zero\n– set if last bit operation has carry\n\npatterns. (Look ‘em up!)\n– set if\n\nor\n\nx86-64 (gas \/ AT&T syntax,\n\n)\n\nPerform x - 0 (does nothing!)\nJump if the result (that is, x)\ndoes not have a set sign bit.\n(x is positive in that case)\nPerform x - y\nJump if the result is 0 or\nif result is negative after overflow\nor positive and didn’t overflow.\n(x is >= y in these cases)\n\n8\n\n\fcmp, simplifying… the confusion\n• Just remember that the order of operands is not the… best order…\n• It’s kinda swapped around in the AT&T syntax we have been looking at:\nJump if x > 0\n\nJump if x >= y\n\nWe negate the\ncondition\nBecause we are\ndeciding when to\nskip the code!\n\nJump if x < y\n\nJump if x != y\n\n9\n\n\ftest… adding some new confusion\n•\n\nis somewhat stranger… and requires some more thought.\n• performs an AND of the arguments and sets flags on result\n\n• Thankfully, generally only commonly used in a couple of cases.\n• Generally to test a value against “true” or “false”.\n• Recall that\nand\nwill look at the zero flag (\n)\n• Keep in mind that jumps are built around\n(which performs:\nWe negate the\ncondition\n\n)…\n\nJump if x != 0\n(\n?)\n\nJump if x == 0\n(\n?)\nCS\/COE 0449 – Spring 2019\/2020\n\n10\n\n\fPatterns\n• Control flow is a\nC\n\n\/\n\nbefore a\n–\n–\nx86-64 (gas \/ AT&T syntax,\n\n)\n\n11\n\n\fAltogether now… Working backward\n\nNegate logic to form “if” logic\n\nNegate logic to form “if” logic\n\n12\n\n\fDeduction, dear watson\nNo use of\n\n… likely no arguments\n\nTwo stack allocations … Two local variables.\n(initialized to 5 and, likely, -2)\n\nLooking at\n\n… This simply returns zero.\n\n13\n\n\fConventional wisdom: counting arguments\n\nReadies\n… second argument!\nReadies\n… first argument!\nSince they are\n… yep! Both 32-bit!\nLike a\nin MIPS. A function call.\n\nStill have to follow the\n\nto the assembly of the function.\n\n14\n\n\fConventional wisdom: counting arguments\n\nCopies\n… function argument!\nCopies\n… second argument!\nSince they are\n… They are both 32-bit!\nis the return address…\nmeans it is a 32-bit return\n\n15\n\n\f","label":[[243,273,"Concept"],[948,964,"Concept"],[1200,1216,"Concept"],[1393,1409,"Concept"],[1530,1546,"Concept"]],"Comments":[]}
{"id":10,"segment": ["test_set", "labeled"],  "course": "cs0449", "lec": "lec06","text":"6\n\nMemory\nManagement\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fOur Story So Far\nYou Hear a Voice Whisper: “The Memory Layout is a Lie”\n\n2\n\n\fReallocating our thoughts\n• A program has several sections:\n• Code\n• Static data\n• Stack\n• Heap\n\n• Today, we take a deeper dive at how dynamic\nmemory is allocated in the heap.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n3\n\n\fReallocating our thoughts\n• We have looked at\n\nand\n\n.\n\n• They stake out space in the heap and return an\naddress.\n• Right now, we live in a nice ideal world.\n• No other programs are running.\n• We have access to all of the memory.\n• Muhahahaha!!\n\n• The OS is lying to our program.\n• This memory is… virtual... reality.\n• We will investigate this lie later in the course.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n4\n\n\fThe World of Allocation\nIt is a puzzle without any optimal solution. Welcome to computers!\n\n5\n\n\fA heap of possibilities\n• Stack access often does not deviate much.\n\nPotential Layout\n(32-bit addresses)\n\n• We allocate a little bit at a time.\n• We allocate and free the memory VERY often.\n\nstack\n• Heap allocations have many access patterns that are\npossible.\n• You might allocate a lot at a time and keep it around for a\nlong time. Or a short time.\n• You might allocate a lot of small things, instead.\n• Maybe you do a little bit of everything?\n\n• Often, such patterns are not easy to predict.\n• Do you get a big file as input? A small file?\n\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n6\n\n\fA heaping helping of good luck\n• Allocations could happen in a nice order.\n• When something is allocated, it can be allocated\nafter everything else.\n\nstack\n\navailable memory\navailable memory\navailable memory\n\n• When freed, it makes room for new things.\n\n• IF ONLY.\n• I mean, it’s possible… but like…\n• the heap and stack are different things for a reason.\n\nheap\nstatic data\ncode\n7\n\n\fDigital potholes… as annoying as real ones\n• Small allocations interfere with large ones.\n• When small gaps interfere with allocation, this is\ncalled fragmentation.\n\nstack\navailable memory\navailable memory\navailable\navailable memory\nmemory\navailable memory\n\nNext\nAllocation\n\nUgh\n\n?\nif we had omniscience of future\nallocations, we could avoid this…\nbut we can’t know ahead of time!\n\nheap\nstatic data\ncode\n8\n\n\fThe worst case\nstack\n\n• When you allocate a lot of small things…\n• Free every other one…\n• And then attempt to allocate a bigger thing…\n\n• Even though there is technically enough memory…\n• There is no continuous space.\n• Therefore, our naïve\nwill fail.\n\n• We have to come up with some strategy.\n\n???\n\nheap\nstatic data\ncode\n9\n\n\fMoving is never easy\nstack\n\n• Why not move things around??\n• A defragmentation process\/algorithm\n\n• Moving around something in the heap is hard!\n• Any pointers referring to data within a block must be updated.\n• Finding these pointers automatically is effectively as difficult\nas garbage collection.\n\n• Because of this, moving blocks around is discouraged.\n(Easier to solve it another way.)\n???\n\nheap\nstatic data\ncode\n10\n\n\fMoving is NEVER easy\nstack\navailable memory\n\n• When blocks move, pointers\nto anything within them must be updated.\n• This is hard to keep track of!\n• C does not check validity of pointers after\n\nheap\nstatic data\ncode\n11\n\n\fStressing it out\n• If we allocate a large array it will be allocated on the\nheap somewhere.\n• Other allocations can also happen, and they go “above”\nthat array.\n\nstack\n\navailable memory\n\nint arr[200]\n\n• What happens when you need to append a 101st\nelement to this array?\n• Uh oh!\nold\ndata:\nint arr[100]\nint\narr[100]\n\n• You will need to allocate more space.\n• And then copy the array contents.\n• Free the old array.\n• How long does that take?\n\nfragmentation\n\nheap\n12\n\n\fStressing it out: Big Arrays\n• This happens in very practical situations!\n• Reallocating means getting rid of a small thing\n• And replacing it with a larger thing.\n• You could have TiBs of memory and this will be a problem.\n\nstack\n\navailable memory\n\n• This affects performance: (in terms of writes:)\n• Appending item arr[0]: 𝑂 1\n• Appending item arr[1]: 𝑂 1\n• …\n• Appending item arr[99]: 𝑂 1\n• Appending item arr[100]: 𝑂 𝑛 + 1 oh no!\n\n• When you would overflow the buffer…\n• You then need to copy all previous values as well.\n\nold\ndata:\nint arr[100]\nint\narr[100]\n\nheap\n13\n\n\fStressing it out: Performance Consistency\n• Big arrays want to be continuous.\n• Ensuring continuous space is difficult when you do not know\nhow much you will ultimately need.\n\n• This is exactly why linked lists exist!\n\nstack\n\navailable memory\n\n• Since a linked list allocates on every append.\n• Each append takes the same amount of time.\n\n• However, everything is a trade-off.\n• Dang it!!!\n• One cost is extra overhead for metadata.\n• Linked list traversal can stress memory caches.\n• It means traversing the array is slower.\n• However, we will mostly ignore this for now.\n\nold\ndata:\nint arr[100]\nint\narr[100]\n\nheap\n14\n\n\fThe Linked List\nA story about trade-offs.\n\n15\n\n\fWhat is a linked list?\n• A linked list is a non-continuous data structure representing an ordered list.\n• Each item in the linked list is represented by metadata called a node.\n• This metadata indirectly refers to the actual data.\n• Furthermore, it indirectly refers to at least one other item in the list.\n\nNode\n\n“struct” required since\nNode is not technically\ndefined until after it is\ndefined!\n\n16\n\n\fKeeping ahead of the list.\n• Creation of a list occurs when one allocates a single node and tracks it in a\npointer. This is the head of our list (first element.)\n\nNode\n\n17\n\n\fAdding some links to our chain\n• If we want to append an item, we can add a node anywhere!\n\n“tail”\n\n“node”\n\nRemember the\n‘\\0’ sentinel!\n\n18\n\n\fWe can add them anywhere!!\n• Consider what happens if we update our append to take any Node:\n\n“curNode”\n\n“node”\n\nTail\n\n19\n\n\fWe can add them anywhere!!\n• This function has very consistent performance (constant time):\n\n• The append always allocates the same amount.\n• It always copies the same amount.\n• Compare to a big array where you may have to copy the entire thing to\nappend something new!\n\n\fTraversal… on the other hand…\n• Accessing an array element is generally very simple.\n•\nis the same as\nbecause its location is very well-known!\n• This is because array items are continuous in memory. Not true for linked lists!\n\n• Here is a function that performs the equivalent for linked lists:\n\nQ: How many times is memory accessed relative to the requested index?\n\n21\n\n\fRemoving… on the other, other hand!\n• One nice thing about linked lists\nis their flexibility to changing\nshape.\n• I used to be able to bend a lot\nbetter, too, when I was in my 20s.\nAlas.\nCan’t find item at index.\nWe are deleting the head.\n\n• Since we don’t have a way to go\n“backward”\n• We first find the node we want to\ndelete (\n)\n• Keeping track of the node of\n– (\n)\n• Rewire\nto cut out\n.\n\nReturns new head (or old head if unchanged).\n\n22\n\n\fRemoving… on the other, other hand!\n• This looks complex, but it really is\na simple traversal.\n• So it takes 𝑂 𝑛 to find the item.\n• And it performs a simple update\nand deallocation. (quick to do)\n\n• A big array, on the other hand.\n• It can find the element to remove\nimmediately.\n• However, removing it means\nshifting over every item after it left.\n• That can be an expensive update!\n(Memory is slow!!)\n23\n\n\fOn your own!\n\nThink about the code you would need to do any of the following:\n• Delete\/free the entire linked list.\n• Sort a linked list.\n• Append a linked list to an existing one.\n• Copy a subset of a linked list to a new list.\nOften, operations can be abstracted in such a way that all of these can be written relatively\nsimply.\nConsider the performance of these operations compared to an Array.\n24\n\n\fLinked lists … link you … to the world!\n• Consider how much cleaner you can make certain operations if you tracked the\nprevious node as well.\n• This is a doubly linked list.\n• This is typically “double-ended” as well: keeping track of both head and tail.\n\nNode\n\nNode\n\nNode\n\n25\n\n\fSeeing the trees through the forest\n• A binary tree can be represented by the same nodes as a linked list.\n• In this case, you have a left and right child node instead of next and prev.\nNode\n\n• The operations are\nvery different, though.\n\nNode\n\nNode\n\nNode\n\n26\n\n\fDe-Stressing it out: Linked Lists\n• We know big arrays want to be continuous.\n• However, ensuring continuous space is difficult when you do\nnot know how much you will ultimately need.\n\n• Linked lists allocate very small chunks of metadata.\n• These chunks can be allocated easily on-demand.\n• And then deallocated without creating wide gaps.\n\nstack\n\navailable memory\n\n• This reduces fragmentation.\n• Deallocating always leaves a small amount of room.\n• It is always the exact amount needed to append!\n• However, it is all at the expense of complexity!\n• And traversal can be expensive (but we can find ways to deal\nwith that.)\n\nsome other data\n\nheap\n27\n\n\fImplementing Malloc\nIt really sounds like some kind of He-Man or She-Ra villain of the week.\n\n28\n\n\fThe malloc essentials\n• The\nfollowing:\n\nstack\n\nfunction does the\n\n• Allocates memory of at least\nbytes.\n• Returns the address to that block of memory (or\nerror)\n\non\n\navailable memory\n\n• Essentially, your program has a potentially large chunk of\nmemory.\n• The\nfunction tears off a piece of the chunk.\n• Also\nmust then allow that chunk to be reused.\n• The job of\nis to do so in the “best” way to reduce\nfragmentation.\nWe want to avoid fragmentation\n29\n\n\fChoosing where to allocate\n• Our first problem is, when\nwe tear off a chunk?\n\nis called, where do\n\n• We can do a few simple things:\n• First-Fit: start at lowest address, find first available section.\n\nstack\navailable memory\n\n• Fast, but small blocks clog up the works.\n\n• Next-fit: Do “First-Fit” but start where we last allocated.\n• Fast and spreads small blocks around a little better.\n\n• Best-Fit: laboriously look for the smallest available section to\ndivide up.\n\nLast Allocated\n\n• Slow, but limits fragmentation.\n\n???\n30\n\n\fManaging that metadata!\n• You have a whole section of memory to divide up.\n• You need to keep track of what is allocated and what is free.\n• One of the least complicated ways of doing so is to use… hmm…\n• A linked list! (or two!) We know how to do this!!\n\n• We can treat each allocated block (and each empty space) as a node in a linked\nlist.\n• Allocating memory is just appending a node to our list.\n\n• The trick is to think about how we want to split up the nodes representing\navailable memory.\n\n31\n\n\fTracking memory: Our fresh new world.\n• Let’s orient our memory visually horizontally.\n• We have control over EVERY byte of it. We can place metadata ANYWHERE.\n\n• Every\n\nis responsible for allocating a block of memory.\n\n• How, then, do we manage where things are allocated and where is empty space?\n• We can have “allocation” reduce to creating a new node in a linked list.\n\navailable memory\n\nWe have the power to write data ANYWHERE!\nSo where do linked list nodes go?\n\n32\n\n\fLinked lists are our friend, here\n• We will augment our normal doubly linked list to be useful for tracking the size of\nthe block it represents. (an explicit list allocator)\n• Here, we will maintain a single linked lists of all allocated or free blocks.\n• The size field denotes how big the block is (how much is used\/available.)\n• We need to know when a block represents allocated space or if it is free.\n• Hmm… we could use a single bit to denote that. Or… negativity!\n• The\n\nis NEVER\n\n. In fact,\n\nfails when requesting size of\n\n.\n\nAllocNode\n\nWe can make other clever\nspace optimizations, but we\nwill start with this.\n\nSigned! Negative number\nmeans a free block.\n\n33\n\n\fTracking memory: High level metadata\n• We can keep track of used\/empty spaces cheaply by having linked list nodes\nat the beginning of them. The nodes track the size of the space.\n• Here we have an allocated block followed by a free and then allocated block.\n• The metadata for the linked list is just smashed into the block itself.\n\navailable memory\n\nQ: What happens when we write over the block boundary?\n\n34\n\n\fImplementing\n• To allocate some amount of space, we find a free block that is at least that\nsize + metadata size. (Which one? Well, first-fit and friends apply!)\n• Then we will want to split that free block.\nx\n\nx\n\nx\n\navailable memory\n\n35\n\n\fImplementing\nCarefully negate size\n\n• Allocating means finding a free\nblock big enough.\n• Including the metadata size.\n\n• Then splitting it into a used block\nand a smaller free block.\nLinked list traversal; 𝑂(𝑛)\n\n• This is incomplete. (Why?)\n\nLinked list append; 𝑂(1)\n\n• (you don’t always split)\n\nRecall that we made size\nnegative for a free block.\nPositive means non-free.\nis negative.\nThink about it!\nQ: This is first-fit. What should be added to implement next-fit? Best-fit?\n36\n\n\fImplementing\n• When freeing the middle block, you will create empty space.\n• Consider allocations… it’s somewhat difficult to see the empty space.\n• You have “false fragmentation,” so you will want to merge adjacent free blocks.\n\navailable memory\n\n37\n\n\fImplementing\n• So, when we free blocks, we look to the left. We look to the right.\n• We coalesce the newly free block with ANY adjacent free blocks.\n• First one…\n• Then the other. (And it is linked list node removal; a constant time operation!)\n\navailable memory\n\n38\n\n\fImplementing\nHeader is just before ptr\n\n• Finding the header metadata\nnode is simple.\n• Look at our\n\n•\n\n’s\n\n.\n\nis slightly less complex.\n• It does not have to search.\n\n• Where\n•\n\nResembles linked list\ndelete; 𝑂(1)\n\nsplits nodes\nmerges them.\n\n• Whenever a block is freed next\nto an existing one…\n• It should merge them!\n\nHowever it subtracts from size\n(which makes reflect a larger space)\n\n• Consider how much a doubly\nlinked list helped.\nQ: Are any changes required here for best-fit?\n\n39\n\n\fThinking about next-fit\n\n• With a typical first-fit version of the malloc function…\n• We can now consider simple improvements.\n• Traversing the list is expensive! 𝑂 𝑛 !\n\n• Next-fit helps because we start from the last allocated item.\n• Generally, what do you think comes after the last allocated item.\n• Consider the normal operation…\n• It splits the node and creates free space.\n\n• Therefore, seems likely free space will exist near the last allocation.\n• Perhaps causing the average case for malloc to bias itself toward 𝑂(1)\n• However, all strategies have their own worst-case!!\n• Think about what that might be.\n\n40\n\n\fThinking about best-fit\n• Best-fit, on the other hand, is not about avoiding traversal.\n• Instead, we focus on fragmentation.\n\n• Allocating anywhere means worst-case behavior splits nodes poorly.\n• If we find a PERFECT fit, we remove fragmentation.\n\n• Traversal is still bad… and we brute force the search...\n• But, hey, solve one problem, cause another. That’s systems!\n• Fragmentation may indeed be a major issue on small memory systems.\n\n• What is the best of both worlds? Next-fit + Best-fit?\n• Hmm.\n• Works best if you keep large areas open.\nCS\/COE 0449 – Spring 2019\/2020\n\n41\n\n\fOther thoughts\n• Don’t need\npointers since adding size to the block’s address will also\nmove there. (unusually, the linked list is always ordered!)\n• You don’t need to keep the used blocks in the list.\n• More complex to understand but removes implementation complexity.\n• Free nodes point to the next and previous free nodes. Used nodes point to their\nneighbors. Traversal is improved since it only visits free nodes; still 𝑂(𝑛)\n\n• The idea is to only keep track of necessary metadata.\n• You only coalesce when free blocks are adjacent.\n• With a list of only free blocks, you can easily tell when that condition is met…\n• just see if\n\nis the same address as\n\n• The only other concern is getting from a used block you want to free to its\nneighboring free block. So those have normal pointers.\n\n42\n\n\fExplicit free lists: giving you VIP access\n• When you allocate, you go through the free list.\n• You don’t care about allocated nodes.\n\n• When you free, you only care about coalescing neighbors.\n\navailable memory\n\nQ: Do free nodes need a\n\npointer?\n\n43\n\n\fTrees are your buddy\n\n• Recall that we easily took the ideas around linked lists and made binary trees.\n• You can manage memory with a binary tree as well.\n\n• This is called a buddy allocator.\n\n45\n\n\fDivide and conquer\n• Buddy allocators divide memory into halves that are powers of two.\n• Can cause internal fragmentation\n\n▪ The total memory,\n, is a\npower of two.\n▪ Each split is, then, also a\npower of two.\n\nAllocation is not a power of two: internal fragmentation\n\n46\n\n\fAllocating with trees\n• Assuming\n\nis\n\n, and we allocate 242MiB:\n\n• We travel left until we find a block\nthat fits.\n\n▪ We travel back up when we can’t go\nfurther left and go right.\n▪ When we find a\nunsplit node that\nfits, we allocate\nthere.\n\n47\n\n\fBurying the hatchet: Chopping the trees\n• Let’s allocate 64MiB. So nice, we will allocate it twice.\n• Again a depth-first search to find\nthe first unsplit node that fits us.\n\n▪ This node is fine! Allocate that!\n▪ Do it again!\n▪ When we find a\nunsplit node that is\ntoo big, we split in\nhalf and keep going.\n\nCS\/COE 0449 – Spring 2019\/2020\n\n48\n\n\fCoalescing friendships (animated)\n• Coalescing happens because every block has a buddy!\n• When both sides of a split node\nare free, coalesce them!\n\n▪ If this keeps happening, it will\ncoalesce larger spaces.\n▪ Repeating as\nmuch as\nnecessary.\n\nx\n\nx\n\nx\n49\n\n\fThinking like an arborist (but only if you are feeling listless)\n• How does a tree-based\nallocation system deal with\nfragmentation?\n• What are some immediate\ndrawbacks from using a\ntree scheme?\n\navailable\nmemory\n\n• Can you imagine a\npossibility of using a\nhybrid approach?\n50\n\n\fLies and Damned Lies!\n• Does your program actually own all of memory?\n• On modern systems, absolutely heckin not.\n\n• Your program still has to request memory allocations from the OS.\n• Generally,\ntakes on this responsibility behind the scenes.\n• In Linux, you request pages in the normal heap in LIFO order with\n• Or, you request specific virtual memory pages with\n.\n\n.\n\n• What is a segmentation fault.\n• Segments are the “code”, “data”, “heap” separation. You fault by doing something\nthe segment does not allow. (write to read-only memory)\n• A historic misnomer since we actually have paging, not segmented memory.\n\n• What is a “page”? Virtual memory??\n• It replaced segments and is part of the much grander lie about sharing memory\nwith multiple applications at the same time. More on this later!\n\n51\n\n\fI want to know MORE\n• If you find this topic interesting, it is a WIDE area of research.\n• Malloc is generally more complex or specialized these days than the options\nhere.\n• Or some kind of hybrid, as the need arises.\n\n• The Linux kernel makes use of a Slab Allocator\n• https:\/\/en.wikipedia.org\/wiki\/Slab_allocation\n\n• Modern C (glibc) uses a hybrid malloc:\n• https:\/\/www.gnu.org\/software\/libc\/manual\/html_node\/The-GNU-Allocator.html\n\n• Professor Knuth has written about several classic algorithms.\n• Buddy Allocation comes from the 60s. Groovy.\n52\n\n\f","label":[[354,368,"Concept"],[2224,2237,"Concept"],[3897,3910,"Concept"],[3998,4010,"Concept"],[8829,8842,"Concept"],[9633,9646,"Concept"],[9786,9795,"Concept"],[9924,9932,"Concept"],[10043,10051,"Concept"],[10155,10168,"Concept"],[12373,12382,"Concept"],[12940,12948,"Concept"],[12950,12958,"Concept"],[13306,13314,"Concept"],[13962,13970,"Concept"],[13993,14001,"Concept"],[14020,14029,"Concept"],[14150,14158,"Concept"],[14615,14623,"Concept"],[14626,14633,"Concept"],[14711,14724,"Concept"],[14834,14847,"Concept"],[14973,14986,"Concept"],[15076,15084,"Concept"],[15087,15095,"Concept"],[15681,15689,"Concept"],[16154,16164,"Concept"],[16411,16426,"Concept"],[16534,16556,"Concept"],[16678,16700,"Concept"],[17297,17307,"Concept"],[17333,17343,"Concept"],[17429,17437,"Concept"],[17480,17488,"Concept"],[17669,17682,"Concept"],[18213,18231,"Concept"],[18461,18465,"Concept"],[18468,18482,"Concept"]],"Comments":[]}
{"id":11,"segment": ["test_set", "labeled"],  "course": "cs1502", "lec": "lec07_finite_automata_06","text":"Finite Automata 06\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fLanguage\nA language is a set of strings\nA set can be empty\nA set can have a finite number of elements\nA set can have an infinite number of elements\n\nRegular or not regular?\nIf L is the empty language,\nL is regular since we can express it using the regular\nexpression ∅\n\nIf L is finite\nL = {s1 , s2 , s3 , . . . , sn }\nfor a number n > 0 and si is a string,\nL is regular since we can express it using the regular expression\ns1 ∪ s2 ∪ s3 ∪ · · · ∪ sn\n\nSo, a non-regular language must be an infinite language\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fRegular Infinite Languages\nBut an infinite language can be a regular language:\n{w | w starts with a 1}\n{w | w contains 011 as a substring}\n{w | w ends with 0110}\n\nTechnically, there are infinite number of regular languages\nthat contains infinite number of strings\nThere must be something that can be used to distinguish\nbetween regular languages and non-regular languages\nBy definition, a language is regular if there are some finite\nstate machines that recognize it\nRecall that the number of states of a finite state machine must\nbe finite\nBut a finite state machine can accept an infinite number of\nstrings\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fRegular Infinite Languages\nWhat is the special property that makes a finite state machine\naccepts an infinite number of strings?\nA loop in a path to an accept state\n0\n\n1\n\n1\n0\n\n0\n\n0\n1\n\n1\n\n0\n\nLet L(M ) be the language of the above machine M :\n10∗ 1 ⊆ L(M )\n10∗ 1 = {11, 101, 1001, 10001, . . . } ⊆ L(M )\nIn other words, 10i 1 ∈ L(M ) for any i ≥ 0\n00(1010)∗ 0 ⊆ L(M )\n00(1010)∗ 0 = {000, 0010100, 00101010100, . . . } ⊆ L(M )\nIn other words, 00(1010)i 0 ∈ L(M ) for any i ≥ 0\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fRegular Infinite Languages\nGiven a DFA M , how to detect that there is a loop in a path\nto an accept state?\nSuppose a DFA M has 5 states and it accepts the string\nw = w1 w2 w3 w4 w5 of length 5\nw1\n\nw2\n\nw3\n\nw4\n\nw5\n\nThere are the total of 6 current states but there are only 5\nstates\nAt least two of them must be the same (Pigeonhole principle)\n\nSuppose the third and the fifth are the same state\n\nw4\nw1\n\nw3\n\nw2\n\nw1 w2 (w3 w4 )i w5 ∈ L(M ) for any i ≥ 0\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\nw5\n\n\fRegular Infinite Languages\nFrom previous example\nAny strings of length at least 5 that is accepted by M will go\nthrough a loop\nIf we let x = w1 w2 , y = w3 w4 , and z = w5 , we can say that\nxy i z ∈ L(M ) for any i ≥ 0\n\nGiven an infinite regular language A, there is a finite state\nmachine M that recognizes it\nBut we have no idea how many states it has\nSuppose it has p states\nAny string s ∈ A of length at least p will go through a loop\ns must be divided into s = xyz where y 6= ε such that\nxy i z ∈ A for any i ≥ 0\nwhere y is the string that takes you around a loop\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fExample\nConsider the following language\nA = {w | w contains 011 as a substring}\nWe need at least a 4-states DFA to recognize A\nLet’s find a string s ∈ A of length at least 4\nLet s = 0111\nx = 011, y = 1, z = ε\nxy 0 z = xz = 011 ∈ A\nxy 1 z = xyz = 0111 ∈ A\nxy 2 z = xyyz = 01111 ∈ A\n..\n.\nxy i z ∈ A for i ≥ 0\n\nLet s = 0101011\nx = 0, y = 1, z = 01011 and xy i z ∈ A for i ≥ 0\nxy 0 z = xz = 001011 ∈ A\nxy 1 z = xyz = 0101011 ∈ A\nxy 2 z = xyyz = 01101011 ∈ A\n..\n.\nxy i z ∈ A for i ≥ 0\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fPumping Lemma\n\nThe pumping lemma states that all regular languages has a\nspecial property\nIf a language lack this property, it is not a regular language\nProperty\nAll strings in the language can be pumped if they are at least as\nlong as a certain special value, called the pumping length. Each\nsuch strings contains a section that can be repeated any number of\ntimes with the resulting string remaining in the language.\npumped: xy i z for any i ≥ 0\nWe can insert the string y in between x and z any number of\ntimes but the result string is still in the language\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fPumping Lemma\nPumping Lemma\nIf A is a regular language, then there is a number p (the pumping\nlength) where if s is any string in A of length at least p, then s\nmust be divided into three pieces, s = xyz, satisfying the following\nconditions:\n1\n\nfor each i ≥ 0, xy i z ∈ A,\n\n2\n\n|y| > 0, and\n\n3\n\n|xy| ≤ p.\n\nwhere\n|s| represents the length of the string s\ny i means that i copies of y are concatenated together\ny 0 equals ε but it does not mean that y = ε\n(010)0 = ε but 010 6= ε\n\nxy 0 z = xz, xy 1 z = xyz, xy 2 z = xyyz, xy 3 z = xyyyz, and so\non\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fProof of the Pumping Lemma\nLet M be a DFA recognizing A and p (the pumping length)\nbe the number of states of M .\nLet s be a string of length at least p.\ns = s1 s2 . . . sn where sx ∈ Σ and n ≥ p.\n\nLet r1 , r2 , . . . , rn+1 be the sequence of states of M when\nprocessing s.\nr1 is the start state of M\nWhen s1 is processed, the state of M is changed to r2 , and so\non.\nδ(r1 , s1 ) = r2\nδ(r2 , s2 ) = r3\n..\n.\nδ(ri , si ) = ri+1 for 1 ≤ i ≤ n.\n..\n.\nδ(rn , sn ) = rn+1\n\nNote that there is no restriction that rx and ry cannot be the\nsame state.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fProof of the Pumping Lemma\nThe sequence r1 , r2 , . . . , rn+1 consists of n + 1 states\nSince n ≥ p, the above sequence has at least p + 1 states.\n\nSince the machine M has only p states, in the first p + 1\nstates of the sequence, at least two states rj and rl must be\nthe same state.\nLet rj be the first occurrence of the repeated state\nLet rl be the second occurrence of the repeated state in the\nabove sequence.\nNote that j < l.\n\nSince rl is in the first p + 1 states of the sequence l ≤ p + 1.\nLet\nx = s1 . . . sj−1\ny = sj . . . sl−1\nz = sl . . . sn\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fProof of the Pumping Lemma\n\nIf M accepts s = s1 s2 . . . sn and s = xyz,\nx takes M from r1 to rj ,\ny takes M from rj to rl , and\nz takes M from rl to rn+1\n\nwhere rn+1 is an accept state.\nLet’s check all conditions of the pumping lemma\n1\n2\n3\n\nThus M accept xy i z for i ≥ 0.\nSince j < l, |y| > 0.\nSince l ≤ p + 1, |xy| ≤ p.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fHow to use the Pumping Lemma\nTo check whether a language B is not regular using the\nPumping Lemma, we use prove by contradiction\nAssume that B is regular\nThere exists a machine M with p states that recognizes B\n\nSelect a string s ∈ B of length at least p so that the conditions\n1, 2, and 3 of the pumping lemma lead to a contradiction\n\nNotes\nThe choice of s must involve p to ensure that s has length at\nleast p (e.g., s = 0p 011, s = ap ba2p or s = bp+1 ap b)\nIt is possible that some choices of s do not produce\ncontradiction. If we do not get a contradiction, we have\nnot proved anything yet\nOnce you pick an s, nothing tells us what x, y, and z should\nbe. We have to show that we must get a contradiction,\nno matter what x, y, and z are, as long as they satisfy\nconditions 1, 2, and 3.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fExample\nShow that B = {0n 1n | n ≥ 0} is not regular.\nAssume that B is regular. In other words, there exists a\nmachine M with p states that recognizes B.\nThere are infinite number of strings in B of length at least p\nJust pick one (for now)\n\nLet s = 0p 1p . Note that s ∈ B and |s| = 2p ≥ p.\nThe pumping lemma says there are strings x, y, and z such\nthat s = xyz satisfying the conditions 1, 2, and 3\n\nRecall that there are multiple ways to divide s into x, y, and z\nsuch that xyz = s = 0p 1p\nExamples:\nx = ε, y = 0, and z = 0p−1 1p\nxyz = ε00p−1 1p = 0p 1p\n2\n3\np−5 p\nx = 0 , y = 0 , and z = 0\n1\nxyz = 02 03 0p−5 1p = 0p 1p\np\np−2\nx = 0 1, y = 1, and z = 1\nxyz = 0p 111p−2 = 0p 1p\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fExample: B = {0n 1n | n ≥ 0}\nSince there are multiple ways to divide s, we are going to\nfocus on all possible way to divide s into x, y, and z satisfying\nonly conditions 2 and 3 first\nWe will try to get a contradiction from the first condition\n\nThe condition 3 says |xy| ≤ p\nSince s starts with p 0s, to satisfy this condition, x and y must\nbe strings that contains only 0s\nIf x contains one 1, for s = 0p 1p = xyz, |x| is already p + 1\n|xy| = |x| + |y| = (p + 1) + |y| > p\nIf y contains one 1, for s = 0p 1p = xyz, |xy| is already p + 1\n\nFormally, to satisfy conditions 3\nx = 0j for some j ≥ 0\ny = 0k for some k > 0\nk > 0 makes |y| > 0 (satisfying condition 2)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fExample: B = {0n 1n | n ≥ 0}\nNow we have\nx = 0j for some j ≥ 0 and\ny = 0k for some k > 0\n\nTo make xyz = s = 0p 1p , z must be 0p−(j+k) 1p\nxyz = 0j 0k 0p−(j+k) 1p = 0j+k+p−(j+k) 1p = 0p 1p\nCondition 1 says that xy i z ∈ B for any i ≥ 0\nWe just need to find an i such that xy i z 6∈ B\nLet i = 0\nxy 0 z = 0j (0k )0 0p−(j+k) 1p\n= 0j 0p−(j+1) 1p\n= 0p−k 1p\nFor 0p−k 1p to be in B = {0n 1n | n ≥ 0}\np − k must be equal to p\nk must be 0 to make p − k = p but k cannot be 0\nContradiction\nB is not regular\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fExample: B = {0n 1n | n ≥ 0}\nThere are multiple is that can lead to a contradiction\nBut i should not be 1 since xy 1 z = xyz = s ∈ B\n\nLet i = 2\nxy 2 z = 0j (0k )2 0p−(j+k) 1p\n= 0j 0k 0k 0p−(j+1) 1p\n= 0p+k 1p\nFor 0p+k 1p to be in B\np + k must be equal to p\nk must be 0 to make p + k = p but k cannot be 0\nContradiction\nB is not regular\n\nIn this example, any i 6= 1 will give you a contradiction\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fExample: B = {0n 1n | n ≥ 0}\nThere are multiple strings s of length at least p that work for\nthis example\nExample: s = 02p 12p\nThis this string s, use exact same proof where x = 0j for any\nj ≥ 0, y = 0k for any k > 0, and z = 02p−(j+k) 12p\np\n\np\n\nExample: s = 0 2 1 2\n\nThis one is a little bit harder since condition 3 does not help\nmuch\nThere are three possibility for the string y\ny contains nothing but 0s (y = 0k for some k > 0)\ncontradiction because xy 2 z will have more 0s than 1s\ny contains some 0s and 1s (y = 0k 1m for some k, m > 0)\np\ncontradiction because xy 2 z = 0j 0k 1m 0k 1m 1 2 −m 6∈ B\nk\ny contains nothing but 1s (y = 1 for some k > 0)\ncontradiction because xy 2 z will have more 1s than 0s\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fRule of Thumb\n\nPick a string s in the language of length at least p such that\nit starts with at least p of the same symbol\n0p 1p\n02p 12p\nCondition 3 will help reducing the amount of proofs that you\nhave to do\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fSome Incorrect Proofs: B = {0n 1n | n ≥ 0}\nLet s = 000111\ns does not have length at least p (p can be any positive\nnumber)\n\nLet s = 0p 12p\ns 6∈ B, cannot use the Pumping lemma\n\nLet s = 0p 1p and x = 0, y = 0p−1 , z = 1p\nThis only show one way of dividing s into x, y, and z such\nthat s = xyz\nThere are multiple ways\nNeed to show them all by using variable (e.g., 0j , 0k , etc)\n\nLet s = 0p 1p and x = 0j , y = 0k , and z = 1p\nxyz = 0j 0k 1p = 0j+k 1p 6= s = 0p 1p\nIf you say j + k = p, it is still incorrect\nYou only show all possible way such that s = xyz where\nz = 1p\nBut z can have some 0s as well\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\fShow that B = {0n 1n | n ≥ 0} is not regular\nAssume that B is regular. Since B is regular, the Pumping lemma says that for any\nstring s ∈ B of length at least p, s can be divided into s = xyz satisfying the\nfollowing conditions:\n1\n\nxy i z ∈ B for any i ≥ 0\n\n2\n\n|y| > 0\n\n3\n\n|xy| ≤ p\n\nLet s = 0p 1p . Since s starts with p 0s, to satisfy the third condition, x and y are\nstrings that contain nothing but 0s. In other words, x = 0j for any j ≥ 0, and y = 0k\nfor any k > 0. Note that k must be greater than 0 because |y| = |0k | = k, and the\ncondition 2 says that |y| > 0. Since x = 0j and y = 0k , z = 0p−(j+k) 1p . Let i = 0.\nWe have\nxy i z = xy 0 z\n= xz\n= 0j 0p−(j+k) 1p\n= 0p−k 1p\nFor the string 0p−k 1p to be in B, the number of 0s must be equal to the number of\n1s. In other words, p − k must be equal to p. This requires k to be 0. But since k\nmust be greater than 0, xy 0 z 6∈ B — contradiction. Therefore, B is not regular.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 06\n\n\f","label":[[622,639,"Concept"],[708,734,"Concept"],[769,785,"Concept"],[3638,3651,"Concept"],[3657,3670,"Concept"]],"Comments":[]}
{"id":12,"segment": ["test_set", "labeled"],  "course": "cs1502", "lec": "lec17_decidability_05","text":"Decidability 05\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fAn Undecidable Language\nTheorem 4.11\nLet ATM = {hM, wi | M is a TM and M accepts w}. ATM is an\nundecidable language.\nAssume that ATM is decidable\nThus, there exists a Turing machine (decider) H that decides\nATM\nGiven a TM M and a string w in the form hM, wi, H can\ndecide whether hM, wi \u2208 ATM or not\n\nLet H(hM, wi) represents running TM H on input hM, wi\nSince H is a decider for ATM , we have\n(\naccept\nH(hM, wi) =\nreject\n\nif hM, wi \u2208 ATM\nif hM, wi 6\u2208 ATM\n\nM accepts w\nM does not accept w\n\nwhere M is a TM and w is a string\nNote that H always halts since it is a decider\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fAn Undecidable Language\n\nNote that an encoding of a machine M denoted by hM i is\nsimply a string\nTherefore, we can run M on input hM i if we want to\n\nSuppose we want to know whether M accepts hM i\nWe cannot simply run M on input hM i\nM may loop indefinitely on input hM i and we may not know\n\nBut now we have TM H (a decider for ATM\nSimply run H on input hM, hM ii\nIf H accepts hM, hM ii, M accepts hM i\nIf H rejects hM, hM ii, M does not accept hM i\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fAn Undecidable Language\n\nNow, create a machine D as follows:\nD = \u201cOn input hM i, where M is a TM:\n1\n2\n\nRun H on input hM, hM ii.\nOutput the opposite of what H outputs. That is, if H accepts,\nreject; and if H rejects, accept.\u201d\n\nThat is\n(\nD(hM i) =\n\naccept\nreject\n\nif H(hM, hM ii) = reject\nif H(hM, hM ii) = accept\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\nM does not accept hM i\nM accepts hM i\n\n\fAn Undecidable Language\nNote that an encoding of a machine D denoted by hDi is also\na string\nTherefore, we can run D on input hDi\n\nLet\u2019s substitute M by D in the definite of TM D on the\nprevious slide\nD(hDi) =\n\n(\naccept\nreject\n\nif H(hD, hDii) = reject\nif H(hD, hDii) = accept\n\nD does not accept hDi\nD accepts hDi\n\nThe above equation says the following:\nTM D accepts hDi if D does not accept hDi\nTM D rejects hDi if D accepts hDi\n\nwhich is contradict to itself\nTherefore, ATM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fUnrecognizable Language\n\nConsider a recognizable language A\nSince A is recognizable, there exists a machine MA that\naccepts every string s \u2208 A\nIf s 6\u2208 A, MA may reject or loop indefinitely on input s\n\nConsider a recognizable language A (the complement of A)\nSince A is recognizable, there exists a machine MA that\naccepts every string s \u2208 A\nIf s 6\u2208 A, MA may reject or loop indefinitely on input s\n\nBoth A and A are recognizable, is A decidable?\nTo show that A is decidable, we have to perform the following:\nConstruct a TM M 0 that we think it is a decider for A\nShow that M 0 accepts all strings in A\nShow that M 0 rejects all strings not in A\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fUnrecognizable Language\nThe following is an example of the TM M 0 :\nM 0 = \u201cOn input s:\n1\n2\n\nRun both MA and MA on input s in parallel.\nIf MA accepts s, accept; if MA accepts s, reject.\u201d\n\nCase 1: Assume that s \u2208 A. Since s \u2208 A and MA recognizes\nA, by running MA on input s, MA will accept s. Since s 6\u2208 A\nand MA recognizes A, by running MA on input s, MA will\nnot accept s. Since MA accepts s, M 0 accepts s.\nCase 2: Assume that s 6\u2208 A. Since s 6\u2208 A and MA recognizes\nA, by running MA on input s, MA will not accept s. Since\ns \u2208 A and MA recognizes A, by running MA on input s, MA\nwill accept s. Since MA accepts s, M 0 rejects s.\nThis show that M 0 is a decider for A\nTherefore, A is decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fUnrecognizable Language\nNow we just prove the following statement:\nIf a language A is recognizable and its complement A is\nrecognizable, A is decidable\nLet\nP \u2261 A is recognizable\nQ \u2261 A is recognizable\nR \u2261 A is decidable\n\nThe above statement is simply (P \u2227 Q) \u2192 R\nFrom boolean algebra:\n(P \u2227 Q) \u2192 R \u2261 \u00ac(P \u2227 Q) \u2228 R\n\u2261 (\u00acP \u2228 \u00acQ) \u2228 R\n\u2261 (\u00acP \u2228 R) \u2228 \u00acQ\n\u2261 \u00ac(P \u2227 \u00acR) \u2228 \u00acQ\n\u2261 (P \u2227 \u00acR) \u2192 \u00acQ\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fUnrecognizable Language\n\nFrom previous slide, we have\n(P \u2227 Q) \u2192 R \u2261 (P \u2227 \u00acR) \u2192 \u00acQ\nBy replacing P , Q, and R on the right side, we have the\nfollowing statement:\nIf A is recognizable and A is undecidable, A is unrecognizable\nRecall the following:\nATM is recognizable\nATM is undecidable\n\nFrom the above statement, ATM is unrecognizable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fConclusions\nTo show that a language A is decidable:\nConstruct a TM M 0 that you think will decide A\nShow that M 0 accepts all strings in A\nShow that M 0 rejects all strings not in A\n\nThe following TMs can be used as helper machines:\nTM M that decides ADFA\nADFA = {hB, wi | B is a DFA that accepts w}\nTM N that decides ANFA\nANFA = {hB, wi | B is a NFA that accepts w}\nTM P that decides AREX\nAREX = {hR, wi | R is a regular expression that generates w}\nTM T that decides EDFA\nEDFA = {hBi | B is a DFA and L(B) = \u2205}\nTM F that decides EQDFA\nEQDFA = {hA, Bi | A and B are DFAs and L(A) = L(B)}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\fConclusions\nTo show that two sets have the same size, simply show a\ncorrespondence between two sets\nA correspondence must be one-to-one and onto\nCan be a definition of a function\nCan be represented by a table (skip duplicate)\n\nTo show that an infinite set is countable, simply show a\ncorrespondence with the set of natural number N\nTo show that an infinite set is uncountable:\nDiagonalization: if all elements in the set is somehow infinite\nShow a correspondence with a known uncountable set\n\nATM = {hM, wi | M is a TM that accepts w} is undecidable\nGiven a TM M and a string w, there is no algorithm that can\ncheck whether M accepts w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 05\n\n\f", "label": [[131, 151, "Concept"], [223, 243, "Concept"], [496, 503, "Concept"], [691, 698, "Concept"], [767, 787, "Concept"], [2255, 2278, "Concept"], [2291, 2303, "Concept"], [2326, 2338, "Concept"], [2467, 2479, "Concept"], [2524, 2536, "Concept"], [4335, 4347, "Concept"], [4357, 4368, "Concept"], [4375, 4389, "Concept"], [4419, 4431, "Concept"], [4439, 4450, "Concept"]], "Comments": []}
{"id":14,"segment": ["test_set", "labeled"],  "course": "cs1502", "lec": "lec06_finite_automata_05","text":"Finite Automata 05\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fRegular Language\n\nQuestion: Given a regular language, can it be expressed by a\nregular expression?\nThere are infinite number of regular languages\nLuckily, all of them have one thing in common\nEach of them has some finite-state machines that recognize it\n\nSo, to try to answer the above question, we need to show a\nway to convert a finite-state machine into a regular expression\nthat expresses the language of the machine\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fGeneralized Nondeterministic Finite Automaton\nA Generalized Nondeterministic Finite Automaton (GNFA) N\nof a DFA M is\na special NFA where L(N ) = L(M )\nN has exactly one accept state\nall transitions of N are regular expressions\nTo transition from one state to another, you need a string in\nthe language expressed by the regular expression instead of a\nsymbol\n\nExample:\nq2\n\n0∗ ∪ 11∗ 0\n\nq5\n\nAt q2 , if you encounter a string in the language 0∗ ∪ 11∗ 0,\nmove to state q5\n0∗ ∪ 11∗ 0 = {ε, 0, 00, . . . } ∪ {10, 110, 1110, . . . }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fConverting a DFA to a GNFA\n1\n2\n\n3\n\nStart with a DFA M\nAdd a new start state s with an ε arrow to the original start\nstate of M\nAdd a new accept state a\nFrom every accept state of M , add an ε arrow to the new\naccept state\nChange all original accept states of M to non-accept states\n\n4\n\n5\n\nTurn transition labels to regular expressions (rule #1 of\nregular expression)\nAdd necessary transition arrows\nChange multiple arrows or multiple labels to a single arrow\nwhile label is the union of the previous labels\na\na∪b\nb\n0, 1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0∪1\nFinite Automata 05\n\n\fConverting a DFA to a GNFA (Example)\n\nA DFA\nb\n\na\n1\n\n2\na\nb\na\n\nb\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fConverting a DFA to a GNFA (Example)\n\nAdd a new start state s\nb\n\na\n1\n\n2\na\nb\n\ns\n\na\n\nb\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fConverting a DFA to a GNFA (Example)\n\nε from s to the original start state\nb\n\na\n1\n\n2\n\nε\n\na\nb\n\ns\n\na\n\nb\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fConverting a DFA to a GNFA (Example)\n\nAdd a new accept state a\nb\n\na\n1\n\n2\n\nε\n\na\nb\n\ns\n\na\n\nb\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\na\n\n\fConverting a DFA to a GNFA (Example)\n\nε from all accepts state to a\nb\n\na\n1\n\n2\n\nε\n\nε\n\na\nb\n\ns\n\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\na\n\na\n\nb\n\nε\n\nFinite Automata 05\n\n\fConverting a DFA to a GNFA (Example)\n\nAll original accept states to non-accept state\nb\n\na\n1\n\n2\n\nε\n\nε\n\na\nb\n\ns\n\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\na\n\na\n\nb\n\nε\n\nFinite Automata 05\n\n\fGNFA to Regular Expression\nNote that by converting a DFA and an equivalent GNFA, the\nlanguage of the machine remain unchanged\nNext, we are going to remove original states of DFA one at a\ntime until there are only two states left\nWhen a state is removed, we have to make sure that the\nlanguage of the machine is not changed\nThe new start state s and the new accept state a\n\nExample:\n3−state\nDFA\n\n5−state\nGNFA\n\n4−state\nGNFA\n\nregular\nexpression\n\n2−state\nGNFA\n\n3−state\nGNFA\n\nThe transition from s to a is a regular expression that\nexpresses the language of the original DFA\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fReducing Number of States of GNFA\nRemoving qrip\nqi\n\nR4\n\nR1\n\nqj\nR3\n\nqrip\n\nR2\n\nHow to go from qi to qj ?\nqi → qj using a string in R4\nqi → qrip → qj\nqi → qrip using a string in R1\nqrip → qrip using a string in R2 any number of times (R2∗ )\nqrip → qj using a string in R3\n\nFrom qi to qj by any strings in R1 R2∗ R3 ∪ R4\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fReducing Number of States of GNFA\nRemoving qrip\nqi\n\nR4\n\nqj\nR3\n\nR1\n\nqi\n\nR1 R2 ∗ R3 ∪ R4\n\nqj\n\nqrip\n\nR2\n\nTo remove qrip\n1\n\nSearch for all possible paths from a state q to qrip and to a\nstate r\nq → qrip → r\n\n2\n\nTurn all paths from previous step to regular expressions\nRemove qrip which results in q → r\nInsert paths q → r back with their regular expressions\n\n3\n4\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fExample\nDFA\n1\n\na\n\nb\n\n2\n\na, b\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fExample\nDFA to GNFA\nε\n1\n\na\n\ns\n\n1\n\nb\n\na\n\nb\nε\n\n2\n\na, b\n\na\n\n2\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\naUb\n\n\fExample\nGet rid of state 2\nε\n1\n\ns\n\na\n\n1\n\na\n\nb\n\nb\nε\n2\n\na, b\n\na\n\n2\n\nε\ns\n\n1\n\na\n\nb(a U b)*\na\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\naUb\n\n\fExample\nGet rid of state 1\nε\n1\n\ns\n\na\n\n1\n\nb\n\na\n\nb\nε\na, b\n\n2\n\na\n\n2\n\nε\ns\n\n1\n\na\n\ns\n\na*b(a U b)*\n\nb(a U b)*\na\n\na\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\naUb\n\n\fExample\n\nDFA to GNFA\nb\n\nb\n\na\n\na\n1\n\n2\n\n1\n\nb\na\n\nb\n\n2\n\nε\n\na\n\nb\n\ns\n\n3\n\nFinite Automata 05\n\na\n\na\n\nb\n\n3\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nε\n\na\n\nε\n\n\fExample\nGet rid of state 1\naa U b\n\nb\n\na\n1\n\n2\n\nε\n\n2\n\na\n\na\n\nb\n3\n\nε\nab\n\nb\n\ns\n\na\n\nε\n\na\n\ns\n\nba U a\nb\n\nε\n\n3\n\nbb\n\nPaths that go through state 1 (q → 1 → r)\ns→1→2\ns→1→3\n2→1→2\n2→1→3\n3→1→2\n3→1→3\n\nεa = a\nεb = b\naa\nab\nba\nbb\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\nε\n\na\n\n\fExample\nGet rid of state 2\naa U b\n\na(aa U b)*\ns\n\n2\na\n\na\n\nε\nab\n\ns\n\nba U a\nb\n\n3\n\na\n(ba U a)(aa U b)* U ε\n\na(aa U b)*ab U b\n\nε\n3\n\nbb\n(ba U a)(aa U b)*ab U bb\n\nPaths that go through state 2 (q → 2 → r)\ns→2→a\ns→2→3\n3→2→a\n3→2→3\n\na(aa ∪ b)∗ ε = a(aa ∪ b)∗\na(aa ∪ b)∗ ab\n(ba ∪ a)(aa ∪ b)∗ ε = (ba ∪ a)(aa ∪ b)∗\n(ba ∪ a)(aa ∪ b)∗ ab\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fExample\n\nGet rid of state 3\na(aa U b)*\ns\n\na\n\na(aa U b)*ab U b\n\n(ba U a)(aa U b)* U ε\n\ns\n\na\n\n3\n(a(aa U b)*ab U b)((ba U a)(aa U b)*ab U bb)*((ba U a)(aa U b)* U ε ) U a(aa U b)*\n\n(ba U a)(aa U b)*ab U bb\n\nPaths that go through state 3 (q → 3 → r)\ns→3→a\n(a(aa∪b)∗ ab∪b)((ba∪a)(aa∪b)∗ ab∪bb)∗ ((ba∪a)(aa∪b)∗ ∪ε)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fTheorem 1.54\n\nEarlier, we show that every regular expression expresses a\nregular language\nWe just show that every state diagram of a DFA can be\nconverted into an equivalent regular expression\nTheorem (1.54)\nA language is regular if and only if some regular expression\ndescribes it.\nProblem: If you cannot express a language using a regular\nexpression, it does not mean it is not regular\nThe language is so complicate that it needs a very long and\ncomplicate regular expression\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fFinite Automaton and Algorithm\n\nFor every finite automaton, there is an equivalent algorithm\nFor example, consider a machine where its language is the set\nof all strings over 0 and 1 that consist of an odd number of 1s.\n0\n\n0\n1\n\nq0\n\nq1\n\n1\n\nThe above machine can be converted into a Java method that\nTakes a string of 0s and 1s as an argument\nReturns true (accept) of the input string consisting of an odd\nnumber of 1s. Otherwise, returns false (reject).\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fFinite Automata and Algorithm\npublic boolean isAccepted(String input)\n{\nString state = \"q0\";\nint length = input.length();\nfor(int i = 0; i < length; i++)\n{\nchar c = input.charAt(i);\nif(state.equals(\"q0\") && c == ’0’)\nstate = \"q0\";\nelse if(state.equals(\"q0\") && c == ’1’)\nstate = \"q1\";\nelse if(state.equals(\"q1\") && c == ’0’)\nstate = \"q1\";\nelse if(state.equals(\"q1\") && c == ’1’)\nstate = \"q0\";\n}\nif(state.equals(\"q1\"))\nreturn true;\nelse\nreturn false;\n}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\fLimitation of Finite Automaton\nFinite Automaton only captures a small subset of algorithms\nProblems that associated with Regular languages\n\nNot all language can be recognized by a finite state machine\nSuch languages is called Nonregular Languages\n\nExample: B = {0n 1n | n ≥ 0}\nB = {ε, 01, 0011, 000111, . . . }\nThe machine need to be able to remember how many 0s and\nhow many 1s it sees so far\nWe need an infinite number of states\n\nIf you cannot express a language using regular expression, it\ndoes not mean it is not a regular language\nToo complicate and requires a huge number of states\n\nWe need a way to determine whether a language is not\nregular.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 05\n\n\f","label":[[134,150,"Concept"],[170,186,"Concept"],[624,669,"Concept"],[672,717,"Concept"],[719,723,"Concept"],[2882,2886,"Concept"],[6218,6236,"Concept"],[6249,6265,"Concept"],[6397,6404,"Concept"],[6425,6443,"Concept"],[6722,6738,"Concept"],[6743,6752,"Concept"],[6764,6780,"Concept"],[6805,6814,"Concept"],[7778,7794,"Concept"],[7795,7811,"Concept"],[7885,7902,"Concept"],[7990,8010,"Concept"]],"Comments":[]}
{"id":15,"segment": ["test_set", "labeled"],  "course": "cs1502", "lec": "lec26_time_complexity_01","text":"Time Complexity 01\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fMeasuring Complexity\nA solvable problem takes time to solve\nSome problems may take too long to solve\n\nWe are going to measure the time to solve a problem by an\nalgorithm\nAn algorithm will be represented by a TM\nCount the maximum number of steps to process an input of\nlength n until it halts\n\nLet M be a deterministic Turing machine that halts on all\ninputs\nThe running time or time complexity of M is the function\nf : N → N, where f (n) is the maximum number of steps that\nM uses on any input of length n\nIf f (n) is the running time of M , M runs in time f (n)\nM is an f (n) time Turing machine\n\nCustomarily we use n to represent the length of the input\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fBig-O and Small-o Notations\n\nGiven a TM M , its running time f (n) may look like the\nfollowing:\nf (n) = 3n3 + 20n2 + 15n + 12\nTo eliminate the complexity of a running time of a Turing\nmachine, we estimate it\nThis is called asymptotic analysis\nConsider only when the Turing machine runs on large input\nConsider only the highest order term\nDiscard the coefficient\n\nWe generally use Big-O and Small-o notations\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fBig-O Definition\n\nLet f and g be functions f, g : N → R+\nWe say that f (n) = O(g(n)) or f (n) is O(g(n)) if positive\nintegers c and n0 exist such that for every integer n ≥ n0 ,\nf (n) ≤ cg(n)\nWhen f (n) = O(g(n)):\ng(n) is an upper bound for f (n), or\nMore precisely, g(n) is an asymptotic upper bound for f (n)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fBig-O Examples\nShow that f (n) = 6n3 + 2n2 + 20n + 45 is O(n3 )\nNeed to find c and n0 such that f (n) ≤ cn3 for n ≥ n0\nWe know the following:\n2n2 ≤ 2n3 for n ≥ 1\n20n ≤ 20n3 for n ≥ 1\n45 ≤ 45n3 for n ≥ 1\n\nThus, we have\nf (n) = 6n3 + 2n2 + 20n + 45 ≤ 6n3 + 2n3 + 20n3 + 45n3\n= (6 + 2 + 20 + 45)n3\n= 73n3 for n ≥ 1\nIn other words,\nf (n) ≤ 73n3\n\nfor n ≥ 1\n\nFrom the above equation, what are c and n0 ?\nc = 73\nn0 = 1\n\nThus, 6n3 + 2n2 + 20 + 45 is O(n3 )\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fBig-O Examples\nShow that f (n) = 2n log2 n + 20n + 10 is O(n log2 n)\nNeed to find c and n0 such that f (n) ≤ c(n log2 n) for n ≥ n0\nWe know the following:\n20n ≤ 20n log2 n for n ≥ 1\n10 ≤ 10n log2 n for n ≥ 1\n\nThus, we have\nf (n) = 2n log2 n + 20n + 10 ≤ 2n log2 n + 20n log2 n + 10n log2 n\n= (2 + 20 + 10)n log2 n\n= 32n log2 n for n ≥ 1\nIn other words,\nf (n) ≤ 32n log2 n for n ≥ 1\nFrom the above equation, what are c and n0 ?\nc = 32\nn0 = 1\n\nThus, 2n log2 n + 20n + 10 is O(n log2 n)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fBig-O Examples\n\nUpper bounds of the form nc where c > 0 is called\npolynomial bounds\nO(n2 )\nO(n10 )\nO(n log2 n) (bounded by O(n2 ))\nδ\n\nUpper bounds of the form 2(n ) is called exponential bounds\nO(2n )\nO(3n )\n3n = (2log2 3 )n = 2log2 3×n and log2 3 is a constant\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fSmall-o Notation\nDefinition 7.5\nLet f and g be functions f, g : N → R+ . Say that f (n) = o(g(n))\nif\nf (n)\n= 0.\nn→∞ g(n)\nlim\n\nIn other words, f (n) = o(g(n)) means that for any real number\nc > 0, a number n0 exists, where f (n) < cg(n) for all n ≥ n0 .\nSimply strictly less than\n√\n\nn = o(n)\nn = o(n log(log n))\nn log(log n) = o(n log n)\nn log n = o(n2 )\nn2 = o(n3 )\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fAnalyzing Algorithms\nConsider a Turing machine M1 that decides\nA = {0k 1k | k ≥ 0}\nM1 = “On input string w:\n1\n\n2\n3\n4\n\nScan across the tape and reject if a 0 is found to the right of a\n1.\nRepeat if both 0s and 1s remain on the tape:\nScan across the tape, crossing off a single 0 and a single 1.\nIf 0s still remain after all the 1s have been crossed off, or if 1s\nstill remain after all the 0s have been crossed off, reject.\nOtherwise, if neither 0s nor 1s remain on the tape, accept.”\n\nFind f (n), the maximum number of steps for M1 to process a\nstring w of length n\nThere are multiple input strings of length n\nThe number of steps of an input of length n depends on the\ninput pattern\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fAnalyzing Algorithm\nConsider all possible inputs of length 4:\nInput\n0000\n0001\n0010\n0011\n0100\n0101\n0110\n0111\n1000\n1001\n1010\n1011\n1100\n1101\n1110\n1111\n\nStep that M1 halts\n4 (0 iterations)\n4 (1 iterations)\n1\n4 (2 iterations)\n1\n1\n1\n4 (1 iterations)\n1\n1\n1\n1\n1\n1\n1\n4 (0 iterations)\n\nThe maximum number of steps on an input of length n occurs\nwhen w = 0k 1k where k = n2\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fAnalyzing Algorithms\n1\n\nScan across the tape and reject if a 0 is found to the right of\na1\nThis step requires TM M1 to move its tape head all the way\nto the right until it encounters a blank symbol and move its\ntape head all the way to the left-most square\nThis step requires either n or 2n steps\nIn other words, this step is O(n)\n\n2\n3\n\nRepeat if both 0s and 1s remain on the tape:\nScan across the tape, crossing off a single 0 and a single 1.\nStep 3 requires n steps or O(n)\nCross the first 0, move the tape head n\/2 step to the first 1,\ncross it off and move the tape head n\/2 step to position on\ntop of the first of the remaining 0s\n\nThere will be n\/2 0s and n\/2 1s\nThe step 3 is repeated n\/2 times\n\nSteps 2 and 3 together is n\/2 × O(n) or O(n2 ).\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fAnalyzing Algorithms\n\n4\n\nIf 0s still remain after all the 1s have been crossed off, or if 1s\nstill remain after all the 0s have been crossed off, reject.\nOtherwise, if neither 0s nor 1s remain on the tape, accept.\nSimply scan the whole tape looking for a 0 or a 1\nThis step can be done in n step\nThis step is O(n)\n\nThe running time of M1 is\nO(n) + O(n2 ) + O(n) = O(n2 + 2n) = O(n2 )\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fTime Complexity Class\nDefinition 7.7\nLet t : N → R+ be a function. Define the time complexity class,\nTIME(t(n)), to be the collection of all languages that are\ndecidable by an O(t(n)) time Turing machine.\nFrom the previous language A = {0k 1k | k ≥ 0} with TM M1\nA ∈ TIME(n2 )\nCan we construct a TM M2 such that f (n) = o(n2 )?\nAsymptotically better than O(n2 )\n\nWhat if we cross off two 0s and two 1s in step 3?\nPractically faster but not asymptotically better\nStep 3 will be repeated n\/4 times but n\/4 × O(n) = O(n2 )\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fAsymptotically Faster for Deciding A\n\nConsider TM M2 that decides A = {0k 1k | k ≥ 0}:\nM2 = “On input string w:\n1\n\n2\n3\n\n4\n\n5\n\nScan across the tape and reject if a 0 is found to the right of a\n1.\nRepeat as long as some 0s and some 1s remain on the tape.\nScan across the tape, checking whether the total number of\n0s and 1s remaining is even or odd. If it is odd, reject.\nScan again across the tape, crossing off every other 0\nstarting with the first 0, and then crossing off every\nother 1 starting with the first 1.\nIf no 0s and no 1s remain on the tape, accept. Otherwise,\nreject.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fAsymptotically Faster for Deciding A\nStep 1 and 5 are O(n) as in M1\nStep 3, to scan and check whether the number of 0s and 1s\nare even or odd is O(n)\nIn step 4, to cross off every other 0s and 1s is O(n)\nEvery time this step is executed, the number of 0s and 1s are\ncut in half\nIf there are n\/2 0s and n\/2 1s, step 4 needs to be repeated\n1 + log2 (n\/2) times\nFor example (n = 16), 8 → 4 → 2 → 1 → 0 (cut in half 4\ntimes) for input 08 18\n\nSteps 2, 3 and 4 all together is\n(1 + log2 (n\/2)) × (O(n) + O(n)) = O(n log n)\nThus, TM M2 run-time is\nO(n) + O(n log n) + O(n) = O(n log n)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fDeciding A in Linear Time\nThe language A can be decided in O(n)\nBut we need the second tape\n\nTwo-tape TM M3 decides A in O(n)\nM3 =“On input string w:\n1\n2\n\n3\n\n4\n\nScan across tape 1 and reject if a 0 is found to the right of a 1.\nScan across the 0s on tape 1 until the first 1. At the same\ntime, copy the 0s onto tape 2.\nScan across the 1s on tape 1 until the end of the input. For\neach 1 read on tape 1, cross off a 0 on tape 2. If all 0s are\ncrossed off before all the 1s are read, reject.\nIf all the 0s have now been crossed off, accept. If any 0s\nremain, reject.”\n\nRun-time of each step is O(n)\nTM M3 runs in O(n)\n\nThe run-time complexity of the language A depends on the\nmodel of computation\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fComplexity Relationships Among Models\nTheorem 7.8\nLet t(n) be a function where t(n) ≥ n. Then every t(n) time\nmultitape Turing machine has an equivalent O(t2 (n)) time\nsingle-tape Turing machine.\nSuppose we have a language A that can be decided by a\nmultitape Turing machine M in t(n) steps where n is the\nlength of an input string\nThe above theorem says that there exists a single-tape TM\nthat can decide A in O(t2 (n))\nt2 (n) = t(n) × t(n)\n\nTo prove the above theorem, we simply need to construct a\nsingle tape TM that decides A in O(t2 (n))\nBut what is the language A?\nSince we do not know what the language, let’s try to simply\nsimulate multitape TM using single-tape TM\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fComplexity Relationships Among Models\nRecall that a multitape TM is a k-tape TM where its input\nappears on tape 1\n0\n\n1\n\n0\n\na\n\na\n\na\n\nb\n\na\n\n1\n\n0\n\nM\n\nWe can simulate multitape TM using a single-tape TM as\nshown below:\nS\n#\n\n0\n\n1\n\n0\n\n1\n\n0\n\n#\n\na\n\na\n\na\n\n#\n\nb\n\na\n\n#\n\nIf TM M halts in t(n) steps, how many step does TM S\nneeds in terms of O?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fProof of Theorem 7.8\nLet M be a k-tape TM for some k ≥ 2 that runs in t(n) on\ninput string of length n\nWe are going to construct a single-tape TM S that simulate\nmultitape TM M\nThe first step is for S to copy contents of all M ’s k tapes into\nits tape\nNote that contents of all M ’s k tapes are fixed except the first\none (input tape)\nSuppose the length of strings on tape 2 to tape k be a\nconstant c and the length of input string is n\nIt requires S to perform c + n + (k + 1) steps to copy\nThe k + 1 steps are from writing # symbols\n\nSince c and k are constant, this step is O(n)\n\nThe next step is to simulate M ’s steps\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fProof of Theorem 7.8\n\nFor each step of M , first S needs to know symbols under M ’s\ntape heads\nS needs to scan and remember all k-dot symbols\nSince M performs t(n) steps, the maximum length of an\nactive portion of each tape is t(n)\nIf the length of the content of a tape is t(n) + 1, it requires\nTM M t(n) step to reach the last symbol\nBy the time it gets to the last symbol, TM M halts\n\nS needs k × t(n) steps to scan for all k-dot symbols\nScanning for all dot symbols is O(t(n))\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fProof of Theorem 7.8\nNext S needs to update all k-dot symbol\nSometime, M may move a tape head to the right over a blank\nsymbol\nBut for TM S, it will be on the # symbol\nIn this case, S needs to shift a portion of its own tape one cell\nto the right\nIf it is M ’s first tape head, it requires to shift the content of\nk − 1 tapes\nRequire (k − 1) × t(n) steps for S to shift\nwhere t(n) is the maximum length of an active portion of\neach tape\nIf it is M ’s second tape head, it requires to shift the content\nof k − 2 tapes\nRequire (k − 2) × t(n) steps for S to shift\nIf all need to be shifted, it requires k(k−1)\n× t(n) steps\n2\n\nThis step is O(t(n)) since k(k−1)\nis a constant\n2\n\nTo simulate one step of M , it is\nscan for dots + update dots = O(t(n)) + O(t(n)) = O(t(n))\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fProof of Theorem 7.8\nNote that M performs t(n) steps\nThus, for S to simulate M for t(n) steps, it needs to perform\nat most\nt(n) × O(t(n)) = O(t2 (n))\nTherefore, the total run-time is\nO(n) + O(t2 (n)) = O(t2 (n))\nwhere\nO(n) steps to copying M ’s tape contents, and\nO(t2 (n)) steps to simulate M ’s t(n) steps\n\nIf a multitape TM M decides a language A in t(n) steps, we\ncan simulate TM M using a single tape TM S which can\ndecide A in O(t2 (n))\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fNondeterministic Turing Machine\nDefinition 7.9\nLet N be a nondeterministic Turing machine that is a decider. The\nrunning time of N is the function f : N → N, where f (n) is the\nmaximum number of steps that N uses on any branch of its\ncomputation on any input of length n.\nDeterministic\n\nNondeterministic\n\nreject\n\nf (n)\n\nf (n)\n\naccept\n\naccept\/reject\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nreject\n\nTime Complexity 01\n\n\fNondeterministic Turing Machine\nTheorem 7.11\nLet t(n) be a function, where t(n) ≥ n. Then every t(n) time\nnondeterministic single-tape Turing machine has an equivalent\n2O(t(n)) time deterministic single-tape Turing machine.\nRecall the nondeterministic Turing machine\nThe computation is a tree\nThe machine accepts as soon as one of its branch enter the\naccept state\nThe machine rejects if all of its branch are in the reject state\n\nSince we have to consider the maximum number of steps on\nany branch, we have to consider the longest number of steps\nfor the machine to accept or reject an input string of length n\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fComputation Tree\nThe following is an example of a computation tree of a\nnondeterministic Turing machine where at each state, it can\nsplit to at most two machines.\n\nThis can be applied to NTM that splits to at most k copies\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fComputation Tree\nWe can not simulate one branch at a time\nThe computation of each branch may not end (enter infinite\nloop)\nNo chance to simulate the next branch\nBut NTM may accept the input string because another branch\nenters the accept state\n\nWe need to simulate like breadth first search\n\n1\n\n2\n\n3\n\n4\n\n7\n\n15\n\n8\n\n16\n\n17\n\n5\n\n9\n\n18\n\n19\n\n10\n\n20\n\n21\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n6\n\n11\n\n22\n\n23\n\n12\n\n24\n\n25\n\nTime Complexity 01\n\n13\n\n26\n\n27\n\n14\n\n28\n\n29\n\n30\n\n\fComputation Tree\n\nRecall that this NTM runs in t(n) steps\nThe height of its computational tree will be t(n)\nThe number of steps to reach is node is at most t(n)\n\nThe number of nodes in a full binary tree of height h is 2h − 1\nSince the height of its computational tree is t(n), the number\nof nodes is 2t(n) − 1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\fComputational Tree\n\nThus, the total run-time is\nt(n) × 2t(n) = 2O(t(n))\nThis is the run-time of multitape Turing machine\nFor a single tape TM, the run-time is the square of multitape\nTM\nThus, the run-time is\n(2O(t(n)) )2 = 22O(t(n)) = 2O(2t(n)) = 2O(t(n))\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 01\n\n\f","label":[[113,128,"Concept"],[144,154,"Concept"],[157,165,"Concept"],[496,508,"Concept"],[512,527,"Concept"],[859,864,"Concept"],[869,876,"Concept"],[907,919,"Concept"],[1002,1012,"Concept"],[1082,1101,"Concept"],[1561,1572,"Concept"],[1614,1636,"Concept"],[2851,2868,"Concept"],[2960,2978,"Concept"],[9449,9458,"Concept"],[12499,12511,"Concept"]],"Comments":[]}
{"id":16,"segment": ["test_set", "labeled"],  "course": "cs1502", "lec": "lec01_mathematical_preliminaries","text":"Mathematics Preliminaries\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fMathematics Preliminaries\n\nYou should review the following topics from the Discrete\nStructures for CS (Discrete Mathematics):\nFunctions\nPredicates\n\nSets\nNotations\nOperations\n\nSequences\nProofs\nConstruction\nContradiction\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fSets\n\nSet is a group of objects represented as a unit.\n{a, b, c}\n{1, 2, 3, . . . }\n{} or ∅ (the empty set)\n\nCharacteristic of Sets\nUnordered\n{a, b, c} = {b, c, a}\n\nNo duplicate\n{1, 2, 3, 2} = {1, 2, 3}\n\nCan be finite, infinite, or empty (finite)\n\nUse ∈ or 6∈ to indicate membership status\n1 ∈ {1, 2, 3}\nc 6∈ {a, b}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fFunctions\n\nFunction: f : D → R\nD is a set (domain)\nR is a set (range)\nf associates each element d ∈ D with some element r ∈ R\nDenoted by f (d) = r.\n\nExample: f (x) = (x + 1) mod 3\nD : {1, 2, 3, 4}\nR : {0, 1, 2}\nAssociations are as follows:\nf (1) = 2\nf (2) = 0\nf (3) = 1\nf (4) = 2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fPredicates\n\nA predicate is a function where its range is {true, false}\nExample: f : N → {true, false}\nf (x) = x is an even number\nf (3) = false\nf (14) = true\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fPredicates and Sets\nA set can be defined using a predicate P\nB = {x | P(x)}\nB is a set that contains all elements x such that P(x) is true\nP(a) = true if and only if a ∈ B\nExample:\nB = {x | x is an even number}\nB = {2, 4, 6, 8, . . . }\n\nExample:\nC = {(x, y) | x, y ∈ N and x + 1 = y}\nC = {(1, 2), (2, 3), (3, 4), . . . }\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fPredicates and Sets\n\nGiven a set B = {x | P(x)}, recall that\nP(a) = true if and only if a ∈ B\nIf y ∈ B, we can conclude that P(y) is true\nSimilarly, if P(y) is true, we can conclude that y ∈ B\nIf z 6∈ B, we can conclude that P(z) is false\nSimilarly, if P(z) is false, we can conclude that z 6∈ B\n\nWe will encounter these a lot in our proofs\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fSet Operations\nMake sure to review the following set operations:\nUnion: ∪\nA ∪ B = {x | x ∈ A or x ∈ B}\n\nIntersection: ∩\nA ∩ B = {x | x ∈ A and x ∈ B}\n\nComplement:\nA = {x | x 6∈ A}\n\nSet Different: − or \\\nA − B = {x | x ∈ A and x 6∈ B}\n\nCartesian Product: ×\nA × B = {(x, y) | x ∈ A and y ∈ B}\nExample:\n{1, 2} × {a, b} = {(1, a), (1, b), (2, a), (2, b)}\n\nIf A is a set |A| denotes the number of elements in the set A\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fFunctions\nWith the notion of the Cartesian product, we can define\nfunctions that take more than one parameters\nExample: f (x, y) = max(x, y)\nf : A × A → A where A = {1, 2, 3}\nThe domain is A × A\nThe range is A\nAssociations:\n\nf (1, 1) = 1\nf (1, 2) = 2\nf (1, 3) = 3\nf (2, 1) = 2\nf (2, 2) = 2\nf (2, 3) = 3\nf (3, 1) = 3\nf (3, 2) = 3\nf (3, 3) = 3\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nSometimes we use\ntable to represent\nassociations\nf\n1\n2\n3\n\n1\n1\n2\n3\n\nMathematics Preliminaries\n\n2\n2\n2\n3\n\n3\n3\n3\n3\n\n\fBig Union\nSuppose we have sets A0 , A1 , . . . An , we can represent the\nfollowing set\nA0 ∪ A1 ∪ · · · ∪ An\nusing the Big Union notation as follows:\nn\n[\n\nAi\n\ni=1\n\nNote that the condition is for every i where 0 ≤ i ≤ n\nSometimes the condition can be for every element in a set\nExample\n[\n\nδ(r, a) = δ(1, a) ∪ δ(3, a)\n\nr∈{1,3}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fSequences\n\nA Sequence is a list of objects in some order where duplicate\nis allowed:\n(a, b, c) 6= (b, c, a)\n(a, b, c, b) 6= (a, b, c)\nFinite Sequences are generally called tuples\nA sequence of k elements is a k-tuple\nA 2-tuple usually called an ordered pair\n\nNote: (a, b) = (x, y) if an only if a = x and b = y.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fAlphabets and Strings\nAn alphabet is a finite set of symbols called Σ (sigma)\nΣA = {a, b, c, . . . , z}\nΣB = {0, 1}\n\nA string over Σ is a finite sequence of symbols from Σ\nThe length of a string s is the number of symbols it contains,\ndenoted by |s|\nExamples:\nLet s1 = (h, e, l, l, o)\ns1 is a string over ΣA\n|s1 | = 5\nFor simplicity, s1 = hello\n\nLet s2 = (1, 0, 1, 1, 0, 1)\ns2 is a string over ΣB\n|s2 | is 6\nFor simplicity, s2 = 101101\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fAlphabets and Strings\n\nA string can be empty\nε is used to represent the empty string.\n\nConcatenation:\nLet s and t be strings over Σ, st is the string obtained by\nappending string t to the end of string s.\nExample\nSuppose s = 101 and t = 10, st = 10110\n\nA string s is a substring of a string t if there exists strings u\nand v such that t = usv\nNote that u and v can be ε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fLanguages\nA language is a set of strings over Σ\nCan be finite, infinite, or empty (∅)\n\nExample:\nLet Σ = {a, b}\nAn example of a language B can be\nB = {a, b, aa, ab, ba, bb, aaa, aab}\n\nWe generally use shortlex order to list a set of strings\n(shorter strings precede longer string)\nThe lexicographic ordering which is dictionary ordering may\ngive you a false information\nConsider listing the set of all strings over {a, b}:\nSortlex Order: {ε, a, b, aa, ab, ba, bb, aaa, aab, . . . }\nLexicographc Order: {ε, a, aa, aaa, aaaa, aaaaa, . . . }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fBoolean Logic\nA boolean value can be either True (1) or False (0)\nBoolean operators\np q ¬p p ∧ q\n0 0 1\n0\n0 1 1\n0\n1 0 0\n0\n1 1 0\n1\n\np∨q\n0\n1\n1\n1\n\np→q\n1\n1\n0\n1\n\np↔q\n1\n0\n0\n1\n\nDeMorgan: ¬(p ∧ q) ⇔ ¬p ∨ ¬q\nDeMorgan: ¬(p ∨ q) ⇔ ¬p ∧ ¬q\nImplication: p → q ⇔ ¬p ∨ q\nEquality: p ↔ q ⇔ (p → q) ∧ (q → p)\nExclusive or: p ⊕ q ⇔ ¬(p ↔ q)\nDistributive: p ∧ (q ∨ r) ⇔ (p ∧ q) ∨ (p ∧ r)\nDistributive: (p ∨ q) ∧ r ⇔ (p ∧ r) ∨ (q ∧ r)\nDistributive: p ∨ (q ∧ r) ⇔ (p ∨ q) ∧ (p ∨ r)\nDistributive: (p ∧ q) ∨ r ⇔ (p ∨ r) ∧ (q ∨ r)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\np⊕q\n0\n1\n1\n0\n\n\fProofs\n\nA typical step in a proof is to derive a statement from initial\nassumptions and hypotheses, or from statements that have\nbeen derived previously, or from other generally accepted\nfacts, using principles of logical reasoning\nWe will focus on these two types of Proofs\nProve by Construction\nProve by Contradiction\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Construction\nProve\nFor every two integers a and b, if a and b are odd, then a × b is\nodd.\nThe above statement is in the form of P → Q where\nP = a and b are odd\nQ = a × b is odd\n\nRecall the truth table of P → Q\nP Q P →Q\nT T\nT\nT F\nF\nF T\nT\nF F\nT\nThe only chance that P → Q will be false is when P is true\nand Q is false\nSo, we need to show that if P is true, Q is also true (cannot\nbe false)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Construction\nProve\nFor every two integers a and b, if a and b are odd, then a × b is\nodd.\nAssume that\na and b are integers that are ood.\n\nNeed to Derive:\na × b is odd.\n\nFact:\nIf x is an integer that is odd, there exists an integer k such\nthat x = 2k + 1.\n\nSince a and b are odd integers, we can derive:\na = 2i + 1 and b = 2j + 1 for some integers i and j\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Construction\n\nLet a = 2i + 1 and b = 2j + 1, we have\na × b = (2i + 1) × (2j + 1)\n= 4ij + 2i + 2j + 1\n= 2(2ij + i + j) + 1\nThus, a × b is an odd integer since there exists an integer\nk = 2ij + i + j such that a × b = 2k + 1.\nNote: By showing a couple of test cases is not a proof\nIn the above proof, we assume that a and b can be any odd\nintegers\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Contradiction\n\nConsider the statement ¬P → False\nSuppose we can show that ¬P → False is true\nRecall that T → F = F and F → F = T\nSince ¬P → F is true, ¬P must be false\nIn other words, P is true.\n\nThus, to show that the statement P is True, we need to show\nthat the statement ¬P → False is true.\nFrom previous example, we need to assume that ¬P is true (or\nP is false) and try to derive a false statement\nA false statement is a statement that contradict with an\ninitial assumption, a hypotheses, a statement that have been\nderived previously, or another generally accepted fact.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Contradiction\nProve\n√\n2 is an irrational number.\nFact: An irrational number is any real number that cannot be\nexpressed as a ratio of integers.\n√\nIn other words, if 2 is an irrational\nnumber, there are no two\nm √\n= 2.\nintegers m and n such that\n√ n\n\nLet P be the statement “ 2 is an irrational number”\n\n√\nThus, ¬P is the statement “ 2 is a rational number”\nIn other words, there exists two integers m and n such that\nm √\n= 2.\nn\n\nm\nm\np\nwe can obtain\n= for some positive\nn\nn\nq\nintegers p and q with no common factors greater than 1\n\nFor any ration\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Contradiction\np √\n= 2 for some positive integers p and q with no\nq\ncommon factor greater than 1\n\nLet\n\nLet’s try to derive some statements\np √\n= 2\nq\n\u0012 \u00132\n√\np\n= ( 2)2\nq\np2\n=2\nq2\np2 = 2q 2\nSince p2 = 2q 2 , p2 is an even number.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Contradiction\nSince p2 = 2q 2 , p2 is an even number.\nSince p2 is an even number, p is an even number\nSince p is an even number, p = 2r for some integer r\nKeep deriving:\np2 = 2q 2\n(2r)2 = 2q 2\n4r2 = 2q 2\n2r2 = q 2\nSince q 2 = 2r2 , q 2 is an even number\nSince q 2 is an even number, q is an even number\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProof by Contradiction\n\np √\n= 2 for some positive integers p\nq\nand q with no common factor greater than 1\n√\nBut we show that if 2 is a rational number both p and q are\neven\nEarlier we stated that\n\nTherefore p and q have a common factor greater than 1 which\nis False according to our assumption\nTherefore, P is true\n√\nIn other words, 2 is an irrational number.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\fProofs in CS1502\n\nThere will be a lot of proofs in this class (Formal Methods)\nHardly any mathematical facts or theorems will be used\nThe type of proof to use is predictable\nTo do this, use prove by construction\nTo do that, use prove by contradiction\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nMathematics Preliminaries\n\n\f","label":[[274,283,"Concept"],[284,294,"Concept"],[296,300,"Concept"],[323,332,"Concept"],[333,339,"Concept"],[443,447,"Concept"],[449,452,"Concept"],[569,573,"Concept"],[694,695,"Concept"],[699,701,"Concept"],[845,853,"Concept"],[865,866,"Concept"],[872,875,"Concept"],[885,886,"Concept"],[892,895,"Concept"],[932,933,"Concept"],[934,935,"Concept"],[1014,1015,"Concept"],[1031,1032,"Concept"],[1190,1200,"Concept"],[1204,1213,"Concept"],[1219,1227,"Concept"],[1424,1434,"Concept"],[1439,1443,"Concept"],[1473,1482,"Concept"],[2237,2240,"Concept"],[2302,2307,"Concept"],[2309,2310,"Concept"],[2313,2314,"Concept"],[2341,2353,"Concept"],[2355,2356,"Concept"],[2359,2360,"Concept"],[2388,2398,"Concept"],[2418,2431,"Concept"],[2433,2434,"Concept"],[2438,2439,"Concept"],[2442,2443,"Concept"],[2472,2489,"Concept"],[2491,2492,"Concept"],[2760,2777,"Concept"],[3229,3238,"Concept"],[3629,3638,"Concept"],[3642,3650,"Concept"],[3801,3807,"Concept"],[3838,3845,"Concept"],[3874,3886,"Concept"],[4017,4026,"Concept"],[4031,4038,"Concept"],[4042,4050,"Concept"],[4136,4142,"Concept"],[4193,4199,"Concept"],[4322,4323,"Concept"],[4616,4629,"Concept"],[4798,4807,"Concept"],[4975,4984,"Concept"],[4987,4995,"Concept"],[5175,5189,"Concept"],[5259,5281,"Concept"],[5397,5410,"Concept"],[5456,5474,"Concept"],[6183,6189,"Concept"],[6211,6216,"Concept"]],"Comments":[]}
{"id":18,"segment": ["test_set", "labeled"],  "course": "cs1502", "lec": "lec12.5_exam_1_review","text":"Exam 1 Review (Chapter 1)\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fDeterministic Finite Automaton (DFA)\n\nState Diagram of a DFA\nFormal Definition of a DFA (Q, Σ, δ, q0 , F )\nQ is a non-empty set of states\nΣ is a finite set of symbols\nδ : Q × Σ → Q transition function\nq0 ∈ Q is the start state\nF is a set of accept states\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fFormal Definition of a DFA\n0\n\n1\n\n0\n\n1\n\nq0\n\nq1\n\nq2\n0, 1\n\nM1 = (Q, Σ, δ, q0 , F )\nQ = {q0 , q1 , q2 }\nΣ = {0, 1}\nδ can be defined using the table below:\nδ\n0\n1\nq0 q0 q1\nq1 q2 q1\nq2 q1 q1\nq0 is the start state\nF = {q1 }\n\nThe state diagram and its formal definition are equivalent\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fDeterministic Finite Automaton (DFA)\n\nHow a DFA process an input string?\nAccept\/Reject\n\nLanguage of a DFA\nThe set of all strings accepted by the DFA\n\nA language is regular is there are finite state machines that\nrecognize it\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fLanguage Operators\n\nA language is a set of strings\nGiven language A and B:\nA ∪ B = {x | x ∈ A or x ∈ B}\nA ◦ B = {xy | x ∈ A and y ∈ B}\nA∗ = {x1 x2 . . . xk | k ≥ 0 and xi ∈ A}\n\nRegular language is closed under union, concatenation, and\nstar operations\nGiven two regular languages A and B\nA ∪ B is a regular language\nAB is a regular language\nA∗ and B ∗ are regular languages\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fTool\n\nGiven two DFA MA and MB\nWe obtain an algorithm to construct a DFA M such that\nL(M ) = L(MA ) ∪ L(MB )\nThe algorithm is defined based on formal definitions of DFAs\nMA and MB\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fL is regular under union operation\n\nLet MA recognizes A, where MA = (QA , Σ, δA , qA , FA )\nLet MB recognizes B, where MB = (QB , Σ, δB , qB , FB )\nMachine M = (Q, Σ, δ, q0 , F ) that recognizes A ∪ B can be\nconstructed as follows:\n1\n2\n\nQ = {(r1 , r2 ) | r1 ∈ QA and r2 ∈ QB }\nFor each (r1 , r2 ) ∈ Q and a ∈ Σ\nδ((r1 , r2 ), a) = (δA (r1 , a), δB (r2 , a))\n\n3\n4\n\nq0 = (qA , qB )\nF = {(r1 , r2 ) | r1 ∈ FA or r2 ∈ FB }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fNondeterministic Finite Automaton (NFA)\n\nCan split into multiple copies of itself\nAt a state, it may have 0, 1, or more than 1 exiting arrow for\neach symbol\nAt a state, it may have 0, 1, or more than 1 exiting arrow for ε\nWe generally use a computational tree to compute an NFA\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fFormal Definition of A Nondeterministic Finite Automaton\nA nondeterministic finite automaton is a 5-tuple\n(Q, Σ, δ, q0, F )\n1\n2\n3\n\nQ is a finite set of states\nΣ is a finite alphabet\nδ : Q × Σε → P(Q) is the transition function,\nΣε = Σ ∪ {ε} and\nP(Q) is the powerset of Q (set of set of states).\n\n4\n5\n\nq0 ∈ Q is the start state\nF ⊆ Q is the set of accept states.\n\nNotes\nIn an NFA, one input symbol can change the state of the\nmachine to multiple states.\nSplit to multiple copies with different current states\nExample: δ(q0 , 1) = {q0 , q1 }\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\n0, 1\n\n0, 1\n\n1\n\nq1\n\n0, ε\n\nq2\n\n1\n\nq3\n\nQ = {q1 , q2 , q3 , q4 }\nΣ = {0, 1} and Σε = {0, 1, ε}\nδ is given as\nδ\n0\n1\nq1 {q1 } {q1 , q2 }\nq2 {q3 }\n∅\nq3\n∅\n{q4 }\nq4 {q4 }\n{q4 }\n\nq4\n\nε\n∅\n{q3 }\n∅\n∅\n\nWe treat ε as a regular input symbol\nIf there is no ε transitions, we can ignore the ε column\n\nq1 is the start state\nF = {q4 }\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fEquivalence of NFAs and DFAs\n\nEvery nondeterministic finite automaton has an equivalent\ndeterministic finite automaton.\nLet N = (Q, Σ, δ, q0 , F ) be the NFA recognizing some\nlanguage A\nWe are going to construct a DFA M = (Q0 , Σ, δ 0 , q00 , F 0 )\nrecognizing A\nLet’s consider the case where N has no ε transitions.\n1\n2\n\nQ0 = P(Q) [\nδ 0 (R, a) =\nδ(r, a)\nr∈R\n\n3\n4\n\nq00 = {q0 }\nF 0 = {R ∈ Q0 | R contains an accept state of N }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\nLet Σ be {0, 1}. The following NFA N recognizes the\nlanguage A where A is a set of strings that end with a 1.\n0, 1\n\n1\n\nq0\n\nq1\n\nN = (Q, Σ, δ, q0 , F )\n1\n2\n3\n\nQ = {q0 , q1 },\nΣ = {0, 1}\nδ is given as\nq0\nq1\n\n4\n5\n\n0\n{q0 }\n∅\n\n1\n{q0 , q1 }\n∅\n\nq0 is the start state\nF = {q1 }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\n\nConstruct a DFA M = (Q0 , Σ, δ 0 , q00 , F 0 )\nQ0 = P(Q) = P({q0 , q1 })\nQ0 = {∅, {q0 }, {q1 }, {q0 , q1 }}\nWe will construct δ 0 later\nq00 = {q0 } where q0 is the start state of the NFA\nF 0 = {R ∈ Q0 | R contains an accept state of N }\nF 0 = {{q1 }, {q0 , q1 }}\nwhere F = {q1 }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\n\nLet’s focus on transition functions\nThe transition function δ of the NFA is as follows:\n0\n1\nq0 {q0 } {q0 , q1 }\nq1\n∅\n∅\nRecall that the set of state of the equivalent DFA is the power\nset of set of state of the NFA\nδ0\n0\n1\n∅\n∅\n∅\n{q0 } {q0 , q1 }\n{q0 }\n{q1 }\n∅\n∅\n{q0 , q1 } {q0 } {q0 , q1 }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\nThe state diagram of the machine M = (Q0 , Σ, δ 0 , q00 , F 0 )\nequivalent to N (L(M ) = L(N )) is shown below:\n0\n\n1\n\n1\n\n{q0 }\n\n{q0 , q1 }\n\n0\n0\n\n0\n\n{q1 }\n\n∅\n1\n\n1\n\nWithout bottom part, it is the same as one of our previous\nexample\nIt is okay for a DFA to have unreachable states\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression\n\nA regular expression expresses a regular language\nA regular expression can be constructed using these rules:\n1\n2\n3\n4\n\n5\n\n6\n\nIf a ∈ Σ, a is a regular expression\nε is a regular expression\n∅ is a regular expression\nIf R1 and R2 are regular expressions, R1 ∪ R2 is a regular\nexpression\nIf R1 and R2 are regular expressions, R1 R2 is a regular\nexpression\nIf R1 is a regular expression, R1∗ is a regular expression\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fTool: Regular Expression to NFA\n\nConstruct an NFA from a regular expression\nBased on closure of union, concatenation, and star\n\nGiven a regular expression\n1\n\n2\n\nBreak it down into steps how to construct the regular\nexpression\nFor each step, slowly construct an NFA\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\n\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\n\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\nb\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\nb\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\nb\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nab U a\na\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\n\na\nε\n(ab U a) *\na\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\n\na\nε\n(ab U a) *\n\nε\na\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\nε\n\na\n\nε\n\nε\n(ab U a) *\n\nε\na\nε\n\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\nb\n\n\fRegular Language\n\nEvery regular language can be expressed by a regular\nexpression\nConvert from an DFA to a regular expression\n1\n\nConvert a given n-state DFA to n + 2 states (GNFA)\nAdd a new start state s with ε transition\nAdd a new accept state a with ε transitions\nConvert all transition function from symbol to regular\nexpression\n\n2\n\n3\n\nGet rid of original states of DFA without changing its language\nuntil we only have two states\nThe label from s to a is a regular expression that expresses the\nlanguage of a given DFA\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\nDFA\n1\n\na\n\nb\n\n2\n\na, b\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fExample\nDFA to GNFA\nε\n1\n\na\n\ns\n\n1\n\nb\n\na\n\nb\nε\n\n2\n\na, b\n\na\n\n2\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\naUb\n\n\fExample\nGet rid of state 2\nε\n1\n\ns\n\na\n\n1\n\na\n\nb\n\nb\nε\n2\n\na, b\n\na\n\n2\n\nε\ns\n\n1\n\na\n\nb(a U b)*\na\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\naUb\n\n\fExample\nGet rid of state 1\nε\n1\n\ns\n\na\n\n1\n\nb\n\na\n\nb\nε\na, b\n\n2\n\na\n\n2\n\nε\ns\n\n1\n\na\n\ns\n\na*b(a U b)*\n\nb(a U b)*\na\n\na\n\n{w | w contains at least one b}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\naUb\n\n\fPumping Lemma\n\nThe Pumping Lemma is based on a property of a regular\nlanguage:\nIf A is regular, for a string s ∈ A of length at least P , s can\nbe divided into s = xyz satisfying the following conditions:\n1\n2\n3\n\nxy i z ∈ A for any i ≥ 0\n|y| > 0\n|xy| ≤ p\n\nTo use the Pumping lemma to prove that a language is not\nregular, we use prove by contradiction\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fShow that A = {ai bj | i = j or 2i = j}\nAssume that A is regular. Since A is regular, the Pumping lemma says that for any\nstring s ∈ A of length at least p, s can be divided into s = xyz satisfying the following\nconditions:\n1\n\nxy i z ∈ D for any i ≥ 0\n\n2\n\n|y| > 0\n\n3\n\n|xy| ≤ p\n\nLet s = ap b2p . In this case, i = p and j = 2p. Since 2i = j, s ∈ A. Since s starts\nwith p as, to satisfy the third condition, x and y are strings that contain nothing but\nas. In other words, x = aq for any q ≥ 0, and y = ar for any r > 0. Note that r must\nbe greater than 0 because |y| = |ak | = r, and the condition 2 says that |y| > 0. Since\nx = aq and y = ar , z = ap−(q+r) b2p . Let i = 0. We have\nxy i z = xy 0 z = xz\n= aq ap−(q+r) b2p\n= ap−r b2p\nFor the string ap−r b2p , i = p − r and j = 2p. Thus, for this string to be in A, p − r\nmust be equal to 2p or 2(p − r) must be equal to 2p. For p − r to be equal to 2p, r\nmust be equal to −p. For 2(p − r) to be equal to 2p, r must be 0. But r must be\ngreater than 0. Thus, ap−r b2p 6∈ A — contradiction. Therefore, A is not regular.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\fNote\nLet Σ = {0, 1} and consider the following language:\nB = {ww | w ∈ Σ∗ }\nSuppose you pick the string s to be 0p 0p\nThis string does not lead to a contradiction\nWe get the usual x, y, and z\nx = 0j for j ≥ 0\ny = 0k for k > 0\nz = 0p−(j+k) 0p\n\nxy i z = 0j (0k )i 0p−(j+k) 0p = 02p+k(i−1)\nFor any i, you can always find k > 0 such that\n2p + k(i − 1) = 2x for some positive integer x\nThis allows us to split 02p+k(i−1) to 0x 0x which is in the\nlanguage B\n\nYou need to pick another s\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nExam 1 Review (Chapter 1)\n\n\f","label":[[148,178,"Concept"],[180,183,"Concept"],[186,199,"Concept"],[205,208,"Concept"],[209,226,"Concept"],[232,235,"Concept"],[830,860,"Concept"],[862,865,"Concept"],[918,935,"Concept"],[982,990,"Concept"],[994,1001,"Concept"],[1042,1051,"Concept"],[1131,1149,"Concept"],[1308,1324,"Concept"],[1341,1346,"Concept"],[1348,1361,"Concept"],[1367,1371,"Concept"],[1393,1410,"Concept"],[2330,2363,"Concept"],[2365,2368,"Concept"],[2684,2740,"Concept"],[2743,2776,"Concept"],[3504,3505,"Concept"],[3733,3766,"Concept"],[3785,3815,"Concept"],[5674,5692,"Concept"],[5705,5721,"Concept"],[10445,10458,"Concept"],[10464,10477,"Concept"]],"Comments":[]}
{"id":19,"segment": ["train_set", "labeled"],  "course": "cs0441", "lec": "lec19", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #19: Generalized Permutations and Combinations\n\nBased on materials developed by Dr. Adam Lee\n\n\fCounting problems can often be harder than\nthose from the last few lectures…\nFor example…\n\nCombinations with repetition\n\nRepeated choice\n\nSUCCESS\nUSSECCS\nSCSCSEU\n…\nPermuting indistinguishable items\n\n\fPermutations with repetition\nRecall: r-permutations are ordered collections of r\nelements drawn from some set\nIf an r-permutation is drawn from a set of size n\nwithout replacement, then there are P(n,r) = n!\/(n-r)!\npossible r-permutations\nIf we select the elements of a permutation with\nreplacement, then we can use the product rule to\ncount the number of possible r-permutations\n\n\fHow many strings of length r can be created\nusing the 26 English letters?\nLet our set S = {A, B, C, …, Z}, with |S| = 26\nTo count the number of r-length strings, note that:\nl 26 ways to choose 1st letter\nl 26 ways to choose 2nd letter (not 25)\nl 26 ways to choose 3rd letter (not 24)\nl …\nl 26 ways to choose rth letter (not 26-r+1)\n\nSo, there are 26r possible ways to choose an r-length string from\nthe set S with replacement\n\nIn general: There are nr possible ways to permute a set of size n\nif repetition of elements is allowed\n\n\fMany times, we want to examine combinations of\nobjects in which repeated choices are allowed\nExample: How many ways can four pieces of fruit be chosen from\na bowl containing at least four apples, four oranges, and four\npears? Assume that only the type of fruit chosen matters, not the\nindividual piece.\nThis is TEDIOUS!!!\n\nSolution #1: Explicit enumeration\n4 apples\n3 apples, 1 orange\n3 oranges, 1 pear\n2 apples, 2 oranges\n2 apples, 1 orange, 1 pear\n\n4 oranges\n3 apples, 1 pear\n3 pears, 1 apple\n2 apples, 2 pears\n2 oranges, 1 apple, 1 pear\n\n4 pears\n3 oranges, 1 apple\n3 pears, 1 orange\n2 oranges, 2 pears\n2 pears, 1 apple, 1 orange\n\nSo, there are 15 possible 4-combinations of a set containing 3\nitems if repetition is allowed\n\n\fLet’s find a nice closed-form expression for\ncounting r-combinations with repetition\nExample: Consider a cash box containing $1 bills, $2 bill, $5 bills,\n$10 bill, $20 bills, $50 bills, and $100 bills. How many ways are\nthere to choose 5 bills if order does not matter and bills within a\nsingle denomination are indistinguishable from one another?\nAssume that there are at least 5 bills of each denomination.\n\nObservations:\nl 7 denominations of bills\nl The order that bills are drawn does not matter\nl At least 5 bills of each denomination\n\nImplication: We are counting 5-combinations with repetition from a\nset of 7 items.\n\n$1\n\n$2\n\n$5\n\n$10\n\n$20\n\n$50\n\n$100\n\n\fAn interesting insight…\nNote:\nl The cash box has 7 compartments\nl These compartments are separated by 6 dividers\nl Choosing 5 bills is the same as arranging 5 placeholders (*) and 6 dividers (|)\n\nExamples:\n1. |||**|||***\n\n$1\n$1\n$1\n\n$10\n$10\n\n2. *|*|**||*||\n\n$5\n\n$20\n$20\n\n$50\n\n$100\n\n\fThis leads us to a nice formula…\nObservation: Arranging 5 stars and 6 bars is the same as choosing\n5 “places” for the stars out of 11 total “places.”\n\n*\n\n|\n\n*\n\n*\n\n|\n\n|\n\n*\n\n|\n\n*\n\n|\n\n|\n\nThis can be done in C(11, 5) = 462 ways.\n\nGeneral Theorem: There are C(n+r-1, r) r-combinations from a\nset with n elements when repetition of elements is allowed.\n\n\fBuying cookies!\nExample: How many ways can we choose six cookies at a cookie\nshop that makes 4 types of cookie? Assume that only the type of\ncookies chosen matters (not the order in which they are chosen or\nthe individual cookies within a given type).\n\nSolution #1:\nl Need six “stars” since we are choosing\nsix cookies\nl Need 3 “bars” to separate the cookies\nby type\nl So, C(9, 6) = 84 ways to choose places\nto put stars.\n\nSolution #2:\nl Since we choose six cookies, r = 6\nl Four possible cookie types means n = 4\nl So, C(6+4-1, 6) = C(9,6) = 84 ways to choose cookies!\n\n\fSolving equations\nExample: How many solutions does the equation x1 + x2 + x3 = 11\nhave if x1, x2, and x3 are non-negative integers?\n\nObservation: Solving this problem is the same as choosing 11\nobjects from a set of 3 objects such that x1 objects of type one\nare chosen, x2 objects of type two are chosen, and x3 objects of\ntype three are chosen.\n\nSolution:\nln=3\nl r = 11\nl So, there are C(3+11-1,11) = C(13, 11) = 78\nways to solve this equation\n\n\fFormula Summary\nType\n\nRepetition?\n\nFormula\n\nr-permutation\n\nNo\n\n𝑛!\n𝑛−𝑟 !\n\nr-combination\n\nNo\n\n𝑛!\n𝑟! 𝑛 − 𝑟 !\n\nr-permutation\n\nYes\n\n𝑛!\n\nr-combination\n\nYes\n\n𝑛+𝑟−1 !\n𝑟! 𝑛 − 1 !\n\n\fHow do we deal with indistinguishable items?\nExample: How many strings can be formed by permuting the\nletters of the word MOM?\nObservation: We can’t simply count permutations\nof the letters in MOM. (Why not?)\nCounting permutations leads to an overcount!\nl Rewrite MOM as M1OM2\nl Possible permutations are:\n➣ M1OM2\n➣ M1M2O\n➣ OM1M2\n➣ M2OM1\n➣ M2M1O\n➣ OM2M1\n\nThese are really the same!\n\nHow do we fix this?\n\n\fRather than permuting all letters as a group,\narrange identical letters separately\nNote: The string MOM contains two Ms and one O.\nWe can count the distinct strings formed by permuting\nMOM as follows:\nl Set up 3 “slots” for letters\nl Count the ways that the 2 Ms can be\nassigned to the these slots\nl Count the ways that the O can be\nassigned to the remaining slots\nl Use the product rule!\n\nC(3,2) = 3\nC(1,1) = 1\nC(3,2) × C(1,1) = 3\n\n\fThis tactic can be stated more generally\nTheorem: The number of different permutations of n\nobjects where there are n1 indistinguishable objects of\ntype 1, n2 indistinguishable objects of type 2, …, and\nnk indistinguishable objects of type k is:\n\n𝐶 𝑛, 𝑛! 𝐶 𝑛 − 𝑛! , 𝑛\" … 𝐶 𝑛# , 𝑛#\nWays to place objects of\ntype 1\nWays to place objects of\ntype 2\n\n𝑛!\n=\n𝑛! ! 𝑛\" ! … 𝑛# !\n\nThere is always only one way to place\nobjects of type k!\n\n\fHow many strings can be formed by permuting\nthe letters in SUCCESS?\nNote: SUCCESS contains\nlS×3\nlU×1\nlC×2\nlE×1\n\nWays to assign each letter group:\nl S: C(7,3)\nl U: C(4,1)\nl C: C(3,2)\nl E: C(1,1)\n\nSo, we can form C(7,3) × C(4,1) × C(3,2) × C(1,1) =\n7!\/(3!2!) = 420 distinct strings using letters from the\nword SUCCESS\n\n\fIn-class exercises\nTop Hat\n\n\fMany counting problems can be solved by\n“placing items in boxes”\nWe can consider two types of objects:\n1.\n\nDistinguishable objects (e.g., “Billy, Chrissy, and Dan”)\n\n2.\n\nIndistinguishable objects (e.g., “three students”)\n\nWe can also consider two types of “boxes”:\n1.\n\nDistinguishable boxes (e.g., “room 123 and room 111”)\n\n123\n2.\n\n111\n\nIndistinguishable boxes (e.g., “two homerooms”)\n\n\fThis leads to four classes of problems…\nDistinguishable objects \/ distinguishable boxes\n\n123\n\n111\n\nIndistinguishable objects \/ distinguishable boxes\n\n123\n\n111\n\nE.g., How many ways can Billy, Chrissy, and Dan be\nassigned to the homeroom 123 and homeroom 111?\n\nE.g., How many ways can three students be\nassigned to the homeroom 123 and homeroom 111?\n\nDistinguishable objects \/ indistinguishable boxes\n\nIndistinguishable objects \/ indistinguishable boxes\n\nE.g., How many ways can Billy, Chrissy, and Dan be\nassigned to two different homerooms?\n\nE.g., How many ways can three students be\nassigned to two different homerooms?\n\n\fCounting assignments of distinguishable items to\ndistinguishable boxes\nExample: How many ways are there to deal 5-card poker hands\nfrom a 52-card deck to each of four players?\n\nSolution:\nl Player 1: C(52,5) ways to deal\nl Player 2: C(47,5) ways to deal\nl Player 3: C(42,5) ways to deal\nl Player 4: C(37,5) ways to deal\n\nTheorem: The number of ways that n distinguishable items can be\nplaced into k distinguishable boxes so that ni objects are placed\ninto box i (1 ≤ i ≤ k) is:\nWe can prove this using the\nproduct rule!\n\n\fHow can we place n indistinguishable items into\nk distinguishable boxes?\nThis turns out to be the same as counting the n-combinations for\na set with k elements when repetition is allowed!\nRecall: We solved the above problem by arranging placeholders\n(*) and dividers (|).\nTo place n indistinguishable items into k distinguishable bins:\n1. Treat our indistinguishable items as *s\n2. Use | to divide our distinguishable bins\n3. Count the ways to arrange n placeholders and k-1 dividers\n\nResult: There are C(n + k – 1, n) ways to place n indistinguishable\nobjects into k distinguishable boxes\n\n\fLet’s see how this works…\nExample: How many ways are there to place 10 indistinguishable\nballs into 8 distinguishable bins?\n\nObservation:\n1. Treat balls as *s\n2. Use 8-1 = 7 dividers to separate bins\n3. Pick 10 positions out of a total 17\nto place balls (all remaining\npositions will be bin dividers)\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nSolution: We have C(10 + 8 – 1, 10) = C(17, 10) = 19,448 ways to\narrange 10 indistinguishable balls into 8 distinguishable bins.\n\n\fSadly, counting the ways to place distinguishable items\ninto indistinguishable boxes isn’t so easy…\nExample: How many ways can Anna, Billy, Caitlin, and Danny be\nplaced into three indistinguishable homerooms?\n\nSolution:\nl Let’s call our students A, B, C, and D\nl Goal: Partition A, B, C, and D into at most 3 disjoint subsets\nl One way to put everyone in the same homeroom\n➣\n\n{A, B, C, D}\n\nl Seven ways to put everyone in two homerooms\n➣\n➣\n\n{{A, B, C}, {D}}, {{A, B, D}, {C}}, {{A, C, D}, {B}}, {{B, C, D}, {A}}\n{{A, B}, {C, D}}, {{A, C}, {B, D}}, {{A, D}, {B, C}}\n\nl Six ways to put everyone into three homerooms\n➣\n➣\n\n{{A, B}, {C}, {D}}, {{A, C}, {B}, {D}}, {{A, D}, {B}, {C}}\n{{B, C}, {A}, {D}}, {{B, D}, {A}, {C}}, {{C, D}, {A}, {B}}\n\nl Total: 14 ways to assign Anna, Billy, Caitlin, and Danny to three\nindistinguishable homerooms\n\n\fIs there some simple closed form that we can use to\nsolve this type of problem?\nNo, but there is a complicated one J\nS(n, j) is a Stirling number of the second kind that tells us the\nnumber of ways that a set of n items can be partitioned into j\nnon-empty subsets.\nS(n, j) is defined as follows:\n\n$%&\n\n𝑆 𝑛, 𝑗 =\n\n1\n+ −1 ! 𝐶 𝑗, 𝑖 𝑗 − 𝑖 '\n𝑗!\n!\"#\n\nResult: The number of ways to distribute n distinguishable objects\ninto k indistinguishable boxes is:\n(\n\n(\n\n$%&\n\n$\"&\n\n$\"&\n\n!\"#\n\n1\n+ 𝑆 𝑛, 𝑗 = + + −1 ! 𝐶 𝑗, 𝑖 𝑗 − 𝑖 '\n𝑗!\n\n\fWhat about distributing indistinguishable\nobjects into indistinguishable boxes?\nExample: How many ways can six copies of the same book be\npacked in at most four boxes, if each box can hold up to six\nbooks?\n\nSolution:\n\n6\n5\n\n4\n\n1\n1\n\n3\n\n4\n1\n\n1\n\n3\n1\n\n1\n\n2\n2\n\n3\n1\n\n2\n\n2\n\n3\n\n2\n\n2\n\n1\n\n1\n\n2\n\nTotal: There are 9 ways to pack 6 identical books into at most 4\nindistinguishable boxes.\n\n\fThat was ugly…\n\nIs there a better way to do this?\nUnfortunately, no.\n\nHere’s why: Placing n indistinguishable objects into k\nindistinguishable boxes is the same as writing n as the sum of at\nmost k positive integers arranged in non-increasing order.\nl i.e., n = a1 + a2 + … + aj, where a1 ≥ a2 ≥ … ≥ aj and j ≤ k\nl We say that a1, a2, …, aj is a partition of n into j integers\n\nThere is no known simple closed formula for counting the\npartitions of an integer, thus there is no solution for placing n\nindistinguishable items into k indistinguishable boxes.\n\n\fIn-class exercises\nTop Hat\n\n\fFinal Thoughts\nn Many counting problems require us to generalize the\nsimple permutation and combination formulas from\nlast time\nn Other problems can be cast as counting the ways to\narrange (in)distinguishable objects into\n(in)distinguishable boxes\nl Problems with indistinguishable boxes are generally more\ndifficult\n\n\f","label":[[121,133,"Concept"],[138,150,"Concept"],[290,302,"Concept"],[436,450,"Concept"],[515,528,"Concept"],[623,637,"Concept"],[669,680,"Concept"],[1344,1356,"Concept"],[4396,4409,"Concept"],[4425,4438,"Concept"],[4459,4472,"Concept"],[4483,4496,"Concept"]],"Comments":[]}
{"id":20,"segment": ["train_set", "labeled"],  "course": "cs0441", "lec": "lec05", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #5: Logic Programming and Nested\nQuantifiers\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s topics\nn Applications of predicate logic\nn Nested quantifiers\n\n\fLogic programming enables automated reasoning\nProlog\nl Programming in logic\nl Developed in the 1970s for\nAI purposes\n\nDatalog\nl Logical formalization of\ndatabases\nl Developed in the 1980s\n\nFor our purposes, we can consider Prolog and Datalog to be the\nsame, though in reality they have very important differences.\n\nTwo main constructs:\n\nLower case = constant\n\nl Facts\n² instructor(bill, cs441)\nUpper case = variable\n² student(smith, cs441)\nl Rules\n² teaches(P,S) :- instructor(P,C), student(S,C)\n\n\fRules and facts define predicates\nFacts define predicates by explicitly listing elements that\nsatisfy those predicates\nl “Dr. Garrison is the instructor for CS441”\nº instructor(bill, cs441)\n\nRules define predicates by combining previously specified\npredicates\nl “Professors teach the students enrolled in the courses for\nwhich they are the instructor” º\nteaches(P,S) :- instructor(P,C), student(S,C)\n\nProlog is an environment that lets us issue queries to\ndetermine which predicates are true!\n\n\fA Security Example\ngrant(U, projector) :- located(U, 105), role(U, presenter)\nlocated(U, R) :- owns(U, D), dev_loc(D, R)\nrole(bob, presenter) owns(alice, laptop12)\nrole(carol, presenter) owns(bob, tablet23)\nowns(carol, cell42)\n\ndev_loc(laptop12, 105)\ndev_loc(tablet23, 105)\ndev_loc(cell42, 105)\n\nCan Bob run the projector?\nl Query: ?grant(bob, projector)\nl Solution: true\nKnowledge base\n\nWho is in room 105?\nl Query: ?located(X, 105)\nl Solution: alice, bob, carol\n\n\fWrite and evaluate the following queries\ngrant(U, projector) :- located(U, 105), role(U, presenter)\nlocated(U, R) :- owns(U, D), dev_loc(D, R)\nrole(bob, presenter) owns(alice, laptop12)\nrole(carol, presenter) owns(bob, tablet23)\nowns(carol, cell42)\n\nn Can Alice use the projector?\nl ?grant(alice, projector)\nl false\n\nn Can Carol use the projector\nl ?grant(carol, projector)\nl true\n\ndev_loc(laptop12, 105)\ndev_loc(tablet23, 105)\ndev_loc(cell42, 105)\n\nn Which devices does Alice own?\nl ?owns(alice, X)\nl laptop12\n\n\fLogic programming is a useful tool!\nName\n\nAge\n\nPhone\n\nAlice\n\n19\n\n555-1234\n\nDanielle\n\n33\n\n555-5353\n\nZach\n\n27\n\n555-3217\n\nCharlie\n\n21\n\n555-2335\n\nArtificial Intelligence\n\nDatabases\n\nRoute\nplanning\n\nSecurity\n\n\fJust for grins…\nIf you are interested in playing around with logic\nprogramming, download SWI-Prolog\nl URL: http:\/\/www.swi-prolog.org\/\n\nThis (free) package is a runtime environment in which\nyou can write logic programs and evaluate queries.\nDave\n\nCharlie\nAlice\n\nElise\n\nBob\n\nBecky\n\nFrank\nSarah\n\nTommy\n\n\fNested quantifiers!?!?\nMany times, we need the ability to nest one quantifier\nwithin the scope of another quantifier\n\nExample: All integers have an additive inverse. That is,\nfor any integer x, we can choose an integer y such that\nthe sum of x and y is zero.\n\n\"x $y (x + y = 0)\nThere is no way to express this statement using only a\nsingle quantifier!\n\n\fDeciphering nested quantifiers isn’t as scary as\nit looks…\n… if you remember to read from left to right!\n\n\"x $y \"z [(x + y)×z = 0]\nFor all x…\n… there exists a y such\nthat…\n… for all z…\n\nAnd think about scope of variables\nlike with programming!\n\n… (x + y)× z = 0\n\n\fA few more examples…\n\"x \"y (x + y = y + x)\n\nThis is the commutative\nlaw for addition!\n\nl For all integers x and for all integers y, x + y = y + x\n\n\"x \"y \"z [(x+y)+z = x+(y+z)]\n\nThis is the associative\nlaw for addition!\n\nl For all integers x, for all integers y, and for all integers z,\n(x+y)+z = x+(y+z)\n\n$x \"y (x× y = 0)\nl There exists an x such that for all y, x× y = 0\n\n\fSince we always read from left to right, the\norder of quantifiers matters!\nConsider: \"x $y (x + y = 0)\n\nClearly true!\nJust set y = -x\n\n➣ Every integer has an additive inverse\nNot true…\n\nTranspose: $y \"x (x + y = 0)\n➣ There exists some integer y such that when added to\nany other integer x, the sum of x and y is 0\n\nRemember: As long as you read from left to right, you\nwon’t have any problems!\n\n\fMany mathematical statements can be translated into\nlogical statements with nested quantifiers\nTranslating mathematical expressions is often easier\nthan translating English statements!\nSteps:\n1. Rewrite statement to make quantification and logical\noperators more explicit\n2. Determine the order in which quantifiers should appear\n3. Generate logical expression\n\n\fLet’s try a translation…\nUniversal quantifier\n\nStatement: Every real number except zero has a\nmultiplicative inverse\nx×y=1\n\nSingular—suggestive of an\nexistential quantifier\n\n\"x\nRewrite: For every real number x, if x ≠ 0, then there\nexists a real number y such that x×y = 1.\n… $y (x × y = 1)\n\n(x ≠ 0) → …\n\nTranslation: \"x [(x ≠ 0) → $y (x × y = 1)] OR\n\"x $y [(x ≠ 0) → (x × y = 1)]\n\n\fMore examples…\n\nStatement: The product of any two negative integers\nis always positive\nl For any integer x and any integer y, if x < 0 and y < 0, then\nx× y > 0\nl \"x \"y [(x < 0 ∧ y < 0) → (x× y > 0)]\n\nStatement: For any real number a, it is possible to\nchoose real numbers b and c such that a2 + b2 = c2\nl For any real number a, there exist real numbers b and c\nsuch that a2 + b2 = c2\nl \"a $b $c (a2 + b2 = c2)\n\n\fTranslating quantified statements to English is\nas easy as reading a sentence!\nLet:\nl C(x) ≡ x is enrolled in CS441\nl M(x) ≡ x has an MP3 player\nl F(x, y) ≡ x and y are friends\nl Domain of x and y is “all students”\n\nStatement: ∀x [C(x) → M(x) ∨ (∃y (F(x,y) ∧ M(y))]\nFor every student x…\n… if x is enrolled in CS441, then…\n… x has an MP3 player…\n… or there exists another student y such that…\n… x and y are friends…\n… and y has an MP3 player.\n\n\fTranslate the following expressions into English\nLet:\nl O(x,y) ≡ x is older than y\nl F(x,y) ≡ x and y are friends\nl The domain for variables x and y is “all students”\n\nStatement: ∃x ∀y O(x,y)\nl There exists a student x, such that for all students y, x is older\nthan y.\nl Alternatively: There exists an oldest student.\n\nStatement: ∃x ∃y [F(x,y) ∧ ∀z [(y≠z) → ¬F(x,z)]]\nl There exists two students x and y such that x and y are friends\nand for all students z, if z ≠ y, then x and z are not friends.\nl Alternatively: There exists a student with only one friend L\n\n\fIn-class exercises\nProblem 1: Translate the following mathematical\nstatement into predicate logic: Every even number is a\nmultiple of 2. Assume that the predicate E(x) means “x\nis even.”\nl\n\nHint: What does “x is a multiple of 2” mean algebraically? Try\nnot to use “mod.”\n\nProblem 2: Translate the following expressions into\nEnglish. Assume that C(x) means “x has a car”, F(x,y)\nmeans “x and y are friends”, and S(x) means “x is a\nstudent.”\nl\nl\n\n∀x (S(x) → C(x) ∨ ∃y [F(x,y) ∧ C(y)])\n∀x ∃y ∃z [C(x) ∨ (F(x,y) ∧ C(y)) ∨ (F(x,y) ∧ F(y,z) ∧ C(z))]\n\n\fTranslating from English to a logical expression with\nnested quantifiers is a little bit more work…\n\nSteps:\n1. If necessary, rewrite the sentence to make quantifiers and\nlogical operations more explicit\n2. Create propositional functions to express the concepts in\nthe sentence\n3. State the domains of the variables in each propositional\nfunction\n4. Determine the order of quantifiers\n5. Generate logical expression\n\n\fLet’s try an example…\nUniversal quantifier\n\nStatement: Every student has asked at least one\nprofessor a question.\nExistential quantifier\n\nRewrite: For every person x, if x is a student, then there\nexists a professor whom x has asked a question.\n\nLet:\nl S(x) ≡ x is a student\nl P(x) ≡ x is a professor\nl Q(x,y) ≡ x has asked y a question\n\nDomains for x and\ny are “all people”\n\nTranslation: ∀x (S(x) → ∃y [P(y) ∧ Q(x,y)])\n\n\fTranslate the following from English\nStatement: There is a man who has tasted every type of\nbeer.\n\nRewrite: There exists a person x such that x is man and\nfor all types of drink y, if y is a beer then x has tasted y.\nDomain: all people\n\nLet:\nl M(x) ≡ x is a man\nl B(x) ≡ x is a beer\nl T(x,y) ≡ x has tasted y\n\nDomain: all drinks\nDomains: x = all people,\ny = all drinks\n\nTranslation: ∃x (M(x) ∧ ∀y [B(y) → T(x,y)])\n\n\fNegating expression with nested quantifiers is\nactually pretty straightforward…\n… you just repeatedly apply DeMorgan’s laws!\n¬[∃x (M(x) ∧ ∀y [B(y) → T(x,y)])]\n≡ ∀x ¬(M(x) ∧ ∀y [B(y) → T(x,y)])\n≡ ∀x (¬M(x) ∨ ¬∀y [B(y) → T(x,y)])\n≡ ∀x (¬M(x) ∨ ∃y ¬[B(y) → T(x,y)])\n≡ ∀x (¬M(x) ∨ ∃y ¬[¬B(y) ∨ T(x,y)])\n≡ ∀x (¬M(x) ∨ ∃y [B(y) ∧ ¬T(x,y)])\n≡ ∀x (M(x) → ∃y [B(y) ∧ ¬T(x,y)])\n\na → b ≡ ¬a ∨ b\n\nIn English: For all people x, if x is a man, then there\nexists some type beer that x has not tasted.\n\nAlternatively: No man has tasted every type of beer.\n\n\fA few stumbling blocks…\nWhether the negation sign is on the inside or the outside of\na quantified statement makes a big difference!\n\nExample: Let T(x) ≡ “x is tall”. Consider the following:\nl ¬∀x T(x)\n➣“It is not the case that all people are tall.”\n\nl ∀x ¬T(x)\n➣“For all people x, it is not the case that x is tall.”\n\nNote: ¬∀x T(x) = ∃x ¬T(x) ≠ ∀x ¬T(x)\nRecall: When we push negation into a quantifier,\n\nDeMorgan’s law says that we need to switch the quantifier!\n\n\fA few stumbling blocks…\nLet:\n\nC(x) ≡ “x is enrolled in CS441”\nS(x) ≡ “x is smart.”\n\nQuestion: The following two statements look the same,\nwhat’s the difference?\nl ∃x [C(x) ∧ S(x)]\nl ∃x [C(x) → S(x)]\n\nThere is a smart\nstudent in CS441.\n\nThere exists a student x\nsuch that if x is in CS441,\nthen x is smart.\n\nSubtle note: The second statement is true if there exists\none person not in CS441, because F→F or F→T.\n\n\fNegate ∀x (S(x) → ∃y [P(y) ∧ Q(x,y)])\n¬∀x (S(x) → ∃y [P(y) ∧ Q(x,y)])\n≡ ∃x ¬(S(x) → ∃y [P(y) ∧ Q(x,y)])\n≡ ∃x ¬(¬S(x) ∨ ∃y [P(y) ∧ Q(x,y)])\n≡ ∃x (S(x) ∧ ¬∃y [P(y) ∧ Q(x,y)])\n≡ ∃x (S(x) ∧ ∀y ¬[P(y) ∧ Q(x,y)])\n≡ ∃x (S(x) ∧ ∀y [¬P(y) ∨ ¬Q(x,y)])\n≡ ∃x (S(x) ∧ ∀y [P(y) → ¬Q(x,y)])\n\nIn English: There exists a student x such that for all people\ny, if y is a professor then x has not asked y a question.\n\nAlternatively: There exists a student that has never asked\nany professor a question.\n\n\fIn-class exercises\nProblem 3: Translate the following English sentences\ninto predicate logic.\na) Every student has at least one friend that is dating a\nSteelers fan.\nb) If a person is a parent and a man, then they are the\nfather of some child.\n\nProblem 4: Negate the results from Problem 3 and\ntranslate the negated expressions back into English.\n\n\fFinal Thoughts\nn Logic programming is an interesting application of\npredicate logic that is used throughout computer\nscience\nn Quantifiers can be nested\nl Nested quantifiers are read left to right\nl Order is important!\nl Translation and negation work the same as they did before!\n\nn Next lecture:\nl Rules of inference\nl Please read sections 1.6–1.7\n\n\f","label":[[229,244,"Concept"],[247,265,"Concept"],[2746,2764,"Concept"],[2804,2808,"Concept"],[2813,2823,"Concept"],[2852,2862,"Concept"],[3112,3130,"Concept"],[3420,3435,"Concept"],[3553,3568,"Concept"],[4210,4228,"Concept"],[7284,7304,"Concept"],[7376,7398,"Concept"],[8208,8223,"Concept"],[10371,10388,"Concept"],[10481,10492,"Concept"],[10500,10506,"Concept"],[10509,10527,"Concept"],[10575,10586,"Concept"],[10591,10599,"Concept"]],"Comments":[]}
{"id":21,"segment": ["train_set", "labeled"],  "course": "cs0441", "lec": "lec09", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #9: Set Identities and Functions\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday\u2019s Topics\nSet identities\nl Methods of proof\nl Relationships to logical equivalences\n\nFunctions\nl Important definitions\nl Relationships to sets, relations\nl Specific functions of particular importance\n\n\fSet identities help us manipulate complex\nexpressions\nRecall from last lecture that set operations bear a\nstriking resemblance to logical operations\nl Disjunction (\u2228) and set union (\u222a)\nl Conjunction (\u2227) and set intersection (\u2229)\nl Negation (\u00ac) and complement ( )\n\nJust as logical equivalences helped us manipulate\nlogical expressions, set identities help us simplify and\nunderstand complex set definitions.\n\n\fSome important set identities\nIdentity\n\nName\n\nA\u222a\u2205=A\nA\u2229U=A\n\nIdentity laws\n\nA\u222aU=U\nA\u2229\u2205=\u2205\n\nDomination laws\n\nA\u222aA=A\nA\u2229A=A\n\nIdempotent laws\nComplementation law\n\nA=A\nA\u222aB=B\u222aA\nA\u2229B=B\u2229A\n\nCommutative laws\n\nA \u222a (B \u222a C) = (A \u222a B) \u222a C\nA \u2229 (B \u2229 C) = (A \u2229 B) \u2229 C\n\nAssociative laws\n\nWe don\u2019t have commutative or associative laws for\nset difference!\n\n\fSome important set identities\nIdentity\n\nName\n\nA \u2229 (B \u222a C) = (A \u2229 B) \u222a (A \u2229 C)\nA \u222a (B \u2229 C) = (A \u222a B) \u2229 (A \u222a C)\n\nDistributive laws\nDeMorgan\u2019s laws\n\nA\u222aB=A\u2229B\nA\u2229B=A\u222aB\nA \u222a (A \u2229 B) = A\nA \u2229 (A \u222a B) = A\n\nAbsorption laws\nComplement laws\n\nA\u222aA=U\nA\u2229A=\u2205\n\n\fThere are many ways to prove set identities\nToday, we\u2019ll discuss four common methods:\n1.\n2.\n3.\n4.\n\nMembership tables\nLogical argument\nUsing set builder notation\nApplying other known set identities\n\n\fMembership tables allow us to write proofs like\nwe did using truth tables!\nThe membership table for an expression has columns\nfor sub-expressions and rows to indicate the ways in\nwhich an arbitrary element may or may not be\nincluded.\n\nExample: A membership table for set intersection\nA\n\nB\n\nA\u2229B\n\n1\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\nAn element is in A \u2229 B iff it is in both A and B\n\n\fProve that A \u2229 (B \u222a C) = (A \u2229 B) \u222a (A \u2229 C)\nA\n\nB\n\nC\n\nB\u222aC\n\nA \u2229 (B \u222a C)\n\nA\u2229B\n\nA\u2229C\n\n(A \u2229 B) \u222a (A \u2229 C)\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nSince the appropriate columns of the membership table\nare the same, we can conclude that A \u2229 (B \u222a C) = (A \u2229 B)\n\u222a (A \u2229 C). \u274f\n\n\fSometimes, it\u2019s easier to make a logical\nargument about a set identity\nRecall: A = B iff A \u2286 B and B \u2286 A\nAs a result, we can prove a set identity by arguing that\neach side of the equality is a subset of the other.\n\nExample: Prove that A \u2229 B = A \u222a B\n1.First prove that A \u2229 B \u2286 A \u222a B\n2.Then prove that A \u222a B \u2286 A \u2229 B\nLet\u2019s see how this is done\u2026\n\n\fProve that A \u2229 B = A \u222a B\nFirst show A \u2229 B \u2286 A \u222a B\nl Let x be an arbitrary element of A \u2229 B\nl By def'n of complement, x \u2209 A \u2229 B\nl By def'n of \u2209, \u00ac(x \u2208 A \u2229 B)\nl By def'n of intersection, \u00ac(x \u2208 A \u2227 x \u2208 B)\nl By DeMorgan's, \u00ac(x \u2208 A) \u2228 \u00ac(x \u2208 B)\nl By def'n of \u2209, x \u2209 A \u2228 x \u2209 B\nl By def'n of complement, x \u2208 A \u2228 x \u2208 B\nl By def'n of union, x \u2208 A \u222a B\nl Therefore, A \u2229 B \u2286 A \u222a B\n\n\fProve that A \u2229 B = A \u222a B\nNext show A \u222a B \u2286 A \u2229 B\nl Let x be an arbitrary element of A \u222a B\nl By def'n of union, x \u2208 A \u2228 x \u2208 B\nl By def'n of complement, x \u2209 A \u2228 x \u2209 B\nl By def'n of \u2209, \u00ac(x \u2208 A) \u2228 \u00ac(x \u2208 B)\nl By DeMorgan's, \u00ac(x \u2208 A \u2227 x \u2208 B)\nl By def'n of intersection, \u00ac(x \u2208 A \u2229 B)\nl By def'n of \u2209, x \u2209 A \u2229 B\nl By def'n of complement, x \u2208 A \u2229 B\nl Therefore, A \u222a B \u2286 A \u2229 B\nSince we have shown A \u2229 B \u2286 A \u222a B and A \u222a B \u2286 A \u2229 B, we have\nshown that A \u2229 B = A \u222a B\n\n\fWe can use set builder notation and logical\ndefinition to make very precise proofs\nExample: Prove that A \u2229 B = A \u222a B\nProof:\n1. A \u2229 B = {x | x \u2209 A \u2229 B}\n2.\n= {x | \u00ac(x \u2208 (A \u2229 B))}\n3.\n= {x | \u00ac(x \u2208 A \u2227 x \u2208 B)}\n4.\n= {x | \u00ac(x \u2208 A) \u2228 \u00ac(x \u2208 B)}\n5.\n= {x | x \u2209 A \u2228 x \u2209 B}\n6.\n= {x | x \u2208 A \u2228 x \u2208 B}\n7.\n= {x | x \u2208 A \u222a B}\n8.\n=A\u222aB\n\nDef\u2019n of complement\nDef\u2019n of \u2209\nDef\u2019n of \u2229\nDeMorgan\u2019s law\nDef\u2019n of \u2209\nDef\u2019n of complement\nDef\u2019n of \u222a\nSet builder notation\n\n\u274f\n\n\fNote: Differences between \u2286 and \u2208\nRecall that A \u2286 B if A is a subset of B, whereas a \u2208 A means\nthat a is an element of A.\n\nExamples:\nl Is {1} \u2208 {1, 2, 3}?\nl Is {1} \u2286 {1, 2, 3}?\nl Is 1 \u2208 {1, 2, 3}?\nl Is {2, 3} \u2286 {1, {2, 3}, {4, 5}}?\nl Is {2, 3} \u2208 {1, {2, 3}, {4, 5}}?\nl Is \u2205 \u2208 {1, 2, 3}?\nl Is \u2205 \u2286 {1, 2, 3}?\n\nNo!\nYes!\nYes!\nNo! But {{2, 3}} is\u2026\nYes!\nNo!\nYes! \u2205 is a subset of every set\n\n\fBe careful when computing power sets\nQuestion: What is P({1, 2, {1, 2}})?\nNote: The set {1, 2, {1, 2}} has three elements\nl 1\nl 2\nl {1, 2}\n\nSo, we need all combinations of those elements:\nl \u2205\nl {1}\nl {2}\nl {{1,2}}\nl {1, 2}\nl {1, {1,2}}\nl {2, {1,2}}\nl {1, 2, {1, 2}}\n\n\u2234 P({1, 2, {1,2}}) = {\u2205, {1}, {2}, {{1,2}},\n{1, 2}, {1, {1,2}},\n{2, {1,2}},\n{1, 2, {1,2} } }\nThis power set has 23 = 8 elements.\n\n\fWe can also construct proofs by repeatedly\napplying known set identities\nExample: Prove that A \u222a (B \u2229 C) = (C \u222a B) \u2229 A\nProof:\n1. A \u222a (B \u2229 C) = A \u2229 (B \u2229 C)\n2.\n= A \u2229 (B \u222a C)\n3.\n= (B \u222a C) \u2229 A\n4.\n= (C \u222a B) \u2229 A\n\nDeMorgan\u2019s law\nDeMorgan\u2019s law\nCommutative law\nCommutative law\n\n\u274f\nNote how similar this process is to that of proving logical equivalences\nusing known logical equivalences.\n\n\fIn-class exercises\nProblem 1: Prove DeMorgan\u2019s law for complement\nover intersection using a membership table.\nProblem 2: Prove the complementation law using set\nbuilder notation.\n\n\fSets give us a way to formalize the concept of a\nfunction\nDefinition: Let A and B be nonempty sets. A function,\nf, is an assignment of exactly one element of set B to\neach element of set A.\nNote: We write f : A \u2192 B to denote that f is a function\nfrom A to B\nNote: We say that f(a) = b if the element a \u2208 A is\nmapped to the unique element b \u2208 B by the function f\nf\n\nA\na\u25cf\n\nf(a) = b\n\nB\n\u25cfb\n\n\fFunctions can be defined in a number of ways\n1. Explicitly\nl\nl\n\nf:Z\u2192Z\nf(x) = x2 + 2x + 1\n\n2. Using a programming language\nl\n\nint min(int x, int y) = { x < y ? return x : return y; }\n\n3. Using a relation\nl\nl\n\nLet S = {Anna, Brian, Christine}\nLet G = {A, B, C, D, F}\n\nf:S\u2192G\nAnna \u25cf\n\n\u25cfA\n\nBrian \u25cf\n\n\u25cfB\n\nChristine \u25cf\n\n\u25cfC\n\u25cfD\n\u25cfF\n\n\fMore terminology\nThe domain of a function is the set that the function maps from, while\nthe codomain is the set that is mapped to\nIf f(a) = b, b is called the image of a, and a is called the preimage of b\nThe range of a function f : A \u2192 B is the set of all images of elements of A\nDomain = S = {Anna, Brian, Christine}\n\nCodomain = G = {A,\nB, C, D, F}\n\nf:S\u2192G\nAnna \u25cf\n\n\u25cfA\n\nBrian \u25cf\n\n\u25cfB\n\nChristine \u25cf\n\n\u25cfC\n\u25cfD\n\u25cfF\n\nRange = {A, C}\n\n\fWhat are the domain, codomain, and range of\nthe following functions?\n1. f : Z \u2192 Z, f(x) = x3\nl Domain: Integers\nl Codomain: Integers\nl Range: Perfect cubes\n\n2. g : R \u2192 R, g(x) = x - 2\nl Domain: Real numbers\nl Codomain: Real numbers\nl Range: Real numbers\n\n3.\n\nint foo(int x, int y) = { return (x*y)%2; }\nl Domain: All (x, y) \u2208 Z\u00d7Z\nl Codomain: Integers\nl Range: {0, 1}\n\n\fA one-to-one function never assigns the same\nimage to two different elements\nDefinition: A function f : A \u2192 B is one-to-one, or\ninjective, iff \u2200x,y\u2208A [(f(x) = f(y)) \u2192 (x = y)]\nAre the following functions injections?\nl f : R \u2192 R, f(x) = x + 1\nl f : Z \u2192 Z, f(x) = x2\nl f : R+ \u2192 R+, f(x) = \u221ax\nl f:S\u2192G\n\nYes\nNo\nYes\nNo\nf:S\u2192G\n\nAnna \u25cf\nBrian \u25cf\nChristine \u25cf\n\n\u25cfA\n\u25cfB\n\u25cfC\n\u25cfD\n\u25cfE\n\n\fAn onto function \u201cuses\u201d every element of its\ncodomain\nDefinition: We call a function f : A \u2192 B onto, or\n\nsurjective, iff for every element b \u2208 B, there is some\nelement a \u2208 A such that f(a) = b.\nThink about an onto function as \u201ccovering\u201d the entirety of\nits codomain.\n\nThe following function is a surjection:\nf:A\u2192B\na\u25cf\nb\u25cf\nc\u25cf\nd\u25cf\n\n\u25cf1\n\u25cf2\n\u25cf3\n\n\fAre the following functions one-to-one, onto,\nboth, or neither?\nf:A\u2192B\na\u25cf\nb\u25cf\nc\u25cf\nd\u25cf\n\nf:A\u2192B\n\u25cf1\n\u25cf2\n\u25cf3\n\na\u25cf\nb\u25cf\nc\u25cf\nd\u25cf\n\nOne-to-one and onto\n(Aside: Functions that are both one-to-one\nand onto are called bijections)\n\nNeither!\n\nf:A\u2192B\na\u25cf\nb\u25cf\nc\u25cf\nd\u25cf\n\nOne-to-one\n\n\u25cf1\n\u25cf2\n\u25cf3\n\u25cf4\n\n\u25cf1\n\u25cf2\n\u25cf3\n\u25cf4\n\u25cf5\n\nf:A\u2192B\na\u25cf\nb\u25cf\nc\u25cf\nd\u25cf\n\n\u25cf1\n\u25cf2\n\u25cf3\n\nOnto\n\n\fBijections have inverses\nDefinition: If f : A \u2192 B is a bijection, the inverse of\nf is the function f-1 : B \u2192 A that assigns to each b \u2208\nB the unique value a \u2208 A such that f(a) = b. That is,\nf-1(b) = a iff f(a) = b.\nf\n\nGraphically:\n\nA\na\u25cf\n\nf(a) = b\n\nB\n\u25cfb\n\nf-1(b) = a\n\nNote: Only a bijection can have an inverse. (Why?)\n\n\fDo the following functions have inverses?\n1. f : R \u2192 R, f(x) = x2\nl No, since this function is not onto\n\n2. g : Z \u2192 Z, g(x) = x + 1\nl Yes, g-1(x) = x \u2013 1\n\n3. h : A \u2192 B\nl Yes\nh:A\u2192B\na\u25cf\nb\u25cf\nc\u25cf\nd\u25cf\n\nh-1 : B \u2192 A\n\u25cf1\n\u25cf2\n\u25cf3\n\u25cf4\n\n1\u25cf\n2\u25cf\n3\u25cf\n4\u25cf\n\n\u25cfa\n\u25cfb\n\u25cfc\n\u25cfd\n\n\fFunctions can be composed with one another\nGiven functions g : A \u2192 B and f : B \u2192 C, the composition\nof f and g, denoted f\u25e6g, is defined as (f\u25e6g)(x) = f(g(x)).\ng\n\nA\na\u25cf\n\ng(a) = b\n\nf\n\nB\n\u25cf g(a)\n\nf(g(a)) = c\n\nC\n\u25cfc\n\nf\u25e6g\n\nNote: For f\u25e6g to exist, the codomain of g must be a\nsubset of the domain of f.\nDefinition: If g : A \u2192 B and f : D \u2192 C and B \u2286 D, f\u25e6g is\na function A \u2192 C where (f\u25e6g)(x) = f(g(x))\n\n\fCan the following functions be composed? If so,\nwhat is their composition?\nLet f : A \u2192 A such that f(a) = b, f(b) = c, f(c) = a\ng : B \u2192 A such that g(1) = b, g(4) = a\n1. (f \u25e6 g)(x)?\n2. (g \u25e6 f)(x)?\n\nYes! f(g(1)) = c, f(g(4)) = b\nNo! A \u2284 B\n\nLet f : Z \u2192 Z, f(x) = 2x + 1\ng : Z \u2192 Z, g(x) = x2\n1. (f \u25e6 g)(x)?\n2. (g \u25e6 f)(x)?\n\nYes! (f \u25e6 g)(x) = 2x2 + 1\nYes! (g \u25e6 f)(x) = 4x2 + 4x + 1\n\nNote: There is not a guarantee that (f \u25e6 g)(x) = (g \u25e6 f)(x).\n\n\fImportant functions\nDefinition: The floor function maps a real number x\nto the largest integer y that is not greater than x. The\nfloor of x is denoted \u230ax\u230b.\n\nDefinition: The ceiling function maps a real number x\nto the smallest integer y that is not less than x. The\nceiling of x is denoted \u2308x\u2309.\n\nExamples:\nl \u230a1.2\u230b = 1\nl \u230a7.0\u230b = 7\nl \u230a-42.24\u230b = -43\n\nl \u23081.2\u2309 = 2\nl \u23087.0\u2309 = 7\nl \u2308-42.24\u2309 = -42\n\n\fWe actually use floor and ceiling quite a bit in\ncomputer science\u2026\nExample: A byte, which holds 8 bits, is typically the\nsmallest amount of memory that can be allocated on\nmost systems. How many bytes are needed to store\n123 bits of data?\nAnswer: We need \u2308123/8\u2309 = \u230815.375\u2309 = 16 bytes\n\nExample: How many 1400 byte packets can be\ntransmitted over a 14.4 kbps modem in one minute?\nAnswer: A 14.4 kbps modem can transmit 14,400*60 =\n864,000 bits per minute. Therefore, we can transmit\n\u230a864,000/(1400*8)\u230b = \u230a77.1428571\u230b = 77 packets.\n\n\fIn-class exercises\nProblem 3: Find the domain and range of each of the\nfollowing functions.\na. The function that determines the number of zeros in some\nbit string\nb. The function that maps an English word to its two\nrightmost letters\nc. The function that assigns to an integer the sum of its\nindividual digits\n\nProblem 4: On Top Hat\n\n\fFinal thoughts\nn Set identities are useful tools!\nn We can prove set identities in a number of\n(equivalent) ways\nn Sets are the basis of functions, which are used\nthroughout computer science and mathematics\nn Next time:\nl Summations (Section 2.4)\n\n\f", "label": [[108, 122, "Concept"],[127, 136, "Concept"],[200, 214, "Concept"],[275, 284, "Concept"],[392, 406, "Concept"],[726, 740, "Concept"],[815, 829, "Concept"],[859, 872, "Concept"],[887, 902, "Concept"],[917, 932, "Concept"],[933, 952, "Concept"],[975, 991, "Concept"],[1046, 1062, "Concept"],[1147, 1161, "Concept"],[1243, 1260, "Concept"],[1261, 1276, "Concept"],[1327, 1342, "Concept"],[1343, 1358, "Concept"],[1403, 1417, "Concept"],[1473, 1490, "Concept"],[1491, 1507, "Concept"],[1514, 1534, "Concept"],[1556, 1570, "Concept"],[1573, 1590, "Concept"],[1652, 1668, "Concept"],[1819, 1835, "Concept"],[2283, 2299, "Concept"],[2430, 2442, "Concept"],[2505, 2517, "Concept"],[3552, 3572, "Concept"],[3577, 3595, "Concept"],[5377, 5385, "Concept"],[5430, 5438, "Concept"],[5716, 5725, "Concept"],[6058, 6064, "Concept"],[6129, 6137, "Concept"],[6196, 6201, "Concept"],[6228, 6236, "Concept"],[6246, 6251, "Concept"],[6318, 6324, "Concept"],[6357, 6365, "Concept"],[6443, 6448, "Concept"],[6473, 6479, "Concept"],[6481, 6489, "Concept"],[6495, 6500, "Concept"],[6555, 6561, "Concept"],[6574, 6582, "Concept"],[6595, 6600, "Concept"],[6646, 6652, "Concept"],[6669, 6677, "Concept"],[6694, 6699, "Concept"],[6765, 6771, "Concept"],[6792, 6800, "Concept"],[6813, 6818, "Concept"],[6831, 6850, "Concept"],[6874, 6879, "Concept"],[6942, 6952, "Concept"],[7023, 7032, "Concept"],[7033, 7043, "Concept"],[7197, 7201, "Concept"],[7202, 7210, "Concept"],[7239, 7247, "Concept"],[7289, 7293, "Concept"],[7299, 7309, "Concept"],[7403, 7407, "Concept"],[7451, 7459, "Concept"],[7490, 7500, "Concept"],[7839, 7843, "Concept"],[7846, 7856, "Concept"],[7901, 7910, "Concept"],[7936, 7944, "Concept"],[8125, 8134, "Concept"],[8410, 8419, "Concept"],[8427, 8435, "Concept"],[8498, 8509, "Concept"],[8653, 8661, "Concept"],[8691, 8697, "Concept"],[8823, 8832, "Concept"],[8836, 8844, "Concept"],[8867, 8878, "Concept"],[9282, 9287, "Concept"],[9288, 9296, "Concept"],[9419, 9426, "Concept"],[9427, 9435, "Concept"],[9512, 9519, "Concept"],[9653, 9658, "Concept"],[9663, 9670, "Concept"],[10521, 10535, "Concept"],[10569, 10583, "Concept"],[10619, 10623, "Concept"],[10641, 10649, "Concept"]], "Comments": []}
{"id":22,"segment": ["train_set", "labeled"],  "course": "cs0441", "lec": "lec23", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #23: Expected Value\n\nBased on materials developed by Dr. Adam Lee\n\n\fWhat is a random variable?\nDefinition: A random variable is a function X from the sample\nspace of an experiment to the set of real numbers R. That is, a\nrandom variable assigns a real number to each possible outcome.\n\nNote: Despite the name, X is not a variable,\nand is not random. X is a function!\n\nExample: Suppose that a coin is flipped three times. Let X(s) be\nthe random variable that equals the numbers of heads that\nappear when s is the outcome. Then X(s) takes the following\nvalues:\nl X(HHH) = 3\nl X(HHT) = X(HTH) = X(THH) = 2\nl X(TTH) = X(THT) = X(HTT) = 1\nl X(TTT) = 0\n\n\fRandom variables and distributions\nDefinition: The distribution of a random variable X on a sample\nspace S is the set of pairs (r, p(X=r)) for all r ∈ X(S), where p(X=r)\nis the probability that X takes the value r.\nNote: A distribution is usually described by specifying p(X=r) for\neach r ∈ X(S)\n\nExample: Assume that our coin flips from the previous slide were\nall equally likely to occur. We then get the following distribution\nfor the random variable X:\nl p(X=0) = 1\/8\nl p(X=1) = 3\/8\nl p(X=2) = 3\/8\nl p(X=3) = 1\/8\n\n\fMany times, we want to study the expected\nvalue of a random variable\nDefinition: The expected value (or expectation) of a random\nvariable X(s) on the sample space S is equal to:\n𝐸 𝑋 = %𝑝 𝑠 𝑋 𝑠\n!∈#\n\nFor every outcome…\n\n… use the probability of\nthat outcome occuring…\n\n… to weight the value of the\nrandom variable for that\noutcome.\n\nNote: The expected value of a random variable defined on an\ninfinite sample space is defined iff the infinite series in the\ndefinition is absolutely convergent.\n\n\fA roll of the dice…\nExample: Let X be the number that comes up when a die is\nrolled. What is the expected value of X?\n\nSolution:\nl 6 possible outcomes: 1, 2, 3, 4, 5, 6\nl Each outcomes occurs with the probability 1\/6\nl E(X) = 1\/6 + 2\/6 + 3\/6 + 4\/6 + 5\/6 + 6\/6\nl\n= 21\/6\nl\n= 7\/2\n\n\fA flip of the coin…\nExample: A fair coin is flipped three times. Let S be the sample\nspace of the eight possible outcomes, and X be the random\nvariable that assigns to an outcome the number of heads in that\noutcome. What is the expected value of X?\n\nSolution:\nl Since coin flips are independent, each outcome is equally likely\nl E(X) = 1\/8[X(HHH) + X(HHT) + X(HTH) + X(THH) + X(TTH)\n+ X(THT) + X(HTT) + X(TTT)]\nl\n= 1\/8[3 + 2 + 2 + 2 + 1 + 1 + 1 + 0]\nl\n= 12\/8\nl\n= 3\/2\n\n\fIf S is large, the definition of expected value can be\ndifficult to use directly\nDefinition: If X is a random variable and p(X=r) is the probability\nthat X = r (i.e., p(X=r) = ∑s∈S,X(s)=r p(s)), then\n𝐸 𝑋 = % 𝑝 𝑋=𝑟 𝑟\n$∈% #\n\nEach value of X…\n\n… is weighted by its probability of\noccurrence.\n\nProof:\nl Suppose that X is a random variable ranging over S\nl Note that p(X=r) is the probability that X takes the value r\nl This means that p(X=r) is the sum of the probabilities of the outcomes s∈S\nsuch that X(s) = r\nl It thus follows that 𝐸 𝑋 = ∑!∈# $ 𝑝 𝑋 = 𝑟 𝑟 ❏\n\n\fRolling two dice\nExample: Let X be the sum of the numbers that appear when a pair of fair\ndice is rolled. What is the expected value of X?\n\nRecall from last week:\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n\nX(1,1) = 2\nX(1,2) = X(2, 1) = 3\nX(1,3) = X(2,2) = X(3,1) = 4\nX(1,4) = X(2,3) = X(3,2) = X(4,1) = 5\nX(1,5) = X(2,4) = X(3,3) = X(4,2) = X(5,1) = 6\nX(1,6) = X(2,5) = X(3,4) = X(4,3) = X(5,2) = X(6,1) = 7\nX(2,6) = X(3,5) = X(4,4) = X(5,3) = X(6,2) = 8\nX(3,6) = X(4,5) = X(5,4) = X(6,3) = 9\nX(4,6) = X(5,5) = X(6,4) = 10\nX(5,6) = X(6,5) = 11\nX(6,6) = 12\n\np(X=2) = 1\/36\np(X=3) = 2\/36 = 1\/18\np(X=4) = 3\/36 = 1\/12\np(X=5) = 4\/36 = 1\/9\np(X=6) = 5\/36\np(X=7) = 6\/36 = 1\/6\np(X=8) = 5\/36\np(X=9) = 4\/36 = 1\/9\np(X=10) = 3\/36 = 1\/12\np(X=11) = 2\/36 = 1\/18\np(X=12) = 1\/36\n\nSo we have that:\nl E(X) = 2(1\/36) + 3(1\/18) + 4(1\/12) + 5(1\/9) + 6(5\/36) + 7(1\/6) + 8(5\/36)\n+ 9(1\/9) + 10(1\/12) + 11(1\/18) + 12(1\/36)\nl\n=7\n\n\fWe can apply this formula to reason about\nBernoulli trials!\nTheorem: The expected number of successes when n independent\nBernoulli trials are performed, in which p is the probability of\nsuccess, is np.\nThe proof of this theorem is straightforward (cf. Sec 7.4 of the text)\nBut let’s think about it intuitively…\nl 6 coin flips, how many will be heads?\nl Bernoulli trials: n = 6, p = 0.5, q = 0.5\nl Intuitively, you’d expect half of your flips to be heads\nl Mathematically, 6 * 0.5 = 3\n\n\fExpected values are linear!\nTheorem: If X1, X2, …, Xn are random variables on S and if a and b\nare real numbers, then\n1.\n2.\n\nE(X1 + X2 + … + Xn) = E(X1) + E(X2) + … + E(Xn)\nE(aX + b) = aE(X) + b\n\nProof:\nl\nl\nl\nl\nl\n\nTo prove the first result for n=2, note that\nE(X1 + X2) = ∑s∈S p(s)(X1(s) + X2(s))\nDef’n of E(X)\n= ∑s∈S p(s)X1(s) + ∑s∈S p(s)X2(s)\nProperty of summations\n= E(X1) + E(X2)\nDef’n of E(X)\nThe case with n variables is an easy proof by induction\n\nl\nl\nl\nl\nl\n\nTo prove the second property, note that\nE(aX + b) = ∑s∈S p(s)(aX(s) + b)\n= ∑s∈S p(s)aX(s) + ∑s∈S p(s)b\n= a∑s∈S p(s)X(s) + b∑s∈S p(s)\n= aE(X) + b ❏\n\nDef’n of E(X)\nProperty of summations\nProperty of summations\nDef’n of E(X), ∑s∈S p(s) = 1\n\n\fDice, revisited\nExample: What is the expected value of the sum of the numbers\nthat appear when two fair dice are rolled?\n\nSolution:\nl Let X1 and X2 be random variables indicating the value on the first and\nsecond die, respectively\nl Want to calculate E(X1+X2)\nl By the previous theorem, we have that E(X1+X2) = E(X1)+E(X2)\nl From earlier in lecture, we know that E(X1) = E(X2) = 7\/2\nl So, E(X1+X2) = 7\/2 + 7\/2 = 7\n\nNote: This agrees with the (more complicated)\ncalculation that we made earlier in lecture.\n\n\fIn-class exercises\nTop Hat\n\n\fSometimes we need more information than the\nexpected value can give us\nThe expected value of a random variable doesn’t tell\nus the whole story…\n\np(X(s)=r)\n\np(X(s)=r)\nX(s)\n\nX(s)\n\np(X(s)=r)\nX(s)\n\n\fThe variance of a random variable gives us information\nabout how wide it is spread\nDefinition: The variance of a random variable 𝑋 on a\nsample space 𝑆 is defined as:\n𝑉 𝑋 = % 𝑋 𝑠 −𝐸 𝑋\n\n$\n\n𝑝 𝑠\n\n!∈#\nSquared difference from\nexpected value\n\nWeighted by probability of\noccurrence\n\nDefinition: The standard deviation of a random\nvariable 𝑋 on a sample space 𝑆 is defined as\n\n𝑉 𝑋 .\n\n\fVariance of a die\nExample: A fair die is rolled. What is the variance of the random\nvariable X representing the face that appears?\n\nSolution:\nl Recall that E(X) = 3.5\nl X(1) = 1, p(1)=1\/6\nl X(2) = 2, p(2)=1\/6\nl X(3) = 3, p(3)=1\/6\nl X(4) = 4, p(4)=1\/6\nl X(5) = 5, p(5)=1\/6\nl X(6) = 6, p(6)=1\/6\nl Thus, V(X) = (1\/6)(1−3.5)2 + (1\/6)(2−3.5)2 + (1\/6)(3−3.5)2 +\n(1\/6)(4−3.5)2 + (1\/6)(5−3.5)2 + (1\/6)(6−3.5)2\nl V(X) = 6.25\/6 + 2.25\/6 + 0.25\/6 + 0.25\/6 + 2.25\/6 + 6.25\/6\nl V(X) = 17.5\/6 ≈ 2.92\n\n\fVariance: The short form\nTheorem: If X is a random variable on a sample space S,\nthen 𝑉 𝑋 = 𝐸 𝑋 $ − 𝐸 𝑋 $.\n\nProof:\nl V(X) = ∑s∈S (X(s) – E(X))2p(s)\nl\n= ∑s∈S X(s)2p(s) – 2E(X)∑s∈S X(s)p(s) + E(X)2∑s∈S p(s)\nl\n= E(X2) – 2E(X)E(X) + E(X)2\nl\n= E(X2) – E(X)2\n❏\n\n\fVariance of a die, revisited\nExample: A fair die is rolled. What is the variance of the random\nvariable X representing the face that appears?\n\nSolution:\nl Recall that E(X) = 3.5\nl X2(1) = 1, p(1)=1\/6\nl X2(2) = 4, p(2)=1\/6\nl X2(3) = 9, p(3)=1\/6\nl X2(4) = 16, p(4)=1\/6\nl X2(5) = 25, p(5)=1\/6\nl X2(6) = 36, p(6)=1\/6\nl Thus, E(X2) = (1\/6)(1) + (1\/6)(4) + (1\/6)(9) + (1\/6)(16) + (1\/6)(25) +\n(1\/6)(36)\nl E(X2) = 1\/6 + 4\/6 + 9\/6 + 16\/6 + 25\/6 + 36\/6 = 91\/6\nl V(X) = E(X2) − E(X)2 = 91\/6 − 3.52 ≈ 2.92\n\n\fMultiple Dice\nExample: Two dice are rolled. What is the variance of the\nrandom variable X((j,k)) = 2j, where j is the number\nappearing on the first die and k is the number appearing on\nthe second die.\n\nSolution:\nl V(X) = E(X2) – E(X)2\nl Note that p(X=m) = 1\/6 for m = 2,4,6,8,10,12 and is 0 otherwise\nl E(X) = (2+4+6+8+10+12)\/6 = 7\nl E(X2) = (22+42+62+82+102+122)\/6 = 182\/3\nl So V(X) = 182\/3 – 49 = 35\/3\n\n\fVariance of a Bernoulli Distribution\nExample: What is the variance of random variable X\nwith X(t)=1 if a Bernoulli trial is a success and X(t)=0\notherwise? Assume that the probability of success is p.\n§7.4 also proves that the variance\nof n Bernoulli trials is npq\nSolution:\nl Note that X takes only the values 0 and 1\nl Hence, X(t) = X2(t)\nl V(X) = E(X2) – E(X)2\nl\n= p – p2\nl\n= p(1-p)\nl\n= pq\nThis tells us that the variance of\nANY Bernoulli distribution is pq!\n\n\fVariance of n Bernoulli trials\nExample: A fair die is rolled 5 times. Let X be the\nrandom variable that assigns to an outcome the\nnumber of throws less than 3. What is the variance of\nX?\n\nSolution:\nl n = 5, p = 1\/3, q = 2\/3\nl V(X) = npq = 5 * 1\/3 * 2\/3 ≈ 1.11\n\n\fIn-class exercises\nTop Hat\n\n\fFinal Thoughts\nn Analyzing the expected value of a random variable\nallows us to answer a range of interesting questions\nn The variance of a random variable tells us about the\nspread of values that the random variable can take\n\n\f","label":[[109,123,"Concept"],[182,197,"Concept"],[213,228,"Concept"],[325,340,"Concept"],[753,769,"Concept"],[774,787,"Concept"],[804,816,"Concept"],[822,837,"Concept"],[976,988,"Concept"],[1305,1319,"Concept"],[1325,1340,"Concept"],[1357,1371,"Concept"],[1376,1387,"Concept"],[3997,4013,"Concept"],[4028,4043,"Concept"],[4064,4075,"Concept"],[4076,4092,"Concept"],[4441,4456,"Concept"],[5882,5890,"Concept"],[5977,5985,"Concept"],[6169,6187,"Concept"],[6254,6262,"Concept"],[8687,8701,"Concept"],[8707,8722,"Concept"],[8782,8790,"Concept"]],"Comments":[]}
{"id":23,"segment": ["train_set", "labeled"],  "course": "cs0441", "lec": "lec21", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #21: Probability Theory\n\nBased on materials developed by Dr. Adam Lee\n\n\fNot all events are equally likely to occur…\n\nSporting events\n\nGames of strategy\n\nNature\n\nInvestments\n\n\fWe can model these types of real-life situations\nby relaxing our model of probability\nAs before, let S be our sample space. Unlike before,\nwe will allow S to be either finite or countable.\nWe will require that the following conditions hold:\n1. 0 ≤ p(s) ≤ 1 for each s ∈ S\n2.\n\nNo event can have a negative likelihood of\noccurrence, or more than a 100% chance\nof occurence\n\nIn any given experiment, some event will\noccur\n\nThe function p : S → [0,1] is called a probability\ndistribution\n\n\fRecap our formulas for the probability of\ncombinations of events\nProperty 1: p(E) = 1 – p(E)\n\nS=\n\nE\n\nl Recall that S = E ∪ E for any event E\nl Further, ∑!∈# 𝑝 𝑠 = 1\nl So, p(S) = p(E) + p(E) = 1\nl Thus, p(E) = 1 – p(E)\n\nE\n\nProperty 2: p(E1 ∪ E2) = p(E1) + p(E2) – p(E1 ∩ E2)\nl Recall that 𝑝 𝐸 = ∑!∈$ 𝑝 𝑠\nS=\nl Let x be some outcome in E1 ∪ E2\nl If x is in one of E1 or E2, then p(x) is counted\nonce on the RHS of the equation\nl If x is in both E1 or E2, then p(x) is counted\n1 + 1 – 1 = 1 times on the RHS of the equation\n\nE1\n\nE2\n\n\fA formula for the probability of pairwise disjoint events\nTheorem: If E1, E2, …, En is a sequence of pairwise disjoint events in\na sample space S, then we have:\n$\n\n$\n\n𝑝 # 𝐸! = & 𝑝 𝐸!\n!\"#\n\n!\"#\n\nRecall: E1, E2, …, En are pairwise disjoint iff Ei ∩ Ej = ∅ for 1 ≤ i,j ≤ n\nS=\nE1\n\nE2\n\n…\n\nEn\n\nWe can prove this theorem using mathematical induction!\n\n\fHow can we incorporate prior knowledge?\nSometimes we want to know the probability of some event given\nthat another event has occurred.\n\nExample: A fair coin is flipped three times. The first flip turns\nup tails. Given this information, what is the probability that an\nodd number of tails appear in the three flips?\n\nSolution:\nl Let F = “the first flip of three comes up tails”\nl Let E = “tails comes up an odd number of times in three flips”\nl Since F has happened, S is reduced to {THH, THT, TTH, TTT}\nl We know:\nl p(E) = |E|\/|S|\nl\n= |{THH, TTT}| \/ |{THH, THT, TTH, TTT}|\nl\n= 2\/4\nl\n= 1\/2\n\n\fConditional Probability\nDefinition: Let E and F be events with p(F) > 0. The conditional\nprobability of E given F, denoted p(E | F), is defined as:\n\n𝑝 𝐸∩𝐹\n𝑝 𝐸 𝐹 =\n𝑝 𝐹\n\nIntuition:\nl Think of the event F as reducing the sample space that can be considered\nl The numerator looks at the likelihood of the outcomes in E that overlap\nthose in F\nl The denominator accounts for the reduction in sample size indicated by our\nprior knowledge that F has occurred\n\n\fBit strings\nExample: Suppose that a bit string of length 4 is generated at\nrandom so that each of the 16 possible 4-bit strings is equally\nlikely to occur. What is the probability that it contains at least\ntwo consecutive 0s, given that the first bit in the string is a 0?\n\nSolution:\nl Let E = “A 4-bit string has at least two consecutive zeros”\nl Let F = “The first bit of a 4-bit string is a zero”\nl Want to calculate p(E | F) = p(E ∩ F)\/p(F)\nl E ∩ F = {0000, 0001, 0010, 0011, 0100}\nl So, p(E ∩ F) = 5\/16\nl Since each bit string is equally likely to occur,\np(F) = 8\/16 = 1\/2\nl So p(E | F) = (5\/16)\/(1\/2) = 10\/16 = 5\/8\n\n\fKids\nExample: What is the conditional probability that a family with\ntwo kids has two boys, given that they have at least one boy?\nAssume that each of the possibilities BB, BG, GB, GG is equally\nlikely to occur.\nBoy is older\n\nSolution:\nl Let E = “A family with 2 kids has 2 boys”\nl E = {BB}\nl Let F = “A family with 2 kids has at least 1 boy”\nl F = {BB, BG, GB}\nl E ∩ F = {BB}\nl So p(E | F) = p(E ∩ F)\/p(F)\nl\nl\n\n= (1\/4) \/ (3\/4)\n= 1\/3\n\nGirl is older\n\n\fDoes prior knowledge always help us?\nExample: Suppose a fair coin is flipped twice. Does knowing that\nthe coin comes up tails on the first flip help you predict whether\nthe coin will be tails on the second flip?\n\nSolution:\nl S = {HH, HT, TH, TT}\nl F = “Coin was tails on the first flip” = {TH, TT}\nl E = “Coin is tails on the second flip” = {TT, HT}\nl p(E) = 2\/4 = 1\/2\nl p(E | F) = p(E ∩ F)\/p(F)\nl\n= (1\/4) \/ (2\/4)\nl\n= 1\/2\nl Knowing the first flip does not help you guess the second flip!\n\n\fIndependent Events\nDefinition: We say that events E and F are independent if and\nonly if p(E ∩ F) = p(E)p(F).\n\nRecall: In our last example…\nl S = {HH, HT, TH, TT}\nl F = {TH, TT}\nl E = {HT, TT}\nl E ∩ F = {TT}\n\nSo:\nl p(E ∩ F) = |E ∩ F|\/|S|\nl\n= 1\/4\nl p(E)p(F) = 1\/2 × 1\/2\nl\n= 1\/4\n\nThis checks out!\n\n\fExample: Bit Strings\nExample: Suppose that E is the event that a randomly generated\nbit string of length four begins with a 1, and F is the event that\nthis bit string contains an even number of 1s. Are E and F\nindependent if all 4-bit strings are equally likely to occur?\n\nSolution:\nl By the product rule, |S| = 24 = 16\nl E = {1111, 1110, 1101, 1011, 1100, 1010, 1001, 1000}\nl F = {0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111}\nl So p(E) = p(F) = 8\/16 = 1\/2\nl p(E)p(F) = 1\/4\nl E ∩ F = {1111, 1100, 1010, 1001}\nl p(E ∩ F) = 4\/16 = 1\/4\nl Since p(E ∩ F) = p(E)p(F), E and F are independent events\n\n\fExample: Distribution of kids\nExample: Assume that each of the four ways that a family can\nhave two children are equally likely. Are the events E that a\nfamily with two children has two boys, and F that a family with\ntwo children has at least one boy independent?\n\nSolution:\nl E = {BB}\nl F = {BB, BG, GB}\nl p(E) = 1\/4\nl p(F) = 3\/4\nl p(E)p(F) = 3\/16\nl E ∩ F = {BB}\nl p(E ∩ F) = 1\/4\nl Since 1\/4 ≠ 3\/16, E and F are not independent\n\n\fIf probabilities are independent, we can use the product rule to\ndetermine the probabilities of combinations of events\nExample: What is the probability of flipping heads 4 times in a\nrow using a fair coin?\n\nAnswer: p(H) = 1\/2, so p(HHHH) = (1\/2)4 = 1\/16\n\nExample: What is the probability of rolling the same number 3\ntimes in a row using an unbiased 6-sided die?\n\nAnswer:\nl\nl\nl\nl\n\nFirst roll agrees with itself with probability 1\n2nd roll agrees with first with probability 1\/6\n3rd roll agrees with first two with probability 1\/6\nSo probability of rolling the same number 3 times is 1 × 1\/6 × 1\/6 = 1\/36\n\n\fIn-class exercises\nTop Hat\n\n\fMany experiments only have two outcomes\n\nP(x)\nCoin flips: Heads or tails?\n\nBit strings: 0 or 1?\n\nPredicates: T or F?\n\nThese types of experiments are called Bernoulli trials\nTwo outcomes:\nl Success\nl Failure\n\nProbability p\nProbability q = 1 – p\n\nMany problems can be solved by examining the probability of k\nsuccesses in an experiment consisting of mutuallyindependent Bernoulli trials\n\n\fExample: Coin flips\nExample: A coin is biased so that the probability of heads is 2\/3.\nWhat is the probability that exactly four heads come up when the\ncoin is flipped seven times, assuming that each flip is independent?\n\nSolution:\nl 27 = 128 possible outcomes for seven flips\nl There are C(7,4) ways that heads can be flipped four times\nl Since each flip is independent, the probability of each of these outcomes is\n(2\/3)4(1\/3)3\nl So, the probability of exactly 4 heads occurring in 7 flips of this biased coin\nis C(7,4)(2\/3)4(1\/3)3 = 560\/2187\n\n7 Choose 4 outcomes\nto make heads\n\nProbability of each tails\ncombined using product rule\nProbability of each heads\ncombined using product rule\n\n\fThis general reasoning provides us with a nice formula…\nTheorem: The probability of exactly k successes in n independent\nBernoulli trials, with probability of success p and probability of\nfailure q = 1-p, is C(n,k)pkqn-k.\n\nProof:\nl The outcome of n Bernoulli trials is an n-tuple (t1, t2, …, tn)\nl Each ti is either S (for success) or F (for failure)\nl C(n,k) ways to choose k tis to label S\nl Since each trial is independent, the probability of each outcome with k\nsuccesses and n-k failures is pkqn-k\nl So, the probability of exactly k successes is C(n,k)pkqn-k. ❏\n\nNotation: We denote the probability of k successes in n independent\nBernoulli trials with probability of success p as b(k; n, p).\n\n\fBits (Again)\nExample: Suppose that the probability that a 0 bit is generated is\n0.9, that the probability that a 1 bit is generated is 0.1, and that\nbits are generated independently. What is the probability that\nexactly eight 0 bits are generated when ten random bits are\ngenerated?\n\nSolution:\nl Number of trials\nl Number of successes\nl Probability of success\nl Probability of failure\nl Want to compute b(k; 10, 0.9)\nl\n= C(10, 8)0.980.12\nl\n= 0.1937102445\n\nn = 10\nk=8\np = 0.9\nq = 1 – 0.9 = 0.1\n\n\fMany probability questions are concerned with some\nnumerical value associated with an experiment\n\nNumber of boys in a family\nNumber of 1 bits generated\n\nBeats per minute of a heart\n\nLongevity of a chicken\n\nNumber of “heads” flips\n\n\fWhat is a random variable?\nDefinition: A random variable is a function X from the sample\nspace of an experiment to the set of real numbers R. That is, a\nrandom variable assigns a real number to each possible outcome.\n\nNote: Despite the name, X is not a variable,\nand is not random. X is a function!\n\nExample: Suppose that a coin is flipped three times. Let X(s) be\nthe random variable that equals the numbers of heads that\nappear when s is the outcome. Then X(s) takes the following\nvalues:\nl X(HHH) = 3\nl X(HHT) = X(HTH) = X(THH) = 2\nl X(TTH) = X(THT) = X(HTT) = 1\nl X(TTT) = 0\n\n\fRandom variables and distributions\nDefinition: The distribution of a random variable X on a sample\nspace S is the set of pairs (r, p(X=r)) for all r ∈ X(S), where p(X=r)\nis the probability that X takes the value r.\nNote: A distribution is usually described by specifying p(X=r) for\neach r ∈ X(S)\n\nExample: Assume that our coin flips from the previous slide were\nall equally likely to occur. We then get the following distribution\nfor the random variable X:\nl p(X=0) = 1\/8\nl p(X=1) = 3\/8\nl p(X=2) = 3\/8\nl p(X=3) = 1\/8\n\n\fExample: Rolling dice\nLet X be the sum of the numbers that appear when a pair of fair dice\nis rolled. What are the values of this random variable for the 36\npossible outcomes (i, j) where i and j are the numbers that appear on\nthe first and second die, respectively?\n\nAnswer:\nl X(1,1) = 2\nl X(1,2) = X(2, 1) = 3\nl X(1,3) = X(2,2) = X(3,1) = 4\nl X(1,4) = X(2,3) = X(3,2) = X(4,1) = 5\nl X(1,5) = X(2,4) = X(3,3) = X(4,2) = X(5,1) = 6\nl X(1,6) = X(2,5) = X(3,4) = X(4,3) = X(5,2) = X(6,1) = 7\nl X(2,6) = X(3,5) = X(4,4) = X(5,3) = X(6,2) = 8\nl X(3,6) = X(4,5) = X(5,4) = X(6,3) = 9\nl X(4,6) = X(5,5) = X(6,4) = 10\nl X(5,6) = X(6,5) = 11\nl X(6,6) = 12\n\np(X=2) = 1\/36\np(X=3) = 2\/36\np(X=4) = 3\/36\np(X=5) = 4\/36\np(X=6) = 5\/36\np(X=7) = 6\/36\np(X=8) = 5\/36\np(X=9) = 4\/36\np(X=10) = 3\/36\np(X=11) = 2\/36\np(X=12) = 1\/36\n\n\fSometimes probabilistic reasoning can lead us to some\ninteresting and unexpected conclusions…\nQuestion: How many people need to be in the same room so that\nthe probability of two people sharing the same birthday is greater\nthan 1\/2?\n\nAssumptions:\n1. There are 366 possible birthdays\n2. All birthdays are equally likely to occur\n3. Birthdays are independent\n\nSolution tactic:\nl Find the probability pn that the n people in a room all have\ndifferent birthdays\nl Then compute 1-pn, which is the probability that at least two\npeople share the same birthday\n\n\fLet’s figure this out…\nLet’s assess probabilities as people enter the room\nl Person 1 clearly doesn’t have the same birthday as anyone else in the\nroom\nl P2 has a different birthday than P1 with probability 365\/366\nl P3 has a different birthday than P1 and P2 with probability 364\/366\nl …\n\nIn general, Pj has a different birthday than P1, P2, …, Pj-1 with\nprobability [366-(j-1)]\/366 = (367-j)\/366\nRecall that pn is the probability that n people in the room all have\ndifferent birthdays. Using our above observations, this means:\n\n\fBut we’re interested in 1-pn …\n\nTo check the minimum number of people need in the room to\nensure that pn > 1\/2, we’ll use trial and error:\nl If n = 22, then 1 – pn ≈ 0.475\nl If n = 23, then 1 – pn ≈ 0.506\n\nSo, you need only 23 people in a room to have a better than 50%\nchance that two people share the same birthday!\n\n\fIn-class exercises\nProblem 3: What is the probability that exactly 2 heads occur\nwhen a fair coin is flipped 7 times?\nProblem 4: Consider a game between Alice and Bob. Over time,\nAlice has been shown to win this game (against Bob) 75% of the\ntime. If Alice and Bob play 6 games in a row, what is the\nprobability that Alice wins every game?\nProblem 5: Consider generating a uniformly-random 4-character\nbit string. Also consider R, a random variable that measures the\nlongest run of 1 bits in the generated string. Determine the\ndistribution of R.\n\n\fFinal Thoughts\nn Today we covered\nl Conditional probability\nl Independence\nl Bernoulli trials\nl Random variables\nl Probabilistic analysis\n\nn Next time:\nl Bayes’ Theorem (Section 7.3)\n\n\fThe proof…\nP(n) ≡ 𝑝 ⋃$!\"# 𝐸! = ∑$!\"# 𝑝 𝐸!\nBase case: P(2): Let E1, E2 be disjoint events.\nBy definition, p(E1 ∪ E2) = p(E1) + p(E2) – p(E1 ∩ E2).\nSince E1 ∩ E2 = ∅, p(E1 ∪ E2) = p(E1) + p(E2)\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn Consider E = E1 ∪ E2 ∪ … ∪ Ek ∪ Ek+1\nn Let J = E1 ∪ E2 ∪ … ∪ Ek, so E = J ∪ Ek+1\nn p(E) = p(J ∪ Ek+1)\nn\n= p(J) ∪ p(Ek+1)\nn\n= p(E1 ∪ E2 ∪ … ∪ Ek) ∪ p(Ek+1)\nn\n= p(E1) ∪ p(E2) ∪ … ∪ p(Ek) ∪ p(Ek+1)\n\nby definition of E\nby I.H.\nby definition of J\nby I.H.\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\f","label":[[738,762,"Concept"],[1328,1352,"Concept"],[1396,1420,"Concept"],[1514,1531,"Concept"],[2231,2254,"Concept"],[2308,2331,"Concept"],[4249,4267,"Concept"],[4311,4322,"Concept"],[5596,5607,"Concept"],[6366,6382,"Concept"],[7397,7408,"Concept"],[7409,7425,"Concept"],[8725,8740,"Concept"],[8756,8771,"Concept"],[9296,9312,"Concept"],[9317,9330,"Concept"],[9347,9359,"Concept"],[9365,9380,"Concept"],[9519,9531,"Concept"],[12615,12638,"Concept"],[12641,12653,"Concept"],[12656,12672,"Concept"],[12675,12691,"Concept"],[12694,12716,"Concept"]],"Comments":[]}
