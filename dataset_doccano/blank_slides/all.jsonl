{"id":130,"segment": "unlabeled", "course": "cs0449", "lec": "lec16", "text":"Network\nProgramming\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fA Client-Server Transaction\n• Most network applications are based on the client-server\nmodel:\n• A server process and one or more client processes\n• Server manages some resource\n• Server provides service by manipulating resource for clients\n• Server activated by request from client (vending machine\nanalogy)\n\n4. Client\nhandles\nresponse\n\nClient\nprocess\n\n1. Client sends request\n3. Server sends response\n\nServer\nprocess\n\nResource\n2. Server\nhandles\nrequest\n\nNote: clients and servers are processes running on hosts\n(can be the same or different hosts)\n\n2\n\n\fHardware Organization of a Network Host\nCPU chip\nregister file\nALU\nsystem bus\n\nmemory bus\nmain\nmemory\n\nI\/O\nbridge\n\nMI\n\nExpansion slots\nI\/O bus\nUSB\ncontroller\n\ngraphics\nadapter\n\nmouse keyboard\n\nmonitor\n\ndisk\ncontroller\n\nnetwork\nadapter\n\ndisk\n\nnetwork\n\n3\n\n\fComputer Networks\n• A network is a hierarchical system of boxes and wires organized by\ngeographical proximity\n• SAN* (System Area Network) spans cluster or machine room\n• Switched Ethernet, Quadrics QSW, …\n\n• LAN (Local Area Network) spans a building or campus\n• Ethernet is most prominent example\n\n• WAN (Wide Area Network) spans country or world\n• Typically high-speed point-to-point phone lines\n\n• An internetwork (internet) is an interconnected set of networks\n• The Global IP Internet (uppercase “I”) is the most famous example of an internet\n(lowercase “i”)\n4\n* Not to be confused with a Storage Area Network\n\n\fLogical Structure of an internet\n\nhost\n\nrouter\n\nhost\n\nrouter\nrouter\n\nrouter\n\nrouter\n\nrouter\n\n• Ad hoc interconnection of networks\n• No particular topology\n• Vastly different router & link capacities\n\n• Send packets from source to destination by hopping through\nnetworks\n• Router forms bridge from one network to another\n• Different packets may take different routes\n\n5\n\n\fThe Notion of an internet Protocol\n• How is it possible to send bits across incompatible\nLANs and WANs?\n• Solution: protocol software running on each host and\nrouter\n• Protocol is a set of rules that governs how hosts and\nrouters should cooperate when they transfer data from\nnetwork to network.\n• Smooths out the differences between the different\nnetworks\n\n6\n\n\fWhat Does an internet Protocol Do?\n• Provides a naming scheme\n• An internet protocol defines a uniform format for host\naddresses\n• Each host (and router) is assigned at least one of these\ninternet addresses that uniquely identifies it\n\n• Provides a delivery mechanism\n• An internet protocol defines a standard transfer unit\n(packet)\n• Packet consists of header and payload\n• Header: contains info such as packet size, source and destination\naddresses\n• Payload: contains data bits sent from source host\n7\n\n\fGlobal IP Internet (upper case)\n• Most famous example of an internet\n• Based on the TCP\/IP protocol family\n• IP (Internet Protocol)\n\n• Provides basic naming scheme and unreliable delivery capability\nof packets (datagrams) from host-to-host\n\n• UDP (Unreliable Datagram Protocol)\n\n• Uses IP to provide unreliable datagram delivery from\nprocess-to-process\n\n• TCP (Transmission Control Protocol)\n\n• Uses IP to provide reliable byte streams from process-to-process over\nconnections\n\n• Accessed via a mix of Unix file I\/O and functions from the\nsockets interface\n8\n\n\fHardware and Software Organization\nof an Internet Application\nInternet client host\n\nInternet server host\n\nClient\n\nUser code\n\nServer\n\nTCP\/IP\n\nKernel code\n\nTCP\/IP\n\nNetwork\nadapter\n\nHardware\nand firmware\n\nNetwork\nadapter\n\nSockets interface\n(system calls)\nHardware interface\n(interrupts)\n\nGlobal IP Internet\n\n9\n\n\fA Programmer’s View of the Internet\n1. Hosts are mapped to a set of 32-bit IP addresses\n• 128.2.203.179\n\n2. The set of IP addresses is mapped to a set of\nidentifiers called Internet domain names\n• 136.142.156.73 is mapped to www.cs.pitt.edu\n\n3. A process on one Internet host can communicate\nwith a process on another Internet host over a\nconnection\n10\n\n\f(1) IP Addresses\n• 32-bit IP addresses are stored in an IP address struct\n• IP addresses are always stored in memory in network byte order\n(big-endian byte order)\n• True in general for any integer transferred in a packet header\nfrom one machine to another.\n• E.g., the port number used to identify an Internet connection.\n\/* Internet address structure *\/\nstruct in_addr {\nuint32_t s_addr; \/* network byte order (big-endian) *\/\n};\n\n\fDotted Decimal Notation\n• By convention, each byte in a 32-bit IP address is\nrepresented by its decimal value and separated by a\nperiod\n• IP address: 0x8002C2F2 = 128.2.194.242\n\n• Use getaddrinfo and getnameinfo functions to\nconvert between IP addresses and dotted decimal\nformat.\n\n\f(2) Internet Domain Names\nunnamed root\n\n.edu\n\n.gov\n\nmit\n\npitt\n\nberkeley\n\namazon\n\nSecond-level domain names\n\nsci\n\nwww\n\nThird-level domain names\n\ncs\n\n.com\n\n54.230.48.28\n\nthoth\n\nFirst-level domain names\n\n.net\n\nwww\n\n136.142.23.51 136.142.156.73\n\n\fDomain Naming System (DNS)\n• The Internet maintains a mapping between IP\naddresses and domain names in a huge worldwide\ndistributed database called DNS\n\n• Conceptually, programmers can view the DNS\ndatabase as a collection of millions of host entries.\n• Each host entry defines the mapping between a set of\ndomain names and IP addresses.\n\n\fProperties of DNS Mappings\n• Can explore properties of DNS mappings using\nnslookup\n• (Output edited for brevity)\n\n• Each host has a locally defined domain name\nlocalhost which always maps to the loopback address\n127.0.0.1\nlinux> nslookup localhost\nAddress: 127.0.0.1\n\n• Use hostname to determine real domain name of local\nhost:\nlinux> hostname\nthoth.cs.pitt.edu\n\n\fProperties of DNS Mappings (cont)\n• Simple case: one-to-one mapping between domain name\nand IP address:\nlinux> nslookup thoth.cs.cmu.edu\nAddress: 136.142.23.51\n\n• Multiple domain names mapped to the same IP address:\nlinux> nslookup cs.pitt.edu\nAddress: 136.142.156.73\nlinux> nslookup sci.pitt.edu\nAddress: 136.142.156.73\n\n\fProperties of DNS Mappings (cont)\n• Multiple domain names mapped to multiple IP\naddresses: linux> nslookup www.twitter.com\nAddress: 104.244.42.65\nAddress: 104.244.42.129\nAddress: 104.244.42.193\nAddress: 104.244.42.1\nlinux> nslookup www.twitter.com\nAddress: 104.244.42.129\nAddress: 104.244.42.65\nAddress: 104.244.42.193\nAddress: 104.244.42.1\n\n• Some valid domain names don’t map to any IP address:\nlinux> nslookup bla.cs.pitt.edu\n(No Address given)\n\n\f(3) Internet Connections\n• Clients and servers communicate by sending streams of bytes\nover connections. Each connection is:\n• Point-to-point: connects a pair of processes.\n• Full-duplex: data can flow in both directions at the same time,\n• Reliable: stream of bytes sent by the source is eventually received by\nthe destination in the same order it was sent.\n\n• A socket is an endpoint of a connection\n• Socket address is an IPaddress:port pair\n\n• A port is a 16-bit integer that identifies a process:\n• Ephemeral port: Assigned automatically by client kernel when client\nmakes a connection request.\n• Well-known port: Associated with some service provided by a server\n(e.g., port 80 is associated with Web servers)\n\n\fWell-known Service Names and Ports\n• Popular services have permanently assigned well-known ports and\ncorresponding well-known service names:\n• echo servers: echo 7\n• ftp servers:\nftp 21\n• ssh servers: ssh 22\n• email servers: smtp 25\n• Web servers: http 80\n\n• Mappings between well-known ports and service names is contained\nin the file \/etc\/services on each Linux machine.\n\n\fAnatomy of a Connection\n• A connection is uniquely identified by the socket\naddresses of its endpoints (socket pair)\n• (cliaddr:cliport, servaddr:servport)\nClient socket address\n128.2.194.242:51213\nClient\n\nServer socket address\n208.216.181.15:80\n\nConnection socket pair\n(128.2.194.242:51213, 208.216.181.15:80)\n\nClient host address\n128.2.194.242\n\n51213 is an ephemeral port\nallocated by the kernel\n\nServer\n(port 80)\n\nServer host address\n208.216.181.15\n\n80 is a well-known port\nassociated with Web servers\n\n\fUsing Ports to Identify Services\nServer host 128.2.194.242\nClient host\nClient\n\nService request for\n128.2.194.242:80\n(i.e., the Web server)\n\nWeb server\n(port 80)\nKernel\nEcho server\n(port 7)\n\nClient\n\nService request for\n128.2.194.242:7\n(i.e., the echo server)\n\nWeb server\n(port 80)\nKernel\nEcho server\n(port 7)\n\n\fSockets Interface\n• Set of system-level functions used in conjunction\nwith Unix I\/O to build network applications.\n• Created in the early 80’s as part of the original\nBerkeley distribution of Unix that contained an\nearly version of the Internet protocols.\n• Available on all modern systems\n• Unix variants, Windows, OS X, IOS, Android, ARM\n\n\fSockets\n• What is a socket?\n• To the kernel, a socket is an endpoint of communication\n• To an application, a socket is a file descriptor that lets the\napplication read\/write from\/to the network\n• Remember: All Unix I\/O devices, including networks, are modeled as\nfiles\n\n• Clients and servers communicate with each other by reading from\nand writing to socket descriptors\nClient\nclientfd\n\nServer\nserverfd\n\n• The main distinction between regular file I\/O and socket I\/O is how\nthe application “opens” the socket descriptors\n\n\fSocket Programming Example\n• Echo server and client\n• Server\n• Accepts connection request\n• Repeats back lines as they are typed\n\n• Client\n• Requests connection to server\n• Repeatedly:\n• Read line from terminal\n• Send to server\n• Read reply from server\n• Print line to terminal\n\n\fEcho Server\/Client Session Example\n\nClient\nthoth $ .\/echoclient\nThis line is being echoed\nThis line is being echoed\nThis one is, too\nThis one is, too\n^D\n\n(A)\n(B)\n(C)\n\nServer\nthoth $ .\/echoserver\nServer connected to client.\nserver received 26 bytes\nserver received 17 bytes\n\n(A)\n(B)\n(C)\n\n\f2. Start client\nClient\n\n1. Start server\nServer\n\nlisten\nconnect\n\nConnection\nrequest\n\nClient \/\nServer\nSession\n\naccept\n\nterminal read\nsocket write\n\nsocket read\n\nsocket read\nterminal write\n\nsocket write\n\nclose\n\n4. Disconnect client\n\nEOF\n\nEcho\nServer\n+ Client\nStructure\nAwait connection\nrequest from client\n\n3. Exchange\ndata\n\nsocket read\n\n5. Drop client\nclose\n\n\f2. Start client\nClient\n\n1. Start server\nServer\n\nlisten\nconnect\n\nConnection\nrequest\n\nClient \/\nServer\nSession\n\naccept\n\nfgets\nwrite\n\nread\n\nread\nfputs\n\nwrite\n\nclose\n\n4. Disconnect client\n\nEOF\n\nEcho\nServer\n+ Client\nStructure\nAwait connection\nrequest from client\n\n3. Exchange\ndata\n\nread\n\n5. Drop client\nclose\n\n\fEcho Server: Main Routine\nC (gcc -o echoserver echoserver.c)\n\n\/\/ Listen for connections\nresult = listen(server_fd, 3);\nif (result < 0) {\nperror(\"listen\");\nexit(EXIT_FAILURE);\n}\n\n#include <stdio.h> \/\/ fgets, etc\n#include <sys\/socket.h> \/\/ socket API\n#include <arpa\/inet.h> \/\/ inet functions, htons\n#include <unistd.h> \/\/ read\/close system calls\n#include <stdlib.h> \/\/ exit\n#include <string.h> \/\/ strlen\n#define PORT 9997\n\n\/\/ Listen will return when a connection is requested...\n\/\/ Accept that connection\nint addrlen = sizeof(address);\nint new_socket = accept(server_fd, (struct sockaddr *)&address,\n(socklen_t*)&addrlen);\nif (new_socket < 0) {\nperror(\"accept\");\nexit(EXIT_FAILURE);\n}\n\nint main(void) {\n\/\/ Creating socket file descriptor (using internet protocol)\nint server_fd = socket(AF_INET, SOCK_STREAM, 0);\nif (server_fd == 0) {\nperror(\"socket failed\");\nWe create a socket.\nexit(EXIT_FAILURE);\n}\n\/\/ We want to use the internet protocol\nstruct sockaddr_in address;\naddress.sin_family = AF_INET;\naddress.sin_addr.s_addr = INADDR_ANY;\naddress.sin_port = htons(PORT);\n\nWe wait until somebody\nrequests a connection.\n\nWe accept that connection.\n\nprintf(\"Server connected to client.\\n\");\nchar buffer[1024] = {0};\nint count = 0;\ndo {\n\/\/ Read data (it waits until data is available)\ncount = read(new_socket, buffer, 1024);\nprintf(\"Server received %d bytes.\\n\", count);\nbuffer[count] = '\\0';\nwrite(new_socket, buffer, strlen(buffer));\n} while(count);\n\nWe define what port and\nprotocol we want\n\n\/\/ Bind socket to the port (so it listens to that port)\nint result = bind(server_fd, (struct sockaddr *)&address, sizeof(address));\nif (result < 0) {\nperror(\"bind failed\");\nWe bind ourselves to that port.\nexit(EXIT_FAILURE);\n}\n}\n\nclose(new_socket);\nclose(server_fd);\nreturn 0;\n\nWe wait until data\narrives and read it.\n\nWe write it back out. Stopping our\nloop when nothing was read.\n\nWe close all of our connections.\n\n\f1. Start server\nServer\n\n2. Start client\nClient\n\nlisten\nconnect\n\nConnection\nrequest\n\nClient \/\nServer\nSession\n\naccept\n\nfgets\nwrite\n\nread\n\nread\nfputs\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nEcho\nServer\n+ Client\nStructure\nAwait connection\nrequest from client\n\n3. Exchange\ndata\n\n\fEcho Client: Main Routine\nC (gcc -o echoclient echoclient.c)\n\nint result = connect(sock,\n(struct sockaddr *)&serv_addr,\nsizeof(serv_addr));\nif (result < 0) {\nprintf(\"\\nConnection Failed \\n\");\nreturn -1;\nIf we got here, the server\n}\naccepted our connection!\n\n#include <stdio.h> \/\/ fgets, etc\n#include <sys\/socket.h> \/\/ socket API\n#include <arpa\/inet.h> \/\/ inet functions, htons\n#include <unistd.h> \/\/ read\/close system calls\n#include <string.h> \/\/ strlen\n#define PORT 9997\nint main(void) {\n\/\/ Creating socket file descriptor (using internet protocol)\nint sock = socket(AF_INET, SOCK_STREAM, 0);\nif (sock < 0) {\nWe create a socket.\nprintf(\"\\n Socket creation error \\n\");\nreturn -1;\n}\nUsing the Internet protocol.\nstruct sockaddr_in serv_addr;\nserv_addr.sin_family = AF_INET;\nserv_addr.sin_port = htons(PORT);\n\nActually request a connection.\n\nchar buffer[1024] = {0};\nint count = 0;\nThis loop reads from stdin (user input)\ndo {\nif (fgets(buffer, 1024, stdin) == NULL) {\nbreak; \/\/ Exit when line is empty (CTRL+D is pressed)\n}\nWe write everything to the server!\nwrite(sock, buffer, strlen(buffer));\ncount = read(sock, buffer, 1023);\nbuffer[count] = '\\0';\nAnd print out everything the\nfputs(buffer, stdout);\nserver sends us.\n} while(count);\n\nConnecting to localhost\n\n\/\/ Convert IPv4 and IPv6 addresses from text to binary form\nif (inet_pton(AF_INET, \"127.0.0.1\", &serv_addr.sin_addr)<=0) {\nprintf(\"\\nInvalid address \/ Address not supported \\n\");\nreturn -1;\n}\n\nclose(sock);\nreturn 0;\n}\n\nWe clean up when the loop ends (when\nno user input via CTRL+D)\n\n\fRead and write system calls\n• Same interface used to read\/write files.\n• Because sockets are also files! Neat.\n#include <unistd.h>\nssize_t read(int fd, void *usrbuf, size_t n);\nssize_t write(int fd, void *usrbuf, size_t n);\nReturn: number of bytes transferred if OK, 0 on EOF (read only), -1 on error\n\n• read returns a count of 0 only if it encounters EOF\n• So, it is useful to notice if the other machine disconnected.\n\n• Calls to read and write can be interleaved arbitrarily on the\nsame file descriptor (socket, file on disk, etc)\n\n\fSockets Interface\n\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nsocket\n\nsocket\n\nlisten\nconnect\n\nbind\n\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fconnect\/accept Illustrated\nlistenfd(3)\nClient\n\nServer\n\nclientfd\n\nConnection\nrequest\nClient\n\nlistenfd(3)\nServer\n\n1. Server blocks in accept,\nwaiting for connection request\non listening descriptor\nlistenfd\n\n2. Client makes connection request by\ncalling and blocking in connect\n\nclientfd\n\nlistenfd(3)\nClient\nclientfd\n\nServer\nconnfd(4)\n\n3. Server returns connfd from accept.\nClient returns from connect.\nConnection is now established between\nclientfd and connfd\n\n\fConnected vs. Listening Descriptors\n• Listening descriptor\n• End point for client connection requests\n• Created once and exists for lifetime of the server\n\n• Connected descriptor\n• End point of the connection between client and server\n• A new descriptor is created each time the server accepts a\nconnection request from a client\n• Exists only as long as it takes to service client\n\n• Why the distinction?\n• Allows for concurrent servers that can communicate over many\nclient connections simultaneously\n• E.g., Each time we receive a new request, we fork a child to handle the request\n\n\fTesting Servers Using telnet\n• The telnet program is invaluable for testing\nservers that transmit ASCII strings over Internet\nconnections\n• Our simple echo server\n• Web servers\n• Mail servers\n\n• Usage:\n• linux> telnet <host> <portnumber>\n• Creates a connection with a server running on <host>\nand listening on port <portnumber>\n\n\fTesting the Echo Server With telnet\noccam.dev $ .\/echoserver 10001\nServer connected to client.\nServer received 11 bytes\nServer received 8 bytes\n\nthoth $ telnet occam.dev 10001\nTrying 142.4.212.185...\nConnected to occam.dev (142.4.212.185).\nEscape character is '^]'.\nHi there!\nHi there!\nHowdy!\nHowdy!\n^]\nß This means CTRL+]\ntelnet> quit\nConnection closed.\nthoth $\n\n\fWeb Server Basics\n• Clients and servers\ncommunicate using the\nHyperText Transfer Protocol\n(HTTP)\n• Client and server establish TCP\nconnection\n• Client requests content\n• Server responds with requested\ncontent\n• Client and server close\nconnection (eventually)\n\n• Current version is HTTP\/1.1\n\nWeb\nclient\n(browser)\n\nHTTP request\nWeb\nserver\nHTTP response\n(content)\n\nHTTP\n\nWeb content\n\nTCP\n\nStreams\n\nIP\n\nDatagrams\n\n• RFC 2616, June, 1999.\nhttp:\/\/www.w3.org\/Protocols\/rfc2616\/rfc2616.html\n\n\fWeb Content\n• Web servers return content to clients\n• content: a sequence of bytes with an associated MIME (Multipurpose\nInternet Mail Extensions) type\n\n• Example MIME types\n• text\/html\n• text\/plain\n• image\/gif\n• image\/png\n• image\/jpeg\n\nHTML document\nUnformatted text\nBinary image encoded in GIF format\nBinar image encoded in PNG format\nBinary image encoded in JPEG format\n\nYou can find the complete list of MIME types at:\nhttp:\/\/www.iana.org\/assignments\/media-types\/media-types.xhtml\n\n\fStatic and Dynamic Content\n• The content returned in HTTP responses can be either static or dynamic\n• Static content: content stored in files and retrieved in response to an HTTP request\n• Examples: HTML files, images, audio clips, Javascript programs\n• Request identifies which content file\n\n• Dynamic content: content produced on-the-fly in response to an HTTP request\n• Example: content produced by a program executed by the server on behalf of the client\n• Request identifies file containing executable code\n\n• Bottom line: Web content is associated with a file that is managed by the\nserver\n\n\fURLs and how clients and servers use them\n• Unique name for a file: URL (Universal Resource Locator)\n• Example URL:\nhttp:\/\/www.google.edu:80\/index.html\n• Clients use prefix (http:\/\/www.google.edu:80) to\ninfer:\n• What kind (protocol) of server to contact (HTTP)\n• Where the server is (www.google.com)\n• What port it is listening on (80)\n\n• Servers use suffix (\/index.html) to:\n\n• Determine if request is for static or dynamic content.\n\n• No hard and fast rules for this\n• One convention: executables reside in cgi-bin directory\n\n• Find file on file system\n\n• Initial “\/” in suffix denotes home directory for requested content.\n• Minimal suffix is “\/”, which server expands to configured default\nfilename (usually, index.html)\n\n\fHTTP Requests\n• HTTP request is a request line, followed by zero or more\nrequest headers\n• Request line: <method> <uri> <version>\n• <method> is one of GET, POST, OPTIONS, HEAD, PUT,\nDELETE, or TRACE\n• <uri> is typically URL for proxies, URL suffix for servers\n• A URL is a type of URI (Uniform Resource Identifier)\n• See http:\/\/www.ietf.org\/rfc\/rfc2396.txt\n\n• <version> is HTTP version of request (HTTP\/1.0 or HTTP\/1.1)\n\n• Request headers: <header name>: <header data>\n• Provide additional information to the server\n\n\fHTTP Responses\n• HTTP response is a response line followed by zero or more response headers, possibly\nfollowed by content, with blank line (“\\r\\n”) separating headers from content.\n• Response line:\n<version> <status code> <status msg>\n• <version> is HTTP version of the response\n• <status code> is numeric status\n• <status msg> is corresponding English text\n•\n\n200\n\nOK\n\nRequest was handled without error\n\n•\n\n301\n\nMoved\n\nProvide alternate URL\n\n•\n\n404\n\nNot found Server couldn’t find the file\n\n• Response headers: <header name>: <header data>\n• Provide additional information about response\n• Content-Type: MIME type of content in response body\n• Content-Length: Length of content in response body\n\n\fExample HTTP Transaction\nwhaleshark> telnet www.cmu.edu 80\nClient: open connection to server\nTrying 128.2.42.52...\nTelnet prints 3 lines to terminal\nConnected to WWW-CMU-PROD-VIP.ANDREW.cmu.edu.\nEscape character is '^]'.\nGET \/ HTTP\/1.1\nClient: request line\nHost: www.cmu.edu\nClient: required HTTP\/1.1 header\nClient: empty line terminates headers\nHTTP\/1.1 301 Moved Permanently\nServer: response line\nDate: Wed, 05 Nov 2014 17:05:11 GMT\nServer: followed by 5 response headers\nServer: Apache\/1.3.42 (Unix)\nServer: this is an Apache server\nLocation: http:\/\/www.cmu.edu\/index.shtml Server: page has moved here\nTransfer-Encoding: chunked\nServer: response body will be chunked\nContent-Type: text\/html; charset=...\nServer: expect HTML in response body\nServer: empty line terminates headers\n15c\nServer: first line in response body\n<HTML><HEAD>\nServer: start of HTML content\n…\n<\/BODY><\/HTML>\nServer: end of HTML content\n0\nServer: last line in response body\nConnection closed by foreign host.\nServer: closes connection\n\nHTTP standard requires that each text line end with “\\r\\n”\n¢ Blank line (“\\r\\n”) terminates request and response headers\n¢\n\n\fExample HTTP Transaction, Take 2\nwhaleshark> telnet www.cmu.edu 80\nClient: open connection to server\nTrying 128.2.42.52...\nTelnet prints 3 lines to terminal\nConnected to WWW-CMU-PROD-VIP.ANDREW.cmu.edu.\nEscape character is '^]'.\nGET \/index.shtml HTTP\/1.1\nClient: request line\nHost: www.cmu.edu\nClient: required HTTP\/1.1 header\nClient: empty line terminates headers\nHTTP\/1.1 200 OK\nServer: response line\nDate: Wed, 05 Nov 2014 17:37:26 GMT\nServer: followed by 4 response headers\nServer: Apache\/1.3.42 (Unix)\nTransfer-Encoding: chunked\nContent-Type: text\/html; charset=...\nServer: empty line terminates headers\n1000\nServer: begin response body\n<html ..>\nServer: first line of HTML content\n…\n<\/html>\n0\nServer: end response body\nConnection closed by foreign host.\nServer: close connection\n\n\fProxies\n• A proxy is an intermediary between a client and an origin\nserver\n• To the client, the proxy acts like a server\n• To the server, the proxy acts like a client\n\n1. Client request\nClient\n\n2. Proxy request\nOrigin\nServer\n\nProxy\n\n4. Proxy response\n\n3. Server response\n\n• This is what you will be implementing in Proxy Lab\n\n\fWhy Proxies?\n• Can perform useful functions as requests and responses\npass by\n• Examples: Caching, logging, anonymization, filtering,\ntranscoding\nClient\nA\n\nRequest foo.html\nProxy\ncache\n\nRequest foo.html\nClient\nB\n\nRequest foo.html\n\nfoo.html\n\nfoo.html\n\nFast inexpensive local network\n\nfoo.html\n\nOrigin\nServer\n\nSlower more expensive global network\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nSA list\n\nSA list\n\nsocket\n\nsocket\n\nopen_listenfd\nopen_clientfd\n\nbind\n\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\naccept\n\nrio_writen\n\nrio_readlineb\n\nrio_readlineb\n\nrio_writen\n\nclose\n\nEOF\n\nrio_readlineb\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Interface: socket\n• Clients and servers use the socket function to create a\nsocket descriptor:\nint socket(int domain, int type, int protocol)\n\n• Example:\nint clientfd = socket(AF_INET, SOCK_STREAM, 0);\n\nIndicates that we are using\n32-bit IPV4 addresses\n\nIndicates that the socket\nwill be the end point of a\nconnection\n\nProtocol specific! Best practice is to use getaddrinfo to\ngenerate the parameters automatically, so that code is\nprotocol independent.\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nSA list\n\nSA list\n\nsocket\n\nsocket\n\nopen_listenfd\n\nlistenfd\n\nclientfd\nopen_clientfd\n\nbind\n\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Interface: bind\n• A server uses bind to ask the kernel to associate the server’s\nsocket address with a socket descriptor:\nint bind(int sockfd, SA *addr, socklen_t addrlen);\n\nRecall: typedef struct sockaddr SA;\n\n• Process can read bytes that arrive on the connection whose\nendpoint is addr by reading from descriptor sockfd\n• Similarly, writes to sockfd are transferred along connection\nwhose endpoint is addr\nBest practice is to use getaddrinfo to supply the arguments\naddr and addrlen.\n\n\fSockets Interface\n\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nSA list\n\nSA list\n\nsocket\n\nsocket\n\nopen_listenfd\n\nlistenfd\n\nclientfd\nopen_clientfd\n\nbind\n\nlistenfd <-> SA\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Interface: listen\n• By default, kernel assumes that descriptor from socket\nfunction is an active socket that will be on the client\nend of a connection.\n• A server calls the listen function to tell the kernel that a\ndescriptor will be used by a server rather than a client:\nint listen(int sockfd, int backlog);\n\n• Converts sockfd from an active socket to a listening\nsocket that can accept connection requests from\nclients.\n• backlog is a hint about the number of outstanding\nconnection requests that the kernel should queue up\nbefore starting to refuse requests.\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nSA list\n\nSA list\n\nsocket\n\nsocket\n\nopen_listenfd\n\nlistenfd\n\nclientfd\nopen_clientfd\n\nbind\n\nlistenfd <-> SA\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\nlistening listenfd\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nrio_readlineb\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Interface: accept\n• Servers wait for connection requests from clients by\ncalling accept:\nint accept(int listenfd, SA *addr, int *addrlen);\n\n• Waits for connection request to arrive on the\nconnection bound to listenfd, then fills in\nclient’s socket address in addr and size of the\nsocket address in addrlen.\n• Returns a connected descriptor that can be used to\ncommunicate with the client via Unix I\/O routines.\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nSA list\n\nSA list\n\nsocket\n\nsocket\n\nopen_listenfd\n\nlistenfd\n\nclientfd\nopen_clientfd\n\nbind\n\nlistenfd <-> SA\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\nlistening listenfd\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Interface: connect\n• A client establishes a connection with a server by calling\nconnect:\nint connect(int clientfd, SA *addr, socklen_t addrlen);\n\n• Attempts to establish a connection with server at socket\naddress addr\n• If successful, then clientfd is now ready for reading and writing.\n• Resulting connection is characterized by socket pair\n(x:y, addr.sin_addr:addr.sin_port)\n• x is client address\n• y is ephemeral port that uniquely identifies client process on\nclient host\n\nBest practice is to use getaddrinfo to supply the\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nSA list\n\nSA list\n\nsocket\n\nsocket\n\nopen_listenfd\n\nlistenfd\n\nclientfd\nopen_clientfd\n\nbind\n\nlistenfd <-> SA\nlisten\n\nconnect\n\nConnection\nrequest\n\nlistening listenfd\naccept\n\nconnected (to SA) clientfd\nClient \/\nServer\nSession\n\nconnected connfd\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nrio_readlineb\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nsocket\n\nsocket\n\nopen_listenfd\nopen_clientfd\n\nbind\n\nlisten\n\nConnection\nrequest\n\nClient \/\nServer\nSession\n\nconnect\n\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Helper: open_clientfd\n• Establish a connection with a server\nint open_clientfd(char *hostname, char *port) {\nint clientfd;\nstruct addrinfo hints, *listp, *p;\n\/* Get a list of potential server addresses *\/\nmemset(&hints, 0, sizeof(struct addrinfo));\nhints.ai_socktype = SOCK_STREAM; \/* Open a connection *\/\nhints.ai_flags = AI_NUMERICSERV; \/* …using numeric port arg. *\/\nhints.ai_flags |= AI_ADDRCONFIG; \/* Recommended for connections *\/\ngetaddrinfo(hostname, port, &hints, &listp);\n\ncsapp.c\n\n\fgetaddrinfo Linked List\nresult\n\naddrinfo structs\nai_canonname\nai_addr\nai_next\n\nSocket address structs\n\nNULL\nai_addr\nai_next\n\nNULL\nai_addr\nNULL\n\n• Clients: walk this list, trying each socket address in turn,\nuntil the calls to socket and connect succeed.\n• Servers: walk the list until calls to socket and bind\nsucceed.\n\n\fSockets Helper: open_clientfd (cont)\n\/* Walk the list for one that we can successfully connect to *\/\nfor (p = listp; p; p = p->ai_next) {\n\/* Create a socket descriptor *\/\nif ((clientfd = socket(p->ai_family, p->ai_socktype,\np->ai_protocol)) < 0)\ncontinue; \/* Socket failed, try the next *\/\n\/* Connect to the server *\/\nif (connect(clientfd, p->ai_addr, p->ai_addrlen) != -1)\nbreak; \/* Success *\/\nclose(clientfd); \/* Connect failed, try another *\/\n}\n\/* Clean up *\/\nfreeaddrinfo(listp);\nif (!p) \/* All connects failed *\/\nreturn -1;\nelse\n\/* The last connect succeeded *\/\nreturn clientfd;\n}\n\ncsapp.c\n\n\fSockets Interface\nClient\n\nServer\n\ngetaddrinfo\n\ngetaddrinfo\n\nsocket\n\nsocket\n\nopen_listenfd\nopen_clientfd\n\nbind\n\nlisten\n\nconnect\n\nClient \/\nServer\nSession\n\nConnection\nrequest\n\naccept\n\nwrite\n\nread\n\nread\n\nwrite\n\nclose\n\nEOF\n\nread\n\nclose\n\nAwait connection\nrequest from\nnext client\n\n\fSockets Helper: open_listenfd\n• Create a listening descriptor that can be used to\naccept connection requests from clients.\nint open_listenfd(char *port)\n{\nstruct addrinfo hints, *listp, *p;\nint listenfd, optval=1;\n\/* Get a list of potential server addresses *\/\nmemset(&hints, 0, sizeof(struct addrinfo));\nhints.ai_socktype = SOCK_STREAM;\n\/* Accept connect. *\/\nhints.ai_flags = AI_PASSIVE | AI_ADDRCONFIG; \/* …on any IP addr *\/\nhints.ai_flags |= AI_NUMERICSERV;\n\/* …using port no. *\/\ngetaddrinfo(NULL, port, &hints, &listp);\n\ncsapp.c\n\n\fSockets Helper: open_listenfd (cont)\n\/* Walk the list for one that we can bind to *\/\nfor (p = listp; p; p = p->ai_next) {\n\/* Create a socket descriptor *\/\nif ((listenfd = socket(p->ai_family, p->ai_socktype,\np->ai_protocol)) < 0)\ncontinue; \/* Socket failed, try the next *\/\n\/* Eliminates \"Address already in use\" error from bind *\/\nsetsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR,\n(const void *)&optval , sizeof(int));\n\/* Bind the descriptor to the address *\/\nif (bind(listenfd, p->ai_addr, p->ai_addrlen) == 0)\nbreak; \/* Success *\/\nclose(listenfd); \/* Bind failed, try the next *\/\n}\n\ncsapp.c\n\n\fSockets Helper: open_listenfd (cont)\n\/* Clean up *\/\nfreeaddrinfo(listp);\nif (!p) \/* No address worked *\/\nreturn -1;\n\/* Make it a listening socket ready to accept conn. requests *\/\nif (listen(listenfd, LISTENQ) < 0) {\nclose(listenfd);\nreturn -1;\n}\nreturn listenfd;\n\ncsapp.c\n\n}\n\n¢\n\nKey point: open_clientfd and open_listenfd are\nboth independent of any particular version of IP.\n\n\fCase Study\nTiny Web Server\n\n\fTiny Web Server\n• Tiny Web server described in textbook (CS:APP)\n• Tiny is a sequential Web server\n• Serves static and dynamic content to real browsers\n• text files, HTML files, GIF, PNG, and JPEG images\n\n• 239 lines of commented C code\n• Not as complete or robust as a real Web server\n• You can break it with poorly-formed HTTP\nrequests (e.g., terminate lines with “\\n” instead\nof “\\r\\n”)\n\n\fTiny Operation\n• Accept connection from client\n• Read request from client (via connected socket)\n• Split into <method> <uri> <version>\n• If method not GET, then return error\n\n• If URI contains “cgi-bin” then serve dynamic content\n• (Would do wrong thing if had file “abcgi-bingo.html”)\n• Fork process to execute program\n\n• Otherwise serve static content\n• Copy file to output\n\n\fTiny Serving Static Content\nvoid serve_static(int fd, char *filename, int filesize)\n{\nint srcfd;\nchar *srcp, filetype[MAXLINE], buf[MAXBUF];\n\/* Send response headers to client *\/\nget_filetype(filename, filetype);\nsprintf(buf, \"HTTP\/1.0 200 OK\\r\\n\");\nsprintf(buf, \"%sServer: Tiny Web Server\\r\\n\", buf);\nsprintf(buf, \"%sConnection: close\\r\\n\", buf);\nsprintf(buf, \"%sContent-length: %d\\r\\n\", buf, filesize);\nsprintf(buf, \"%sContent-type: %s\\r\\n\\r\\n\", buf, filetype);\nwrite(fd, buf, strlen(buf));\n\/* Send response body to client *\/\nsrcfd = open(filename, O_RDONLY, 0);\nsrcp = mmap(0, filesize, PROT_READ, MAP_PRIVATE, srcfd, 0);\nclose(srcfd);\nwrite(fd, srcp, filesize);\nmunmap(srcp, filesize);\n}\n\ntiny.c\n\n\fServing Dynamic Content\n• Client sends request to\nserver\n\nGET \/cgi-bin\/env.pl HTTP\/1.1\nClient\n\n• If request URI contains the\nstring “\/cgi-bin”, the\nTiny server assumes that\nthe request is for dynamic\ncontent\n\nServer\n\n\fServing Dynamic Content (cont)\n\n• The server creates a child\nprocess and runs the\nprogram identified by the\nURI in that process\n\nClient\n\nServer\nfork\/exec\nenv.pl\n\n\fServing Dynamic Content (cont)\n\n• The child runs and\ngenerates the dynamic\ncontent\n• The server captures the\ncontent of the child and\nforwards it without\nmodification to the client\n\nClient\n\nContent\n\nServer\nContent\nenv.pl\n\n\fIssues in Serving Dynamic Content\n• How does the client pass program\narguments to the server?\nClient\n• How does the server pass these\narguments to the child?\n• How does the server pass other\ninfo relevant to the request to the\nchild?\n• How does the server capture the\ncontent produced by the child?\n• These issues are addressed by the\nCommon Gateway Interface (CGI)\nspecification.\n\nRequest\nContent\nContent\n\nServer\nCreate\nenv.pl\n\n\fCGI\n• Because the children are written according to the CGI spec, they are often\ncalled CGI programs.\n• However, CGI really defines a simple standard for transferring information\nbetween the client (browser), the server, and the child process.\n• CGI is the original standard for generating dynamic content. Has been\nlargely replaced by other, faster techniques:\n• E.g., fastCGI, Apache modules, Java servlets, Rails controllers\n• Avoid having to create process on the fly (expensive and slow).\n\n\fThe add.com Experience\nhost\n\nport\n\nCGI program\narguments\n\nOutput page\n\n\fServing Dynamic Content With GET\n• Question: How does the client pass arguments to the\nserver?\n• Answer: The arguments are appended to the URI\n• Can be encoded directly in a URL typed to a browser\nor a URL in an HTML link\n• http:\/\/add.com\/cgi-bin\/adder?15213&18213\n• adder is the CGI program on the server that will do the\naddition.\n• argument list starts with “?”\n• arguments separated by “&”\n• spaces represented by “+” or “%20”\n\n\fServing Dynamic Content With GET\n• URL suffix:\n• cgi-bin\/adder?15213&18213\n\n• Result displayed on browser:\nWelcome to add.com: THE Internet addition portal.\nThe answer is: 15213 + 18213 = 33426\nThanks for visiting!\n\n\fServing Dynamic Content With GET\n• Question: How does the server pass these\narguments to the child?\n• Answer: In environment variable QUERY_STRING\n• A single string containing everything after the “?”\n• For add: QUERY_STRING = “15213&18213”\n\/* Extract the two arguments *\/\nif ((buf = getenv(\"QUERY_STRING\")) != NULL) {\np = strchr(buf, '&');\n*p = '\\0';\nstrcpy(arg1, buf);\nstrcpy(arg2, p+1);\nn1 = atoi(arg1);\nn2 = atoi(arg2);\n}\nadder.c\n\n\fServing Dynamic Content with GET\n• Question: How does the server capture the content produced by the child?\n• Answer: The child generates its output on stdout. Server uses dup2 to\nredirect stdout to its connected socket.\nvoid serve_dynamic(int fd, char *filename, char *cgiargs)\n{\nchar buf[MAXLINE], *emptylist[] = { NULL };\n\/* Return first part of HTTP response *\/\nsprintf(buf, \"HTTP\/1.0 200 OK\\r\\n\");\nRio_writen(fd, buf, strlen(buf));\nsprintf(buf, \"Server: Tiny Web Server\\r\\n\");\nRio_writen(fd, buf, strlen(buf));\nif (Fork() == 0) { \/* Child *\/\n\/* Real server would set all CGI vars here *\/\nsetenv(\"QUERY_STRING\", cgiargs, 1);\nDup2(fd, STDOUT_FILENO);\n\/* Redirect stdout to client *\/\nExecve(filename, emptylist, environ); \/* Run CGI program *\/\n}\nWait(NULL); \/* Parent waits for and reaps child *\/\n}\n\ntiny.c\n\n\fServing Dynamic Content with GET\n¢\n\nNotice that only the CGI child process knows the content\ntype and length, so it must generate those headers.\n\n\/* Make the response body *\/\nsprintf(content, \"Welcome to add.com: \");\nsprintf(content, \"%sTHE Internet addition portal.\\r\\n<p>\", content);\nsprintf(content, \"%sThe answer is: %d + %d = %d\\r\\n<p>\",\ncontent, n1, n2, n1 + n2);\nsprintf(content, \"%sThanks for visiting!\\r\\n\", content);\n\/* Generate the HTTP response *\/\nprintf(\"Content-length: %d\\r\\n\", (int)strlen(content));\nprintf(\"Content-type: text\/html\\r\\n\\r\\n\");\nprintf(\"%s\", content);\nfflush(stdout);\nexit(0);\n\nadder.c\n\n\fServing Dynamic Content With GET\nbash:makoshark> telnet whaleshark.ics.cs.cmu.edu 15213\nTrying 128.2.210.175...\nConnected to whaleshark.ics.cs.cmu.edu (128.2.210.175).\nEscape character is '^]'.\nGET \/cgi-bin\/adder?15213&18213 HTTP\/1.0\n\nHTTP request sent by client\n\nHTTP\/1.0 200 OK\nServer: Tiny Web Server\nConnection: close\nContent-length: 117\nContent-type: text\/html\n\nHTTP response generated\nby the server\n\nHTTP response generated\nWelcome to add.com: THE Internet addition portal. by the CGI program\n<p>The answer is: 15213 + 18213 = 33426\n<p>Thanks for visiting!\nConnection closed by foreign host.\nbash:makoshark>\n\n\fFor More Information\n• W. Richard Stevens et. al. “Unix Network\nProgramming: The Sockets Networking API”,\nVolume 1, Third Edition, Prentice Hall, 2003\n• THE network programming bible.\n\n• Michael Kerrisk, “The Linux Programming\nInterface”, No Starch Press, 2010\n• THE Linux programming bible.\n\n• Code examples\n• csapp.{.c,h}, hostinfo.c, echoclient.c, echoserveri.c,\ntiny.c, adder.c\n• You can use any of this code in your assignments.\n\n\fBONUS\nSLIDES\nThe following slides are for those curious.\nYou will NOT be expected to know this material.\n\n\fLowest Level: Ethernet Segment\nhost\n100 Mb\/s\n\nhost\nhub\n\nhost\n100 Mb\/s\nport\n\n• Ethernet segment consists of a collection of hosts\nconnected by wires (twisted pairs) to a hub\n• Spans room or floor in a building\n• Operation\n• Each Ethernet adapter has a unique 48-bit address (MAC address)\n\n• E.g., 00:16:ea:e3:54:e6\n\n• Hosts send bits to any other host in chunks called frames\n• Hub slavishly copies each bit from each port to every other port\n• Every host sees every bit\n[Note: Hubs are obsolete. Bridges (switches, routers) became cheap enough to replace them]\n\n\fNext Level: Bridged Ethernet Segment\nA\nhost\n\nhost\nhub\n\nB\nhost\n100 Mb\/s\n\nhost\nX\nbridge\n\n100 Mb\/s\n\n1 Gb\/s\nhub\nhost\n\nhost\n\n100 Mb\/s\n\nbridge\n\n100 Mb\/s\n\nY\nhost\n\nhost\n\nhub\n\nhost\n\nhost\n\nhub\nhost\n\nhost\nC\n\n• Spans building or campus\n• Bridges cleverly learn which hosts are reachable from which\nports and then selectively copy frames from port to port\n\n\fConceptual View of LANs\n• For simplicity, hubs, bridges, and wires are often shown as a collection of\nhosts attached to a single wire:\n\nhost\n\nhost\n\n...\n\nhost\n\n\fNext Level: internets\n• Multiple incompatible LANs can be physically connected by\nspecialized computers called routers\n• The connected networks are called an internet (lower case)\n\nhost\n\nhost ...\n\nhost\n\nhost ...\n\nhost\n\nLAN 1\n\nhost\nLAN 2\n\nrouter\n\nWAN\n\nrouter\n\nWAN\n\nrouter\n\nLAN 1 and LAN 2 might be completely different, totally incompatible\n(e.g., Ethernet, Fibre Channel, 802.11*, T1-links, DSL, …)\n\n\fTransferring internet Data Via Encapsulation\nLAN1\n\n(1)\n\nclient\n\nserver\n\nprotocol\nsoftware\n\ndata\n\nPH\n\ndata\n\nPH\n\nLAN1\nadapter\n\nPH: internet packet header\nFH: LAN frame header\n\nLAN1\nadapter\ndata\n\n(8)\n\ndata\n\n(7)\n\ndata\n\nPH\n\nFH2\n\n(6)\n\ndata\n\nPH\n\nFH2\n\nFH2\n\n(5)\n\nLAN2\nadapter\n\nRouter\n\nFH1\n\n(4)\n\nLAN2\n\nprotocol\nsoftware\n\nFH1\n\nLAN1 frame\n\n(3)\n\nHost B\n\ndata\n\ninternet packet\n(2)\n\nHost A\n\nPH\n\nLAN2\nadapter\n\nFH1\n\nLAN2 frame\ndata\n\nprotocol\nsoftware\n\nPH\n\n\fAside: IPv4 and IPv6\n• The original Internet Protocol, with its 32-bit addresses,\nis known as Internet Protocol Version 4 (IPv4)\n• 1996: Internet Engineering Task Force (IETF) introduced\nInternet Protocol Version 6 (IPv6) with 128-bit\naddresses\n• Intended as the successor to IPv4\n\n• Majority of Internet traffic still carried by IPv4\nIPv6 traffic at Google\n\n• We will focus on IPv4, but will show you how to write\nnetworking code that is protocol-independent.\n\n\fSocket Address Structures\n• Generic socket address:\n• For address arguments to connect, bind, and accept\n• Necessary only because C did not have generic (void *) pointers when the\nsockets interface was designed\n• For casting convenience, we adopt the Stevens convention:\ntypedef struct sockaddr SA;\nstruct sockaddr {\nuint16_t sa_family;\nchar\nsa_data[14];\n};\n\n\/* Protocol family *\/\n\/* Address data *\/\n\nsa_family\n\nFamily Specific\n\n\fSocket Address Structures\n• Internet (IPv4) specific socket address:\n• Must cast (struct sockaddr_in *) to (struct sockaddr *) for\nfunctions that take socket address arguments.\nstruct sockaddr_in {\nuint16_t\nsin_family; \/* Protocol family (always AF_INET) *\/\nuint16_t\nsin_port;\n\/* Port num in network byte order *\/\nstruct in_addr sin_addr;\n\/* IP addr in network byte order *\/\nunsigned char\nsin_zero[8]; \/* Pad to sizeof(struct sockaddr) *\/\n};\n\nsin_port\nAF_INET\n\nsin_addr\n\n0\n\n0\n\nsa_family\nsin_family\n\nFamily Specific\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n\fHost and Service Conversion: getaddrinfo\n• getaddrinfo is the modern way to convert string representations of\nhostnames, host addresses, ports, and service names to socket address\nstructures.\n• Replaces obsolete gethostbyname and getservbyname funcs.\n\n• Advantages:\n\n• Reentrant (can be safely used by threaded programs).\n• Allows us to write portable protocol-independent code\n• Works with both IPv4 and IPv6\n\n• Disadvantages\n\n• Somewhat complex\n• Fortunately, a small number of usage patterns suffice in most cases.\n\n\fHost and Service Conversion: getaddrinfo\nint getaddrinfo(const char *host,\n\/* Hostname or address *\/\nconst char *service,\n\/* Port or service name *\/\nconst struct addrinfo *hints,\/* Input parameters *\/\nstruct addrinfo **result);\n\/* Output linked list *\/\nvoid freeaddrinfo(struct addrinfo *result);\n\n\/* Free linked list *\/\n\nconst char *gai_strerror(int errcode);\n\n\/* Return error msg *\/\n\n• Given host and service, getaddrinfo returns result that points to a linked list\nof addrinfo structs, each of which points to a corresponding socket address struct,\nand which contains arguments for the sockets interface functions.\n• Helper functions:\n• freeadderinfo frees the entire linked list.\n• gai_strerror converts error code to an error message.\n\n\fLinked List Returned by getaddrinfo\nresult\n\naddrinfo structs\nai_canonname\nai_addr\nai_next\n\nSocket address structs\n\nNULL\nai_addr\nai_next\n\nNULL\nai_addr\nNULL\n\n• Clients: walk this list, trying each socket address in turn, until the calls\nto socket and connect succeed.\n• Servers: walk the list until calls to socket and bind succeed.\n\n\faddrinfo Struct\nstruct addrinfo {\nint\nai_flags;\n\/* Hints argument flags *\/\nint\nai_family;\n\/* First arg to socket function *\/\nint\nai_socktype; \/* Second arg to socket function *\/\nint\nai_protocol; \/* Third arg to socket function *\/\nchar\n*ai_canonname; \/* Canonical host name *\/\nsize_t\nai_addrlen;\n\/* Size of ai_addr struct *\/\nstruct sockaddr *ai_addr;\n\/* Ptr to socket address structure *\/\nstruct addrinfo *ai_next;\n\/* Ptr to next item in linked list *\/\n};\n\n• Each addrinfo struct returned by getaddrinfo contains\narguments that can be passed directly to socket\nfunction.\n• Also points to a socket address struct that can be\npassed directly to connect and bind functions.\n\n\fHost and Service Conversion: getnameinfo\n• getnameinfo is the inverse of getaddrinfo, converting a socket address\nto the corresponding host and service.\n• Replaces obsolete gethostbyaddr and getservbyport funcs.\n• Reentrant and protocol independent.\n\nint getnameinfo(const SA *sa, socklen_t salen, \/* In: socket addr *\/\nchar *host, size_t hostlen,\n\/* Out: host *\/\nchar *serv, size_t servlen,\n\/* Out: service *\/\nint flags);\n\/* optional flags *\/\n\n\fConversion Example (writing our own nslookup)\nint main(int argc, char **argv)\n{\nstruct addrinfo *p, *listp, hints;\nchar buf[MAXLINE];\nint rc, flags;\n\/* Get a list of addrinfo records *\/\nmemset(&hints, 0, sizeof(struct addrinfo));\n\/\/ hints.ai_family = AF_INET;\n\/* IPv4 only *\/\nhints.ai_socktype = SOCK_STREAM; \/* Connections only *\/\nif ((rc = getaddrinfo(argv[1], NULL, &hints, &listp)) != 0) {\nfprintf(stderr, \"getaddrinfo error: %s\\n\", gai_strerror(rc));\nexit(1);\n}\n\nhostinfo.c\n\n\fConversion Example (cont)\n\n\/* Walk the list and display each IP address *\/\nflags = NI_NUMERICHOST; \/* Display address instead of name *\/\nfor (p = listp; p; p = p->ai_next) {\ngetnameinfo(p->ai_addr, p->ai_addrlen,\nbuf, MAXLINE, NULL, 0, flags);\nprintf(\"%s\\n\", buf);\n}\n\/* Clean up *\/\nfreeaddrinfo(listp);\nexit(0);\n}\n\nhostinfo.c\n\n\fRunning hostinfo\nwhaleshark> .\/hostinfo localhost\n127.0.0.1\nwhaleshark> .\/hostinfo whaleshark.ics.cs.cmu.edu\n128.2.210.175\nwhaleshark> .\/hostinfo twitter.com\n199.16.156.230\n199.16.156.38\n199.16.156.102\n199.16.156.198\nwhaleshark> .\/hostinfo google.com\n172.217.15.110\n2607:f8b0:4004:802::200e\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":131,"segment": "unlabeled", "course": "cs0449", "lec": "lec01", "text":"0\n\nIntroduction\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\nSummer 2021\n\n\fWhat do I need to know now!\nThe classes will be recorded!\n• You will be able to access the videos online\n▪ They are for your personal use only!\n▪ Do not distribute them!\n\n• You don’t need to turn on your camera\n▪ If you do, you may be recorded\n\n• You can ask questions via text!\n▪ Chat is great for that. If I don’t stop and read your questions, ask them again\n▪ But feel free to interrupt me at any point.\n\n2\n\n\fSyllabus \/ Administrivia\nI’m obligated to inform you that this is, in fact, a university course.\n\n3\n\n\fWelcome!\n• My name is Luis (pronounced Loo-eesh, but I don’t really care ☺ )\n• I’m not from these parts as you can tell from my accent\n▪ I come from Portugal\n\ni before e except after c…\nand this guy’s name!\n\n• course site: luisfnqoliveira.gitlab.io\/cs449_su2021\n▪ all the stuff I talk about today is on the course site\n\n• email: loliveira@pitt.edu\n• office: 5421 SENSQ (haha – more like Zoom) (check site)\n• office Hours: TBD (check the site)\n4\n\n\fThe Textbooks\n• The ANSI C Programming\nLanguage (2nd Edition)\n▪ By Brian Kernighan and Dennis Ritchie\n▪ Published by Prentice Hall, 1988\n▪ Often called the K&R book.\n▪ Conventions referred to as K&R style.\n▪ Old but trusty!\n\n• Computer Systems:\n\nA Programmer’s Perspective (3rd Edition)\n▪ By Randal E. Bryant and David R. O’Hallaron\n▪ Published by Pearson, 2016\n\n5\n\n\fCourse Layout\n• Lectures\n\n▪ Present high-level concepts.\n\n• Recitations\n\n▪ Applied concepts and introduce tools and skills for lab-work.\n▪ Clarify lectures and review topics.\n\n• Programming Assignments (Labs)\n\n▪ THE BULK OF YOUR COURSEWORK!\n▪ Roughly one-two weeks per assignment.\n▪ Provide deeper dive into some new skill or systems concept.\n▪ Programming, measurement, design.\n\n• Exams (Midterm + Final)\n\n▪ Tests comprehension of concepts\n\n6\n\n\fPolicies: Lab Assignments\n• Collaboration\n▪ You MUST WORK ALONE on all lab assignments.\n\n• Submission\n▪ Electronic submission using Gradescope (no exception)\n▪ Check due dates on the course website\n\n• Thoth Machine\n▪ Many labs will assume the use of a specialized machine.\n▪ You must use this machine:\n•\n\n▪ Use your Pitt username and password.\n▪ Talk to your TA if you have any issues. (Do NOT start assignments late!)\n7\n\n\fPolicies: Late Work\n• You get 5 Late Days\n▪ Covers most normal setbacks and life and schedule mishaps.\n▪ A maximum of 2 Late Days per lab assignment.\n▪ That is, assignments 3 days late will always take at least 1 penalty day.\n\n• When you run out…\n▪ Late penalty incurs a 15% penalty for each day. (out of original 100%)\n▪ An assignment cannot be submitted after the 3rd penalty day.\n• Four days late: that’s a 0.\n\n• Emergencies\n▪ Major emergencies require haste communication with me and your advisor.\n\n• Start everything early!!\n8\n\n\fPolicies: Grading\n• I don’t keep track of attendance\n▪ But you should come to class!\n▪ A lot of the concepts are best demonstrated interactively.\n\n• Labs: 50% (Weighted according to effort)\n• Quizzes: 5%\n• Midterm: 15%\n• Final: 20% (Necessarily Cumulative)\n• Homework: 10% (Online problem sets)\n• You CANNOT pass without doing the lab assignments.\n9\n\n\fPolicies: Conduct \/ Academic Integrity\n• Disability Resources \/ Services:\n▪ Contact DRS 412-648-7890; TTY: 412-383-7355\n▪ They will email me, and I will listen to what they tell me to do.\n\n• Cheating:\n▪ The default penalty is to be removed from the course with a failing grade\n▪ Pro-tip: DON’T CHEAT. Start early. Ask appropriate staff for help.\n▪ The syllabus online has a more thorough policy.\n\n• Conduct:\n▪ Jokes\/comments about sex, gender, race, ethnicity, religion, etc are not\ntolerated. Includes any online spaces involved.\n\n10\n\n\fMore Notes about Cheating\n• Again, do not cheat.\n• I’m not grading lab assignments, but I still look at your work.\n• Ask for help (There are PLENTY of resources)\n▪ TAs and my own office hours\n▪ Undergraduate Helpdesk (CRC)\n▪ We want you to succeed!\n\n• I can definitely tell when someone cheats.\n▪ It is very obvious.\n▪ Do not do it.\n▪ The University is justifiably strict about it.\n\n• Do not publish your code until after the semester (if at all)\n11\n\n\fTeaching Pedagogy \/ Philosophy\n• I want you to walk away with a direction\/goal to do something else.\n▪ Hopefully, you find something to be inspired by.\n\n• I trust my students that they could learn on their own.\n▪ But don’t want them to have to do so.\n▪ Ask questions! Challenge concepts! Ask for help!\n\n• No questions are dumb!\n▪ You ARE learning! You are not supposed to know things…\n▪ Even if you are supposed to know that, you don’t… so ask!\n▪ So ASK QUESTIONS!\n▪ DON’T STRUGGLE IN SILENCE\n\n12\n\n\fLessons learned\n• I scare some students (source: OMETs)\n▪ Sorry :’)\n▪ Don’t be scared! I like to help!\n\n• Regret #1 of my students:\n▪ I should have started earlier ;)\n\n• Come to lectures synchronously if you can!\n▪ You have access to me, we can interact\n▪ You can ask questions, and get the answers promptly!\n▪ Please… be interactive ☺\n\n13\n\n\fQuiz: CS 447 situation\n• What is your CS 447 situation?\n(A) I am *not* taking CS 447 (CS minor?)\n\n(B) I am taking CS 447 *this* semester\n(C) I completed CS 447 *last* semester\n(D) I completed CS 447 at least 2 semesters ago\n\n14\n\n\fFLEXing @ Pitt\n\nhttps:\/\/www.kasperonbi.com\/w\norking-from-home-for-thelast-3-years-tips-and-tricks\/\n\n15\n\n\fIDK you tell me\n• Let’s discuss some things:\n▪ What were your experiences last semester with remote teaching?\n• Good\/bad\/ugly\n\n▪ If the university authorizes, do you want to attend classes physically?\n• Why? (I really want to know your reasons!)\n• Note the question is not do you prefer\n(cause, ya know, we all preferred no CoViD pandemic)\n• My opinion: There is no benefit in coming that overweighs your health.\n\n▪ Who has timezone restrictions?\n• How should I distribute my office hours?\n\n▪ Anything else?\n\n16\n\n\fWhat I know!\n▪ Lectures will be synchronous! And online! ALWAYS!\n• But video will be available!\n• If you can, attend synchronously!\n\n▪ You can attend lectures asynchronously\n• Have a job? some scheduling problem? Idk?\n\n▪ Recitations are ALL remote!\n• I think…\n\n▪ IF we go back to the classroom\n• First lecture, I’ll be in the classroom, you will be at home. Why?\n• If technical issues come, I’ll just relocate to teach remotely\n• You can experience how it feels (teacher in classroom, students remote)\n\n▪ EVERYONE WILL FOLLOW THE RULES (distance, mask, etc.!)\n• https:\/\/www.coronavirus.pitt.edu\/\n• NO EXCEPTION (class will be dismissed)\n\n17\n\n\fCourse Overview\nIf food were knowledge, this would be, like, our restaurant menu.\n\n18\n\n\fTopics (Subject to deviation)\n• We’re going to (tentatively) learn SO MUCH fun stuff!\n• The C Systems Programming Language\n▪ Some x86 Assembly\n\n• Memory Models\n\n▪ Addresses and Pointers\n▪ Memory management\n\n• Memory Caches\n• Operating Systems\n\n▪ Processes \/ Signals\n▪ Interprocess Communication\n▪ The Basics of Virtual Memory\n▪ Basic Network Programming\n19\n\n\fSkills\n• C Programming\n▪ Abstractions and coping without them\n▪ x86 assembly (ISA) \/ calling conventions (ABI)\n▪ Interactive debugging\n▪ Data representation\n▪ We gain an appreciation of abstraction (and respecting limitations)\n\n• Systems Design\n▪ Learning the “why” for many systems abstractions\n▪ Manipulating systems and existing programs\n▪ Thinking about how systems might change in the future\n▪ We demystify software so as to no longer be a hostage to its design\n\n20\n\n\fWhat is Systems?\n• Systems is broad\n▪ A subfield of CS dealing with the interactions between software\/hardware.\n▪ A layer that provides abstractions and must constantly reevaluate them.\n• Operating Systems\n• File Systems\n• Program Analysis \/ Debugging Tools\n• Intra\/Inter System Protocols\n\n▪ A house built from trade-offs in approach…\n• Do you build better hardware? Add more memory?\n• Or, do you design better software?\n\n▪ And trade-offs in design…\n• Do you choose the specialized path?\n• Or, do you create a general system?\n• Both??\n\n▪ Very opinionated!!!!!!!!!!\n21\n\n\fWhat is Systems??\nLooking for guidance by looking at recent research:\n• Research Conferences\n\n▪ SOSP\/OSDI\/EuroSys – OS design, kernel design, virtualization\n• Parit models: erasure-coded resilience for prediction serving systems\n• Teechain: a secure payment network with asynchronous blockchain access\n• Finding semantic bugs in file systems with an extensible fuzzing framework\n• File systems unfit as distributed storage backends: lessons from 10 years of Ceph evolution\n• Snap: a microkernel approach to host networking\n\n▪ HotOS – Positions on Systems’ future\n• Machine Learning Systems are Stuck in a Rut\n• Granular Computing\n• I\/O Is Faster Than the CPU -- Let's Partition Resources and Eliminate (Most) OS Abstractions\n• I'm Not Dead Yet!: The Role of the Operating System in a Kernel-Bypass Era\n• Unikernels: The Next Stage of Linux's Dominance\n• The Case for I\/O-Device-as-a-Service\n\n22\n\n\fWhy the C Programming Language?\n• Because B sucks and D wasn’t invented yet. J\/K.\n• C was invented in 1972 alongside UNIX to an effort to aid\napplication development of that system.\n• Eventually UNIX itself was rewritten in C cementing C as a systems\nlanguage.\n• As such, C provides a high-level abstraction of assembly \/ machinecode and a low-level abstraction of memory, from the perspective\nof the C programmer.\n▪ This is important for programming systems code!\n▪ Allows full manipulation of memory (to one’s peril, often.)\n▪ This, in turn, allows for full manipulation of cpu\/hardware.\n23\n\n\fWhy the C Programming Language??\n• Learning C helps you understand Systems.\n• Understanding Systems lets you\nmake them better.\n▪ Or break them. ☺\n\n• C reveals the underlying memory\nmodel and execution environment.\n▪ Lets you understand any program.\n▪ Even if you do not have the original code.\n\n• Failing at C helps you learn…\n▪ Because then you debug your program.\n▪ And debuggers are very useful tools.\n24\n\n\fHow people use these skills\n• Writing Operating Systems\n▪ not the entire thing hopefully\n▪ … but parts are generally gonna be C\/C-like\n▪ Understanding systems means knowing how to\nmitigate\/improve performance.\n• Important that your abstractions don’t hurt performance\nbecause EVERY user application suffers.\n• Yet, performance is not the only consideration;\nunderstanding abstractions should help alleviate design\nfatigue. https:\/\/wilkie.how\/posts\/kaashoeks-law\n\n▪ Linux and Device Drivers: 10+ million lines of C\n• Yikes.\n• But, learning C means you can potentially read this and\nlearn more about \/ improve \/ extend Linux.\n\n25\n\n\fHow people use these skills\n• Debugging Higher-level Programs\n▪ Yes, even Python itself crashes!\n▪ … and the Python interpreter is written in C …\n▪ … and computers don’t understand C …\n▪ … so it’s gonna give you an assembly dump.\n\n26\n\n\fHow people use these skills\n• Creating Art\n▪ Real-time art includes not just video games.\n▪ There is a lot of fun and skill involved.\n▪ Being creative within a constraint has been\nvery alluring.\n▪ The Demoscene is such a community.\n\n27\n\n\fHow people use these skills\n• Breaking Things for Great Good\n\n▪ Or great bad… I’m not your parents.\n▪ Why? Old programs with copy-protection\nare still useful.\n• Original source code backed up??\nWhat time do you think this is?? Never????\n\n▪ And it is technically legal to reverse-engineer\nand\/or change them.\n• The best kind of legal.\n• But I’m not a lawyer and this is not legal advice. lol\n\n▪ You will typically use a “debugger” to break\ndown a program’s behavior.\n▪ And then patch it to do \/ not-do things.\n▪ Generally done professionally by librarians\/archivists.\n▪ We will also do this!!\n\n28\n\n\fHow YOU will use these skills\n• All of the above!!\n• And, of course, TO HAVE FUN!!\n\n29\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":132,"segment": "unlabeled", "course": "cs0007", "lec": "lec00", "text":"CS 0007: Introduction to Java\nLecture 0\nNathan Ong\nUniversity of Pittsburgh\n\n\fIntroductions\n• Name\n• Year\n• Why this course?\n• Random fact\n\n\fA Note for CS and Other\nComputing Majors\nCS 0401 (Intermediate Programming in\nJava) is more likely to be suitable for\nyou. This course is geared towards nonmajors. Talk to your academic advisor.\n\n\fSyllabus\n• Can be found on Courseweb\n\n\fTeaching Style\n• Because this course is geared towards\nnon-majors, the course will be taught in\na human-language inspired manner.\n• However, students should recognize\nthat human languages are fluid and\nbendable. Programming languages\n(like Java) are not.\n\n\fThe Programming Mindset\nImagine talking to your (possibly\nimaginary) non-cooperative little\nbrother who claims to be following\ndirections, but taking them as literally\nas possible.\nThis is what it is like programming a\ncomputer.\n\n\fRegarding Collaboration\n• I encourage all of you to collaborate\nand learn from each other by doing the\nhomework.\n• The only requirement: please write at\nthe top of your homework who you\ncollaborated with. Leaving this\ninformation out while continuing to\ncollaborate constitutes cheating and is\nNOT PERMITTED.\n\n\fRegarding Plagiarism\n• Plagiarism, which includes copying or\nusing code from any source without my\napproval, is strictly prohibited.\n• First offense: A zero for the graded item\n• Second offense: A zero for the course\n\n\fAcademic Integrity\n• The University’s official Academic\nIntegrity Policy can be found here:\nhttp:\/\/www.as.pitt.edu\/fac\/policies\/acad\nemic-integrity\n\n\fHeader\n• For those of you that don't check your\ne-mail on a daily basis, you should start.\n• Questions should be asked as soon as\nyou have one.\n\n\fQuestions?\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":133,"segment": "unlabeled", "course": "cs0441", "lec": "lec12", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #12: Primes, GCDs, and Representations\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nPrimes & Greatest Common Divisors\nl Prime representations\nl Important theorems about primality\nl Greatest Common Divisors\nl Least Common Multiples\nl Euclid’s algorithm\n\n\fOnce and for all, what are prime numbers?\nDefinition: A prime number is a positive integer p that is\ndivisible by only 1 and itself. If a number is not prime, it is called\na composite number.\n\nMathematically: p is prime ⇔ ∀ x∈Z+ [(x≠1 ∧ x≠p) → x \/| p]\nExamples: Are the following numbers prime or composite?\nl 23\nl 42\n\nPrime\nComposite, 42 = 2× 3× 7\n\nl 17\nl 3\nl 9\n\nPrime\nPrime\nComposite, 9 = 32\n\n\fAny positive integer can be represented as a\nunique product of prime numbers!\nTheorem (The Fundamental Theorem of Arithmetic): Every\npositive integer greater than 1 can be written uniquely as a prime\nor the product of two or more primes where the prime factors are\nwritten in order of non-decreasing size.\n\nExamples:\nl 100 = 2× 2× 5× 5 = 22× 52\nl 641 = 641\nl 999 = 3× 3× 3× 37 = 33× 37\nl 1024 = 2× 2× 2× 2× 2× 2× 2× 2× 2× 2 = 210\n\nNote: Proving the fundamental theorem of arithmetic requires\nsome mathematical tools that we have not yet learned.\n\n\fThis leads to a related theorem…\nTheorem: If n is a composite integer, then n has a\nprime divisor less than or equal to √n.\n\nProof:\nl If n is composite, then it has a positive integer factor a with\n1 < a < n by definition. This means that n = ab, where b is\nan integer greater than 1.\nl Assume a > √n and b > √n. Then ab > √n√n = n, which is a\ncontradiction. So either a ≤ √n or b ≤ √n.\nl Thus, n has a divisor less than or equal to √n.\nl By the fundamental theorem of arithmetic, this divisor is\neither prime, or is a product of primes. In either case, n\nhas a prime divisor less than or equal to √n. ❏\n\n\fApplying contraposition leads to a naive\nprimality test\nCorollary: If n is a positive integer that does not have\na prime divisor less than or equal to √n, then n is\nprime.\n\nExample: Is 101 prime?\nl The primes less than or equal to √101 are 2, 3, 5, and 7\nl Since 101 is not divisible by 2, 3, 5, or 7, it must be prime\n\nExample: Is 1147 prime?\nl The primes less than or equal to √1147 are 2, 3, 5, 7, 11,\n13, 17, 23, 29, and 31\nl 1147 = 31× 37, so 1147 must be composite\n\n\fThis approach can be generalized\nThe Sieve of Eratosthenes is a brute-force algorithm for finding all\nprime numbers less than some value n\nStep 1: List the numbers less than n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n245\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\n43\n\n44\n\n45\n\n46\n\n47\n\n48\n\n49\n\n50\n\n51\n\n52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\n63\n\n64\n\n65\n\n66\n\n67\n\n68\n\n69\n\n70\n\n71\n\nStep 2: If the next available number is less than √n, cross out all\nof its multiples\nStep 3: Repeat until the next available number is > √n\nStep 4: All remaining numbers are prime\n\n\fHow many primes are there?\nTheorem: There are infinitely many prime numbers.\nProof: By contradiction\nl Assume that there are only a finite number of primes p1, …, pn\nl Let Q = p1 × p2 × … × pn + 1\nl By the fundamental theorem of arithmetic, Q can be written as\nthe product of two or more primes.\nl Note that no pj divides Q, for if pj | Q, then pj also divides Q – p1\n× p2 × … × pn = 1.\nl Therefore, there must be some prime number not in our list. This\nprime number is either Q (if Q is prime) or a prime factor of Q (if\nQ is composite).\nl This is a contradiction since we assumed that all primes were\nlisted. Therefore, there are infinitely many primes. ❏\n\nThis is a non-constructive existence proof!\n\n\fIn-class exercises\nProblem 1: What is the prime factorization of 984?\nProblem 2: Is 157 prime? Is 97 prime?\nProblem 3: Is the set of all prime numbers countable\nor uncountable? If it is countable, show a 1-to-1\ncorrespondence between the prime numbers and\nthe natural numbers.\n\n\fGreatest common divisors\nDefinition: Let a and b be integers, not both zero.\nThe largest integer d such that d | a and d | b is called\nthe greatest common divisor of a and b, denoted by\ngcd(a, b).\nNote: We can (naively) find GCDs by comparing the\ncommon divisors of two numbers.\n\nExample: What is the GCD of 24 and 36?\nl Factors of 24: 1, 2, 3, 4, 6, 8, 12, 24\nl Factors of 36: 1, 2, 3, 4, 6, 9, 12, 18, 36\nl ∴ gcd(24, 36) = 12\n\n\fSometimes, the GCD of two numbers is 1\nExample: What is gcd(17, 22)?\nl Factors of 17: 1, 17\nl Factors of 22: 1, 2, 11, 22\nl ∴ gcd(17, 22) = 1\n\nDefinition: If gcd(a, b) = 1, we say that a and b are\nrelatively prime, or coprime. We say that a1, a2, …,\nan are pairwise relatively prime if gcd(ai, aj) = 1\n∀i,j.\n\nExample: Are 10, 17, and 21 pairwise coprime?\nl Factors of 10: 1, 2, 5, 10\nl Factors of 17: 1, 17\nl Factors of 21: 1, 3, 7, 21\n\nYes!\n\n\fWe can leverage the fundamental theorem of\narithmetic to develop a better algorithm\nLet:\nThen:\n\nGreatest multiple of p1 in both\na and b\n\nand\n\nGreatest multiple of p2 in both\na and b\n\nExample: Compute gcd(120, 500)\nl 120 = 23× 3× 5\nl 500 = 22× 53\nl So gcd(120, 500) = 22× 30× 5 = 20\n\n\fBetter still is Euclid’s algorithm\nObservation: If a = bq + r, then gcd(a, b) = gcd(b, r)\n\nProved in section 4.3 of the book\n\nSo, let r0 = a and r1 = b. Then:\nl r0 = r1q1 + r2\nl r1 = r2q2 + r3\nl…\nl rn-2 = rn-1qn-1 + rn\nl rn-1 = rnqn\n\ngcd(a, b) = rn\n\n0 ≤ r2 < r1\n0 ≤ r3 < r2\n0 ≤ rn < rn-1\n\n\fExamples of Euclid’s algorithm\nExample: Compute gcd(414, 662)\nl 662 = 414 × 1 + 248\nl 414 = 248 × 1 + 166\nl 248 = 166 × 1 + 82\nl 166 = 82 × 2 + 2\nl 82 = 2 × 41\n\ngcd(414, 662) = 2\n\nExample: Compute gcd(9888, 6060)\nl 9888 = 6060 × 1 + 3828\nl 6060 = 3828 × 1 + 2232\nl 3828 = 2232 × 1 + 1596\nl 2232 = 1596 × 1 + 636\nl 1596 = 636 × 2 + 324\nl 636 = 324 × 1 + 312\nl 324 = 312 × 1 + 12\nl 312 = 12 × 26\n\ngcd(9888, 6060) = 12\n\n\fLeast common multiples\nDefinition: The least common multiple of the integers\na and b is the smallest positive integer that is divisible\nby both a and b. The least common multiple of a and\nb is denoted lcm(a, b).\n\nExample: What is lcm(3,12)?\nl Multiples of 3: 3, 6, 9, 12, 15, …\nl Multiples of 12: 12, 24, 36, …\nl So lcm(3,12) = 12\n\nNote: lcm(a, b) is guaranteed to exist, since a\ncommon multiple exists (i.e., ab).\n\n\fWe can leverage the fundamental theorem of\narithmetic to develop a better algorithm\nLet:\nThen:\n\nGreatest multiple of p1 in either\na or b\n\nand\n\nGreatest multiple of p2 in\neither a or b\n\nExample: Compute lcm(120, 500)\nl 120 = 23× 3× 5\nl 500 = 22× 53\nl So lcm(120, 500) = 23× 3× 53 = 3000 << 120× 500 =\n60,000\n\n\fLCMs are closely tied to GCDs\nNote: ab = lcm(a, b)× gcd(a, b)\n\nExample: a = 120 = 23 × 3 × 5, b = 500 = 22 × 53\nl 120 = 23× 3× 5\nl 500 = 22× 53\nl lcm(120, 500) = 23× 3× 53 = 3000\nl gcd(120, 500) = 22× 30× 5 = 20\nl lcm(120, 500)× gcd(120, 500)\n= 23× 3× 53× 22× 30× 5\n= 25× 3× 54\n= 60,000 = 120× 500\n\n✔\n\n\fIn-class exercises\nProblem 4: Use Euclid’s algorithm to compute\ngcd(92928, 123552).\nProblem 5: Compute gcd(24, 36) and lcm(24, 36). Verify\nthat gcd(24, 36)× lcm(24, 36) = 24× 36.\n\n\fFinal Thoughts\nn Prime numbers play an important role in number\ntheory\nn There are an infinite number of prime numbers\nn Any number can be represented as a product of\nprime numbers; this has implications when\ncomputing GCDs and LCMs\nn Next time: Proof by Induction\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":134,"segment": "unlabeled", "course": "cs0449", "lec": "lec05", "text":"5\n\nIntroduction\nto Memory\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fThe Memory Model\nIf you forget how addressing works, I have a few pointers for you.\n\n2\n\n\fThe C Memory Model\n• Memory is a continuous series of bits.\n\nPotential Layout\n(32-bit addresses)\n\n• It can be logically divided into bytes or words.\n\n• We will treat it as byte-addressable which\nmeans individual bytes can be read.\n\nstack\n\n• This is not always the case!!\n• Consider masking and shifting to know the\nworkaround!\n\ncurrently unused but\navailable memory\n\n• With byte-addressable memory, each and\nevery byte (8 bits) has its own unique\naddress.\n• It’s the place it lives!! Memory is JUST LIKE US!\n• Address starts at 0, second byte is at address 1,\nand increases (“upward”) as you add new data.\n\nheap\nstatic data\n\ncode\n3\n\n\fThe C Memory Model\n• There are two main parts of a\nprogram: code and data\n• “code” is sometimes called “text”\n\n• Where in memory should each go?\n• Should we interleave them?\n• Which do you think is usually largest?\n\n• How do we use memory dynamically?\n• That is, only when we know we need it,\nin the moment.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n4\n\n\fThe C Memory Model: Code\n• Code has a few known properties:\n\nPotential Layout\n(32-bit addresses)\n\n• It likely should not change.\n• It must be loaded before a program can start.\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n5\n\n\fThe C Memory Model: Static Data\n• Static Data is an oft forgotten but useful section.\n• It does change. (contrary to its name)\n• It generally must be loaded before a program starts.\n• The size of the data and section is fixed.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n6\n\n\fThe C Memory Model: The Stack\n• The Stack is a space for temporary dynamic data.\n• Holds local variables and function arguments.\n• Allocated when functions are called. Freed on return.\n• Grows “downward”! (Allocates lower addresses)\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nStack Allocation allows\nrecursion. However, the more\nyou recurse, the more you\nuse! (Stack is only freed on return)\n\nstatic data\n\ncode\n7\n\n\fRevisiting our past troubles:\n\n4. Stack Allocation (No initialization!)\nIt reuses what is already there!!\n2. Stack Allocation\n\n1. Function Call\n3. Function Call\nQ: Hmm. Where is the value for ‘x’ coming from? Why?\n\n8\n\n\fThe C Memory Model: The Heap\n• The Heap is the dynamic data section!\n• Managing this memory can be very complex.\n• No garbage collection provided!!\n• We will revisit it in greater detail very soon.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n😱\n\n9\n\n\fCan an emoji be a variable name?\nThe questions that really matter 😃\nCompiler and version\n\nSuccess\n\ngcc main.c -std=c99\n\n😔\n\ngcc main.c -std=c11\n\n😔\n\ngcc main.c -std=c18\n\n😔\n\nclang main.c -std=c99\n\n😔\n\nclang main.c -std=c11\n\n😃\n\nclang main.c -std=c18\n\n😃\n\n#include <stdio.h>\nint main(void) {\nint 😱 = 3;\nprintf(\"😱 is %d\\n\", 😱);\nreturn 0;\n}\nlun8@thoth:~\/code_449$ .\/Hello\n😱 is 3\n\nAnswer: So… Not really, so avoid it (unless it’s for fun)!\n10\n\n\fPointers\nThey point to things. They are not the things. But they are things!?\n\n11\n\n\fThe “Memory Address” Variable Type\n• In C, we have integer types, floating point types…\n• Now we introduce our dedicated address type!\n• A pointer is a specific variable type that holds a memory address.\n\n• You can create a pointer that points to any address in memory.\n• Furthermore, you can tell it what type of data it should interpret that\nmemory to be: Just place that\nat the end.\n\n12\n\n\fInterpreting Pointers: Basics\n👉\n\n👉\n\nMemory\n(32-bit addresses)\n\n3.14159\n\n42\n\n• Pointers can pointer to individual sections of\nmemory.\n• They interpret whatever binary information is there.\n13\n\n\fInterpreting Pointers: Hmm\n👉\n\n👉\n\nMemory\n(32-bit addresses)\n\n3.14159\n\n42 or 0.1543e10(-8) ?\n\n• Pointers can refer to the same address as other\npointers just fine.\n• They interpret whatever binary information is\nthere.\n\n14\n\n\fInterpreting Pointers: A Sign of Trouble\n👉\n\nMemory\n(32-bit addresses)\n\nHelp I’m Lost!\n\n👉\n\n3.14159\n\n\n\n42 or 0.1543e10(-8) ?\n\n• Without the pointer, allocated data may linger\nforever without a way to reference it again!\n• C does not manage freeing memory for you.\n15\n\n\fDereferencing Pointers: A Star is Born\n• So, we have some ambiguity in our language.\n• If we have a variable that holds an address, normal operations\nchange the address not the value referenced by the pointer.\n• We use the dereference operator (\n\n)\n\n👉\n\n👉\n👉\n\n👉\n16\n\n\fDereferencing Pointers: A Star is Born\n• Remember: C implicitly coerces whatever values you throw at it…\n• Incorrectly assigning a value to an address or vice versa will be…\n• … Well … It will be surprising to say the least.\n\n• Generally, compilers will issue a warning.\n• But warnings mean it still compiles!! (You should eliminate warnings in practice)\n\n👉\n👉\n👉 🤔\n👉\n\n👉\n\n17\n\n\fReferencing Data: An… &… is Born?\n• Again… ambiguity. When do you want the address or the data?\n• We can pull out the address to data and assign that to a pointer.\n• Sometimes we refer to pointers as ‘references’ to data.\n\n• We use the reference operator (\n\n)\n\n👉\n\n👉\n👉\n18\n\n\fTurtles all the way down\n👉_\n\n👉_👉_\n\n👉_\n\n👉_\n\n👉_👉_\n👉_👉_\n\n19\n\n\fRemoving the emoji\n\n20\n\n\fLike skipping rocks on the lake…\n\n00\n\n01\n\n02\n\n03\n\n04\n\n05\n\n06\n\n42\n\n07\n\n08\n\n09\n\n0A\n\n0B\n\n0C\n\n0D\n\n0E\n\n0F\n\n10\n\n11\n\n12\n\n13\n\n4\ndata\n\ndataptr\n21\n\n\fLike skipping rocks on the lake…\n\n00\n\n01\n\n02\n\n03\n\n04\n\n05\n\n06\n\n42\n\n07\n\n08\n\n09\n\n0A\n\n0B\n\n0C\n\n4\ndata\n\n0D\n\n0E\n\n0F\n\n10\n\n11\n\n12\n\n13\n\nA\ndataptr\n\ndataptrptr\n22\n\n\fThe C Memory Model: The Heap\n• The Heap is the dynamic data section!\n• You interact with the heap entirely with pointers.\n•\nreturns the address to the heap with at\nleast the number of bytes requested. Or\non\nerror.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n23\n\n\fArrays\nIt is what all my fellow teachers desperately need: Arrays.\n\n24\n\n\fMany ducks lined up in a row\n• An array is simply a continuous span of memory.\n\n• You can declare an array on the stack:\n\n• You can declare an array on the heap:\n\nwriting in a pedantic style, you\nwould write the cast here.\n\n25\n\n\fInitialization\n• You can initialize them depending on how they are allocated:\n• You can initialize an array as it is allocated on the stack:\n\n• And the heap (for values other than 0, you’ll need a loop):\n\nQ: Why is using\n\nimportant here?\n\n26\n\n\fCarelessness means the Stack; Can stab you in the back!\n— “A poem about betrayal” by wilkie\n\n• Remember: Variables declared on the stack are temporary.\n• All arrays can be considered pointers, but addresses to the stack are not\nreliable:\n\nStack allocation\nArrays are indeed just pointers! This is an address on the stack.\nStack deallocation (oh no!)\n\n• This may work sometimes.\n\n• However calling a new function will overwrite the array. Don’t trust it!!\n\n• Instead: Allocate on the heap and pass in a buffer. (next slide)\n27\n\n\fAppropriate use of arrays. Approp-array-te.\nArrays don’t store length. Gotta pass it in.\nPointers allow for passing arguments “by reference”\n\nPointers can indeed be array-like!\n\nHeap allocation!\nAlthough we overwrite all values, using calloc to\ninitialize array elements to 0 reduces surprises.\nQ: What happens if we pass 20 instead of 10 to powers_of_two?\n\n28\n\n\fQuick notes on function arguments, here…\n• All arguments are passed “by value” in C.\n• This means the values are copied into temporary space (the stack, usually)\nwhen the functions are called.\n• This means changing those values does not change their original sources.\n\n• However, we can pass “by reference” indirectly using pointers:\n• Similar to how you pass “by reference” in Java by using arrays.\nThe “value” of the argument is the address.\n\n29\n\n\fCareful! No guard rails… You might run off the edge…\n• Since arrays are just pointers… and the length is not known…\n• Accessing any element is correct regardless of actual intended length!\n• No array bounds checking is the source of many very serious bugs!\n• Can pull out and leak arbitrary memory.\n• Can potentially cause the program to execute arbitrarily code.\n\nWhat if this is too big?\n\nA simple mistake, but it will gleefully write to it!\n\n30\n\n\fPointer arithmetic (Warning: it’s wacky)\n• Because pointers and arrays are essentially the same concept in C…\n• Pointers have some strange interactions with math operations.\n\n• Ideally pointers should “align” to their values in memory.\n• Goal: Incrementing an\npointer should go to the next\n• That is, not part way between two\nvalues.\n\nin memory.\n\n• Therefore, pointer sum is scaled to the element size.\n• Multiplication and other operators are undefined and result in a compiler\nerror.\n\n31\n\n\fPointer arithmetic in practice:\nAlternative (and less common) way of expressing a pointer.\n\nThe\n(postfix-increment) happens AFTER the dereference.\nThis is defined by the C language and is really confusing in practice.\n(but you’ll see it. often.)\n\n32\n\n\fThe C Memory Model: The Heap\n• The Heap is the dynamic data section!\n• You interact with the heap entirely with pointers.\n•\nreturns the address to the heap with at least the\nnumber of bytes requested. Or\non error.\n\nPotential Layout\n(32-bit addresses)\n\nstack\ncurrently unused but\navailable memory\n\nheap\nstatic data\n\ncode\n33\n\n\fStrings\nNo longer just for cats!\n\n34\n\n\fStrings\n• They are arrays and, as such, inherit all their limitations\/issues.\n• The size is not stored.\n• They are essentially just pointers to memory.\n\n• Text is represented as an array of\n\nelements.\n\n• Representing text is hard!!!\n• Understatement of the dang century.\n• Original ASCII is 7-bit, encodes Latin and Greek\n• Hence\n\nbeing the C integer byte type.\n\n• Extended for various locales haphazardly.\n• 7-bits woefully inadequate for certain languages.\n\n• Unicode mostly successfully unifies a variety of glyphs.\n• Tens of thousands of different characters! More than a byte!!\n\n35\n\n\fHow long is your string?\n• Arrays in C are just pointers and as such do not store their length.\n• They are simply continuous sections of memory!\n• Up to you to figure out how long it is!\n• Misreporting or assuming length is often a big source of bugs!\n\n• So, there are two common ways of expressing length:\n• Storing the length alongside the array.\n• Storing a special value within the array to mark the end. (A sentinel\nvalue)\n\n• Strings in C commonly employ a sentinel value.\n• Such a value must be something considered invalid for actual data.\n• How do you know how long such an array is?\n• You will have to search for the sentinel value! Incurring a 𝑂(𝑛) time cost.\n\n36\n\n\fThe string literal.\n• String literals should be familiar from Java.\n• However, in C, they are\npointers. (That is:\n)\n• The contents of the literal are read-only (immutable) so it is a:\n• Modifying it crashes your program!!\n• A pointer that can’t change pointing to an immutable string is a\n\nLet’s ignore this! ☺\n(for now)\n\nThe variable is allocated on the stack,\nwhich is a pointer. The string itself is\n37\nlikely in the static data segment!\n\n\fHow long is your string? Let’s find out.\n• The\n\nstandard library function reports the length of a string.\n\n• This is done in roughly 𝑂(𝑛) time as it must find the sentinel.\n• The following code investigates and prints out the sentinel:\n\n38\n\n\fWhen good strings go bad.\n• What happens if that sentinel… was not there?\n• Well… it would keep counting garbage memory until it sees a 0.\n\nThis syntax copies the string literal on to the stack.\nThis allows us to modify it. (otherwise, it is immutable)\n\nThe length here depends on the state of memory in the stack.\n\n39\n\n\fUsing stronger strings. A… rope… perhaps.\n• To ensure that malicious input is less likely to be disastrous…\n• We have alternative standard functions that set a maximum length.\n\nstrnlen will stop after the 12th character if it does not see a sentinel.\n\n40\n\n\fComparing “Apples” to “Oranges”\n• When you compare strings using\n\nit compares the addresses!\n\n• Since string literals are constant, they only exist in the executable once.\n• All references will refer to the same string!\n\n41\n\n\fComparing “Apples” to “Oranges”\n• When the addresses differ, they are not equal.\n• So, you have to be careful when comparing them.\n• This is similar to Java when considering\nversus\n\n42\n\n\fComparing “Apples” to “Oranges”\n• To compare values instead, use the standard library’s\n\n.\n\n• This will perform a byte-by-byte comparison of the string.\n• Upon finding a difference, it returns rough difference between those contrary bytes.\n• When they are the same, then the difference is 0!\n\n• Therefore, it is case sensitive! It also has a 𝑂 𝑛 time complexity.\n\nwill return 0 when the strings\nare equal.\n\n43\n\n\fAppropriate string construction. A-rope-riate.\n• C is a very deliberate language.\nis important here! Ensures string has\na length of 0. (is initially empty, not garbage!)\nLike a ballroom. Empty, but spacious.\n\nis the bounded form of\nOverwrites string.\n\n.\n\nis the bounded form of\n.\nConcatenates to end of existing string.\n44\n\n\fMemory\/Strings: Summary\n• Memory Allocation\n•\n•\n•\n•\n\nReturns pointer to length bytes\n\nReturns pointer to (count*size) bytes, zeros them\nDeallocates memory at ‘ptr’ so it can be allocated elsewhere\n\n• Strings\n•\n•\n•\n•\n•\n•\n•\n• Generally safer to use the bounded forms.\n\nCopies src to dst overwriting dst.\nCopies up to ‘max’ to dst.\nCopies string from src to end of dst.\nCopies up to ‘max’ to end of dst.\nReturns difference between strings. (0 if equal)\nCompares up to ‘max’ bytes.\n\n45\n\n\fInput\/Output: Summary\n• Input\n•\n•\n•\n•\n\n• Output\n•\n•\n•\n•\n•\n•\n•\n\nCopies string input by user into buffer (unsafe!)\nCopies up to 10 chars into buffer\n(my_buffer needs to be >= 11 bytes for sentinel)\nInterprets input and places value into int variable.\n\nupdates your variable, so you need to pass the address.\n(\ndoes not need it. Strings are already\n)\nPrints string. (technically unsafe)\nPrints up to 10 chars from string.\n(safe as long as my_buffer is >= 10 bytes)\nPrints int variable. (d for decimal, unfortunately)\nPrints int variable in hexadecimal. (x for hex)\nPrints long variable.\nPrints unsigned long variable.\n\n• Lots more variations! Generally\n\nand\n\nshare terms. Look them up!\n\n46\n\n\fStructures\nIt may not have class, but it has style.\n\n47\n\n\fQuick note on allocated structures…\n• You are gonna allocate a lot of structures…\n• They are big… you want them around… therefore, not good on the stack.\n• You could make them globals… except when you want them dynamically.\n\n48\n\n\fPointing to structure fields…\n• A shorthand for\n• The “arrow” syntax works only on\n\nis\npointers and dereferences a field.\n\n49\n\n\fPointing to structure fields…\n• Recall that\n• If you want a\n\nis what names types.\ndata type, you can use\n\nto do so:\n\n50\n\n\fIt took humanity thousands of years to discover the NULL pointer error.\n\n• So, what do we use to denote that we are not pointing to anything?\n• Same as Java… we use a Null value and we hope nobody dereferences it.\n• It is not a built-in thing! We have to include\nto use it.\n\n51\n\n\fWhen malloc … goes bad\n• When your request for memory cannot be made, malloc returns\n\n!\n\n• In your perfect program, you would always check for this.\n\n52\n\n\fWhen malloc … goes bad\n• You can check if\n\nis null with\n\n• You might say, “hey!\nis not defined as\nby the C standard!”\n• Yet, C specifically considers any pointer equal to\nto be a false value.\n• Regardless of the value of\n\nwhich is usually\n\nanyway.\n\n53\n\n\fExamples\nSome nice examples that address addressing!\n\n54\n\n\fSumming it all up.\n\n55\n\n\fSearching for values\n\nRemember that\n\nwants pointers to data.\n\n56\n\n\fPaving a new path\n\nRemember that\n\nwants pointers to data.\n\nWhen it sees more than 20 characters… what\nwill it do? (What will the next call to\ndo?)\n\n57\n\n\fPaving a new path (arbitrary number of directories!)\n\nwill resize the allocated space, copying the old\nvalue to a new chunk of memory if necessary.\nLook it up on your own!\n\n58\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":135,"segment": "unlabeled", "course": "cs0441", "lec": "lec17", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #17: Inclusion\/Exclusion, Pigeonhole Principle\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nn Inclusion\/exclusion principle\nn The pigeonhole principle\n\n\fSometimes when counting a set, we count the\nsame item more than once\nFor instance, if something can be done n1 ways or n2\nways, but some of the n1 ways are the same as some of\nthe n2 ways.\nIn this case n1 + n2 is an overcount of the ways to\ncomplete the task!\nWhat we really want to do is count the n1 + n2 ways to\ncomplete the task and then subtract out the common\nsolutions.\nThis is called the inclusion\/exclusion principle.\n\n\fWe can formulate this concept using set theory\nSuppose that a task T can be completed using a\nsolution drawn from one of two classes: A1 and A2\nAs in the sum rule, we can define the solution set for\nthe task T as S = A1 ∪ A2\nThen |S| = |A1 ∪ A2|\n= |A1| + |A2| - |A1 ∩ A2|\n\nA1\n\nDo you remember this from earlier this semester?\n\nA2\n\n\fCounting Bit Strings\nExample: How many bit strings of length 8 start with\na 1 or end with 00?\n\nSolution:\nl 27 = 128 8-bit strings start with a 1\nl 26 = 64 8-bit strings end with 00\nl 25 = 32 8-bit strings start with a 1 and end with 00\n\nSo, we have 128 + 64 – 32 = 160 ways to construct an\n8-bit string that starts with a 1 or ends with 00.\n\n\fJob Applications\nExample: A company receives 350 applications.\nSuppose 220 of these people majored in CS, 147 majored\nin business, and 51 were CS\/business double-majors.\nHow many applicants majored in neither CS nor business?\n\nSolution:\nl Let C be the set of CS majors, B be the set of business majors\nl |C ∪ B| = |C| + |B| - |C ∩ B|\nl\n= 220 + 147 – 51\nl\n= 316\n\nSo of the 350 applications, 350 – 316 = 34 applications\nneither majored in CS nor business.\n\n\fThe pigeonhole principle is an incredibly simple\nconcept that is extremely useful!\nThe pigeonhole principle: If k is a positive integer and\nk+1 objects are placed in k boxes, then at least one\nbox contains at least two objects.\n\nExample: k = 4\n\n\fThe pigeonhole principle is also easy to prove\nThe pigeonhole principle: If k is a positive integer and\nk+1 objects are placed in k boxes, then at least one\nbox contains at least two objects.\n\nProof: Assume that each of the k boxes contains at\nmost 1 item. This means that there are at most k\nitems, which is a contradiction of our assumption that\nwe have k+1 items, so at least one box must contain\nmore than one item. ❏\n\n\fExamples\nExample: Among any group of 367\npeople there are at least two with the\nsame birthday, since there are only 366\npossible birthdays.\n\nExample: Among any 27 English\nwords, at least two will start with\nthe same letter.\n\n\fThe pigeonhole principle can be used to prove a\nnumber of interesting results\nClaim: Every integer n has a multiple whose decimal\nrepresentation contains only 1s and 0s\n\nProof:\n\nThis number has n+1 ones\n\nl Let n be a positive integer\nl Consider the set of n+1 integers S = {1, 11, 111, …, 111…1}\nl Note that when any integer x is divided by n, there are n\npossible remainders (0 through n-1)\nl Since S contains n+1 elements, at least two elements of S\nhave the same remainder when divided by n (call them x\nand y, with x > y)\nl Since x ≡ y (mod n), n | (x – y), and thus na = (x – y)\nl Finally, we note that x – y contains only 0s and 1s ❏\n\n\fIn-class exercises\nTop Hat\n\n\fThere is a more general form of the pigeonhole\nprinciple that is even more useful\nThe generalized pigeonhole principle: If N objects are\nplaced into k boxes, then there is at least one box\ncontaining at least ⌈N\/k⌉ items.\n\nProof:\nl Assume that no box contains more than ⌈N\/k⌉ - 1 objects\nl Note that ⌈N\/k⌉ < (N\/k) + 1\nl So, k (⌈N\/k⌉ - 1) < k((N\/k + 1) – 1) = N\nl This contradicts our assumption that we had N objects ❏\n\n\fExample\nWhat is the minimum number of students needed to\nguarantee that at least six students receive the same\ngrade, if possible grades are A, B, C, D, and F?\n\nSolution:\nl Need the smallest integer N such that ⌈N\/5⌉ = 6\nl With 25 students, it would be possible (though maybe\nunlikely) to have 5 students get each possible grade\nl By adding a 26th student, we guarantee that at least 6\nstudents get one possible grade\nl So, the smallest such N is 5 × 5 + 1 = 26 ❏\n\n\fFrom the casino…\nHow many cards must be drawn from a standard 52card deck to guarantee that three cards of the same\nsuit are drawn?\n\nSolution:\nl Let’s make 4 piles: one for each suit\nl We want to have ⌈N\/4⌉ ≥ 3\nl We can do this using 4 × 2 + 1 = 9 cards\n\nNote: We don’t need 9 cards to end up with three\nfrom the same suit—if we did, we could never get a\nflush in poker!\n\n\fWe can’t always use the pigeonhole principle directly\nHow many cards would we need to draw to ensure\nthat we picked at least three hearts?\n\nIn the worst case, we would need to draw every club,\nspade, and diamond before getting three hearts…\nSo, to guarantee three hearts, we need to draw 3 ×\n13 + 3 = 42 cards!\n\n\fMa Bell…\nWhat is the least number of area codes needed to\nguarantee that the 25 million phones in some state can\nbe assigned distinct 10-digit phone numbers of the\nform NXX-NXX-XXXX?\n\nSolution:\nl The product rule tells us that there are 8 million phone\nnumbers of the form NXX-XXXX\nl Think of phones as objects and phone numbers as boxes\nl By the generalized pigeonhole principle, we know that\nsome “box” contains at least ⌈25,000,000\/8,000,000⌉ = 4\n“objects”\nl This means that we need 4 area codes to ensure that each\nphone gets a unique 10-digit number ❏\n\n\fThis has been easy so far, right?\nUnfortunately, life isn’t always easy!\nSometimes, we need to be clever when we are\ndefining our “boxes” or assigning objects to\nthem\n\nFor example…\n\n\fSports!\nDuring a month with 30 days, a baseball team plays at\nleast one game per day, but no more than 45 games total.\nShow that there must be some period of consecutive days\nin which exactly 14 games are played.\n\nSolution:\nl Let aj be the number of games played on or before the jth day of\nthe month. Note that the sequence {aj} is strictly increasing.\nl Note also that {aj + 14} is also an increasing sequence\nl Now, consider a1, a2, …, a30, a1 + 14, a2 + 14, …, a30 + 14\nl There are 60 terms in this sequence, all ≤ (45 + 14) = 59\nl By the pigeonhole principle, at least two terms are equal\nl Note: Each aj for j = 1, 2, …, 30 is distinct, as is each aj + 14\nl This means there exists some ai that is equal to some aj + 14, so\n14 games were played from day j + 1 to day i ❏\n\n\fNumber theory\nShow that among any n+1 positive integers not\nexceeding 2n, there must be an integer that divides\none of the other integers.\n\nProof:\nl Call our 𝑛 + 1 positive integers 𝑎! , 𝑎\" , … , 𝑎#$!\nl Write each 𝑎% as 2&! 𝑞% , where 𝑞% is an odd positive integer\nand 𝑘% is non-negative (i.e., 𝑘% might be zero)\nl Note that there are 𝑛 odd positive integers less than 2𝑛\nl By the pigeonhole principle, at least two of 𝑞! , 𝑞\" , … , 𝑞#$!\nmust be equal\nl This means we have some 𝑎% = 2&! 𝑞% and some 𝑎' = 2&\" 𝑞%\nl If 𝑘% < 𝑘' , then 𝑎% ∣ 𝑎' . If 𝑘% > 𝑘' , then 𝑎' ∣ 𝑎% ❏\n\n\fIn-class exercises\nProblem 4: Top Hat\nProblem 5: A drawer contains a dozen brown socks and a\ndozen black socks, all unmatched. How many socks must be\ndrawn to find a matching pair? How many socks must be\ndrawn to find a pair of black socks?\nProblem 6: Let A be some subset of {1, 2, …, 50} where\n|A|=10. Show that there are at least three subsets of A that\nhave the same sum. (This question appeared on a previous\nfinal exam.)\n\n\fFinal Thoughts\nn The inclusion\/exclusion principle is useful when we\nneed to avoid overcounting\nn The pigeonhole principle and its generalized form\nare useful for solving many types of counting\nproblems\nn Next time:\nl Permutations and combinations (Section 6.3)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":136,"segment": "unlabeled", "course": "cs0447", "lec": "lec0D", "text":"#D\nCS 0447\nIntroduction to\nComputer Programming\n\nAdding circuits and\nPlexers\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n● none\n\n2\n\n\fCan I add with transistors?\n\n3\n\n\fSolving a problem\n● Let’s say we want to create a circuit to add two bits.\no How do we do that?\n● In 3 very simple steps:\no Create a truth table that accurately represents the problem\no Interpret the truth table into a logic function\no Translate the function into a circuit\n\n4\n\n\fThe Tables of Truth\n● let's try to come up with a truth table for adding two bits\n● each column will hold 1 bit\nlet's name the\n\nA B\n0 0\nfor the input values, 0 1\nwe count up in\n1 0\nbinary\n1 1\nlet's name the\ninputs A and B\n\nC S\n0 0\n0 1\n0 1\n1 0\n\noutputs C and S,\nfor Carry and Sum\n\nnow let's fill in the\noutput values\nhey, this C column\nlooks familiar… so\ndoes the S column\ngreat! But\nthis is wrong.\n5\n\n\fHalf-truth tables\n● what we just made was a half-adder\n● it has a carry output but not a carry input\no (which might be useful for the lowest bit)\n\n● to make a full adder, we need 3 input bits\n\nCo C i\n\n00111 110\nA\n1011 0010\nB\n+0010 1111\n1110 0001\nS\n\nCi A B Co S\n0 0 0 0 0\n0\n0\n0\n1\n\n0\n1\n1\n0\n\n1\n0\n1\n0\n\n0 1\n0 1\n1 0\n\n0\n1 0 1 1\n1 1 0 1\n1 1 1 1\n\n1\n0\n0\n1\n6\n\n\fThe logic of it all\n● it looks a little messy, but it kinda makes\nsense if you think of it like this:\no it counts how many input bits are \"1\"\no Co and S are a 2-bit number!\n● if we look at the outputs in isolation:\no S is 1 if we have an odd number of \"1s\"\no Co is 1 if we have 2 or 3 \"1s\"\n● it's a little weird, but we can build this out of\nAND, OR, and XOR gates\n\nCi A B Co S\n0 0 0 0 0\n0\n0\n0\n1\n\n0\n1\n1\n0\n\n1\n0\n1\n0\n\n0 1\n0 1\n1 0\n\n0\n1 0 1 1\n1 1 0 1\n1 1 1 1\n\n1\n0\n0\n1\n7\n\n\fLet’s build the adder Co circuit\n\nA B Ci Co S 𝐂𝐨𝐮𝐭 = 𝐀\nഥ 𝐁𝐂𝐢𝐧 + 𝐀𝑩\nഥ 𝐂𝐢𝐧 + 𝐀𝐁𝐂𝐢𝐧 + 𝐀𝐁𝐂𝐢𝐧\n0 0 0 0 0\n0\n0\n0\n1\n\n0\n1\n1\n0\n\n1\n0\n1\n0\n\n0 1\n0 1\n1 0\n\nഥ 𝐁𝐂𝐢𝐧\n𝐀\n\n1\n0\n0\n1\n\nഥ 𝐂𝐢𝐧\n𝐀𝑩\n𝐀𝐁𝐂𝐢𝐧\n𝐀𝐁𝐂𝐢𝐧\n\n0\n1 0 1 1\n1 1 0 1\n1 1 1 1\n\n8\n\n\fSweeping that under the rug…\n● in programming, we use functions to be able to reuse code\n● in hardware, we can group gates into a component\n● here's the symbol for a one-bit full adder\n\nCo\nA\nB\nthe inputs are\nlike arguments\n\n+\nCi\n\nthe outputs are like\nreturn values\n\nS\nnow we don't have to care how\nit adds, just that it does\n\n9\n\n\fAdding longer numbers\n\n10\n\n\fWhere do the carries go?\n● when you add one place, you might get a carry out\n● that becomes the carry in for the next higher place\n\nBit\nBucket..?\n\n1 0 1 1 0 0 1 0\n+0 0 1 0 1 1 1 1\n11\n\n\fRipple Carry Adder\n● if we want to add two three-bit numbers, we'll\nneed three one-bit adders\n● we chain the carries from each place to the next\nhigher place, like we do on paper\n● we have to split the numbers up like so:\n\nA2 A1 A 0\n\n+ B2 B1 B0\nS2 S1 S0\n\nA2\n\nB2\nA1\nB1\nA0\nB0\n\n+\n\nS2\n\n+\n\nS1\n\n+\n\nS0\n\n12\n\n\fFlip side\n● We could come up with a separate subtraction circuit, but…\n● Since algebra tells us that x - y = x + (-y)\no Negation meaning flip the bits and add 1\nA1\n● Flipping the bits uses NOT gates\n● How do we add 1 without any extra circuitry?\n~B1\no we use a full adder for the LSB, and when\nwe're subtracting, set the \"carry in\" to 1\n\nA0\n\n~B0\n\n+\n\nS1\n\n+\n\nS0\n\n1\n\n13\n\n\fWhat makes a good word size?\n● can you think of an example of…\no 100 of something?\no a million of something? One thousand million?\no One billion? more?\n● 28 = 256, 216 ≅ 65,000, 232 ≅ 4000 million, 264 ≅ lots-of-a-lot\n● for a given word size, all the circuitry has to be built to support it\no 64 1-bit adders\no 128 wires going in\no 64 wires coming out\n\n14\n\n\fGate Delay\n● electrical signals can't move infinitely fast\n● transistors can't turn on and off infinitely fast\n● since each digit must wait for the next smaller digit to\ncompute its carry…\no ripple carry is linear in the number of digits\n● this is a diagram of how the outputs of a 16-bit ripple\ncarry adder change over time\no it's measured in picoseconds! so ~100ps total\n● but if we went to 32 bits, it'd take 200ps\no and 64 bits, 400ps...\n● there are more efficient ways of adding\n\n(courtesy of Kate Temkin)\n\n15\n\n\fWhat about overflow?\n● For unsigned addition, it's easy\no For an n-bit adder:\n▪ just look at the Co of the MSB\n▪ if it's 1, it's an overflow.\no what about subtraction?\n● For signed, is a bit strange\no Compare the last 2 carry bits\no If they are different\n▪ Then there is overflow\n\nOVF\nA2\nB2\n\nA1\nB1\nA0\n\nB0\n\n+\n\nS2\n\n+\n\nS1\n\n+\n\nS0\n16\n\n\fBut why?\n● When does signed addition overflow?\no If:\n▪ Both addends have the same sign\n▪ The result has a different sign\n● How can we detect that?\no Looking at the last bit!\n● Where is the overflow?\n\nThere is overflow in signed addition if:\n\nOn the last bit, the carry-in and carryout have different bit values.\n\nThe last bit:\nCi A B Co S\n0 0 0 0 0\n\n0\n0\n0\n1\n\n0\n1\n1\n0\n\n1\n0\n1\n0\n\n0 1\n0 1\n1 0\n\n0\n1 0 1 1\n1 1 0 1\n1 1 1 1\n\n1\n0\n0\n1\n\n17\n\n\fMuxes and demuxes, encoders and\ndecoders\n\n18\n\n\fHardware that makes decisions\n● a multiplexer (mux) outputs one of its inputs based on a select.\n\nA\n\nA\n\nQ\n\nQ\n\nB\n\nB\nS\n\nA\n\nQ\n\nB\nS=0\n\nS=1\n\nThis is the select input.\n\n19\n\n\fMultiplexer truth table\n● let's make a truth table for a two-input 1-bit multiplexer.\n\nA\n\n0 0 0 0\n\nA\nQ\n\nB\n\nQ\nB\n\nS=0\n\nS A B Q\n\n0 0 1 0\n0 1 0 1\n0 1 1 1\n\nS=1\n\n1 0 0 0\n1 0 1 1\n1 1 0 0\n1 1 1 1\n20\n\n\fDoing everything and throwing most of it away\n● I want a circuit that does this:\n\nif(select == 1)\noutput = A – B\nelse\noutput = A + B\n\n● let's see what that looks like\n● a mux is like a hardware if-else statement\n● but unlike in software…\no the \"condition\" comes at the \"end\" (the output)\no instead of doing one or the other, we do both, choose the one that we care\nabout, and ignore the rest!\n\n21\n\n\fAmusing muxes\n● Let’s go to Logisim!\n\n22\n\n\fWhat's that enable input?\n● if you don't understand tristate buses or high\nimpedance states, do not turn on the enable input.\n● if you ever see blue wires, you are in weird,\nconfusing territory.\n● if you know this stuff, fine, but otherwise…\n\n23\n\n\fDemultipliexers\n● a demux does the opposite: it sends its input to one of its outputs\n● the rest of the outputs are 0s\n\nIn\n\nIn\nS\n\nIn\nS=0\n\nS=1\n\n24\n\n\fLooking in a mirror\n● it can be confusing if all you see is this:\n\nwhich is which???\n\nLogisim distinguishes these with names\nI’ll do it with arrows\n\n25\n\n\fEncoders\n● They encode 2n inputs into n outputs. Specifically…\n● you give it several 1-bit inputs, and it tells you which one is 1.\n\n1\n\n0\n\n0\n\n1\n\n0\n\n2\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n2\n\n3\n\n0\n\n3\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n2\n\n0\n\n3\n\n0\n\nEnc 1\n\nEnc\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n2\n\n1\n\n3\n\n0\n\nEnc 1\n\nEnc\n\n1\n0\n\n0\n\n1\n\n1\n\n1\n\nI0 I1 I2 I3 Out\n1 0 0 0 00\n0 1 0 0 01\n0 0 1 0 10\n0 0 0 1 11\n\n26\n\n\fEncoder issues\n● That table seems VERY incomplete!!!\no What about the other entries???\n\nThe output is not valid!\n\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n0\n\n3\n\nEnc\n\n0\n\n1\n\nvalid\n\n???\n\n???\n0\n\nI0\n0\n0\n1\n1\n…\n\nI1\n0\n1\n1\n1\n…\n\nI2\n0\n1\n1\n1\n…\n\nI3 Out Valid\n0 ??\n0\n0 ??\n0\n0 ??\n0\n1 ??\n0\n… …\n…\n27\n\n\fEnter: Priority Encoders\n● In a priority encoder … you give it several 1-bit inputs, and it tells you the highest\ninput with a 1.\n\n1\n\n0\n\n0\n\n1\n\n0\n\n2\n\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n2\n\n3\n\n0\n\n3\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n2\n\n0\n\n3\n\n0\n\nPri 1\n\nPri\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n2\n\n1\n\n3\n\n0\n\nPri 1\n\nPri\n\n1\n0\n\n0\n\n1\n\n1\n\n1\n\nI0 I1 I2 I3 Out\n1 0 0 0 00\nX 1 0 0 01\nX X 1 0 10\nX X X 1 11\nWhat are\nthese?\n28\n\n\fidc\n● we don't even care about the that input\n● we can put X in the inputs we don't care about\n● we call these don't cares\no yep, really\n● what these mean is:\no when we make this into a boolean function, we can ignore those inputs\n▪ we won't even need to write em\n\nI0 I1 I2 I3 Out\n1 0 0 0 00\nX 1 0 0 01\nX X 1 0 10\nX X X 1 11\n29\n\n\fStill…\n● All zeros is still an invalid input :(\n\nif none of the inputs is 1,\nthen logisim gives you X…\nThese are not don’t cares :’)\n0\n\n0\n\n0\n\n1\n\n0\n\n2\n\n0\n\n3\n\n0\n\nPri 1\nvalid\n\nX\nX\n0\n\n30\n\n\fDecoders\n● a decoder is like a 1-bit demux whose input is always 1\n● It does pretty much the opposite of an encoder ☺\n\n1\nS\n\nS\n\nexactly one output is 1,\nand the rest are 0s\n31\n\n\fUses for encoders, decoders and demuxes\n● uhhhhhhhhhhh\no Ummmmmmmmm… for now …\n▪ unless you're using tristate (blue) wires, they're not too useful…\n● most of the time, you don't have to \"direct\" a signal to a location\no instead, you hook up the inputs to everything that needs them\n● we'll use them more when we get to sequential logic\n\n32\n\n\fCombinational vs Sequential\n● combinational logic: the outputs of a circuit depend entirely on their current inputs\no AND, OR, NOT, XOR gates\no adders\no muxes, demuxes, encoders, and decoders\n● sequential logic is coming up soon\no the outputs can depend on the current and previous inputs\no it remembers\n● logic minimization techniques only work on combinational logic!\no …or combinational pieces of a larger sequential circuit\n\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":137,"segment": "unlabeled", "course": "cs0007", "lec": "lec02", "text":"CS 0007: Introduction to Java\nLecture 2\nNathan Ong\nUniversity of Pittsburgh\nSeptember 6, 2016\n\n\fOutline\n• Recap of last lecture\n• Java’s “Place of Residence”\n• Your First Program\n\n\fWhy Learn Java?\n• Widely used\n• Constantly updating and receiving\nsupport\n• Easily understood due to standardized\nand consistent syntax\n• Forces code readability\n• Virtual Machine isolates running code\n\n\fSoftware Layers\n• Applications: Software\nthat interfaces with\nthe user\n• System Software:\nOperating System\n(e.g. Windows, Mac\nOSX, Linux)\n• Device Drivers:\nSoftware to talk to\nnon-computer devices\n(e.g. printers,\nwebcams, etc.)\nSource:\n\n\fWHAT ABOUT JAVA?\n\n\fVirtual Machine\n• Java is run on a \"Virtual Machine.\"\n• A Virtual Machine (VM) is an emulation\nof all of the components (hardware or\nsoftware) necessary to run a certain\npiece of software.\n• It is technically an application.\n\n\fJava Flowchart\nJava\nProgra\nm\n\nC\nO\nM\nP\nI\nL\nE\nR\n\nJava\nByteco\nde\n\nJava\nVM\n\nApplication Software\nLevel\n\n\fVM Pros and Cons\nPros:\nCons:\n• Code and bytecode\n• Generally slower\nworks independently\nruntime (than\nfrom the platform\nnatively compiled\n(operating system) it\nlanguages like C++).\nwas compiled on.\n• Arguably provides\n• More secure due to\nless fine-tuned\nruntime isolation.\nusage of lower levels\nof software\n\n\fMy First Program\nCOMPILING AND COMMENTING\n\n\fHello World!\n• A \"Hello World\" program is frequently a\nstudent's first program of any language.\n• Similar to learning \"Hello\" greetings in\nother languages.\n• Note: all code is CaSe SeNsItIvE\n\n\fHello World!\npublic class HelloWorld\n{\npublic static void main(String[] args)\n{\nSystem.out.println(\"Hello World!\");\n}\/\/end method main\n}\/\/End class HelloWorld\n\n\fCompilation\n\n\fCompiling\njavac – compiling a\njava file\n(javac FileName.java)\njava – running a\ncompiled java file\n(java FileName)\n\n\fHello World!\npublic class HelloWorld\n{\npublic static void main(String[] args)\n{\nSystem.out.println(\"Hello World!\");\n}\/\/end method main\n}\/\/End class HelloWorld\n\n\fSystem.out.println(\"Hello World!\");\n• Statement\n• Ends with a semicolon (like a period in\nEnglish)\n• Prints stuff\n– Where to?\n– What stuff?\n\n• To the console\n• Strings\n\n\fHello World v2\npublic class Hello\n{\npublic static void main(String[] args)\n{\nSystem.out.println(\"My name is\nNathan Ong! YAAAAYYYY LALALALA\nRANDOM TYPING\");\n}\/\/end method main\n}\/\/End class Hello\n\nWhat is the output?\n\n\fString\n• Characters in between quotation marks\n• Some valid Strings\n– \"Hi\"\n– \"#&^@!*(QQQQQQQQ\"\n\n• What if we want quotation marks in the\nString?\n\n\f\\\n• Escape character \\ tells the computer\nthat the next character should not be\ntaken literally\n– \\\"\n– Tab? \\t\n– New line? \\n\n– Slash? \\\\\n• \"Java said, \"OMG SYNTAX ERROR\"\"\n\n• How do we make the line above a valid\nString?\n\n\fHello World v2\npublic class Hello\n{\npublic static void main(String[] args)\n{\nSystem.out.println(\"My name is\nNathan Ong! YAAAAYYYY LALALALA\nRANDOM TYPING\");\n}\/\/end method main\n}\/\/End class Hello\n\nWhat if I want to use my name in another\nprint statement?\n\n\fRecycling\n• What if I want to use the string\nsomewhere else?\n• I'm so lazy, I don't want to type in my\nname when printing it out all the time.\n• String name = \"Nathan Ong\";\n• Now I can call name wherever I want\n(not really)!\n-->Variables<--\n\n\fString name = \"Nathan\nOng\";\n\n\fString name = \"Nathan\nOng\";\n\nTyp\ne\n\n\fString name = \"Nathan\nOng\";\n\n\fString name = \"Nathan\nOng\";\nvalu\ne\n\n\fTo Declare a Variable:\nType name = value;\n\n\fTo Use a Variable:\nname\n\n\fNaming\n• The name of a variable (and other\nthings) are called identifiers.\n• There are some rules regarding valid\nidentifiers, and further conventions that\nyou should follow.\n\n\fValid Identifiers\n• You must have at least one character in\nthe name\n• You can only use alphanumeric\ncharacters (letters and numbers),\nunderscores (_), and dollar signs ($) in\nthe name\n• You CANNOT use a number as the first\ncharacter.\n• You CANNOT use reserved Java\nkeywords.\n\n\fReserved Java Keywords\nboolean\ndo int\nstatic\nbreak double long super\nbyte else\nnew\nswitch\ncase final package\nthis\ncatch finally private\nthrow\nchar float protected throws\nclass for\npublic try\ncontinue if\nreturn void\ndefault\nimport short while\n\n\fConventional Naming\n• Variables and functions:\nlowerCamelCase\n– First word starts with a lowercase letter,\nsubsequent words have uppercase first\nletters\n\n• Class names: UpperCamelCase\n– All words have uppercase first letters\n\n• Never use dollar signs or underscores\n\n\fHello World v3\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.println(name);\nSystem.out.println(\" is \");\nSystem.out.println(name);\n}\/\/end method main\n}\/\/End class Name\n\nWhat is the output?\n\n\fOh I changed my name…Nathan Ong is\nno longer Nathan Ong…What am I going\nto do?\n\n\fWhat can we do with it?\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.println(name);\nSystem.out.println(\" is \");\nString name = \"Brandon Ong\";\nSystem.out.println(name);\n}\/\/end method main\n}\/\/End class Name\n\nCan we do this?\nNo, because we re-declared a variable!\n\n\fDeclaring a new variable:\nType name = value;\nChanging the old variable:\nname = newValue;\n\n\fWhat can we do with it?\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.println(name);\nSystem.out.println(\" is \");\nname = \"Brandon Ong\";\nSystem.out.println(name);\n}\/\/end method main\n}\/\/End class Name\n\nWhat is the output?\n\n\fOkay so I'm not really changing my name.\nRegardless, the output is really annoying.\nHow do we put it all in one line?\n\n\fWhat can we do with it?\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.print(name);\nSystem.out.print(\" is \");\nSystem.out.println(name);\n}\/\/end method main\n}\/\/End class Name\n\nUse print instead. No new line added.\nSaying print print print is kind of annoying…\n\n\f*cough* \\n\n*cough*\nAhhhhh, that's better.\nBut just for fun, how do we get it back to\nthe multiple-line version using the print\nstatements instead of println?\n\n\fWhat can we do with it?\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.print(name);\nSystem.out.print(\"\\n is \\n\");\nSystem.out.println(name);\n}\/\/end method main\n}\/\/End class Name\n\nGetting the same output from before with only print.\nBut we don't like that, so we'll take that out\nSaying print print print is kind of annoying…\n\n\fWhat can we do with it?\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.println(name + \" is \" + name);\n}\/\/end method main\n}\/\/End class Name\n\nUsing + with Strings is called\nconcatenation\n(Note: there's no such thing as – for\nStrings)\n\n\fWhat can we do with it?\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nSystem.out.println(name + \" is \" +\nname);\n}\/\/end method main\n}\/\/End class Name\n\nConcatenation does not add spaces\nautomatically.\n\n\fNicknames\n• System.out.println(name +\n\" is \" + name);\n• Seems useful, like if someone had a\nnickname\n• System.out.println(nickName +\n\" is \" + name);\n• But I don't have one…how do we make\nit so it doesn't matter?\n\n\fNicknames\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nString nickName = name;\nSystem.out.println(nickName + \" is \" +\nname);\n}\/\/end method main\n}\/\/End class Name\n\nOh so we can make nickName the exact same thing\nas name without having to type everything again!\nWhat is the output?\n\n\fSame thing\n\n\fComments\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan Ong\";\nString nickName = name;\nSystem.out.println(nickName + \" is \" +\nname);\n}\/\/end method main\n}\/\/End class Name\n\n\fCommenting\n• You must comment every line of useful\ncode.\n• Don't forget the \/\/\n• If you need a block of comments, use \/*\n*\/\n• Everything in between is commented\nout\n• Careful about nested comments…\n• \/* This \/* Comment Ends *\/ Here???*\/\n\n\fLine-by-Line Commenting\n\/\/The file name\n^Bad comment\n\/\/Prints out statistics for\n\/\/Nathan Ong\n^Good Comment\npublic class Name\n\n\fLine-by-Line Commenting\n\/\/The main method\n^Bad comment\n\/\/The program begins here\n^Good Comment\npublic static void main(String[]\nargs)\n\n\fLine-by-Line Commenting\n\/\/A String\n^Bad comment\n\/\/The first name held in a\n\/\/String\n^Good Comment\nString name = \"Nathan\";\n\n\fLine-by-Line Commenting\n\/\/Prints out stuff\n^Bad comment\n\/\/Prints out the names\n^Okay comment\n\/*Prints the nickname and links it to\nthe original name.*\/\n^Good comment\nSystem.out.println(nickName +\n\" is \" + name);\n\n\fRecap\n• Hello World!\n• System.out.println (and print)\n• Strings and String concatenation\n• Compiling and running\n• Variables\nType name = value;\nname = newValue;\n• Commenting\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":138,"segment": "unlabeled", "course": "cs0441", "lec": "lec02", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #2: Propositional Logic\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topic: Propositional Logic\nn What is a proposition?\nn Logical connectives and truth tables\nn Translating between English and propositional logic\n\n\fLogic is the basis of all mathematical and analytical reasoning\n\nGiven a collection of known truths, logic allows us to\ndeduce new truths\nExample\nBase facts:\nIf it is raining, I will not go outside\nIf I am inside, Lisa will stay home\nLisa and I always play video games if we are together during the\nweekend\nToday is a rainy Saturday\n\nConclusion: Lisa and I will play video games today\n\nLogic allows us to advance mathematics through an\niterative process of conjecture and proof\n\n\fPropositional logic is a very simple logic\n\nDefinition: A proposition is a precise statement that is\neither true or false, but not both.\nExamples:\nl 2 + 2 = 4 (true)\nl All dogs have 3 legs (false)\nl x2 < 0 (false)\nl Washington, D.C. is the capital of the USA (true)\n\n\fNot all statements are propositions\n\nn Charlie is handsome\nl “Handsome” is a subjective term.\n\nn x3 < 0\nl True if x < 0, false otherwise.\n\nn Springfield is the capital\nl True in Illinois, false in Massachusetts.\n\n\fWe can use logical connectives to build complex\npropositions\n\nWe will discuss the following logical connectives:\nl ¬ (not)\nl Ù (conjunction \/ and)\nl Ú (disjunction \/ or)\nl Å (exclusive disjunction \/ xor)\nl ® (implication)\nl « (biconditional)\n\n\fNegation\nThe negation of a proposition is true iff the proposition is\nfalse\nWhat we want\nto know\n\nWhat we know\nOne row for\neach possible\nvalue of “what\nwe know”\n\np\n\n¬p\n\nT\nF\n\nF\nT\n\nThe truth table for negation\n(I’ll sometimes use ⊤ and ⊥)\n\n\fNegation Examples\nNegate the following propositions\nl Today is Monday\n➣Today is not Monday\n\nl 21 * 2 = 42\n➣21 * 2 ¹ 42\n\nWhat is the truth value of the following propositions\nl ¬(9 is a prime number)\n➣true\n\nl ¬(Pittsburgh is in Pennsylvania)\n➣false\n\n\fConjunction\nThe conjunction of two propositions is true iff both\npropositions are true\np\n\nq\n\npÙq\n\nT\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nT\n\nF\n\nF\n\nF\n\nF\n\nThe truth table for conjunction\n22 = 4 rows since we know both p and q!\n\n\fDisjunction\nThe disjunction of two propositions is true iff at least one\nproposition is true\np\n\nq\n\npÚq\n\nT\n\nT\n\nT\n\nT\n\nF\n\nT\n\nF\n\nT\n\nT\n\nF\n\nF\n\nF\n\nThe truth table for disjunction\n\n\fConjunction and disjunction examples\nThis symbol means “is defined as”\nor “is equivalent to”\n(sometimes I’ll use ≜)\n\nLet:\n\nl p º x2 ≥ 0\nl q º A lion weighs less than a mouse\nl r º 10 < 7\nl s º Pittsburgh is located in Pennsylvania\n\nWhat are the truth values of these expressions:\nlpÙq\nlpÙs\nlpÚq\nlqÚr\n\nfalse\ntrue\ntrue\nfalse\n\n\fIn-class Exercises\nProblem 1: Let p º 2+2=5, q º eagles can fly, r º 1=1.\nDetermine the value for each of the following:\nlpÙq\nl ¬p Ú q\nl p Ú (q Ù r)\nl (p Ú q) Ù (¬r Ú ¬p)\n\n\fExclusive or (XOR)\nThe exclusive or of two propositions is true iff exactly one\nproposition is true\np\n\nq\n\npÅq\n\nT\n\nT\n\nF\n\nT\n\nF\n\nT\n\nF\n\nT\n\nT\n\nF\n\nF\n\nF\n\nThe truth table for exclusive or\nNote: Exclusive or is typically used to natural language to\nidentify choices. For example “You may have a soup or\nsalad with your entree.”\n\n\fImplication\nThe implication p ® q is false if p is true and q is false,\nand true otherwise\nTerminology\nl p is called the hypothesis\nl q is called the conclusion\n\np\n\nq\n\np®q\n\nT\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nT\n\nT\n\nF\n\nF\n\nT\n\nThe truth table for implication\n\n\fImplication (cont.)\nThe implication p ® q can be read in a number of\n(equivalent) ways:\nl If p then q\nl p only if q\nl p is sufficient for q\nl q whenever p\n\n\fImplication examples\nLet:\nl p º Jane gets a 100% on her final exam\nl q º Jane gets an A on her final exam\n\nWhat are the truth values of these implications:\nlp®q\nlq®p\n\ntrue\nfalse\n\n\fOther conditional statements\nGiven an implication p ® q:\nl q ® p is its converse\nl ¬q ® ¬p is its contrapositive\nl ¬p ® ¬q is its inverse\n\nWhy might this\nbe useful?\n\nNote: An implication and its contrapositive always have\nthe same truth value\n\n\fBiconditional\nThe biconditional p « q is true if and only if p and q assume the same\ntruth value\n\np\n\nq\n\np«q\n\nT\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nT\n\nF\n\nF\n\nF\n\nT\n\nThe truth table for the biconditional\nNote: The biconditional statement p « q is often read as\n“p if and only if q” or “p is a necessary and sufficient\ncondition for q.”\n\n\fTruth tables can also be made for more complex expressions\nExample: What is the truth table for (p Ù q) ® ¬r?\n\n23 =\n\n8r\nows\n\nSubexpressions of\n“what we want to\nknow”\n\nWhat we want to know\n\np\n\nq\n\nr\n\npÙq\n\n¬r\n\n(p Ù q) ® ¬r\n\nT\nT\nT\nT\nF\nF\nF\nF\n\nT\nT\nF\nF\nT\nT\nF\nF\n\nT\nF\nT\nF\nT\nF\nT\nF\n\nT\nT\nF\nF\nF\nF\nF\nF\n\nF\nT\nF\nT\nF\nT\nF\nT\n\nF\nT\nT\nT\nT\nT\nT\nT\n\n\fLike mathematical operators, logical operators\nare assigned precedence levels\n1. Negation\nl\n\n¬q Ú r means (¬q) Ú r, not ¬(q Ú r)\n\n2. Conjunction\n3. Disjunction\nl\n\nq Ù r Ú s means (q Ù r) Ú s, not q Ù (r Ú s)\n\n4. Implication\nl\n\nq Ù r ® s means (q Ù r) ® s, not q Ù (r ® s)\n\n5. Biconditional\nIn general, we will try to use parenthesis to\ndisambiguate these types of expressions.\n\n\fIn-class Exercises\nProblem 2: Show that an implication p ® q and its\ncontrapositive ¬q ® ¬p always have the same value\nl Hint: Construct two truth tables\n\nProblem 3: Construct the truth table for the compound\nproposition p Ù (¬q Ú r) ® s\n\n\fEnglish sentences can often be translated into propositional\nsentences\nBut why would we do that?\n\nReasoning about law\nPhilosophy and epistemology\n\nVerifying complex system\nspecifications\n\n\fExample #1\n\nExample: You can see an R-rated movie only if you are\nover 17 or you are accompanied by your legal guardian.\nLet:\n\nFind logical connectives\n\nTranslate fragments\nl r º “You can see an R-rated movie”\nl o º “You are over 17”\nl a º “You are accompanied by your legal guardian”\n\nTranslation: r ® (o Ú a)\nCreate logical expression\n\n\fExample #2\n\nExample: You can have free coffee if you are senior\ncitizen and it is a Tuesday\nLet:\nc º “You can have free coffee”\nn s º “You are a senior citizen”\nn t º “It is a Tuesday”\nn\n\nTranslation: (s Ù t) ® c\n\n\fExample #3\nExample: If you are under 17 and are not accompanied\nby your legal guardian, then you cannot see the\nR-rated movie.\nLet:\nn r º “You can see the R-rated movie”\nn u º “You are under 17”\nn a º “You are accompanied by your legal guardian”\nTranslation: (u Ù ¬a) ® ¬r\nNote: The above translation is the contrapositive of the\ntranslation from example 1!\n\n\fLogic also helps us understand bitwise operations\nn Computers represent data as sequences of bits\nl e.g., 0101 1101 1010 1111\n\nn Bitwise logical operations are often used to manipulate\nthese data\nn If we treat 1 as true and 0 as false, our logic truth\ntables tell us how to carry out bitwise logical operations\n\n\fBitwise logic examples\n1010 1110\nÙ 1110 1010\n1010 1010\n\n1110\nÚ 1010\n1110 1010\n1110 1110\n\n1110\nÅ 1010\n1110 1010\n0100 0100\n\n\fIn-class Exercises\nProblem 4: Translate the following sentences\nl If it is raining and today is Saturday then I will either play\nvideo games or watch a movie\nl You get a free salad only if you order off of the extended\nmenu and it is a Wednesday\n\nProblem 5: Solve the following bitwise problems\n1000\nÅ 1011\n1010 0110\n\n1000\nÙ 1011\n1010 0110\n\n\fFinal Thoughts\nn Propositional logic is a simple logic that allows us to\nreason about a variety of concepts\nn In recitation:\nl More examples and practice problems\nl Be sure to attend!\n\nn Next:\nl Logic puzzles and logical equivalences\nl Please read sections 1.2 and 1.3\n➣In general: do the assigned reading!\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":139,"segment": "unlabeled", "course": "cs0441", "lec": "lec04", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #4: Predicates and Quantifiers\n\nBased on materials developed by Dr. Adam Lee\n\n\fTopics\nn Predicates\nn Quantifiers\nn Logical equivalences in predicate logic\nn Translations using quantifiers\n\n\fPropositional logic is simple, therefore limited\nPropositional logic cannot represent some classes of\nnatural language statements…\nGiven: Kody is one\nof my dogs\n\nGiven: All of my dogs like peanut butter\n\nPropositional logic gives us no way to draw the\n(obvious) conclusion that Kody likes peanut butter!\n\n\fPropositional logic also limits the mathematical truths\nthat we can express and reason about\nConsider the following:\nl p1 º 2 has no divisors other than 1 and itself\nl p2 º 3 has no divisors other than 1 and itself\nl p3 º 5 has no divisors other than 1 and itself\nl p4 º 7 has no divisors other than 1 and itself\nl p5 º 11 has no divisors other than 1 and itself\nl…\n\nThis is an inefficient way to reason about the\nproperties of prime numbers!\nGeneral problem: Propositional logic has no way of\nreasoning about instances of general statements.\n\n\fHistorical Context\nThe previous examples are called syllogisms\nAristotle used syllogisms in his Prior\nAnalytics to deductively infer new facts\nfrom existing knowledge\nMajor premise\n\nAll men are mortal\nSocrates is a man\nSocrates is mortal\nConclusion\n\nMinor premise\n\n\fPredicate logic allows us to reason about the properties\nof individual objects and classes of objects\nPredicate logic allows us to use propositional functions\nduring our logical reasoning\n\nP(x) º x3 > 0\nvariable\n\npredicate\n\nNote: A propositional function P(x) has no truth value\nunless it is evaluated for a given x or set of xs.\n\n\fExamples\nAssume P(x) º x3 > 0. What are the truth values of the\nfollowing expressions:\nl P(0)\nfalse\nl P(23)\ntrue\nl P(-42)\nfalse\nWe can express the prime number property using\npredicate logic:\nl P(x) º “x is prime”\nl D(x) º “x has no divisors other than 1 and itself”\nl P(x) « D(x)\n\n\fPredicates can also be defined on more than one\nvariable\nLet P(x, y) º x + y = 42. What are the truth values of\nthe following expressions:\nl P(45, -3)\ntrue\nl P(23, 23)\nfalse\nl P(1, 119)\nfalse\nLet S(x, y, z) º x + y = z. What are the truth values of\nthe following expressions:\ntrue\nl S(1, 1, 2)\nfalse\nl S(23, 24, 42)\ntrue\nl S(-9, 18, 9)\n\n\fPredicates play a central role in program control\nflow and debugging\nIf\/then statements:\nl if x > 17 then y = 13\n\nLoops:\nl while y <= 14 do\n…\nend while\n\nDebugging in C\/C++:\nl assert(strlen(passwd) > 0);\n\nThis is a predicate!\n\n\fQuantifiers allow us to make general statements that\nturn propositional functions into propositions\nIn English, we use quantifiers on a regular basis:\nl All students can ride the bus for free\nl Many people like chocolate\nl I enjoy some types of tea\nl At least one person will sleep through their final exam\n\nQuantifiers require us to define a universe of discourse\n(also called a domain) in order for the quantification\nto make sense\nl “Many like chocolate” doesn’t make sense!\n\nWhat are the universes of discourse for the above\nstatements?\n\n\fUniversal quantification allows us to make statements\nabout the entire universe of discourse\nExamples:\nl All of my dogs like peanut butter\nl Every even integer is a multiple of two\nl For each positive integer x, 2x > x\n\nGiven a propositional function P(x), we express the\nuniversal quantification of P(x) as \"x P(x)\nWhat is the truth value of \"x P(x)?\nl true if P(x) is true for every x in the universe of discourse\nl false if P(x) is false for even one x in the universe of discourse\n\n\fExamples\nAll rational numbers are greater than 42\nl Domain: rational numbers\nl Propositional function: Let G(x) º “x is greater than 42”\nl Statement: \"x G(x)\nl Truth value: false (counterexample: ½)\n\nIf a natural number is prime, it has no divisors other than 1\nand itself\nl Domain: natural numbers\nl Propositional functions:\n➣Let P(x) º “x is prime”\n➣Let D(x) º “x has no divisors other than 1 and itself”\n\nl Statement: \"x [P(x) ® D(x)]\nl Truth value: true (by definition)\n\n\fExistential quantifiers allow us to make\nstatements about some objects\nExamples:\nl Some elephants are scared of mice\nl There exist integers a, b, and c such that the equality\na2 + b2 = c2 is true\nl There is at least one person who did better than John on the\nmidterm\n\nGiven a propositional function P(x), we express the\nexistential quantification of P(x) as $x P(x)\nWhat is the truth value of $x P(x)?\nl true if P(x) is true for at least one x in the universe of discourse\nl false if P(x) is false for each x in the universe of discourse\n\n\fExamples\nThe inequality x + 1 < x holds for at least one integer\nl Domain: Integers\nl Propositional function: P(x) º x + 1 < x\nl Statement: $x P(x)\nl Truth value: false\n\nFor some integers, the equality a2 + b2 = c2 is true\nl Domain: Integers\nl Propositional function: P(a, b, c) º a2 + b2 = c2\nl Statement: $a,b,c P(a,b,c)\nl Truth value: true\n\n\fWe can restrict the domain of quantification\nThe square of every natural number less than 4 is no more\nthan 9\nl Domain: natural numbers\nl Statement: \"x<4 (x2 ≤ 9)\nl Truth value: true\n\nThis is equivalent to writing\n\n\"x [(x < 4) ® (x2 ≤ 9)]\n\nSome integers between 0 and 6 are prime\nl Domain: Integers\nl Propositional function: P(x) º “x is prime”\nl Statement: $0≤x≤6 P(x)\nl Truth value: true\nThis is equivalent to writing\n\n$x [(0≤x≤6) Ù P(x)]\n\n\fPrecedence of quantifiers\nThe universal and existential quantifiers have the\nhighest precedence of all logical operators\nFor example:\nl \"x P(x) Ù Q(x) actually means (\"x P(x)) Ù Q(x)\nl $x P(x) ® Q(x) actually means ($x P(x)) ® Q(x)\n\nFor the most part, we will use parentheses to\ndisambiguate these types of statements\nBut you are still responsible\nfor understanding precedence!\n\n\fIn-class exercises\nSee on Top Hat\n\n\fWe can extend the notion of logical equivalence to\nexpressions containing predicates or quantifiers\n\nDefinition: Two statements involving predicates and\nquantifiers are logically equivalent iff they take on the\nsame truth value regardless of which predicates are\nsubstituted into these statements and which domains\nof discourse are used.\n\n\fProve: $x [P(x) Ú Q(x)] º $x P(x) Ú $x Q(x)\nWe must prove each “direction” of the equivalence.\nAssume that P and Q have the same domain.\nFirst, prove $x [P(x) Ú Q(x)] ® $x P(x) Ú $x Q(x):\nl If $x [P(x) Ú Q(x)] is true, this means that there is some value v in\nthe domain such that either P(v) is true or Q(v) is true\nl If P(v) is true, then $x P(x) is true and [$x P(x) Ú $x Q(x)] is true\nl If Q(v) is true, then $x Q(x) is true and [$x P(x) Ú $x Q(x)] is true\nl Thus $x [P(x) Ú Q(x)] ® $x P(x) Ú $x Q(x)\n\n\fProve: $x [P(x) Ú Q(x)] º $x P(x) Ú $x Q(x)\nThen, prove $x P(x) Ú $x Q(x) ® $x [P(x) Ú Q(x)]:\nl If $x P(x) Ú $x Q(x) is true, this means that there is some\nvalue v in the domain such that either P(v) is true or Q(v) is\ntrue\nl If P(v) is true, then $x [P(x) Ú Q(x)] is true\nl If Q(v) is true, then $x [P(x) Ú Q(x)] is true\nl Thus $x P(x) Ú $x Q(x) ® $x [P(x) Ú Q(x)]\n\nSince $x [P(x) Ú Q(x)] ® $x P(x) Ú $x Q(x) and\n$x P(x) Ú $x Q(x) ® $x [P(x) Ú Q(x)] then\n$x [P(x) Ú Q(x)] º $x P(x) Ú $x Q(x).\n\n\fWe also have DeMorgan’s laws for quantifiers\nNegation over universal quantifier: ¬\"x P(x) º $x ¬P(x)\nIntuition: If P(x) is not true for all x, then\nthere is at least one x for which P(x) is\nfalse\n\nNegation over existential quantifier: ¬$x P(x) º \"x ¬P(x)\nIntuition: If P(x) is not true for at least\none value x, then P(x) is false for all x\n\nThese are very useful logical equivalences, so let’s prove\none of them…\n\n\fProve: ¬\"x P(x) º $x ¬P(x)\nn ¬\"x P(x) ® $x ¬P(x)\nl ¬\"x P(x) is true if and only if \"x P(x) is false\nl \"x P(x) is false if and only if there is some v such that ¬P(v)\nis true\nl If ¬P(v) is true, then $x ¬P(x)\n\nn $x ¬P(x) ® ¬\"x P(x)\nl $x ¬P(x) is true if and only if there is some v such that ¬P(v)\nis true\nl If ¬P(v) is true, then clearly P(x) does not hold for all\npossible values in the domain and thus we have ¬\"x P(x)\n\nTherefore ¬\"x P(x) º $x ¬P(x).\n\n\fTranslations from English\nTo translate English sentences into logical expressions:\n1. Rewrite the sentence to make it easier to translate\n2. Determine the appropriate quantifiers to use\n3. Look for words that indicate logical operators\n4. Formalize sentence fragments\n5. Put it all together\n\n\fExample: At least one person in this classroom is named Bill and\nhas lived in Pittsburgh for 8 years\nExistential quantifier\n\nRewrite: There exists at least one person who is in this\nclassroom, is named Bill, and has lived in Pittsburgh\nfor 8 years\nConjunction\n\nFormalize:\nl C(x) º “x is in this classroom”\nl N(x) º “x is named Bill”\nl P(x) º “x has lived in Pittsburgh for 8 years”\n\nFinal expression: $x [C(x) Ù N(x) Ù P(x)]\n\n\fExample: If a student is taking CS441, then they have\ntaken high school algebra\nUniversal quantifier\n\nRewrite: For all students, if a student is in CS 441,\nthen they have taken high school algebra\n\nFormalize:\n\nImplication\n\nl C(x) º “x is taking CS441”\nl H(x) º “x has taken high school algebra”\n\nFinal expression: \"x [C(x) ® H(x)]\n\n\fNegate the previous example\nDeMorgan’s law for\nnegation over the universal\nquantifier\n\n¬\"x [C(x) ® H(x)] º $x ¬[C(x) ® H(x)]\nº $x ¬[¬C(x) Ú H(x)]\nº $x [¬¬C(x) Ù ¬H(x)]\na ® b º ¬a Ú b\nº $x [C(x) Ù ¬H(x)]\nDeMorgan’s law for\nDouble negation law\n\nnegation over\ndisjunction\n\nTranslate back into English:\nl There is a student taking CS441 that has not taken high\nschool algebra!\n\n\fExample: Jane enjoys drinking some types of tea\nRewrite: There exist some types of tea that Jane\nenjoys drinking\n\nFormalize:\nl T(x) º “x is a type of tea”\nl D(x) º “Jane enjoys drinking x”\n\nFinal expression: $x [T(x) Ù D(x)]\nNegate the previous example:\n¬$x [T(x) Ù D(x)] º \"x ¬[T(x) Ù D(x)]\nº \"x [¬T(x) Ú ¬D(x)]\nº \"x [T(x) ® ¬D(x)]\n“For all types of drink, if x is a tea, Jane does not enjoy\ndrinking it.”\n\n\fIn-class exercises\nProblem 3: Translate the following sentences into\nlogical expressions.\na) Some cows have black spots\nb) At least one student likes to watch football or ice hockey\nc) Any adult citizen of the US can register to vote if he or\nshe is not a convicted felon\n\nProblem 4: Negate the translated expressions from\nproblem 3. Translate these back into English.\n\n\fFinal Thoughts\nn The simplicity of propositional logic makes it unsuitable for\nsolving certain types of problems\nn Predicate logic makes use of\nl Propositional functions to describe properties of objects\nl The universal quantifier to assert properties of all objects within\na given domain\nl The existential quantifier to assert properties of some objects\nwithin a given domain\n\nn Predicate logic can be used to reason about relationships\nbetween objects and classes of objects\nn Next lecture:\nl Applications of predicate logic and nested quantifiers\nl Please read section 1.5\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":140,"segment": "unlabeled", "course": "cs0441", "lec": "lec03", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #3: Logic Puzzles and Propositional\nEquivalence\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s topics\nn Logic puzzles\nn Propositional equivalences\n\n\fA technical support conundrum\nAlice and Bob are technical support agents. If an agent\nis having a bad day, he or she will always lie to you. If\nan agent is having a good day, he or she will always\ntell you the truth. Alice tells you that Bob is having a\nbad day. Bob tells you that he and Alice are both\nhaving the same type of day. Can you trust the advice\nyou receive from Alice during your call?\n\n\fSolving logic puzzles is easy!\n\nStep 1: Identify rules\nand constraints\n\nStep 2: Assign propositions\nto key concepts\n\nStep 3: Make assumptions\nand reason logically!\n\n\fTechnical support revisited\nAlice and Bob are technical support agents. If an agent is having a\nbad day, he or she will always lie to you. If an agent is having a\ngood day, he or she will always tell you the truth. Alice tells you\nthat Bob is having a bad day. Bob tells you that he and Alice are\nboth having the same type of day. Can you trust the advice you\nreceive from Alice during your call?\n\nStep 1: Identify the rules of the puzzle\nl Good day = tell the truth\nl Bad day = lie!\n\nStep 2: Assign propositions to the key concepts in the puzzle\nl a º “Alice is having a good day”\nl b º “Bob is having a good day”\n\n\fStep 3: Make assumptions and reason logically\n\n\fAnother example\nConsider a group of friends: Frank, Anna, and Chris. If\nFrank is not the oldest, then Anna is. If Anna is not the\nyoungest, then Chris is the oldest. Determine the\nrelative ages of Frank, Anna, and Chris.\nPropositions:\nl\nl\nl\nl\n\nf = “Frank is the oldest”\na = “Anna is the oldest”\na’ = “Anna is the youngest”\nc = “Chris is the oldest”\n\nRules:\n1. ¬f ® a\n2. ¬a’ ® c\n\n\fStep 3: Make assumptions and reason logically\n\n\fSometimes no solution is a solution!\nAlice and Bob are technical support agents. Alice says\n“I am having a good day.” Bob says “I am having a good\nday.” Can you trust either Alice or Bob?\n\nStep 1: Identify rules\nl Good day = tell the truth\nl Bad day = lie\n\nStep 2: Assign propositions\nl a = Alice is having a good day\nl b = Bob is having a good day\n\n\fStep 3: Make assumptions and reason logically\n\n\fIn-class exercises\nProblem 1: Alice and Bob are technical support agents\nworking to fix your computer. Alice tells you that Bob\nis having a bad day today and that you should expect a\nlong wait before your computer is fixed. Bob tells you\nnot to worry, Alice is just having a bad day—your\ncomputer will be ready in no time.\n\nQuestion: Can you draw any conclusions about when\nyour computer will be fixed? If so, what\ncan you learn?\n\n\fPropositional equivalences: preliminaries\nDefinition: A tautology is a compound proposition that is\nalways true, regardless of the truth values of\nthe propositions occurring within it.\nDefinition: A contradiction is a compound proposition\nthat is always false, regardless of the truth\nvalues of the propositions occurring within it.\nDefinition: A contingency is a compound proposition\nwhose truth value is dependent on the\npropositions occurring within it.\n\n\fExamples\nAre the following compound propositions tautologies,\ncontradictions, or contingencies?\np\n¬p\np Ú ¬p\nn p Ú ¬p\n\ntautology\n\nn ¬p Ù p\n\ncontradiction\n\nnpÚq\n\ncontingency\n\nT\n\nF\n\nT\n\nF\n\nT\n\nT\n\np\n\n¬p\n\np Ù ¬p\n\nT\n\nF\n\nF\n\nF\n\nT\n\nF\n\np\n\nq\n\npÚq\n\nT\n\nT\n\nT\n\nT\n\nF\n\nT\n\nF\n\nT\n\nT\n\nF\n\nF\n\nF\n\n\fWhat are logical equivalences and why are they\nuseful?\nDefinition: Compound propositions p and q are\nlogically equivalent if p « q is a tautology.\nThe notation p º q means that p and q are\nlogically equivalent.\nLogical equivalences are extremely useful!\nl Aid in the construction of proofs\nl Allow us to simplify compound propositions\n\nExample: p ® q º ¬p Ú q\n\nHow do we\nprove this type\nof statement?\n\n\fIt is easy to prove propositional equivalences\nWe can prove simple logical equivalences using our\ngood friend the truth table!\nProve: p ® q º ¬p Ú q\np\n\nq\n\n¬p\n\n¬p Ú q\n\np®q\n\nT\n\nT\n\nF\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nF\n\nF\n\nT\n\nT\n\nT\n\nT\n\nF\n\nF\n\nT\n\nT\n\nT\n\n\fDeMorgan’s laws allow us to distribute negation\nover compound propositions\nIf “p or q” isn’t true, then\nneither p nor q is true\n\nTwo laws:\nl ¬(p Ú q) º ¬p Ù ¬q\nl ¬(p Ù q) º ¬p Ú ¬q\n\nIf “p and q” isn’t true, then at\nleast one of p or q is false\n\nProve: ¬(p Ú q) º ¬p Ù ¬q\np\n\nq\n\npÚq\n\n¬(p Ú q)\n\n¬p\n\n¬q\n\n¬p Ù ¬q\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nF\n\nT\n\nF\n\nT\n\nF\n\nF\n\nT\n\nF\n\nF\n\nT\n\nT\n\nF\n\nT\n\nF\n\nF\n\nF\n\nF\n\nF\n\nT\n\nT\n\nT\n\nT\n\n\fUsing DeMorgan’s laws\nUse DeMorgan’s laws to negate the following expressions:\nn “Bob is wearing blue pants and a sweatshirt”\nlb Ù s\nl ¬(b Ù s) º ¬b Ú ¬s\nl Bob is not wearing blue pants or is not wearing a\nsweatshirt\nn “I will drive or I will walk”\nld Ú w\nl ¬(d Ú w) º ¬d Ù ¬w\nl I will not drive and I will not walk\n\n\fIn-class exercises\nProblem 2: Prove that ¬(p Ù q) and ¬p Ú ¬q are\nlogically equivalent, i.e., ¬(p Ù q) º ¬p Ú ¬q. This is\nthe second DeMorgan’s law.\nProblem 3: Use DeMorgan’s laws to negate the\nfollowing propositions:\nl Today I will go running or ride my bike\nl Tom likes both pizza and beer\n\n\fSometimes using truth tables to prove logical\nequivalencies can become cumbersome\nRecall that for an equivalence with n propositions, we\nneed to build a truth table with 2n rows\nl Fine for tables with n = 2, 3, or 4\nl Consider n = 30—we would need 1,073,741,824 rows in the\ntruth table!\n\nAnother option: Direct manipulation\nof compound propositions using\nknown logical equivalencies\n\n\fThere are many useful logical equivalences\nEquivalence\n\nName\n\npÙTºp\npÚFºp\n\nIdentity laws\n\npÙFºF\npÚTºT\n\nDomination laws\n\npÙpºp\npÚpºp\n\nIdempotent laws\n\n¬(¬p) º p\n\nDouble negation law\n\npÚqºqÚp\npÙqºqÙp\n\nCommutative laws\n\n\fMore useful logical equivalences\nEquivalence\n\nName\n\n(p Ù q) Ù r º p Ù (q Ù r)\n(p Ú q) Ú r º p Ú (q Ú r)\n\nAssociative laws\n\np Ù (q Ú r) º (p Ù q) Ú (p Ù r)\np Ú (q Ù r) º (p Ú q) Ù (p Ú r)\n\nDistributive laws\n\n¬(p Ú q) º ¬p Ù ¬q\n¬(p Ù q) º ¬p Ú ¬q\n\nDeMorgan’s laws\n\np Ú (p Ù q) º p\np Ù (p Ú q) º p\n\nAbsorption laws\n\np Ú ¬p º T\np Ù ¬p º F\n\nNegation laws\n\nMore equivalencies in the book!\n\n\fProve that (p Ù q) ® (p Ú q) is a tautology\n\n\fProve: (p ® q) Ú (p ® r) º p ® (q Ú r)\n\n\fFinal Thoughts\nn Logic can help us solve real world problems and play\nchallenging games\nn Logical equivalences help us simplify complex\npropositions and construct proofs\nl More on proofs later in the course\n\nn Next:\nl Predicate logic and quantification\nl Please read section 1.4\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":141,"segment": "self_training_1", "course": "cs0447", "lec": "lec0B", "text":"#B\n\nMultiplication\nand Division\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n\n2\n\n\fMultiplication by repeated addition\n● in A × B, the product (answer) is “B copies of A, added together\"\n\n3 × 6 = 18\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9 10 11 12\n\n13 14 15 16 17 18\nhow many additions would it take to calculate\n\n2 x 500,000,000?\n\n3\n\n\fBack to grade school\n\n● remember your multiplication tables?\n● binary is so much easier\n● if we list 0 too, the product logic looks awfully familiar…\n✕\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n2\n\n2\n\n4\n\n6\n\n8 10 12 14 16 18\n\n3\n\n3\n\n6\n\n9 12 15 18 21 24 27\n\n4\n\n4\n\n8 12 16 20 24 28 32 36\n\n5\n\n5 10 15 20 25 30 35 40 45\n\n6\n\n6 12 18 24 30 36 42 48 54\n\n7\n\n7 14 21 28 35 42 49 56 63\n\n8\n\n8 16 24 32 40 48 56 64 72\n\n9\n\n9 18 27 36 45 54 63 72 81\n\n✕\n\n1\n\n1\n\n1\n\nA\n0\n0\n1\n1\n\nB P\n0 0\n1 0\n0 0\n1 1\n4\n\n\fJust like you remember\n● you know how to multiply, riiiight?\n\nthese are partial\nproducts. how\nmany additions\nare we doing?\n\n0101 =\n× 0110 =\n0000\n01010\n010100\n000\n0000___\n0011110\n\n5 Multiplicand\n× 6 Multiplier\n30\nwait, what operation\nare we doing here...?\n\n5\n\n\fWait, why does this work?\n● what are we actually doing with this technique?\n● remember how positional numbers are really polynomials?\n\nFOIL…\n\n78×54 = 70×50 + 70×4 + 8×50 + 8×4\nwe're eliminating many addition\nsteps by grouping them together.\n\n= 78×50 + 78×4\nwe group them together by\npowers of the base.\n\n6\n\n\fHow many bits?\n\n● when we added two n-digit\/bit numbers, how\nmany digits\/bits was the sum?\n● how about for multiplication?\n● when you multiply two n-digit\/bit numbers,\nthe product will be at most 2n digits\/bits\n● so if we multiply two 32-bit numbers…\no we could get a 64-bit result! AAAA!\no if we just ignored those extra 32 bits, or\ncrashed, we'd be losing a lot of info.\no so we have to store it.\n\n99\n× 99\n9801 9999\n×\n9999\n99980001\n1111\n×\n1111\n11100001\n7\n\n\fHow (and why) MIPS does it\n● MIPS has two more 32-bit registers, HI and LO. if you do this:\n\nmult t0, a0\n\n● then HI = upper 32 bits of the product and LO = lower 32 bits\n● to actually get the product, we use these:\n\nmfhi t0 # move From HI (t0 = HI)\nmflo t1 # move From LO (t1 = LO)\n\n● the mul pseudo-op does a mult followed by an mflo\n● MIPS does this for 2 reasons:\no multiplication can take longer than addition\no we'd otherwise have to change two different registers at once\n● if you wanted to check for 32-bit multiplication overflow, how could you do\nit?\n8\n\n\fSigned multiplication\n\n9\n\n\fGrade school (but like, 6th, instead of 3rd)\n● if you multiply two signed numbers, what's the rule?\n\nProduct\n\nA B\nP\n3\n5 15\n3 -5 -15\n-3\n5 -15\n-3 -5 15\n\nSign\n\nA\n+\n+\n-\n\nB\n+\n+\n-\n\nS\n+\n+\n\nif the signs of the\noperands differ, the\noutput is negative.\n\n10\n\n\fDon't repeat yourself\n● we already have an algorithm to multiply unsigned numbers\n● multiplying signed numbers is exactly the same (except for the signs)\n● so why not use what we already made?\n\nlong prod = unsigned_mult(abs(A), abs(B));\nif(sgn(A) == sgn(B))\nreturn prod;\nelse\nreturn –prod;\n\n11\n\n\fDivision\nLike multiplication, except… not really\n\n12\n\n\fIf multiplication is repeated addition…\n● …is division repeated subtraction?\no yes. it is.\n● in A ÷ B, the quotient (answer) is \"how many times can you\nsubtract B from A until you can't anymore?\"\n\n20 ÷ 3 = 6 R 2\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nwhat about\nthese lil guys?\n\nhow many subtractions would it take to calculate\n\n1,000,000,000 ÷ 2?\n\n13\n\n\fThat's not what you learned in school, was it\n● You learned something a tiiiiiny bit more complicated\n\n005 4 R51\n77 4209\n- 3 8 5 =77×5\n359\n- 3 0 8=77×4\n51\nFinally!!\n14\n\n\fWhat's going on?\n● division is multiplication backwards. it's like ctrl+z.\n\nfinding the partial products\n77 we're\nthat add up to a total product\n×54\ndivision goes left-to-right because we find the\npartial products from biggest to smallest\n308\nand to make it even more interesting,\n3850\nthere might be a remainder\nand then there's division by 0.\n4158\n+R51 multiplication is multiplying polynomials.\ndivision is factoring polynomials.\n4209\n15\n\n\fAnother way of looking at it (animated)\n● let's say we want to do… (calculating random number)… 696÷4\n\nfirst we ask how\nmany 400s fit\nthen how many 40s\nthen how many 4s\nso we're saving time\nby doing groups of\nsubtractions from\nbiggest to smallest\n\n1\n\n40\n\n40\n\n40\n\n7\n4\n\n40\n\n400\n\n40\n\n4\n4\n4\n4\n\n40\n40\n16\n\n\fThanks, tiny multiplication table. Thable.\n● at least multiplication in binary is easy, which simplifies division\n\n0 0 0 0 1 1 0R 1\n1100 1001001\n-1100\n1100\nin binary, each step becomes a\n1\n1\n0\n0\nyes-no choice: does the divisor fit\n01\ninto the remainder?\n17\n\n\fDivisor? Dividend? Remainder?\nthe divisor\ndivides the\ndividend\n\nthe dividend is\nthe number that is\nbeing divided\n\n1100 1001001\n-1100\nthe remainder is the\nnumber we're trying to\nthis is the new\n0\n1\n1\n1\n0\nfit the divisor into\nremainder.\nit starts off as the\ndividend…\n\nbut really, when we\nsubtract something, we\nare making the\nremainder smaller.\n\n18\n\n\fFinding partial products, biggest to smallest (animated)\nessentially we're starting with the divisor shifted\nall the way left, and sliding it right\n\n0 0 0 0 1 1 0R 1\n1100 100\n11001\n1100000000\nso let's ALGORITHM-IZE IT\n\n19\n\n\fA few more odds and ends\n\n20\n\n\fDivide-and-conquer… in parallel\n\n● an n×n digit multiplication can be broken into n, n×1 digit ones\n● the partial products can be summed in any order (thanks, commutativity)\n\n1011×0101 =\n1011×1\n+\n\n1011×0\n\n+1011×100\n\n+\n\n1011×0\n\nall operations in the same\ncolumn can be done in parallel.\n\n+\n+\n+\n\nnow our multiplication takes\nonly 3 steps instead of 4.\nbut this is a O(log(n))\nalgorithm! so for 32 bits…\nit takes 6 steps instead of 32!\n21\n\n\fBut division…\n● if we try to do something similar, well…\nwhat's the difference between\naddition and subtraction?\nsubtraction is not commutative.\n\n1011÷101 =\n1011÷101000 = 0\nyou can do the steps in any\norder… but you can't do\n1011÷10100 = 0\nthem at the same time.\n1011÷1010 = 1 R 1\n1011÷101 = 1 R 110??\n\nwe cannot know the\nanswer to this step… …until we know the answer\nto the previous one.\n\n22\n\n\fDivision is fundamentally slower\n● each step depends on the previous one.\n● we cannot split it up into subproblems like with multiplication.\n● the only way to make division faster is to guess.\no SRT Division is a way of predicting quotient bits based on the next few bits\nof the dividend and divisor\no but it can make mistakes and they have to be corrected\no the original Pentium CPU in 1994 messed this up\n▪ and Intel pretended everything was OK\n▪ and people got mad\n▪ and they had to recall millions of them\n▪ and Intel lost half a billion dollars\n– lol\n\n23\n\n\fDoing modulo with AND\n● in decimal, dividing by powers of 10 is trivial.\n\n53884 ÷ 1000 = 53 R 884\n● in binary, we can divide by powers of 2 easily with shifting\n● and we can get the remainder by masking!\n\n10010110 ÷ 1000 = 10010 R 110\n10010110 >> 11 = 10010\n10010110 & 0111 = 110\nmore generally: a AND (2n-1) = a % 2n\nmore generally: a AND ((1<<n)-1) = a % 2n\n24\n\n\fSigned division\n\n25\n\n\fAll roads lead to Rome… er, the Dividend\n● how did we extend our multiplication algorithm to signed numbers?\n● but how exactly do the rules work when you have two results?\nthe four values are related as:\nDividend = (Divisor × Quotient) + Remainder\nIf you do… Java says… Python says…\n\n7 \/ 2\n7 \/ -2\n\n3 R\n-3 R\n\n1\n1\n\n3 R 1\n-4 R -1\n\n-7 \/ 2\n-7 \/ -2\n\n-3 R -1\n3 R -1\n\n-4 R 1\n3 R -1\n\nmathematicians would\nexpect the remainder\nto always be positive,\nso the last row would\nbe 4 R 1!\n\ncheck out https:\/\/en.wikipedia.org\/wiki\/Modulo_operation for this travesty\n\nIn Java -7\/2 = -(7\/2)\nIn Python -7\/2 ≠ -(7\/2)\n26\n\n\fWhaaaaaaaaaaaaaaaat\n● no, really, it's not well-defined. there's no \"right\" answer.\n● watch out for this.\no I think I ran into it once because I was doing maths with angles in the\nrange [-pi, pi)\no most of the time, when you're dealing with modulo, you're dealing with\npositive values\no Most languages I had used did (-7 \/ 2) as -(7 \/ 2)\n▪ this is \"truncated division\" (rounds towards 0)\no but Python is gaining popularity and can sometimes be confusing\n▪ it uses \"flooring division\" (rounds towards -∞)\n● so which does arithmetic right shift do?\no it does flooring division.\n\n27\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":142,"segment": "unlabeled", "course": "cs0007", "lec": "lec12", "text":"CS 0007: Introduction to Java\nLecture 12\nNathan Ong\nUniversity of Pittsburgh\nOctober 20, 2016\n\n\fAnnoucements\n• Project 0 submission link is now\navailable via CourseWeb\n• When submitting, please organize your\nsource code and your essay into one zip\nfile. The zip file name should be\nLastnameFirstname_Project0.zip\n• Please DO NOT include any *.class files\n\n\fType[] name = new Type[size];\n\n\fint[] empty = new int[10];\n\n\fint[] list = {1,2,3,4};\n\n\fwhile(){…}\n• While a condition is true, run the block\n• While not at the end of the list, print out\nthe next element\nwhile(not at end of list)\n{\nSystem.out.println(next element);\n}\/\/end loop while(not at end of list)\n\n\fwhile(){…}\nint currElement = 0;\nwhile(currElement < list.length)\n{\nSystem.out.println(list[currElement]);\ncurrElement++;\n}\/\/end loop\nwhile(currElement<list.length)\n\n\fSo Many Things We Have to\nRemember…WHYYYYYYYYYY\nint counter = start;\nwhile(condition)\n{\n\/\/Do stuff\ncounter increment\n}\/\/end loop while(condition)\n• There must be a better way\n• WILD KEYWORD APPEARS\n\n\ffor(;;){…}\n• The for loop allows us to put everything\ntogether for us!\n• for(counter = start; condition; counter\nincrement)\n• Now we don't have to worry about the\nplacement of the counter and\nincrementing it!\n\n\fEQUIVALENCE\nint counter = start;\nwhile(condition)\n{\n\/\/Do stuff\ncounter\nincrement\n}\/\/end loop\n\/\/while(condition)\n\nfor(int counter = start;\ncondition; counter\nincrement)\n{\n\/\/Do stuff\n}\/\/end loop\nfor(condition)\n\n\ffor(;;){…}\nfor(int currElement = 0; currElement <\nlist.length; currElement++)\n{\nSystem.out.println(list[currElement]);\n}\/\/end loop for(currElement <\nlist.length)\n\n\fA Note\n• Programmers are lazy\n• Therefore, currElement is too long for\na name\n• Conventional counter names are:\ni, j , k , l , m , n , c , a , b , d , num ,\ncount , counter\n\n\ffor(;;){…}\nfor(int i = 0; i < list.length; i++)\n{\nSystem.out.println(\nlist[currElement]);\n}\/\/end loop for(i < list.length)\n\n\fLet's Test It on List\n\nint[] list = {1,2,3,4};\n\n\fLet's Test It on List\npublic class Test\n{\npublic static void main(String[] args)\n{\nint[] list = {1,2,3,4};\nfor(int i = 0; i < list.length; i++)\n{\nSystem.out.println(\"Counting...\nlist[i]);\n}\/\/end loop for(i < list.length)\n}\/\/end method main\n}\/\/End class Test\n\nWhat is the output?\n\nNow at \" +\n\n\fIt works, YAAAAAYYYYYY!!!\n\n\fUsage\n• When do I use For Loops?\n– Iteration, or going through all of the\nelements of an array or other type of list.\n– When the number of times you need to\nloop is known, either explicitly with a value,\nor through a variable.\n\n• While loops are used otherwise\n\n\fMulti-dimension Arrays\n• Arrays don't just have to be lists\n• They can also be grids, cubes, hypercubes, …\n\n\fType[]…[] name = new Type[size1]…\n[sizeN];\n\n\fType[]…[] name = values…s;\n\n\fint[][] listList = {{1,2,3},\n{4,5,6},{7,8,9}};\n\n\fThings to Remember\n• In 2-D Arrays, referencing an element\nrequires TWO PAIRS of square brackets\n• To get the #1 from listList\n• listList[0][0]\n• To get the #4 from listList\n• listList[1][0]\n• Remember: element  listList[row][col]\n\n\fHow Do We Go Through a 2-D\nArray?\n• If it took one for loop to go though a 1D array, then…\n• It will take two for loops to go through a\n2-D array\n• It will take N for loops to go through an\nN-D array\n• If you don't have to go through\neverything, don't. It just wastes time\n\n\fWe need to modify this\npublic class Test\n{\npublic static void main(String[] args)\n{\nint[][] listList =\n{{1,2,3},{4,5,6},{7,8,9}};\nfor(int i = 0; i < listList.length; i++)\n{\nSystem.out.println(\"Counting...\n\nNow at \" + listList[i]);\n\n}\/\/end loop for(i < list.length)\n}\/\/end method main\n}\/\/end class Test\n\nThere is only one loop. How do we write the next one?\n\n\fInner For Loop\nfor(int i = 0; i < listList.length;\ni++)\n{\n\/\/inner for loop\n\/\/…\n\/\/end inner for loop\n}\/\/end outer-loop for(i <\n\/\/listList.length)\nWhat does the outer for loop go through?\nWhat should the inner for loop go through?\n\n\flistList[row][col]\n\n\fInner For Loop\nfor(int i = 0; i < listList.length;\ni++)\n{\n\/\/inner for loop\n\/\/…\n\/\/end inner for loop\n}\/\/end outer-loop for(i <\n\/\/listList.length)\nThe outer loop goes through the row\nThe inner loop goes through the columns\n\n\fInner For Loop\n\/\/inner for loop\nfor(int i = 0; i < listList.length;\ni++)\n{\nSystem.out.println(\"Counting…\nNow at \" + listList[i][i]);\n}\/\/end inner-loop for(i <\n\/\/listList.length)\nIs this allowed?\nNo because i is already used\n\n\fInner For Loop\n\/\/inner for loop\nfor(int j = 0; j < listList.length; j++)\n{\nSystem.out.println(\"Counting…\nNow at \"\n+ listList[i][j]);\n}\/\/end inner-loop for(j <\n\/\/listList.length)\nIs this allowed?\nNo because listList.length only gets the number of\nrows!\nWhat can we do?\nWhat if we use each individual row's length?\n\n\fInner For Loop\n\/\/inner for loop\nfor(int j = 0; j <\nlistList[i].length; j++)\n{\nSystem.out.println(\"Counting…\nNow at \" + listList[i][j]);\n}\/\/end inner-loop for(j <\n\/\/listList[i].length)\nDoes this work?\n\n\fLet's Test It on listList\npublic class Test\n{\npublic static void main(String[] args)\n{\nint[][] list = {{1,2,3},{4,5,6},{7,8,9}};\nfor(int i = 0; i < list.length; i++)\n{\nfor(int j = 0; j < list[i].length; j++)\n{\nSystem.out.println(\"Counting...\nNow at \" + list[i][j]);\n}\/\/end inner-loop for(j < list[i].length)\n}\/\/end outer-loop for(i < list.length)\n}\/\/end method main\n}\/\/End class Test\n\nFor convenience sake, listList  list\nWhat is the output?\n\n\fIt works, YAAAAAYYYYYY!!!\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":143,"segment": "unlabeled", "course": "cs0007", "lec": "lec08", "text":"CS 0007: Introduction to Java\nLecture 8\nNathan Ong\nUniversity of Pittsburgh\nSeptember 27, 2016\n\n\fAnnouncement\n• Your midterm exam is scheduled for\nTuesday, October 11, during the normal\nclass period.\n• Anything from the lecture, homework,\nquiz, labs, is fair game.\n• You should also be comfortable with\nanalyzing previously unseen code in\nconjunction with using the API (as seen\nin several homeworks).\n\n\fFunction Components\n\n1.\n2.\n3.\nFunction 4.\nHeader 5.\n\nFunction\nBody 6.\n\nVisibility type (public\/protected\/private)\nstatic (For now, required)\nReturn Type\nfunctionName\nParentheses “()”\n–\na)\nb)\nc)\n\nParameters\nType1 parameterName1\nType2 parameterName2\n…\n\nCurly Brackets\/Braces “{}”\n–\n\nreturn a value\n\n\fFunction Overloading\n• Sometimes, the function name that was\nalready used fits perfectly with a\nfunction you need to create\n• They have similar functions, but operate\non different parameters\n• Turns out, you can use the same name,\nin what is called function overloading\n\n\fFunction Overloading Rules\n• Functions with the same name MUST\nhave different typed parameters, or a\ndifferent number of parameters.\n• We have seen this before!!!\n(System.out.println())\n\n\fExamples\npublic static void britishGreeting\n(String fullName)\npublic static void britishGreeting\n(String yourName, String childName)\n\npublic static int britishGreeting\n(int numOfPeople)\npublic static char britishGreeting\n(String firstNameOnly)\n\n\fSpecial Functions\n• Constructors: Functions that create new\nobjects, preceded by the keyword new\nbefore calling it.\n• main: The first function that is always\nrun by any Java program. It always has\na String[] as the only parameter.\n• Functions common to all objects: we\nmay return to this later.\n\n\fAnd More Booleans and Scope\n\nIF STATEMENTS\n\n\fIf Statements\n• English usage?\n• “If it will rain today, then I will bring my\numbrella.”\n• Rain today would indicate that I have\nbrought my umbrella\n• Cause and effect relationship: when a\ncondition is met, a behavior is elicited.\n\n\fSkeleton Statement\nif(<boolean condition>)\n{\n…\n}\n…\n\n\fBoolean Condition\n• Must evaluate to true in order for the\nstatements within the if-block to\nexecute.\n• Evaluating to false will skip the block.\n\n\fJava Example\n\/* checkWeather returns true if\nit\n* will rain today.*\/\nboolean rainToday = checkWeather();\nif(rainToday)\n{\nbringUmbrella();\n}\ngoToSchool();\n\n\fRemember this?\n\"has a Facebook: true“\nHow do we fix this?\n\n\fCode Reminder\npublic class Name\n{\npublic static void main(String[] args)\n{\nString firstName = \"Nathan\";\nchar midInitial = 'R';\nString lastName = \"Ong\";\nint age = 19;\nboolean hasFacebook = true;\nSystem.out.println(firstName + \" \" + midInitial +\n\" \" + lastName + \",\nage: \"\n+ age +\n\", has a Facebook: \" + hasFacebook);\n}\/\/end method main\n}\/\/End class Name\n\n\fA More Useful Output\n• if hasFacebook is true, then print what?\n• \"has a Facebook\"\n• Otherwise, if hasFacebook is false, then\nprint what?\n• \"does not have a Facebook\"\n\n\fif()\n• if hasFacebook is true, then print \"has a\nFacebook\"\nif(hasFacebook is true)\n{\nSystem.out.println(\"has a\nFacebook\");\n}\/\/end block if(hasFacebook is true)\n\n\fhasFacebook is true\n• How do we model this?\n• hasFacebook = true; \/\/?\n• But that let us do name-changing!\n• name = \"Brandon Ong\";\n\n\fMaking a new thing:\nType name = value;\nChanging the old thing:\nname = newValue;\n\n\fhasFacebook is true\n• How do we model this?\n• hasFacebook = true \/\/?\n• But that let us do name-changing!\n• name = \"Brandon Ong\";\n• So no, we can't do this to check for\nequality\n• We instead use double equals (==)\n\n\f== vs =\n• = is for assignment\n\n– String name = \"Nathan Ong\";\n– name has value?\n– int age = 19;\n– age has value?\n\n• == is for comparison (equal or not equal)\n– age == 19;\n– name == \"Nathan Ong\";\n\n• What values do we get for the last two\nstatements?\n• What type are they?\n\n\fWARNING\n• Be careful when comparing nonprimitive values!!!\n• name == \"Nathan Ong\" could actually\ngive us false!\n• Instead you will need to use equals()\n• name.equals(\"Nathan Ong\");\n\n\fif()\n• if hasFacebook is true, then print \"has a\nFacebook\"\nif(hasFacebook == true)\n{\nSystem.out.println(\"has a\nFacebook\");\n}\/\/end block if(hasFacebook == true)\n• But how do I represent when hasFacebook is\nfalse?\n\n\felse if()\n• If an if condition fails (the boolean\ncondition evaluates to false), it will\ncheck the condition for the subsequent\nelse if.\n• Can also be placed directly after\nanother else if block (i.e. you can\nhave several).\n\n\felse if()\n• Otherwise, if hasFacebook is false, then\nprint \"does not have a Facebook\"\nelse if(hasFacebook == false)\n{\nSystem.out.println(\"does not\nhave a Facebook\");\n}\/\/end block else if(hasFacebook\n\/\/ == false)\n\n\felse if()\nif(hasFacebook == true)\n{\nSystem.out.println(\"has a Facebook\");\n}\/\/end block if(hasFacebook == true)\nelse if(hasFacebook == false)\n{\nSystem.out.println(\"does not have a\nFacebook\");\n}\/\/end block else if(hasFacebook == false)\n\nSeems kinda wordy…\n\n\fhasFacebook == true\n• What does that give us?\n• What type?\n• hasFacebook == true when\nhasFacebook = true gives us?\n• hasFacebook == true when\nhasFacebook = false gives us?\n• hasFacebook == true \nhasFacebook\n• *Gasp*\n\n\fif()\nif(hasFacebook)\n{\nSystem.out.println(\"has a Facebook\");\n}\/\/end block if(hasFacebook)\nelse if(hasFacebook == false)\n{\nSystem.out.println(\"does not have a\nFacebook\");\n}\/\/end block else if(hasFacebook == false)\n\nStill kinda wordy…\n\n\fhasFacebook == false\n• We want to use the same trick we did\nbefore, except this time with false\ninstead of true.\n• Remember the boolean operators?\n\n\fif()\nif(hasFacebook)\n{\nSystem.out.println(\"has a Facebook\");\n}\/\/end block if(hasFacebook)\nelse if(!hasFacebook)\n{\nSystem.out.println(\"does not have a\nFacebook\");\n}\/\/end block else if(!hasFacebook)\n\nStill kinda wordy…\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":144,"segment": "unlabeled", "course": "cs0441", "lec": "lec11", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #11: Integers and Modular Arithmetic\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nIntegers and division\nl The division algorithm\nl Modular arithmetic\nl Applications of modular arithmetic\n\n\fWhat is number theory?\nNumber theory is the branch of mathematics that\nexplores the integers and their properties.\nNumber theory has many applications within computer\nscience, including:\nl Organizing data\nl Encrypting sensitive data\nl Developing error correcting codes\nl Generating “random” numbers\nl …\n\nWe will only scratch the surface…\n\n\fThe notion of divisibility is one of the most basic\nproperties of the integers\nDefinition: If a and b are integers and a ≠ 0, we say\nthat a divides b if there is an integer c such that b = ac.\nWe write a | b to say that a divides b, and a \/| b to say\nthat a does not divide b.\n\nMathematically: a | b ⇔ ∃ c∈Z (b = ac)\nNote: If a | b, then\nla is called a factor of b\nlb is called a multiple of a\n\nWe’ve been using the notion of divisibility all along!\nlE = {x | x = 2k ∧ k ∈ Z}\n\n\fDivision examples\nExamples:\nl Does 4 | 16?\nl Does 3 | 11?\nl Does 7 | 42?\n\nYes, 16 = 4 × 4\nNo, because 11\/3 is not an integer\nYes, 42 = 7 × 6\n\nQuestion: Let n and d be two positive integers. How\nmany positive integers not exceeding n are divisible by\nd?\nAnswer: We want to count the number of integers of the\nform dk that are less than n. That is, we want to know\nthe number of integers k with 0 ≤ dk ≤ n, or 0 ≤ k ≤\nn\/d. Therefore, there are ⌊n\/d⌋ positive integers not\nexceeding n that are divisible by d.\n\n\fImportant properties of divisibility\nProperty 1: If a | b and a | c, then a | (b + c)\n\nProperty 2: If a | b, then a | bc for all integers c.\n\nProperty 3: If a | b and b | c, then a | c.\n\n\fDivision algorithm\nTheorem: Let a be an integer and let d be a positive\ninteger. There are unique integers q and r, with\n0 ≤ r < d, such that a = dq + r.\n\nFor historical reasons, the above theorem is called the\ndivision algorithm, even though it isn’t an algorithm!\n\nTerminology: Given a = dq + r\nl a is called the dividend\nl d is called the divisor\nl q is called the quotient\nl r is called the remainder\nl q = a div d\nl r = a mod d\n\ndiv and mod are operators\n\n\fExamples\nQuestion: What are the quotient and remainder when 123 is\ndivided by 23?\nAnswer: We have that 123 = 23 × 5 + 8. So the quotient is 123\ndiv 23 = 5, and the remainder is 123 mod 23 = 8.\nQuestion: What are the quotient and remainder when -11 is\ndivided by 3?\nAnswer: Since -11 = 3 × -4 + 1, we have that the quotient is -4\nand the remainder is 1.\nRecall that since the remainder must be non-negative, 3 × -3 − 2\nis not a valid use of the division theorem!\n\n\fMany programming languages use the div and\nmod operations\nFor example, in Java, C, and C++\nl \/ corresponds to div when used on integer arguments\nl % corresponds to mod\n\nPrints out 1\n\npublic static void main(String[] args)\n{\nint x = 2;\nPrints out 2, not 2.5!\nint y = 5;\nfloat z = 2.0;\nSystem.out.println(y\/x);\nSystem.out.println(y%x);\nSystem.out.println(y\/z);\n\nPrints out 2.5\n\n}\nThis can be a source of many errors, so be careful in\nyour future classes!\n\n\fIn-class exercises\nProblem 1 & 2: On Top Hat\nProblem 3: Show that if 𝑎 is an integer and 𝑑 is an\ninteger greater than 1, then the quotient and remainder\n!\n!\nobtained dividing 𝑎 by 𝑑 are \" and 𝑎 − 𝑑 \" ,\nrespectively.\n\n\fSometimes, we care only about the remainder of an\ninteger after it is divided by some other integer\nExample: What time will it be 22 hours from now?\n\nAnswer: If it is 6am now, it will be (6 + 22) mod 24 =\n28 mod 24 = 4 am in 22 hours.\n\n\fSince remainders can be so important, they have\ntheir own special notation!\nDefinition: If a and b are integers and m is a positive\ninteger, we say that a is congruent to b modulo m if\nm | (a – b). We write this as a ≡ b (mod m).\nNote: a ≡ b (mod m) iff a mod m = b mod m.\n\nExamples:\nl Is 17 congruent to 5 modulo 6?\nl Is 24 congruent to 14 modulo 6?\n\nYes, since 6 | (17 – 5)\nNo, since 6 |\n\/ (24 – 14)\n\n\fProperties of congruencies\nTheorem: Let m be a positive integer. The integers a\nand b are congruent modulo m (a ≡ b (mod m)) iff\nthere is an integer k such that a = b + km.\n\nTheorem: Let m be a positive integer. If a ≡ b (mod\nm) and c ≡ d (mod m), then\nl (a + c) ≡ (b + d) (mod m)\nl ac ≡ bd (mod m)\n\n\fCongruencies have many applications within\ncomputer science\nToday we’ll look at three:\n1. Hash functions\n2. The generation of pseudorandom numbers\n3. Cryptography\n\n\fHash functions allow us to quickly and\nefficiently locate data\nProblem: Given a large collection of records, how can we find the\none we want quickly?\nSolution: Apply a hash function that determines the storage\nlocation of the record based on the record’s ID. A common hash\nfunction is h(k) = k mod n, where n is the number of available\nstorage locations.\n\nMemory:\n\n0\n\n42 mod 8 = 2\nID: 42\n…\n…\n\n1\n\n2\n\n3\n\n4\n\n276 mod 8 = 4\nID: 276\n…\n…\n\n5\n\n6\n\n7\n\n23 mod 8 = 7\nID: 23\n…\n…\n\n\fMany areas of computer science rely on the ability to\ngenerate pseudorandom numbers\n\nCoding algorithms\nHardware, software, and\nnetwork simulation\nSecurity\n\nNetwork protocols\n\n\fCongruencies can be used to generate\npseudorandom sequences\nStep 1: Choose\nl A modulus m\nl A multiplier a\nl An increment c\nl A seed x0\n\nStep 2: Apply the following\nl xn+1 = (axn + c) mod m\n\nExample: m = 9, a = 7, c = 4, x0 = 3\nl x1 = 7x0 + 4 mod 9 = 7×3 + 4 mod 9 = 25 mod 9 = 7\nl x2 = 7x1 + 4 mod 9 = 7×7 + 4 mod 9 = 53 mod 9 = 8\nl x3 = 7x2 + 4 mod 9 = 7×8 + 4 mod 9 = 60 mod 9 = 6\nl x4 = 7x3 + 4 mod 9 = 7×6 + 4 mod 9 = 46 mod 9 = 1\nl x5 = 7x4 + 4 mod 9 = 7×1 + 4 mod 9 = 11 mod 9 = 2\nl…\n\n\fThe field of cryptography makes heavy use of\nnumber theory and congruencies\nCryptography is the study of secret messages\nUses of cryptography:\nl Protecting medical records\nl Storing and transmitting military secrets\nl Secure web browsing\nl…\n\nCongruencies are used in cryptosystems from antiquity, as\nwell as in modern-day algorithms\nSince modern algorithms require quite a bit of background to\ndiscuss, we’ll examine an ancient cryptosystem\n\n\fThe Caesar cipher is based on congruencies\nTo encode a message using the Caesar cipher:\nl Choose a shift index s\nl Convert each letter A-Z into a number 0-25\nl Compute f(p) = p + s mod 26\n\nExample: Let s = 9. Encode “ATTACK”.\nl ATTACK = 0 19 19 0 2 10\nl f(0) = 9, f(19) = 2, f(2) = 11, f(10) = 19\nl Encrypted message: 9 2 2 9 11 19 = JCCJLT\n\n\fDecryption involves using the inverse function\nThat is, f-1(p) = p - s mod 26\n\nExample: Assume that s = 3. Decrypt the message\n“UHWUHDW”.\nl UHWUHDW = 20 7 22 20 7 3 22\nl f-1(20) = 17, f-1(7) = 4, f-1(22) = 19, f-1(3) = 0\nl Decrypted result: 17 4 19 17 4 0 19 = RETREAT\n\n\fIn-class exercises\nProblem 4 & 5: On Top Hat\n\n\fFinal thoughts\nn Number theory is the study of integers and their\nproperties\nn Divisibility, modular arithmetic, and congruency are\nused throughout computer science\nn Next time:\nl Prime numbers, GCDs, integer representation (Section 4.2\nand 4.3)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":145,"segment": "self_training_2", "course": "cs1502", "lec": "lec00_syllabus_handout", "text":"CS1502\nFormal Methods in Computer Science\nSyllabus\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nPlease see updated information on the Canvas\n1\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fInstructor\nName: Thumrongsak Kosiyatrakul\nFeel free to call me Tan, Dr. Tan, or Professor Tan\n\nEmail: tkosiyat@pitt.edu\nPlease start the subject line with [CS1502]...\n\nOffice Location: 6215 SENSQ\nOffice Hours: (TuTh) 9:30 AM to 11:30 AM (EST)\nOffice Hours URL: Check Canvas for the updated URL\n2\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fTeaching Assistant\n\nName: TBA\nEmail: TBA\nOffice Location: TBA\nOffice Hours: TBA\nOffice Hours URL: TBA\n\n3\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fMeeting Information\n\nLecture:\nDate\/Time: According to your section\nLocation (URL): Check Canvas for the updated URL\n\nFor Spring 2021:\nPre-recorded lectures will be provided\nAlmost like the flip classroom style\nWatch them in advance is recommended\nAll slides are provided with the lecture\nAvailable under Modules\n\nLecture sessions will be used for summaries, quick overview,\npractice questions, and Q&A\nAvailable under Panopto\n\n4\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fTextbook\nAuthors: Michael Sipser\nName: Introduction to the Theory of Computation\nEdition: 2nd or 3rd edition\n\n5\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fOutcome Measurement\n\nExams: 62% (2 exams 31% each)\nAssignments: 31% (≈ 7 – 8 written assignments)\nNo late submissions will be accepted\nUpload your assignment to the Canvas\n.pdf format is preferred\nHandwritten, take pictures, and convert them into a single\npdf file is okay\n\nTake-Home Quizzes: 5% (≈ 10 – 12 quizzes)\nAssessment Tests: 2% (2 tests)\n≥ 95%\nA+\n\n≥ 90%\nA\n\n≥ 89%\nA-\n\n≥ 88%\nB+\n\n≥ 80%\nB\n\n≥ 79%\nB-\n\n≥ 78%\nC+\n\n≥ 70%\nC\n\n6\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fCourse Web Site\n\nCanvas will be our LMS\nSlides\nRecorded lectures\nExams\nAssignments\nTake-home quizzes\nAssessment Tests\n\nAlways start from the Canvas front page for updated URLs\n\n7\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fOutline Schedule\n\nChapter 1: Finite Automata\nChapter 3: Turing Machines\nMidterm Exam\nChapter 4: Decidability\nChapter 5: Reducibility\nChapter 7: Time Complexity\nFinal Exam (last day of lecture)\n\n8\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fOther Information\n\nAcademic Integrity\nDisability Services\nStatement on Classroom Recording\n\n9\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fIntroduction to CS1502\n\nThere are some problems that cannot be solved\nGiven a polynomial with multiple variables:\n6x3 yz 2 + 3xy 2 − x3 − 10\nDoes it has an integral root?\nGiven a set of dominoes:\n(\u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015)\na\nca\nabc\nb\n,\n,\n,\nca\nab\na\nc\nIs it possible to make a horizontal line of one or more\ndominoes, with duplicates allowed, so that the string obtained\nby reading across the top halves matches the one obtained by\nreading across the bottom?\nPost Correspondence Problem\n\n10\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fIntroduction to CS1502\nThere are infinite number of algorithms\nHow to prove that none of them can be used to solve a\nproblem?\n\nProblem\nNo mathematical representation that can be used to express all\nalgorithms\n\nComputational Models:\nFinite Automata:\nCapture a subset of algorithms\n\nTuring Machine:\nCapture all algorithms\n\nLearn about limitations of computational models (limitations\nof algorithms) and how to formally verify their limitations\n\n11\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fIntroduction to CS1502\nThere are some problem that take too long to solve\nUnrealistically solvable\n\nModified Travel Salesman Problem\nShortest path to visit 1000 cities?\nThere are 1000! possible path\n1000! ≈ 4 × 102567\nSuppose you can check the distance of each path in 1 ns, it\nwill take ≈ 4 × 102558 seconds\n≈ 4 × 102556 minutes\n≈ 4 × 102554 hours\n≈ 4 × 102552 days\n≈ 4 × 102549 years\n\nDo we have a faster algorithm?\nMaybe not\n\nNeed to learn how to spot these kind of problems in advance\n12\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fProofs\n\nTo prove that a given statement is true is to demonstrate the\ntruth of the given statement\nA typical step in a Proof is to derive a true statement using\nprinciples of logical reasoning from one of the following:\nAssumptions\nHyptheses\nStatements that have been derived previously\nGeneral accepted facts\nDefinitions\n\n13\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 1\nLet Z = {. . . , −2, −1, 0, 1, 2, . . . }\nDefinition (simple one): An integer is a number in Z\nFormally: n is an integer if an only if n ∈ Z\nLet P (n) be “n is an integer”\nLet Q(n) be “n ∈ Z”\nThe above statement is P (n) ↔ Q(n)\n\nProve that “5 is an integer”\nThe statement “5 is an integer” is P (5)\nWe need to show that P (5) is true\nSince P (5) ↔ Q(5), if Q(5) is true, P (5) is true\nSo, we need to show that Q(5) is true\nQ(5) is “5 ∈ Z”\nSince 5 ∈ Z, Q(5) is true\nTherefore, the statement “5 is an integer” is true\n14\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 2\n\nDefinition: An even number n is an integer of the form\nn = 2k where k is an integer\nFormally: n is even if an only if n is an integer and n = 2k\nand k is an integer\nProve that “12 is even”\nWe need to show the following:\n12 is an integer (done)\n12 = 2k\nk=6\n6 is an integer (done)\n\nTherefore, “12 is even is true”\n\nDefinition: An odd number n is an integer of the form\nn = 2k + 1 where k is an integer\n\n15\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 3\nProve\nFor every two integers a and b, if a and b are odd, then a × b is\nodd.\nThe above statement is in the form P → Q where\nP is “a and b are odd”\nQ is “a × b is odd”\n\nRecall the truth table of P → Q\nP Q P →Q\nT T\nT\nT F\nF\nF T\nT\nF F\nT\nSo, to show that P → Q is true, we need to show that if P is\ntrue, Q must be true\nNeed to assume P to be true and show that Q is also true\n16\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 3\nProve\nFor every two integers a and b, if a and b are odd, then a × b is\nodd.\nInitial Assumptions:\na and b are integers.\na and b are odd.\nNote: a and b can be any integers that are odd.\n\nStatement to Derive:\na × b is odd.\n\nFact:\nIf x is an integer that is odd, there exists an integer k such\nthat x = 2k + 1.\n\nFrom the initial assumptions that a and b are odd integers, we\ncan derive:\nThere exists two integers i and j such that a = 2i + 1 and\nb = 2j + 1.\n17\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 3\nProve\nFor every two integers a and b, if a and b are odd, then a × b is\nodd.\nLet a = 2i + 1 and b = 2j + 1:\na × b = (2i + 1) × (2j + 1)\n= 4ij + 2i + 2j + 1\n= 2(2ij + i + j) + 1\nThus, a × b is an odd integer since there exists an integer k\n(2ij + i + j) such that a × b = 2k + 1.\nNote: By showing a couple of test cases is not a proof. In the\nabove proof, we assume that a and b can be any odd integers.\n18\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fProof by Contradiction\n\nA statement P is logically equivalent to ¬P → False\nP\n¬P\n¬P → False\nFalse True\nFalse\nTrue False\nTrue\nThus, to show that the statement P is True, we can show\nthat the statement ¬P → False is true.\nIn other words, assume that P is False and derive a statement\nthat contradict with an initial assumption, an hypotheses, a\nstatement that have been derived previously, or another\ngenerally accepted fact.\n\n19\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 4\nProve\nFor every integer p, if p2 is even, p is even.\nFormally: “p2 is even” → “p is even”\nAssume that the above statement is false\nRecall that\n¬(p2 is even → p is even) ≡ ¬(¬(p2 is even) ∨ p is even)\n≡ p2 is even ∧ ¬(p is even)\n≡ p2 is even ∧ p is odd\nNote that\nThe statement “p is odd” is either true or false\nIf “p is odd” is false, the above statement is false\nIf “p is odd” is true, “p2 is even” is false\nIn other word, the above statement implies false\n\nTherefore, “if p2 is even, p is even” is true\n20\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fExample 5\n\n−3\nDefinition: A rational number is a number such as\nthat\n7\np\ncan be expressed as the quotient or fraction of two integers,\nq\na numerator p and a non-zero denominator q\nEvery rational number may be expressed in a unique way as\na\nan irreducible fraction , where a and b are coprime integers\nb\nand b > 0\nNo common integer factor greater than 1 between a and b\nCanonical form\n\n21\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fProof by Contradiction\nProve\n√\n2 is an irrational number.\nFact: An irrational number is any real number that cannot be\nexpressed as a ratio of integers.\nIn other words, there is not exists two integers m and n such\nm √\nthat\n= 2.\nn\n√\n\nLet P be the statement ” 2 is an irrational number”.\n√\nThus ¬P is ” 2 is a rational number”.\n\nIn other words, there exists two integers m and n such that\nm √\n= 2.\nn\n\nm\nm\np\nwe can obtain\n= for some\nn\nn\nq\npositive integers p and q with no common factors.\nFact: For any ration\n\n22\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fProof by Contradiction\nProve\n√\n2 is an irrational number.\np √\nLet = 2 for some positive integers p and q with no\nq\ncommon factor.\n√\nNote that the above statement says that\nnumber\n\n2 is a rational\n\nLet’s try to derive some statements\np √\n= 2\nq\n\u0012 \u00132\n√\np\n= ( 2)2\nq\np2\n=2\nq2\np2 = 2q 2\n23\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fProof by Contradiction\nProve\n√\n2 is an irrational number.\nSince p2 = 2q 2 , p2 is an even number.\nIn other words, p is an even number or there exists an integer\nr such that p = 2r.\n\nKeep deriving:\np2 = 2q 2\n(2r)2 = 2q 2\n4r2 = 2q 2\n2r2 = q 2\nSince q 2 = 2r2 , q 2 is an even number (q is an even number)\n24\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\fProof by Contradiction\n\nProve\n√\n2 is an irrational number.\np √\n= 2 for some positive integers p\nq\nand q with no common factor.\n√\nBut we show that if 2 is a rational number both p and q are\neven.\nEarlier we stated that\n\nTherefore p and q have a common factor which is False\naccording to our assumption.\nSo, we get ¬P → False which is equivalent to P .\n√\nThus, 2 is an irrational number.\n\n25\/25\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nCS1502 Formal Methods in Computer Science Syllabus\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":146,"segment": "unlabeled", "course": "cs0447", "lec": "lec10", "text":"#10\n\nThe Register File\nand Building an\nALU\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\n1\n\nFall 2020\n\n\fThe Register File\nAbstracting out the flip-flops.\n\n2\n\n\fRemember this?\n\nProgram\n\ninstruction\n\nControl\n\n3\n\n5\n\n8\n\nRegisters\n\n+\n\nA\n\nB\n\nC\n\nMemory\n\nUnresolved questions:\n● What's the control?\n● What's the datapath?\n● How does it know what\ninstruction to get next?\n● How does it know what\nregisters to access?\n● How does it know to add,\nsubtract, etc.?\n\nDatapath\nProcessor\n\n3\n\n\fZooming in\n\nif(add)\ndo this\nelse if...\n\nfp\ns4\nat\nsp\neax?\n\nControl\n\nt0\n\nRegisters\n\nregisters hold the\nvalues being computed\n\ncontrol signals!\n\nvalues move\nbetween them\n\n...oh yeah, memory too\n\nthe control tells\neverything else what\nto do, and when\n\n+ -÷ ×\n⊕∫ ☃\n\n&\n\n● There are a few major parts of any CPU:\n\nDatapath\n\nthe datapath computes\nnew values\n4\n\n\fA bit less abstract\n\nControl\n\n● The registers are grouped together into the register file\n● The ALU (arithmetic and logic unit) is the main part of the datapath\n● The control is doing its thing, somehow…? (we’ll see)\n\nRegister\nFile\n\nALU\n\nwe can get the values of two\nregisters at once\n\n5\n\n\fIt doesn't have to be this way\nCISC CPUs usually have small\nsets of registers, and many have\nspecial purposes or behaviors\n\n8086\n\nz80\n\n6502\n\nPDP8\n\nax\nbx\ncx\ndx\nsi\ndi\nsp\nbp\n\na f\nb c\nd e\nh l\nix\niy\nsp\n\nA\nX\nY\n\nAC\n\n16 bits\n\n12 bits\n\nRISC CPUs usually have 32* mostlyinterchangeable registers: MIPS,\nRISC, SPARC, ARMv8, RISC-V…\nr0 r1 r2 r3 r4 r5 r6 r7\nr8 r9 r10 r11 r12 r13 r14 r15 32\/64\n\nr16 r17 r18 r19 r20 r21 r22 r23 bits\nr24 r25 r26 r27 r28 r29 r30 r31\n\n8 bits\n\nwhy is this? well, what do you\nremember about the differences\nbetween RISC and CISC?\n*or 32 at a time\n\n6\n\n\fTug-of-war\n● Register file design is constrained by many competing factors\n\ncompilers love lots of\nISA says instructions identical registers! …but there are diminishing returns.\nhave 2 operands\nand 1 destination\nfast L1 cache? not as\n…except for this one\nmany regs needed\nD\nQ\ninstruction that has\nD QQ\nD\n2 destinations.\nmulti-issue CPU: need to\nhumans like intuitive\nassembly language!\nwith lots of registers,\nfunction calls are faster! …but context switches\nare slower.\n\nread 4 regs and write 2\n\nmore registers means\nmore silicon…\n7\n\n\fA word of advice\n● You will see many imperfect designs in your life\n● But in problem-solving, perfection isn't always the goal\no everyone has to work within the constraints they're given\n● and if everyone does something the same way…\no there are probably problems\/constraints you don't know about\no don't waste your time reinventing the wheel.\n▪ find out why it's done that way first.\n● When it comes to register files, 32 registers is just a nice number\no not too many, not too few, a nice middle-ground\n\n● also don't be a judgmental ass about someone else's design because one, it's shitty, and two, they know more about why\nit was designed that way, so you're just being presumptuous\n\n8\n\n\fSoooo registers… How do we create a register?\n● If we create a 32-bit register out of D Flip-Flops:\n\nD\n\nQ\n\nD\n\nQ\n\nD\n\nQ\n\nD\n\nQ\n\nD Flip-Flop\n\nD Flip-Flop\n\nD Flip-Flop\n\nD Flip-Flop\n\n3rd bit\n\n2nd bit\n\n1st bit\n\n0th bit\n\nWe abstract\naway to this:\n\n32\n\nD\n\nQ\n\n32\n\nRegister\n\nThankfully, so does\nLogisim!\n9\n\n\fCombined into…\n● Then, we can combine many of those together:\n\nRegister File\n32\n\nD\n\nQ\n\n32\n\nRegister\n\n32\n\nD\n\nQ\n\n32\n\nRegister\n\n…\n32\n\nD\n\nQ\n\n32\n\nRegister\n10\n\n\fThe MIPS register file\n● In the instruction add t0, t1, t2, how many registers are read?\no how many are written?\no how many different registers are accessed?\n\nthere's one input or\nwrite port\nit needs a clock signal.\nwhat other control\nsignals does it need?\n\nRegister\nFile\nWE\n\nhow about a\nwrite enable?\n\nrd\n\nrs\n\nrt\n\nthere are two output\nor read ports\neach port can read a\ndifferent register\n\nand inputs to select\nthe registers?\n\n11\n\n\fReading from one register\n● You have two registers, and you want to choose one to read\n\nwhat kind of component chooses?\nD\n\nA WE83\n\nQ\n\n29\n83\n\nB\n\nD\n\nreading from a register is\ntechnically combinational\n\nQ\n\n29\n\nWE\n\n0\n1\n\na read port is made of a\nselect signal, a MUX, and a\ndata output\n12\n\n\fWriting\n● For the write port, we only want to write to one register at a time\n● We'll have a select signal again…\n\nwhen should we write to A?\nselect = 0\nDo we ALWAYS write to a\nregister?\nWE = 1\nHow about in\nbeq A, 3, top?\n\nD\n\nQ\n\n83\n\nWE\n\nwhen should we write to B? D\nselect = 1\n\nWE = 1\n\nQ\n\n29\n\nWE\n\nA\nB\n13\n\n\fClose the door\n● when a register's write enable is 0, what happens to the data?\n● so we can hook up the data input to all registers at once.\n\nData\n\nD\n\nQ\n\n83\n\nWE\n\nA\nonly the register with\nWE=1 will store the data\n\nD\n\nQ\n\n29\n\nWE\n\nB\n14\n\n\fChekhov's Gun\n● there's a component we haven't seen in a while which only sends an input value to\none of its outputs (demux)\nD\n\nWE\n0\nWE\n\n1\n0\n\nQ\n\n83\n\nWE\n\nD\n\nWE\n0\n\na write port is made of a select signal, a data\ninput, a write enable, and some kinda logic to\nsend the write enable to one register\n\nQ\n\n29\n\nWE\n\nA\nB\n\n15\n\n\fThe Register File\n● And then, we can abstract our subcircuit to the following:\no This presumes we have 32 registers which are 32-bits in size\no (like MIPS!)\n\nWriteEnable\nWriteData\nRegister1\nRegister2\n\n32\n\n32\n5\n5\n\nRegister File\n\n32\n\nReadData1\nReadData2\n\n5\n\n16\n\n\fDiving in\n\nControl\n\n● We have a complete register file!\n● Now… let’s look more closely at building an ALU.\n\nRegister\nFile\n\nALU\n\n17\n\n\fBuilding Out a Basic ALU\nDoing the stuff.\n\n18\n\n\fStarting small, the one-bit adder\n● Who remembers how to use an adder to subtract?\n\nCarry in\nA\nB\n\n+\n\nResult\n\nCarry out\n\n19\n\n\fStarting small, the one-bit adder\n● Here is a simple ALU. It can Add A and B together.\no There are a few control signals leading into it and several outputs.\no Consider how this ALU subcircuit, as it is, can perform “A – B”\nA\nBinvert\nCarry in\nB\n\n0\n1\n\n+\n\nResult\n\nCarry out\n\nThis is a\nMUX\n20\n\n\fA basic 1-bit ALU – Addition\nBinvert\n\nCarry in\n\n0\n\n0\n\nPut it in a\nbox\n\nA\n\nResult\nB\n\n0\n1\n\n+\nCarry out\n21\n\n\fA basic 1-bit ALU – Subtraction\n2s Complement\n1- Invert\n\nBinvert\n\nCarry in\n\n1\n\n1\n\n2-Add one\n\nA\n\nResult\nB\n\n0\n1\n\n+\nCarry out\n22\n\n\fA basic 1-bit ALU – Expanding the adder\nALU – Arithmetic and Logic Unit\n● This ALU can perform the\narithmetic operations add,\nand subtract.\n● And the logic operations\nAND, OR, and NOT\n● Operation is selected by\nthe signal Operation:\n\n(2-bits)\n00 – AND; 01 – OR;\n10 – ADD; 11 – SLT\n\nBinvert\n\nCarry in Operation\n2\n\nA\n0\n\n1\nB\n\n0\n1\n\n+\n\nLess\n\nResult\n\n2\n\n3\nCarry out\n23\n\n\fA basic 1-bit ALU – Addition\nBinvert\n\nCarry in Operation\n\n0\n\n0\n\n2 10\n\nA\n\nThe other\noutputs are\nbeing\ncalculated.\n\n0\n\n1\nB\n\n0\n1\n\n+\n\nLess\n\n2\n\nResult\n\nBut not\npropagated\n\n3\nCarry out\n24\n\n\fA basic 1-bit ALU – Subtraction\nBinvert\n\nCarry in Operation\n\n1\n\n1\n\n2 10\n\nA\n0\n\n1\nB\n\n0\n1\n\n+\n\nLess\n\nResult\n\n2\n\n3\nCarry out\n25\n\n\fA basic 1-bit ALU – AND\nBinvert\n\nCarry in Operation\n\n0\n\nX\n\n2 00\n\nA\n0\n\n1\nB\n\n0\n1\n\n+\n\nLess\n\nResult\n\n2\n\n3\nCarry out\n26\n\n\fA basic 1-bit ALU – OR\nBinvert\n\nCarry in Operation\n\n0\n\nX\n\n2 01\n\nA\n0\n\n1\nB\n\n0\n1\n\n+\n\nLess\n\nResult\n\n2\n\n3\nCarry out\n27\n\n\fA basic 1-bit ALU – NAND and NOR?\nRemember Boolean algebra?\n\n𝐴 + 𝐵 = 𝐴. 𝐵\n\n𝐴𝐵 = 𝐴 + 𝐵\n\n𝐴 𝐵 𝐴 + 𝐵 𝐴 𝐵 𝐴. 𝐵\n0 0 1\n1 1 1\n0 1 0\n1 0 0\n1 0 0\n0 1 0\n1 1 0\n0 0 0\n\n𝐴 𝐵 𝐴𝐵 𝐴 𝐵 𝐴 + 𝐵\n0 0 1 1 1 1\n0 1 1 1 0 1\n1 0 1 0 1 1\n1 1 0 0 0 0\n28\n\n\fA basic 1-bit ALU – NAND\nWe need to\nadd another\ninverter and\nmutex\n\nAinvert Binvert\n1\n\nA\n\nCarry in Operation\n\n1\n\nX\n\n0\n\n2 01\n\n0\n\n1\n1\nB\n\n0\n1\n\n+\n\nLess\n\nResult\n\n2\n\n3\nCarry out\n29\n\n\fA basic 1-bit ALU – NOR\nAinvert Binvert\n1\n\nA\n\nCarry in Operation\n\n1\n\nX\n\n0\n\n2 00\n\n0\n\n1\n1\nB\n\n0\n1\n\n+\n\nLess\n\nResult\n\n2\n\n3\nCarry out\n30\n\n\fA basic 1-bit ALU, with overflow detection\n● This ALU can detect overflow\n\nAinvert\n\n● Also allows to perform the SLT operation!\n● Remember the slt instruction?\nblt t0, t1, label\no It’s equivalent to\nslt at, t0, t1\nbnq at, zero, label\n\nA\n\nBinvert\n\n0\n\nOperation\n2\n0\n\n1\n1\nB\n\n● SLT – Set if Less Then\n\no “Set” = 1 if a<b\n\nCarry in\n\n0\n1\n\n+\n\nLess\n\n2\n3\n\no “Set” = 0 if a>=b\n\nResult\n\nSet\n\nOverflow\nCarry out\n31\n\n\fA basic 1-bit ALU, with overflow detection\n● This ALU can detect overflow\n\nAinvert\n\n● Also allows to perform the SLT operation!\n● Remember the slt instruction?\nblt t0, t1, label\no It’s equivalent to\nslt at, t0, t1\nbnq at, zero, label\n\nA\n\nBinvert\n\nOperation\n2\n\n0\n\n0\n\n1\n1\nB\n\n● SLT – Set if Less Then\n\no “Set” = 1 if a<b\n\nCarry in\n\n0\n1\n\n+\n\nResult\n\n2\n\nLess\n\n3\n\no “Set” = 0 if a>=b\n\nSet\n\n● What is the propagation delay?\nOverflow\nCarry out\nAssume: Not: 2ns, Mux: 6ns,\nAdder: 10ns, AND\/OR\/XOR: 4ns\n\n(26ns)\n\n32\n\n\fBuilding it up!\n● Combine multiple 1-bit ALUs\n\n● We can combine the Binvert and\nCarry in signals\no They are used simultaneously\nfor subtractions\no Otherwise, we don’t care about\nthe Carry in\n\nAinvert Bnegate\n1\n0\n\nCarry in can be\nconnected to\nBinvert\n\nA0\nB0\n\nCarry in\n\nLess\n\nCarry out\n\nA1\nB1\n\nCarry in\n\n0\n\nALU 0\n\nALU 1\n\nLess\n\nCarry out\n\nA2\nB2\n\nCarry in\n\n0\n\nALU 2\n\nLess\n\nCarry out\n\nA31\nB31\n\nCarry in\n\n0\n\nLess\n\nALU 31\nCarry out\n\n2\n\nOperation\n10\n\nResult0\n\n1\n\nResult1\n\n0\n\nResult2\n\n0\n\nResult31\nSet\n\n0\n\nOverflow\n\n1\n\n33\n\n\fImplementing SLT\n● SLT uses subtraction\nslt at, t0, t1\nt0<t1: t0-t1<0\no Set is 1\n● Note how Set is connected to\nALU0’s Less input\n\nAinvert Bnegate\n1\n0\nA0\nB0\n\nCarry in\n\nLess\n\nCarry out\n\nA1\nB1\n\nCarry in\n\n0\n\n● Could we use Result31 instead?\no No, note how the output is 0,\nnot 1\n\n2\n\nALU 0\n\nALU 1\n\nLess\n\nCarry out\n\nA2\nB2\n\nCarry in\n\n0\n\nALU 2\n\nLess\n\nCarry out\n\nA31\nB31\n\nCarry in\n\n0\n\nLess\n\nALU 31\nCarry out\n\nOperation\n11\n\nResult0\n\n1\n\nResult1\n\n0\n\nResult2\n\n0\n\nResult31\nSet\n\n0\n\nOverflow\n\n1\n\n34\n\n\fA 32-bit ALU\nAinvert Bnegate\n\n4 ALU operation\nA\n\n32\n\n32\nALU\n\nB\n\n32\n\n2\n\nA0\nB0\n\nCarry in\n\nLess\n\nCarry out\n\nOperation\n\nResult0\n\nALU 0\n\nZero detection is so\ncommon that is usually\nsupported by ALUs\nE.g. beq, bne\n\nA1\nB1\n\nCarry in\n\nResult\nZero\n\n0\n\nLess\n\nCarry out\n\nOverflow\n\nA2\nB2\n\nCarry in\n\n0\n\nLess\n\nA31\nB31\n0\n\nLess\n\nResult1\n\nALU 1\n\nZero\n\nResult2\n\nALU 2\nCarry out\n\nCarry in\n\nALU 31\nCarry out\n\nResult31\nSet\n\nOverflow\n\nAs we saw when we\ntalked about overflow.\nIt can be detected in\nthe MSB\n35\n\n\fA basic 32-bit ALU?\n● ALUs are many times in the\nreal world built as multiple\n1-bit ALUs.\no Called bit-slicing\n● However, we can happily\nlive in our ideal world for a\nbit longer!\n● We can build it much like\nthe 1-bit model, but just tell\nLogisim to make the\ncomponents’ “Data Size”\n32-bits.\no i.e., for your project.\n● Whew!\n\nAinvert Bnegate\n1\nA\n\nOperation\n\n1\n\n2 00\n\n0\n\n0\n\n1\n1\nB\n\n+\n\n0\n1\n\nResult\n\n2\n\n<\n\n3\n\nCarry out\nOverflow\n\n36\n\n\fZooming Out Again\n● Now, how to we wire up the Register File with the ALU??\no How do we know which registers to use?\no How do we pull in instructions?\n● Tune in next time for…\no Control and Datapath\n4 ALU operation\nA\n\nWriteEnable\nWriteData\nRegister1\nRegister2\nWriteRegister\n\n32\n\n32\n\n5\n5\n\nRegister File\n\n32\n\n32\n\n32\nALU\n\nReadData1\n\nB\n\n32\n\nResult\nZero\nOverflow\n\nReadData2\n\n5\n37\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":147,"segment": "self_training_1", "course": "cs0447", "lec": "lec03", "text":"#3\n\nPrograms,\nInstructions, and\nRegisters\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce Childers,\nDavid Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fWhat do I need to know now!\nThe classes will be recorded!\n● You will be able to access the videos online\no They are for your personal use only!\no Do not distribute them!\n● You don’t need to turn on your camera\no If you do, you may be recorded\n● You can ask questions via text!\no Chat is great for that. If I don’t stop and read your questions, ask them again\no But feel free to interrupt me at any point.\n\n2\n\n\fClass announcements\n● Don’t forget!\no Use the MARS I have on the course website:\n● It has been modified!!\n\n3\n\n\fPrograms and Instructions\n\n4\n\n\fWhat are they?\n● An instruction is a single, simple operation for the computer to carry out, such as:\no \"add two numbers together\"\no “copy a number from one location to another\"\no \"go to a different place in the program\"\no \"search a string for a character“\n● A program is a series of these tiny instructions\no How do we create these instructions?\no And how does the computer \"understand\" them?\n▪ does the computer understand anything?\n\n5\n\n\fMachine language and Assembly language\n● Machine language instructions are the patterns of bits that a processor\nreads to know what to do\n● Assembly language (or \"asm\") is a human-readable, textual\nrepresentation of machine language\nMIPS asm\n\nMIPS machine language\n\nlw t0, 1200(t1)\n\n100011 01001 01000 0000010010110000\nlw\nt1\nt0\n1200\n\nadd t2, s2, t0\n\n000000 10010 01000 01010 00000 100000\n<math>\ns2\nt0\nt2\nn\/a\nadd\n\nsw t2, 1200(t1)\n\n101011 01001 01010 0000010010110000\nsw\nt1\nt2\n1200\n6\n\n\fWhat can a CPU do?\n\nMaths\n\nLoad\/Store things\nfrom\/to memory\n\nGo execute\nsomewhere else\n\nExample: Count up to 10\n1. Load variable from memory (memory)\n2. If value equals 10 stop (cond. Go execute)\n3. Add 1 to variable value (maths)\n4. Place new value in the variable (memory)\n5. Go back to the top (incond. Go execute)\n6. stop\n\n7\n\n\fIS THAT ENOUGH?\n\n8\n\n\fCPUs are WAY\nmore complex\n\nRemember the Turing machine?\n\n● Has infinite memory represented by a single tape.\no A head moves along the tape and can read and write values.\n\n▪ The movement (left or right) is based upon the value read and the state of the machine.\n\n● The machine:\n1. Reads\/writes the memory (tape)\n2. Compares that data and decides where to move (execute) next\n● Everything that can be computed, is computed by a Turing machine\nRulebook 1\n\n➔\n\nRulebook 20\n\nRead\n\nWrite\n\nMove\n\nNext\n\nRead\n\nWrite\n\nMove\n\nNext\n\n0\n\n1\n\n\n\n20\n\n0\n\n0\n\n→\n\n15\n\n1\n\n0\n\n→\n\n12\n\n1\n\n1\n\n→\n\n15\n9\n\n\fHow a CPU runs a program\n\n1. read an instruction\n2. do what it says\n3. go to step 1\n\no ...okay there's a little more to it than that\n\n10\n\n\fHow a CPU runs a program\n\nProgram\n3\n\n5\n\n8\n\ninstruction\n\nControl\n\n\"C = A + B\"\n\nA\n\nB\n\nC\n\nRegisters\n\n+\nDatapath\n\nProgram\n\nPersistent\nStorage\n\nMemory\n\n…and repeat!\n\nProcessor\n\n11\n\n\fISAs\n\n12\n\n\fInstruction Set Architecture (ISA)\n● An ISA is the interface that a CPU presents to the programmer\no When we say “architecture”, this is what we mean\n● ISAs define:\no WHAT the CPU can do (add, subtract, call functions, etc.)\no WHAT registers it has (we'll get to those)\no WHAT the machine language is\n▪ Machine language: the bit patterns used to encode instructions\n\n● ISAs do not define:\no HOW the CPU does it\no HOW to design the hardware!\n▪ …if there's any hardware at all\n– Java\n\nWHY? O.o\n13\n\n\fISAs example: x86\n● Descended from 16-bit 8086 CPU from 1978\no Implemented in a rush by intel\n● Extended to 32 bits, then 64\n● Each version can run all programs from the\nprevious version\no you can run programs written in 1978 on\na brand new CPU!\n● So why don't we learn x86 in this course?\no It can do a lot of things\no Its machine language is very complex\no Making an x86 CPU is… difficult\no Ultimately, we would waste a ton of time\n\n14\n\n\fAll three processors run the exact same programs…\n● but they're TOTALLY different on the inside\n\nI’m an x86\nCPU!\n\nIntel Core i7\n\nVIA Nano\n\nI’m an\nx86\nCPU!\n\nAMD Zen\n\nI’m an x86 CPU!\n15\n\n\fKinds of ISAs: CISC\n● CISC: \"Complex Instruction Set Computer\"\n● ISA designed for humans to write asm\no from the days before compilers!\n● lots of instructions and ways to use them\n● complex (multi-step) instructions to shorten\nand simplify programs\no \"search a string for a character\"\no \"copy memory blocks\"\no \"check the bounds of an array access\"\n● x86 is very CISCy\nprguitarman.com\n\n16\n\n\fKinds of ISAs: RISC\n● RISC: \"Reduced Instruction Set Computer\"\n● ISA designed to make it easy to:\no build the CPU hardware\no make that hardware run fast\no write compilers that make machine code\n● a small number of instructions\n● instructions are very simple\n● MIPS is very RISCy\n● MIPS and RISC were the original RISC architectures\ndeveloped at two universities in California\no the research leads were… Patterson and Hennessy…\n\n17\n\n\fPopular ISAs today\n● x86 (these days, it’s x86-64 or “amd64”)\no most laptops\/desktops\/servers have one\no (modern x86 CPUs are just RISC CPUs that can read the weird x86 instructions)\n▪ (unless you ask Intel, they will say otherwise ☺)\n● ARM\no almost everything else has one\no ARMv8 (AArch64) is pretty similar to MIPS!\n▪ More than to ARMv7: “the main similarity between ARMv7 and ARMv8 is the\nname” – Comp. Org. & Design page 159\n● Everything else: Alpha, Sparc, POWER\/PPC, z, z80, 29K, 68K, 8051, PIC, AVR, Xtensa,\nSH2\/3\/4, 68C05, 6502, SHARC, MIPS...\no microcontrollers, mainframes, some video game consoles, and historical\/legacy\napplications\n● despite its limited use today, MIPS has been incredibly influential!\n18\n\n\fThe MIPS ISA:\nRegisters\n\n19\n\n\fThe registers\n\nGeneral Purpose\n#\n\nName\n\nErm…\n\n0\n\nzero\n\nAvoid Totally\n\n1\n\nat\n\n2, 3\n\nv0, v1\n\n4..7\n\na0..a3\n\n8..15\n\nt0..t7\n\n16..23\n\ns0..s7\n\n24, 25\n\nt8, t9\n\nDon’t need\nthese\n\n26, 27\n\nk0, k1\n\n28\n\ngp\n\nHI\n\nFor later ;)\n\n29\n\nsp\n\nLO\n\nDon’t matter\n\n30\n\nfp\n\nPC\n\nAlso for later ;)\n\n31\n\nra\n\n● Registers are a small and fast temporary memory\ninside the CPU\n● The CPU can only operate (add, etc.) on data in\nregisters\n● MIPS has 32 registers, and each is 32 bits (one word)\n● The registers are numbered 0 to 31…\no …but they also have nice names\n▪ The MARS version on the course website is modified\n– so you don't have to use them $ signs in the registers\n– $s0, $t1 vs. s0, t1\nUsed for multiplication\n(more on that later)\nKeeps track of the next\ninstruction to be executed\n\nSpecial\npurpose\n\nUsed for\nfunctions\n\nUsed for almost\neverything else\n\n20\n\n\fThe juggler\n● Registers are… like….. hands\n● You have a limited number and they can only hold small things\n● Your program's variables primarily live in memory\n● The registers are just a temporary stopping point for those values\n\nIMPORTANT!\nless important\n3\n\n5\n\n8\n\nRegisters\n\nA\n\nB\n\nC\n\nMemory\n21\n\n\fReally, you don't have that many\n● You cannot write every program using only registers\no Don't try to\n▪ please.\n● Every piece of your program has to SHARE the registers.\no Unlike high-level languages\no Where everyone gets their own locals\no Not in assembly!\n\n22\n\n\fThe s (saved) and the t(temporary) registers\n● There are ten temporary registers, t0 through t9\no These are used for temporary values – values that are used briefly\n\nName\nt0..t9\n● There are 8 saved registers, s0 through s7\no These are kinda like… local variables inside a function\n\nName\ns0..s7\n23\n\n\fWhen to use each\n● We'll learn more about this in the coming weeks\n● Rule of thumb:\no Use t register\no Unless you need the value to persist when calling functions\n▪ ok that's not too clear yet\n● 90% (made up percentage) of your code will use s and t registers\n\n24\n\n\fThe MIPS ISA:\nWHAT can it do?\n\n25\n\n\fWe have a semester to learn ;)\n\nFor now:\nli\n→ Loads a number (Immediate)\nadd → It adds 2 numbers\nsub → It subtracts 2 numbers\nmul → It multiplies 2 numbers\ndiv → It multiplies 2 numbers\nmove → It … ermmm… COPIES a number\n26\n\n\fExample: Loading immediates and adding them\n\ns0 = 3;\nli s0, 3\ns1 = 5;\nli s1, 5\ns2 = s0 + s1; add s2, s0, s1\n● li stands for \"load immediate.\" what does it look like it does?\no \"immediate\" means \"number inside the instruction\"\n● add, uh, makes coffee. ¬_¬\n● Just like in Java, C, whatever: the destination is on the left\n\n27\n\n\fExample: Complex expression\n● We can re-use registers (t0 in the example) as a temporary\no For example, say we had a longer expression:\n\ns4 = (s0 + s1 – s2) * s3\n\n● What does algebra say about what order we should do this in?\n\nadd t0, s0, s1\nsub t0, t0, s2\nmul s4, t0, s3\n\n28\n\n\fYou will be thinking like a compiler\n● Writing ASM is a different way of programming than you're used to\n● To make the transition easier, try to reduce your cognitive load\no cognitive load is \"the set of ideas you have to keep in your mind to perform some\ntask.\"\no high-level languages (HLLs) reduce cognitive load by hiding the machine code,\nusing a compiler to write it for you\n● you can do the same thing: think about how to write a program in e.g. C, and then\nturn that into asm\n\nadd c, a, b\n\nc=a+b\nadd s2, s0, s1\n29\n\n\fThe other way around\n● going the other way is also useful\n\nmul t0, s2, 33\ndiv t1, s3, s4\nsub s1, t0, t1\n\nhow would we write this in C\/Java?\n\nt0 = s2 * 33\nt1 = s3 \/ s4\ns1 = t0 – t1\n\nor, if we rolled it all together,\n\ns1 = (s2 * 33) – (s3 \/ s4)\nthat's what this asm does\n\n30\n\n\fWhy do you need to know this?\n● CS0447 is about building a mental model of how a computer works\n\n● Understanding what is happening when you write code or run programs gives you a\nmuch deeper understanding\no \"why should I avoid using this programming language feature in this speed-critical\npart of my code?\"\no \"why wouldn't this crazy idea be very fast on current architectures?\"\no \"this program is breaking in a really confusing way, I have to look at the asm to\ndebug it\"\n● This stuff is specialized but hey you're majoring\/minoring in it right\n\n31\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":148,"segment": "unlabeled", "course": "cs0441", "lec": "lec15", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #15: Recursion and Structural Induction\n\nBased on materials developed by Dr. Adam Lee\n\n\fThere are many uses of induction in computer\nscience!\nProof by induction is often used to reason about:\nl Algorithm properties (correctness, etc.)\nl Properties of data structures\nl Membership in certain sets\nl Determining whether certain expressions are well-formed\nl…\n\nTo begin looking at how we can use induction to prove\nthe above types of statements, we first need to\nlearn about recursion\n\n\fSometimes, it is difficult or messy to define\nsome object explicitly\nRecursive objects are defined in terms of (other\ninstances of) themselves\nWe often see the recursive versions of the following\ntypes of objects:\nl Functions\nl Sequences\nl Sets\nl Data structures\n\nLet’s look at some examples…\n\n\fRecursive functions are useful\nWhen defining a recursive function whose domain is\nthe set of natural numbers, we have two steps:\n1. Basis step: Define the behavior of f(0)\n2. Recursive step: Compute f(n+1) using f(0), …, f(n)\nDoesn’t this look a little bit like strong induction?\n\nExample: Let f(0) = 3, f(n+1) = 2f(n) + 3\nl\nl\nl\nl\nl\n\nf(1) = 2f(0) + 3 = 2(3) + 3 = 9\nf(2) = 2f(1) + 3 = 2(9) + 3 = 21\nf(3) = 2f(2) + 3 = 2(21) + 3 = 45\nf(4) = 2f(3) + 3 = 2(45) + 3 = 93\n…\n\n\fSome functions can be defined more precisely\nusing recursion\nExample: Define the factorial function F(n) recursively\n1. Basis step: F(0) = 1\n2. Recursive step: F(n+1) = (n+1) × F(n)\n\nNote: F(4) = 4 × F(3)\nThe recursive definition\n= 4 × 3 × F(2)\navoids using the “…”\nshorthand!\n= 4 × 3 × 2 × F(1)\n= 4 × 3 × 2 × 1 × F(0)\n= 4 × 3 × 2 × 1 × 1 = 24\nCompare the above definition our old definition:\nl\n\nF(n) = n × (n-1) × … × 2 × 1\n\n\fIt should be no surprise that we can also define\nrecursive sequences\nExample: The Fibonacci numbers, {fn}, are defined as follows:\nl f0 = 1\nl f1 = 1\nl fn = fn-1 + fn-2\n\nThis is like strong induction, since we need\nmore than fn-1 to compute fn.\n\nCalculate: f2, f3, f4, and f5\nl f2 = f1 + f0 = 1 + 1 = 2\nl f3 = f2 + f1 = 2 + 1 = 3\nl f4 = f3 + f2 = 3 + 2 = 5\nl f5 = f4 + f3 = 5 + 3 = 8\n\nThis gives us the sequence {fn} = 1, 1, 2, 3, 5, 8, 13, 21, 34, …\n\n\fRecursion is used heavily in the study of strings\nLet: ∑ be defined as an alphabet\nl Binary strings: ∑ = {0, 1}\nl Lower case letters: ∑ = {a, b, c, …, z}\n\nWe can define the set ∑* containing all strings over the\nλ is the empty string\nalphabet ∑ as follows:\ncontaining no characters\n1. Basis step: λ ∈ ∑*\n2. Recursive step: If w ∈ ∑* and x ∈ ∑, then wx ∈ ∑*\n\nExample: If ∑ = {0, 1}, then ∑* = {λ, 0, 1, 01, 11, …}\n\n\fThis recursive definition allows us to easily\ndefine important string operations\nDefinition: The concatenation of two strings can be\ndefined as follows:\n1. Basis step: if w ∈ ∑*, then w⋄λ = w\n2. Recursive step: if w1 ∈ ∑*, w2 ∈ ∑*, and x ∈ ∑, then\nw1⋄(w2x) = (w1⋄w2)x\n\nExample: Concatenate the strings “Hello” and “World”\n1. Hello⋄World = (Hello⋄Worl)d\n2.\n= (Hello⋄Wor)ld\n3.\n= (Hello⋄Wo)rld\n4.\n= (Hello⋄W)orld\n5.\n= (Hello⋄λ)World\n6.\n= HelloWorld\n\n\fThis recursive definition allows us to easily\ndefine important string operations\nDefinition: The length l(w) of a string can be defined\nas follows:\n1. Basis step: l(λ) = 0\n2. Recursive step: l(wx) = l(w) + 1 if w ∈ ∑* and x ∈ ∑\n\nExample: l(1001) = l(100) + 1\n= l(10) + 1 + 1\n= l(1) + 1 + 1 + 1\n= l(λ) + 1 + 1 + 1 + 1\n=0+1+1+1+1\n=4\n\n\fWe can define sets of well-formed formulae\nrecursively\nThis is often used to specify the operations permissible in\na given formal language (e.g., a programming language)\n\nExample: Defining propositional logic\n1. Basis step: ⟙, ⟘, and s are well-formed propositional logic\nstatements (where s is a propositional variable)\n2. Recursive step: If E and F are well-formed statements, so are\n➣\n➣\n➣\n➣\n➣\n\n(¬E)\n(E ∧ F)\n(E ∨ F)\n(E → F)\n(E ↔ F)\n\n\fExample\nQuestion: Is ((p ∧ q) → (((¬r) ∨ q) ∧ t)) well-formed?\nl\nl\nl\nl\nl\n\nBasis tells us that p, q, r, t are well-formed\n1st application: (p ∧ q), (¬r) are well-formed\n2nd application: ((¬r) ∨ q) is well-formed\n3rd application: (((¬r) ∨ q) ∧ t)\n4th application: ((p ∧ q) → (((¬r) ∨ q) ∧ t)) is well-formed\n\n✔\n\n\fIn-class exercises\nProblem 1: Construct a recursive definition of the\nsequence 𝑎# where the 𝑛$% term is a natural number\ncomputed by adding the 𝑛 − 1 $% term to the square\nof the 𝑛 − 3 $% term. Assume that the first three\nterms of this sequence are 1, 1, 1.\nProblem 2: Top Hat\n\n\fLike other forms of induction, structural induction\nrequires that we consider two cases\nBasis step: Show that the result holds for the objects\nspecified in the basis case of the recursive definition\nRecursive step: Show that if the result holds for the\nobjects used to construct new elements using the\nrecursive step of the definition, then it holds for the\nnew object as well.\nTo see how this works, let’s revisit string length…\n\n\fRecall from earlier…\nDefinition: The length l(w) of a string can be defined\nas follows:\n1. Basis step: l(λ) = 0\n2. Recursive step: l(wx) = l(w) + 1 if w ∈ ∑* and x ∈ ∑\n\nExample: l(1001) = l(100) + 1\n= l(10) + 1 + 1\n= l(1) + 1 + 1 + 1\n= l(λ) + 1 + 1 + 1 + 1\n=0+1+1+1+1\n=4\n\n\fProve that l(x⋄y) = l(x) + l(y) for x,y ∈ ∑*\nP(n) ≡ l(x⋄y) = l(x) + l(y) whenever x ∈ ∑* and l(y) = n\nBase case: P(0): l(x⋄λ) = l(x) = l(x) + 0 = l(x) + l(λ) ✔\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn Consider the string x⋄ya, where x,y ∈ ∑*, a ∈ ∑ and l(y) = k\nn l(x⋄ya) = l(x⋄y) + 1 by the recursive definition of l()\nn\n= l(x) + l(y) + 1 by the I.H.\nn Since l(ya) = l(y) + 1 by the recursive defintion of l(), we have\nthat l(x⋄ya) = l(x) + l(ya), where ya is a string of size k+1\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by structural induction ❏\n\n\fMany common data structures used in computer\nscience have recursive definitions\nExample: Rooted Trees\n\nBase step: A single node is a rooted tree\nRecursive step: If T1, T2, …, Tn are disjoint rooted trees\nwith roots r1, r2, …, rn then introducing a new root r\nconnected to r1, r2, …, rn forms a new rooted tree.\n\n\fExample Rooted Trees\nBase case:\nOne application:\n\n…\n\nTwo applications:\n\n…\n\n…\n\n\fMany common data structures used in computer\nscience have recursive definitions\nExample: Extended binary trees\n\nBase step: The empty set is an extended binary tree\nRecursive step: If T1 and T2 are disjoint extended binary\ntrees with roots r1 and r2, then introducing a new root\nr connected to r1 and r2 forms a new extended binary\ntree.\n\n\fExample Extended Binary Trees\nBase case:\n\n∅\n\nStep 1:\n\nStep 2:\n\nStep 3:\n\n…\n\n…\n\n\fMany common data structures used in computer\nscience have recursive definitions\nExample: Full binary trees\n\nBase step: A single root node r is a full binary tree\nRecursive step: If T1 and T2 are disjoint full binary trees\nwith roots r1 and r2, then introducing a new root r\nconnected to r1 and r2 forms a new full binary tree.\n\n\fExample Full Binary Trees\nBase case:\nStep 1:\n\nStep 2:\n\n…\n\n\fTrees are used to parse expressions\n\n456\n\n(((3 + 6) × 7) – (4 + 2)) × 8\n\n×\n57\n63\n\n-\n\n8\n\n×\n\n6\n\n+\n\n9\n+\n3\n\n7\n6\n\n4\n\n2\n\n\fTrees are used to enable fast searches\nConsider the set S = {56, 22, 34, 89, 99, 77, 16}\n34 < 56\n\n22\n\n34 > 22\n\n34\n\n16\n\n56\n\n262 > 56\n\n89\n77\n\n262 > 89\n\n99\n\nQuestion: Is 34 ∈ S?\n\nQuestion: Is 262 ∈ S?\n\nYES!\n\nNO!\n\n\fAs with other recursively defined objects, we can\ndefine many properties of trees recursively\nDefinition: Given a tree T, we can define the height of\nT recursively, as follows:\n1. Basis step: If T consists only of the root node r, then h(T) = 0\n2. Recursive step: If T consists of a root r that connects to\nsubtrees T1, …, Tn, then h(T) = 1 + max(h(T1), …, h(Tn))\n\nExample: What is the height of this tree T?\nh(T) = 1 + max(h(L), h(R)) = 3\nh(L) = 1 + h(L1) = 2\nh(L1) = 1 + max(h(L11), h(L12)) = 1\nh(L11) = 0\n\nh(R) = 1 + h(R1) = 1\nh(R1) = 0\nh(L12) = 0\n\n\fIf T is a full binary tree, then the number of nodes in T\n(denoted n(T)) is less than or equal to 2h(T)+1-1\nClaim: n(T) ≤ 2h(T)+1-1\nBase case: T contains only a root node. In this case n(T) = 1 and\nh(T) = 0. Note that 20+1-1 = 1, so the claim holds.\nI.H.: Assume that claim holds for a tree of height k\nInductive step: Show that the claim holds for trees of height k+1\nn Let T1 and T2 be disjoint full binary trees of height k\nn By the I.H., n(T1) ≤ 2h(T1)+1-1 and n(T2) ≤ 2h(T2)+1-1\nn Let r be a unique root element, and let T be the tree formed\nusing r as a root, T1 as the left subtree of r, and T2 as the right\nsubtree of r\n\nr\nT1\n\nT2\n\n\fIf T is a full binary tree, then the number of nodes in T\n(denoted n(T)) is less than or equal to 2h(T)+1-1\nr\nT1\n\nT2\n\nInductive step (cont.): We have that\nn n(T) = 1 + n(T1) + n(T2)\nn\n≤ 1 + 2h(T1)+1 - 1 + 2h(T2)+1 - 1\nn\n≤ 2h(T1)+1 + 2h(T2)+1 – 1\nn\n≤ 2 ×max(2h(T1)+1, 2h(T2)+1) – 1\nn\n≤ 2 ×2max(h(T1), h(T2))+1 – 1\nn\n≤ 2 ×2h(T) – 1\nn\n≤ 2h(T)+1 – 1\n\nby recursive formula of n(T)\nby I.H.\nsum of 2 terms ≤ twice larger term\nmax(2x, 2y) = 2max(x,y)\nby recursive def’n of h(T)\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by structural induction ❏\n\n\fIn-class exercises\nProblem 3: Use structural induction to prove that\nchecking whether some number is contained in a\nbinary search tree T involves at most h(T)+1\ncomparison operations.\n\n\fFinal Thoughts\nn Structural induction can be used to prove properties\nof recursive\nl Functions\nl Sequences\nl Sets\nl Data structures\n\nn Next time, we start learning about counting and\ncombinatorics (Section 6.1)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":149,"segment": "unlabeled", "course": "cs0447", "lec": "lec0E", "text":"#E\n\nMinimization and\nK-maps\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n● Check the schedule\no I’ll post examples of circuits and suggestions of exercises\n● Labs will be submitted via Gradescope\n\n2\n\n\fHow ICs are made in 3 slides\n(another For Fun™ section)\n\n3\n\n\fHow ICs (integrated circuits) are made\n● silicon is purified and grown into a\nmonolithic crystal (extremely expensive)\n● this is sliced thinly to make wafers\n\no gooey caramel is put between them to make stroopwafel\n\n● a series of complicated photochemical\nprocesses do things like:\no change its electrical properties\no make wires to connect things\no make inert insulating layers\n\n4\n\n\fHow ICs (integrated circuits) are made\n● many ICs are printed on one wafer\n● the wafers are diced (chopped up)\n● the ICs are tested\n● the ICs are mounted in a package ->\n● they're tested again\n● then they're ready to sell\n\n5\n\n\fManufacturing yield\n● ICs are tiny and complex\n● silicon crystals can have defects\n● a tiny speck of dust during production can ruin an entire chip\n● the yield is the percentage of usable chips\no bigger chips have smaller yields: more opportunities for mistakes!\n● the size of the silicon is the biggest factor in the price of an IC\no huge ICs (several cm on each side!), such as very high-resolution camera\nsensors, can cost tens of thousands of dollars!\n● manufacturers can also bin resulting chips\no bug-free ones can be sold as Core i7s for lotsa money\no slightly malformed ones can be sold as Core i5s and i3s\no the ones they sweep off the floor are the Celerons\n\n6\n\n\fLogic Minimization\n\n7\n\n\fSilicon is expensive, rocks are slow\n● Logic minimization means using the smallest number of gates\/transistors\npossible to implement a boolean function\no a boolean function is anything we've talked about\no it has boolean inputs, and boolean outputs\n▪ Less inputs can also improve speed\n● Fewer transistors means:\no smaller area:\n▪ cheaper chips!\n▪ more stuff on one chip!\n▪ smaller chance of manufacturing defects!\no less gate delay:\n▪ faster circuits!\n\n8\n\n\fMinimizing Booleans\n● How do we minimize Boolean functions?\no Logical adjacency!\n● If in two Boolean terms being ORed only one of the variables changes then it\ncan be removed\n\nഥ + 𝐀𝐁 = 𝐀 𝑩\nഥ +𝑩 =𝑨\no 𝑸 = 𝐀𝑩\nഥ = 𝑨𝑩\nഥ = 𝑨𝑩\nഥ 𝑪 + 𝑨𝑩\nഥ𝑪\nഥ 𝑪+𝑪\nഥ\no 𝑸 = 𝑨𝑩\n● To minimize a Boolean expression\no Find the terms where only one variable changes\no Eliminate that variable\n\n9\n\n\fA first try\n● The truth table for a two-input 1-bit\nmultiplexer.\n\nA\n\nA\nQ\n\nB\n\nQ\n\nB\nS=0\n\nS=1\n\nS Q\n\nS A B Q\n\n0 A\n\n0 0 0 0\n\n0 A\n\n0 0 1 0\n\n0 A\n\n0 1 0 1\n\n0 A\n\n0 1 1 1\n\n1 B\n\n1 0 0 0\n\n1 B\n\n1 0 1 1\n\n1 B\n\n1 1 0 0\n\n1 B\n\n1 1 1 1\n10\n\n\fA first try\n● The truth table for a two-input 1-bit multiplexer.\n\nS A B Q\n0 0 0 0\n0 0 1 0\n\nഥ 0 1 0 1\n𝐒ത𝐀𝑩\n𝐒ത𝐀𝐁 0 1 1 1\n1 0 0 0\n\nഥ𝐁 1 0 1 1\n𝐒𝑨\n1 1 0 0\n\n𝐒𝐀𝐁 1 1 1 1\n\nഥ + 𝐒ത𝐀𝐁 + 𝐒𝑨\nഥ 𝐁 + 𝐒𝐀𝐁\n𝑸 = 𝐒ത𝐀𝑩\nAre there any adjacencies?\nWell… I guess we don’t need\nthose variables\n\n𝑸 = 𝐒ത𝐀 + 𝐒ത𝐀 + 𝐒𝐁 + 𝐒𝐁\nThis seems easy…\n\n𝑸 = 𝐒ത𝐀 + 𝐒𝐁\n11\n\n\fIt makes sense, right?\n● We know that if S=0 then the output is A\n● We know that if S=1 then the output is B\no We don’t care about one of the variables\n● So, let’s solve this truth table\n1. find all rows with an output of 1\n2. for each one, write an AND of all the inputs,\nwith NOTs on the 0s\n3. eliminate duplicate terms\n4. OR the remaining terms together\n\nത + 𝐒𝐁\n𝐐 = 𝐒𝐀\n\n● Getting the sum-of-products\no an OR of multiple ANDed terms\n\nS A B Q\n0 0 X 0\n\n0 0 X 0\n\n𝐒ത𝐀 0 1 X 1\n𝐒ത𝐀 0 1 X 1\n1 X 0 0\n\n𝐒𝐁 1 X 1 1\n1 X 0 0\n\n𝐒𝐁 1 X 1 1\n12\n\n\fTurning that expression into gates\n● making 𝐐 = 𝐒ത𝐀 + 𝐒𝐁 into gates is pretty straightforward:\n\nS\n\nA\nQ\nB\n\n13\n\n\fMinimization using Boolean\nAlgebra\n\n14\n\n\fBoolean algebra\n● Idempotent\no 𝑎. 𝑎 = 𝑎 + 𝑎 = 𝑎\n● Commutative\no 𝑎. 𝑏 = 𝑏. 𝑎\no 𝑎+𝑏 =𝑏+𝑎\n● Associative\no 𝑎. 𝑏. 𝑐 = 𝑎. 𝑏 . 𝑐\no 𝑎+ 𝑏+𝑐 = 𝑎+𝑏 +𝑐\n● Distributive\no 𝑎. 𝑏 + 𝑐 = 𝑎. 𝑏 + 𝑎. 𝑐\no 𝑎 + 𝑏. 𝑐 = 𝑎 + 𝑏 . (𝑎 + 𝑐)\n\n● De Morgan’s laws\no 𝑎. 𝑏 = 𝑎ത + 𝑏ത\no 𝑎 + 𝑏 = 𝑎.\nത 𝑏ത\n● Other\no 𝑎 + 𝑎. 𝑏 = 𝑎\no 𝑎. 𝑎 + 𝑏 = 𝑎\no 𝑎ത = 𝑎\no 𝑎 + 𝑎ത = 1\no 𝑎. 𝑎ത = 0\n\n15\n\n\fWhat if the function is more complex?\n● if we use that method on the Cout of a full adder:\n\nA\n0\n0\n0\n0\n1\n1\n1\n1\n\nB\n0\n0\n1\n1\n0\n0\n1\n1\n\nഥ 𝐁𝐂𝐢𝐧 + 𝐀𝑩\nഥ 𝐂𝐢𝐧 + 𝐀𝐁𝐂𝐢𝐧 + 𝐀𝐁𝐂𝐢𝐧\nCin Cout 𝐂𝐨𝐮𝐭 = 𝐀\n0\n0\n● It feels too complex, somehow\n1\n0\n● each NOT gate uses 2 transistors\n0\n0\n● each AND\/OR gate uses 6\nഥ 𝐁𝐂𝐢𝐧 ● this will use 72 transistors\n1\n1 𝐀\no just for the carry\n0\n0\nഥ 𝐂𝐢𝐧 o of one one-bit addition\n1\n1 𝐀𝑩\n0\n1 𝐀𝐁𝐂𝐢𝐧\n1\n1 𝐀𝐁𝐂𝐢𝐧\n16\n\n\fMinimizing equations –WHYYYYY!!\n𝑨𝑩𝑪 + 𝑨𝑩𝑪 + 𝑨𝑩𝑪 + 𝑨𝑩𝑪\nUsing distributivity 𝐴𝐵 + 𝐴𝐶 = 𝐴(𝐵 + 𝐶) :\n𝐴𝐵 𝐶 + 𝐶 + 𝐴𝐵𝐶 + 𝐴𝐵𝐶\n\n72 transistors\n\nAdjacency #1 tackled:\n𝐴𝐵 + 𝐴𝐵𝐶 + 𝐴𝐵𝐶\nWhat about now? Two variables change \n\n17\n\n\fMinimizing equations –WHYYYYY!!\n𝑨𝑩𝑪 + 𝑨𝑩𝑪 + 𝑨𝑩𝑪 + 𝑨𝑩𝑪\nAdding (ORing) the same term multiple times is ok:\n𝐴𝐵 + 𝐴𝐵𝐶 + 𝐴𝐵𝐶 + 𝐴𝐵𝐶 + 𝐴𝐵𝐶\nNote the two adjacencies :D\n𝐴𝐵 + 𝐴𝐶 𝐵 + 𝐵 + 𝐵𝐶 𝐴 + 𝐴\nAdjacency #2 and #3 tackled:\n𝑨𝑩 + 𝑪𝑨 + 𝑪𝑩\n30 transistors\n18\n\n\fIt’s hard!\n\nIf only we had a better tool to help us\n\n19\n\n\fKarnaugh Maps\n\n20\n\n\fGray Code\n● Gray code is a way of encoding binary where\nonly one bit changes on each step\n● How can we construct it?\n● Let’s revisit our binary number table:\no Start by creating the first two 1-bit entries\no Mirror them and complete the next column\no Repeat!\n● See how only one bit changes from the\nlast to the first number!\n● We’ll only be using 2-bit code.\n\n000\n001\n011\n010\n110\n111\n101\n100\n\n21\n\n\fKarnaugh Maps (K-maps) – Setting up\n● Karnaugh Maps are a tool for minimizing boolean functions\n● It helps us finding adjacencies\no let's start with a function that has two inputs\nTruth Table\n\nK-map\n\nA\n0\n0\n1\n1\n\n0\n\nB\n0\n1\n0\n1\n\nQ\n1\n0\n1\n1\n\nഥ\n𝐁\n\nഥ\n0 𝐀\n\n1\n\n𝐁\n\n1 0\n1 𝐀 1 1\n\n1. Karnaugh maps are\nrepresented as a table\n2. write input values in Gray\ncode along axes.\no (there's only one input on\neach side here, it's easy)\n3. Fill in cells from truth table.\n\n22\n\n\fKarnaugh Maps (K-maps) – Finding rects\nK-map\n\nഥ\n𝐁\n\nഥ\n𝐀\n\n𝐁\n\n1 0\n𝐀 1 1\n\n3. find rectangles of 1s with these rules:\no width and height can only be 1, 2, or 4\n▪ NEVER 3\no overlapping is totally fine! it's good!\no use the biggest rectangles possible\no use the fewest rectangles possible\n\n23\n\n\fKarnaugh Maps (K-maps) – Interpreting rects\nK-map\n\nഥ\n𝐁\n\n𝐁\n\nഥ\n𝐀\n\n1 0\n𝐀 1 1\nഥ\nRed: 𝐁\n\nBlue: 𝐀\n\nഥ\n𝐐=𝐀+𝐁\n\n4. for each rectangle, look at the values of\nthe variables along the axes. some\nvariables change, and others don't.\no which variable changes in the red\nrectangle? which doesn't?\no what about the blue rectangle?\n5. each rectangle is an AND minterm\no write the variables that stay the same\nfor that rect (keeping the NOT bars)\no ignore the variables that change\n6. OR all the terms together\n7. WHEW!\n24\n\n\fI'd like to place an order for the carry-out bit\n● With more than 2 variables, put two along one axis (GRAY CODE!)\ntry to make the rectangles as big as\nC A B Co\npossible. overlap is goooood.\n\n0\n0\n0\n0\n1\n1\n1\n1\n\n0\n0\n1\n1\n0\n0\n1\n1\n\n0\n1\n0\n1\n0\n1\n0\n1\n\n0\n0\n0\n1\n0\n1\n1\n1\n\n00\n\n01\n\n11\n\n10\n\nഥ𝐁\nഥ 𝐀\nഥ 𝐁 𝐀𝐁 𝐀𝐁\nഥ Red: 𝐀𝑪\n𝐀\n0 𝐂ത\n\n0 0 1 0\n1 𝐂 0 1 1 1\n\nGreen: 𝐀𝐁\nBlue: 𝐁𝐂\n\n𝐂𝐨 = 𝐀𝑪 + 𝐀𝐁 + 𝐁𝑪\n25\n\n\fI'd like to place an order for the carry-out bit\n● With more than 2 variables, put two along one axis (GRAY CODE!)\ntry to make the rectangles as big as\nC A B Co\npossible. overlap is goooood.\n\n0\n0\n0\n0\n1\n1\n1\n1\n\n0\n0\n1\n1\n0\n0\n1\n1\n\n0\n1\n0\n1\n0\n1\n0\n1\n\n0\n0\n0\n1\n0\n1\n1\n1\n\n00\n\n01\n\n11\n\n10\n\nഥ𝐁\nഥ 𝐀\nഥ 𝐁 𝐀𝐁 𝐀𝐁\nഥ\n𝐀\n0 𝐂ത\n\n0 0 1 0\n1 𝐂 0 1 1 1\nDid you notice this? 𝐴𝐵𝐶\nThis is the term we needed to add multiple times :D\n26\n\n\fJust like a 2D RPG world map…\n● rectangles on K-maps can wrap around (left-right AND top-bottom!)\n\n00\n\n01\n\n11\n\n10\n\nഥ𝐘\nഥ 𝐗\nഥ𝐘 𝐗𝐘 𝐗𝐘\nഥ\n𝐗\nത\n0 𝐙\n\n1 1 0 1\n1 𝐙 1 0 0 1\n\nഥ\nRed: 𝐘\nഥ𝐙ത\nBlue: 𝐗\n\nഥ+𝐗\nഥ𝐙ത\n𝐐=𝐘\n\nthis is really a 2x2 rectangle.\nit's just… doing its best.\n27\n\n\fOkay, maybe it's not perfect.\n● let's try the Sum output of a full adder\n\n00\n\n01\n\n11\n\n10\n\na 1x1 rectangle\nbecomes a term that\nuses all the variables\n\nഥ𝑩\nഥ𝑪\nഥ𝐁\nഥ 𝐀\nഥ 𝐁 𝐀𝐁 𝐀𝐁\nഥ Red: 𝐀\n𝐀\n\nഥ𝐢\n0 𝐂\n\n0 1 0 1\n1 𝐂𝐢 1 0 1 0\n\nഥ\nഥ 𝐁𝑪\nGreen: 𝐀\nBlue: 𝐀𝐁𝐂𝐢\nഥ\nഥ𝑪\nPurple: 𝐀𝐁\n\nഥ + 𝐀𝐁𝑪 + 𝐀𝐁\nഥ\nഥ𝑩\nഥ𝑪 + 𝐀\nഥ 𝐁𝑪\nഥ𝑪\n𝐒𝐮𝐦 = 𝐀\nwait, can’t we do that as:\n\n𝐒𝐮𝐦 = 𝐀⨁𝐁⨁𝑪 (that's xor!)\n28\n\n\fTradeoffs, tradeoffs\n● there are extensions to K-maps to detect XORs\n● but…\no XOR gates are slower than AND\/OR gates\no if area is a concern, an XOR make sense\no if speed is a concern, AND\/OR gates make sense\n● what do real hardware designers do?\no they use programs to do this stuff for them lol\no things like GALs, CPLDs, and FPGAs are reconfigurable hardware which\nusually use \"sum-of-products\" to do logic, so ANDs and ORs are all you've\ngot\n\n29\n\n\fSome more examples\n● Can you solve this?\n\n00\n\n01\n\n11\n\n10\n\n● In AB: A is both {0,1}\n● In CD: D is both {0,1}\n\n0\nഥ𝑫 0\n01 𝑪\n11 𝑪𝑫 0\nഥ 0\n10 𝑪𝑫\n\n0\n0\n1\n1\n\n0\n0\n1\n1\n\n0\n0\n0\n0\n\n● Eliminate both A,D\n● We get\n𝐶𝐵\n\nഥ𝐁\nഥ 𝐀\nഥ 𝐁 𝐀𝐁 𝐀𝐁\nഥ\n𝐀\nഥ𝑫\nഥ\n00 𝑪\n\n30\n\n\fSome more examples\n● Can you solve this?\n\n00\n\n01\n\n11\n\n10\n\n● In AB: Both have {0,1}\n● In CD: C is both {0,1}\n\n0\nഥ𝑫 1\n01 𝑪\n11 𝑪𝑫 1\nഥ 0\n10 𝑪𝑫\n\n0\n1\n1\n0\n\n0\n1\n1\n0\n\n0\n1\n1\n0\n\n● Eliminate both A, B, C\n● We get\n\nഥ𝐁\nഥ 𝐀\nഥ 𝐁 𝐀𝐁 𝐀𝐁\nഥ\n𝐀\nഥ𝑫\nഥ\n00 𝑪\n\n𝐷\n\n31\n\n\fSome more examples\n● Can you solve this?\n\n00\n\n01\n\n11\n\n10\n\n● In AB: A has both {0,1}\n● In CD: C is both {0,1}\n\n1\nഥ𝑫 0\n01 𝑪\n11 𝑪𝑫 0\nഥ 1\n10 𝑪𝑫\n\n0\n0\n0\n0\n\n0\n0\n0\n0\n\n1\n0\n0\n1\n\n● Eliminate both A, C\n● We get\n\nഥ𝐁\nഥ 𝐀\nഥ 𝐁 𝐀𝐁 𝐀𝐁\nഥ\n𝐀\nഥ𝑫\nഥ\n00 𝑪\n\nഥ𝐷\nഥ\nB\n\nthis is still a 2x2 rectangle!\n32\n\n\f7 segment LED display\n● This is a 7 segment LED display\no It displays numbers\no It has 8 LEDs (one for the decimal point)\n● Problem\no Given a 4-bit number, draw the\ncorresponding numeral.\no E.g. 0000 is “0”; 1001 is “9”.\no Ignore the dot\n● Solution\no Create a Boolean function for each\nsegment.\n\nd2\n\nd1\n\nd3\nd0\n\nd7\n\nd5\nd6\n\nd4\n\n33\n\n\f7 segment LED display\nNumber 0\n01110111\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n34\n\n\f7 segment LED display\nNumber 1\n00010100\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n35\n\n\f7 segment LED display\nNumber 2\n10110011\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n36\n\n\f7 segment LED display\nNumber 3\n10110110\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n37\n\n\f7 segment LED display\nNumber 4\n11010100\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n38\n\n\f7 segment LED display\nNumber 5\n11100110\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n39\n\n\f7 segment LED display\nNumber 6\n11100111\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n40\n\n\f7 segment LED display\nNumber 7\n00110100\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n41\n\n\f7 segment LED display\nNumber 8\n11110111\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n42\n\n\f7 segment LED display\nNumber 9\n11110110\n\nd2\n\nd1\n\nd0\n\nd7\n\nd3\nd0\n\nd7\n\nd5\n\nd6\n\nd4\n\nHex Digit LED\n7 segments, 1 decimal point\nTurn each segment on\/off\nState: 0=OFF, 1=ON\n“Draw” numbers 0 to 9\n43\n\n\f7 segment LED display\nHow to approach this?\n● Create a truth table\no Inputs are i0 to i3 (4 bits)\no Outputs are numbered d0, to d7, corresponding to segments\n\n● Minimize the circuit using a K-map\no Create the table\no Follow the rules!\n● Draw the numerals by setting d0 to d7 to 1s or 0s\no Build the circuit!\n\n44\n\n\f7 segment LED display\ni3\n\ni2\n\ni1\n\ni0\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n0\n\n1\n\n0\n\n0\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n0\n\n1\n\n0\n\n1\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n0\n\n1\n\n1\n\n0\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n0\n\n1\n\n1\n\n1\n\n0\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n1\n\n0\n\n0\n\n0\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n1\n\n0\n\n0\n\n1\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n1\n\n0\n\n1\n\n0\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n45\n\n\f7 segment LED display\ni3\n\ni2\n\ni1\n\ni0\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n0\n\n1\n\n0\n\n1 It can\n1\n…\n…\nonly\n0\n1\n…\n…\ndisplay\n\n…\n\n…\n\n0\n\n1\n\n0\n\n1 numbers\n1\n…up …\n\n0\n\n1\n\n1\n\n0\n\n1to 9!…\n\n…\n\n…\n\n…\n\n0\n\n1\n\n1\n\n1\n\n0\n\n…\n\n…\n\n…\n\n1\n\n0\n\n0\n\n0\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n1\n\n0\n\n0\n\n1\n\n1\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n1\n\n0\n\n1\n\n0\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\nSo we “don’t\n…\n…\n…\ncare” about\n…\n…\n…\n…\nthe outputs\n…\n…\n…\n…\nfor larger\n…\n…\n…\n…\nnumbers\n\n46\n\n\f7 segment LED display\n● Now use a K-map for each output\nfunction d0-d7 - Let’s start with d0\n\n00\n\n01\n\n11\n\n10\n\n𝒊ഥ𝟏 𝒊ഥ𝟎 𝒊ഥ𝟏 𝒊𝟎 𝒊𝟏 𝒊𝟎 𝒊𝟏 𝒊ഥ𝟎\n00 𝒊ഥ𝟑 𝒊ഥ𝟐\n\n0\n01 𝒊ഥ𝟑 𝒊𝟐 1\n11 𝒊𝟑 𝒊𝟐 X\n10 𝒊𝟑 𝒊ഥ𝟐 1\n\n0\n1\nX\n1\n\n1\n0\nX\nX\n\n1\n1\nX\nX\n\ni3\n\ni2\n\ni1\n\ni0\n\nd0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\nX\n\n…\n\n…\n\n…\n\n…\n\n…\n\n47\n\n\f7 segment LED display\n● Now use a K-map for each output function d0-d7\n● Let’s start with d0\n\n00\n\n01\n\n11\n\n10\n\n0\n01 𝒊ഥ𝟑 𝒊𝟐 1\n11 𝒊𝟑 𝒊𝟐 X\n10 𝒊𝟑 𝒊ഥ𝟐 1\n\n0\n1\nX\n1\n\n1\n0\nX\nX\n\n1\n1\nX\nX\n\n𝒊ഥ𝟏 𝒊ഥ𝟎 𝒊ഥ𝟏 𝒊𝟎 𝒊𝟏 𝒊𝟎 𝒊𝟏 𝒊ഥ𝟎\n00 𝒊ഥ𝟑 𝒊ഥ𝟐\n\nOutput\n“don’t cares”\ncan be\nwhatever we\nneed them to\nbe ☺\n\n48\n\n\f7 segment LED display\n00\n\n01\n\n11\n\n10\n\n𝒊ഥ𝟏 𝒊ഥ𝟎 𝒊ഥ𝟏 𝒊𝟎 𝒊𝟏 𝒊𝟎 𝒊𝟏 𝒊ഥ𝟎\n00 𝒊ഥ𝟑 𝒊ഥ𝟐\n\n0\n01 𝒊ഥ𝟑 𝒊𝟐 1\n11 𝒊𝟑 𝒊𝟐 X\n10 𝒊𝟑 𝒊ഥ𝟐 1\n\n0\n1\nX\n1\n\n1\n0\nX\nX\n\n1\n1\nX\nX\n\n● Four minterms:\n\no 𝒊𝟑\no 𝒊𝟐 𝒊𝟏\no 𝒊𝟏 𝒊𝟐\no 𝒊𝟏 𝒊𝟎\n\n𝒅𝟎 = 𝒊𝟑 + 𝒊𝟐 𝒊𝟏 + 𝒊𝟏 𝒊𝟐 + 𝒊𝟏 𝒊𝟎\n\n49\n\n\fOn your own\n● Try making a circuit in Logisim using the 7 segment LED display\no Find it in input\/output\n● Connect the logic you just solved into d0 (top-left pin) and test it\n\n● Solve for the remaining segments\no It’s fun and practice\n\n50\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":150,"segment": "unlabeled", "course": "cs0447", "lec": "lec06", "text":"#6\n\nFlow, Conditionals,\nand Loops\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fSo far…\n● Putting numbers into registers\n\nli a0, 3\n\nla a0, x\n\nlabel\n\n.data\nx: .word 4\n\n● COPYing register contents\n\nmove a0, t0\n\nThese do zero\nUnsigned! \nextension\n\n● COPYing from\/to memory\n\nlw\/sw, lh\/lhu\/sh, lb\/lbu\/sb\nla t1, x\nlw t0, 0(t1)\nlw t0, x\n\nla t1, x\nsw t0, 0(t1)\nsw t0, x\n\nDo the\nsame thing\n2\n\n\fIn another perspective\nlw, lh, lhu, lb, lbu\nmove\nCPU\n\nMemory\nRegisters\n\nli, la\n\nsw, sh, sb\nOther operations\n\nDatatypes\n\nadd\n\nword\n\nsub\n\nhalf\n\nmul\n\nbyte\n\nsyscall\n\nasciiz\n\n…\n\n…\n\n3\n\n\fIntroduction to conditions\n● What distinguishes a computer from a calculator?\n● It can make decisions based on values that it calculates\no If the value of this register is this, do something.\no Otherwise, do something else.\n● The possible decisions make up the potential control flow of the program.\no When there is no possible route to a piece of code in your program, that is\ncalled dead code.\n It’s like procrastination!\n\nif(false) {\ndo_some_work()\n}\n4\n\n\fControl flow\n\n5\n\n\fWith great power…\n● Control flow determines the order that your instructions run in\no What kinds of control flow statements do you know of?\no What about functions?\n● In asm, the only thing you get for free is that instructions run in order\n● You're responsible for coming up with everything else.\no If you screw up your control flow, the CPU doesn't care\no You'll just have a broken, malfunctioning program\n And it'll be half an hour before the lab is due\n– And you'll be sad\n» This is like 90% of the bugs\n\n6\n\n\fGetting a little further from familiarity\n● all control flow is done with branches and jumps\no these are instructions which say \"go somewhere else\"\n● for example…\n\nthis is an infinite loop,\nwhich is sometimes useful\nbut not too interesting\n\n_main_loop:\n# clear screen\n# draw one thing\n# sleep\n# draw another thing\nj stands for ”jump\" – go\n# etc\nsomewhere else\nj _main_loop\n\n7\n\n\fBuilding blocks\n● A basic block is a chunk of code that has no control flow in it\n● Control flow statements separate basic blocks\n\nif(x == w - 1) {\ndo_thing()\n} else {\nother_thing()\n}\nthird_thing()\n\nx == w - 1?\n\nother_thing\n\ndo_thing\n\nthird_thing\n\nthinking about this is REAL HELPFUL\n8\n\n\fEssentially…\n● The way control flow works in asm is you make basic blocks\no You gotta name (label) them\n● Then, you use special instructions to choose where to go\no Ask yourself “Which basic block runs next?\"\no Select the instruction you need!\n Don’t worry, we look into these instructions in a moment\n● And don’t forget!\no Write pseudo-code (with comments) to keep track of control flow\no Or make a drawing of a flow-chart!\no Or … any other guide you think it’s helpful\n\n9\n\n\fConditionals: if and if-else\n\n10\n\n\fMIPS ISA: conditional branch instructions\n● conditional branch instructions do one of two things:\no if the condition is met, we go to the label\no otherwise, nothing happens, and we go to the next instruction\n\nInstruction\n\nMeaning\n\nbeq a, b, label\n\nif(a == b) { goto label }\n\nbne a, b, label\n\nif(a != b) { goto label }\n\nabove, a must be a register, but b can be a register or immediate\n(by the powers of the pseudo-instruction)\n11\n\n\fHow do these work?\n\nPrevious\ninstruction\n\nThis is the branch\n\nbeq t0, t1, label\n# branch if equal\n\nt0==t1\n\nTrue\n\nFalse\n\nNext\ninstruction\n\nlabel:\n\nOther\ninstruction\n12\n\n\fHow do these work?\nlabel:\n\nbeq t0, t1, label\n# branch if equal\n\nOther\ninstruction\nPrevious\ninstruction\n\nTrue\n\nt0==t1\nThis is the branch\nFalse\n\nNext\ninstruction\n13\n\n\fHow to write asm (again!)\n● Remember:\n\nWRITE PSEUDOCODE\n\nALWAYS\n\nREALLY!!!\n\nif(x == w - 1) {\ndo_thing()\n} else {\nother_thing()\n}\n\n14\n\n\fLike mad libs, but for code\n● From now on, I’ll use these 'blocks' to represent the basic blocks\no cause they don’t matter\n\nif(some condition) {\nblock A\n} else {\nblock B\n}\nblock C\n\n15\n\n\fA simple conditional block (if)\n● If there is no else, it's pretty simple.\n\nif(s0 == 30) {\nblock A\n}\nblock B\n\nbne s0, 30, blockB\nblockA:\nblockB:\n\n16\n\n\fA simple conditional block (if)\n● If there is no else, it's pretty simple.\n\nif(s0 == 30) {\nblock A\n}\nblock B\n\nbne s0, 30, blockB\n\nIn Java\/C what happens in an if?\nYou JUMP OVER when the condition is true or\nfalse?\n\nWhen its FALSE!!\n\n17\n\n\fA simple conditional block (if)\n\n●In MIPS you jump when the condition is TRUE\nif(s0 == 30) {\nblock A\n}\nblock B\n\nbne s0, 30, blockB\nblockA:\nblock A\nblockB:\nblock B\n\n18\n\n\fAn if-else with a simple condition\n● more blocks now…\n\nif(s0 == 30) {\nblock A\n}\nelse {\nblock B\n}\nblock C\n\nbne s0, 30, blockB\nblock A\nj blockC\nblockB:\nblock B\nblockC:\nblock C\n\nwe NEED THIS – the CPU doesn't\nsee\/care about your labels!!\n\n19\n\n\fThe other way around\n● Because in HLL we “execute smth if” and In assembly we “jump over if”\n● We usually negate the condition in the assembly to skip over code\no It’s a preference.\no You can still invert the process\n How?\nif(s0 == 30) {\n}\nelse {\n}\n\nblock A\nblock B\n\nblock C\n\nbeq s0, 30, blockA\nj blockElse\nblockA:\nblock A\nj blockExit # skip the else\nblockElse:\nblock B\nblockExit:\nblock C\n20\n\n\fMIPS ISA: conditional branch instructions\n● MIPS also supports instructions that compare to zero\n\nInstruction\n\nMeaning\n\nbltz a, label\n\nif(a < 0) { goto label }\n\nblez a, label\n\nif(a <= 0) { goto label }\n\nbgtz a, label\n\nif(a > 0)\n\nbgez a, label\n\nif(a >= 0) { goto label }\n\n{ goto label }\n\n21\n\n\fMIPS ISA: set if less than\n● And…\n\nInstruction\nslt\n\nc, a, b\n\nMeaning\nif(a < b) { c = 1 } else { c = 0 }\n\nSet if Less Than: register c will be set to 1 if a<b.\nOtherwise, register c will be set to 0.\nUsing slt together with bne and beq all conditionals can be implemented!\na=b , a≠b, a>b, a≥b, a<b, a≤b\n\nThanks, De Morgan\n\n22\n\n\fMIPS ISA: conditional branch instructions\n● Or… we can just use the pseudo-instructions :D\n\nInstruction\n\nMeaning\n\nblt a, b, label\n\nif(a < b)\n\n{ goto label }\n\nble a, b, label\n\nif(a <= b) { goto label }\n\nbgt a, b, label\n\nif(a > b)\n\nbge a, b, label\n\nif(a >= b) { goto label }\n\n{ goto label }\n\nabove, a must be a register, but b can be a register or immediate\n\n23\n\n\fExamples\nExample 1: branch if a>b\nbgt a, b, label\n\n# Goto label if a>b\n\nSolution: branch if b<a\nslt t, b, a\nbne t, zero, label\n\n# t=1 if b<a\n# Goto label if t≠0\n\nExample 2: branch if a≥b\nbge a, b, label\n\n# Goto label if a≥b\n\nSolution: branch if !(a<b)\nslt t, a, b\nbeq t, zero, label\n\n# t=1 if a<b\n# Goto label if t=0\n24\n\n\fComplex conditionals\n\n25\n\n\fIn this code…\nif(dog_size < 10 || dog_name() == \"Fluffy\")\n\nif dog_size is 3, is dog_name() called?\n\nNO!\n\nthis is short circuit evaluation.\n\nfor || (logical OR), if the first condition is true, the\nsecond one is skipped. (cause there's no way for\nthe result of the OR to be false.)\nfor && (logical AND), if the first condition is\nfalse, the second one is skipped.\n\n26\n\n\fIn this code…\nif(dog_size < 10)\nsmall();\nif dog_size is 3, is this\nelse if(dog_size < 20)\ncondition checked?\nmedium();\nNO!\nelse if(dog_size < 30)\nlarge();\nelse\nonce a true condition is found, no\nenormous();\nmore conditions are checked.\nafter small(), it comes down here.\n\n27\n\n\fAnd-and!\n● Block A is run if both conditions are true.\no to think of it another way… it's skipped if? What’s the inverse?\no either condition is false…\n\nif(s0 == 30 &&\ns1 > 1) {\nblock A\n}\n\nbne s0, 30, skipA\nble s1, 1, skipA\nblock A\nskipA:\n\n28\n\n\fOr-or!\n● We go to block A if either condition is true.\no to think of it another way… it's skipped if? What’s the inverse?\no all conditions are false.\n\nif(s0 == 30 ||\ns1 > 1)\n{\nblock A\n}\n\nbeq s0, 30, blockA\nble s1, 1, skipA\nblockA:\nblock A\nskipA:\n29\n\n\fLooooops\no o\no\no\no\no\noo oo\n30\n\n\fDis-assembling a for-loop\n● How does a for loop work?\n\nWhat is the first thing a for does?\n\nInitialize: i=0\nfor(i=0; i<10; i++) And???\nCheck condition:\n{\nexecute while i<10\nblock A\nThen…\n}\n\/\/ carry on\nblock A\nFinally?\n\nIncrement: i++\nGo back up to the top\n\n31\n\n\fLooping in MIPS assembly\nWhat’s the first\nli\ns0,\n0\n__________ thing a for\nloop_top:\ndoes?\nfor(i=0; i<10; i++)\nwhich conditional branch?\n__________________\n{\nblock A\nblock A\n}\nHow do we\n\/\/ carry on\nincrement?\naddi s0, s0, 1\n_______________\nj loop_top\n____________\nLet’s start with a\nHow do\nrecipe\ncarry_on:\nwe go up?\n# carry on\n\n● Let’s use s0 to hold i\n\n32\n\n\fThat’s bge, actually\nWe want to leave the loop…\nwhen the opposite of i<10\nhappens!\n● In HLL we “execute smth if”\n● In assembly we “jump over if”\n● Thus negate the condition in\nthe assembly to skip over\ncode\n\nli s0, 0\n__________\nloop_top:\nbge s0, 10, carry_on\n_____________________\ns0\nmove a0, ___\nli v0, 1\nsyscall\naddi s0, s0, 1\n_______________\nj loop_top\n____________\ncarry_on:\n# carry on\n33\n\n\fThe other way around\nli s0, 0\n__________\nloop_top:\nblt s0, 10, loop_code\nb carry_on\nloop_code:\ns0\nmove a0, ___\nli v0, 1\nsyscall\naddi s0, s0, 1\n_______________\nj loop_top\n____________\ncarry_on:\n# carry on\n\n34\n\n\fWhile looks the same, no initialization or increment\n\nwhile(s2 < 10)\n{\n\/\/ stuff!!\n}\n\/\/ more stuff\n\nloop_top:\nbge s2, 10, more_stuff\n________\nstuff:\n# stuff!!\nj loop_top\nmore_stuff:\n# more stuff\n\n35\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":151,"segment": "unlabeled", "course": "cs0449", "lec": "lec02", "text":"1\n\nData\nRepresentation\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fBinary Encoding\ni\nBits, Bytes, and Nybbles\n\n2\n\n\fPositional number systems\n\n• The numbers we use are written positionally: the position of a digit\nwithin the number has a meaning.\n\n3\n2 x 10\n\n2000\n2\n0 0 0 0 x 10\n=\n2021=\n1\n2 0 2 x 10\n0\n+\n1 1 x 10\n3\n\n\fPositional number systems\n● The numbers we use are written positionally: the position of a digit within\nthe number has a meaning.\n\n2021\n\nMost Significant\n\nLeast Significant\n\n1000s\n\n100s\n\n10s\n\n1s\n\n103\n\n102\n\n101\n\n100\n\n● How many (digits) symbols do we have in our number system?\n○ 10: 0, 1, 2, 3, 4, 5 ,6 ,7, 8, 9\n4\n\n\fRange of numbers\n\nSuppose we have a 4-digit numeric display.\n• What is the smallest number it can show?\n\n• What is the biggest number it can show?\n• How many different numbers can it show?\n• 9999 - 0 + 1 = 10,000\n• What power of 10 is 10,000?\n• 104\n\n5\n\n\fSo… with the numbers we use every day…\n• A number represented by the digits\n• Has the value\n\n𝑑𝑛 𝑑𝑛−1 … 𝑑1 𝑑0\n\n𝑑𝑛 × 10𝑛 + 𝑑𝑛−1 × 10𝑛−1 + ⋯ + 𝑑1 × 101 + 𝑑0 × 100\n• Using 𝑛 digits we can represent 10𝑛 different numbers\n• The smallest non-negative number representable with 𝑛 digits is 0\n\n• The largest number representable with 𝑛 digits is 10𝑛 − 1\n• Using 10 symbols: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n\n6\n\n\fNumeric Bases\n• These 10s keep popping up… and for good reason\n• We use a base-10 (decimal) numbering system\n• 10 different symbols, and each place is a power of 10\n\n• But we can use (almost) any number as a base!\n• The most common bases when dealing with computers are base-2 (binary),\nbase-16 (hexadecimal), and (rarely) base-8 (octal)\n• When dealing with multiple bases, you can write the base as a subscript to be\nexplicit about it:\n\n510 = 1012\n7\n\n\fMaking a number system\nUsing base B\n• A number represented by the digits\n𝑑𝑛 𝑑𝑛−1 … 𝑑1 𝑑0\n\n• Has the value\n\n𝑑𝑛 × B 𝑛 + 𝑑𝑛−1 × B 𝑛−1 + ⋯ + 𝑑1 × B1 + 𝑑0 × B 0\n• Using 𝑛 digits we can represent B 𝑛 different numbers\n• The smallest non-negative number representable with 𝑛 digits is 0\n• The largest number representable with 𝑛 digits is B 𝑛 − 1\n• Using B symbols\n\nIf you use base 0 you\ndon’t need to remember\n8\nany symbols\n\n\fBinary – Base 2\n\n9\n\n\fLet's make a base-2 number system\nUsing base 2\n• A number represented by the digits\n• Has the value\n\n𝑑𝑛 𝑑𝑛−1 … 𝑑1 𝑑0\n\n𝑑𝑛 × 2𝑛 + 𝑑𝑛−1 × 2𝑛−1 + ⋯ + 𝑑1 × 21 + 𝑑0 × 20\n• Using 𝑛 digits we can represent 2𝑛 different numbers\n• The smallest non-negative number representable with 𝑛 digits is 0\n• The largest number representable with 𝑛 digits is 2𝑛 − 1\n• Using 2 symbols: 0, 1\n\n10\n\n\fBinary (base-2)\n\n• We call a Binary digIT a bit – a single 1 or 0\n• When we say an n-bit number, we mean one with n binary digits 1 × 128 +\nMSB\n\nLSB\n\n1001 0110 =\n27 2 6\n25 24\n128s 64s 32s 16s\n\n23\n8s\n\n22\n4s\n\n21\n2s\n\n20\n1s\n\nTo convert binary to decimal: ignore 0s, add up\nIt’s the\nplace values wherever you see a 1.\nonly odd\n\n0 × 64 +\n0 × 32 +\n1 × 16 +\n0×8+\n1×4+\n1×2+\n0×1\n\n= 15010\n\nnumber!\n\n11\n\n\fConverting the other way around\n\n• Ok! then. Let’s go back to decimal for a bit\n\n2021\n\nHow would you\nextract this\nnumber???\n\nHow I like to think of it:\n\nWhen you divide by the BASE\nyou are moving the decimal\npoint in that BASE\nJust divide by\n10!\n\n10\n10\n\n10\n\n2\n\nR0\n\n20\n\nR2\n\n202 R1\n2 0 21\n12\n\n\fWhat happens when you divide by 10?\n\n• Turns out that dividing by 10 in any base has the same outcome\n\n0b10\n\n210\n\n110012\n\n1\n\nR1\n\n210\n210\n\n3\n\nR0\n\n6\n\n210\n\n12\n\nR0\nR1\n\n210\n\n25\n\n13\n\n\fBits, bytes, nibbles, and words\n• A bit is one binary digit, and its unit is lowercase b.\n• A byte is an 8-bit value, and its unit is UPPERCASE B.\n\n• This is (partially) why your 30 megabit (Mbps) internet connection can only give you\nat most 3.57 megabytes (MB) per second!\n\n• A nibble (also nybble) is 4 bits – half of a byte\n• Corresponds nicely to a single hex digit.\n\n• A word is the \"most comfortable size\" of number for a CPU.\n• When we say \"32-bit CPU,\" we mean its word size is 32 bits.\n• This means it can, for example, add two 32-bit numbers at once.\n\n• BUT WATCH OUT:\n\n• Some things (Windows, x86) use word to mean 16 bits and double word (or dword)\nto mean 32 bits.\n14\n\n\fWhy binary? Whynary?\n• Why indeed?\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n• What color is this?\n\n15\n\n\fWhy binary? Whynary?\n• Why indeed?\n0\n\n1\n\n• What color is this?\n\n16\n\n\fBinary Representation\n• Computers translate\nelectrical signals to\neither 0 or 1.\n\n• It is relatively easy to\ndevise electronics\nthat operate this way.\n• In reality, there is no\nsuch thing as\n“binary” so we often\nhave to approximate\nand mitigate error.\nOscilloscope visualization of several digital wires. From @computerfact on Twitter.\n\n17\n\n\fEverything in a computer is a number\n• So, everything on a computer is represented in binary.\n• everything.\n01100101 01110110 01100101 01110010 01111001 01110100 01101000 01101001 01101110\n01100111 00001010 00000000\n\n• Java strings are encoded using UTF-16\n• Most letters and numbers in the English alphabet are < 128.\n• “Strings are numbers”\n• 83 116 114 105 110 103 115 32 97 114 101 32 110 117 109 98 101 114 115 0\n\n• ASCII is also pretty common (the best kind of common)\n• That’s what we will be using → 8 bit numbers represent characters\n• Letters and numbers (and most\/all ascii characters) have the same value as UTF16\nDo try this at home: what does this mean?\n• 71 111 111 100 32 74 111 98 0\n18\n\n\fHexadecimal – Base 16\nThe binary shorthand\n\n19\n\n\fHexadecimal\n• Binary numbers can get really long, quickly.\n▪ 3,927,66410 = 11 1011 1110 1110 0111 00002\n\n• But nice \"round\" numbers in binary look\narbitrary in decimal.\n▪ 10000000000000002 = 32,76810\n\n• This is because 10 is not a power of 2!\n• We could use base-4, base-8,\nbase-16, base-32, etc.\n▪ Base-4 is not much denser than binary\n• e.g. 3,927,66410 = 120 3331 2323 00004\n\n▪ Base-32 would require 32 digit symbols. Yeesh.\n• They do, oddly, have their place… but not really in this\ncontext.\n\n▪ Base-8 and base-16 look promising!\n20\n\n\fHexadecimal or “hex” (base-16)\n• Digit symbols after 9 are A-F, meaning 10-15 respectively.\n• Usually we call one hexadecimal digit a hex digit. No fancy name :(\n\n003B EE70 =\n167 166 165 164\n\n163 162 161 160\n\nTo convert hex to decimal: use a dang calculator\nlol\n\n0 × 167 +\n0 × 166 +\n3 × 165 +\n11 × 164 +\n14 × 163 +\n14 × 162 +\n7 × 161 +\n0 × 160 =\n\n3,927,66410\n21\n\n\fBinary to Hex\n\n(animated)\n\n0100 1100 1010 0010 0000 0010 0110 0001\n\n4 C A 2 0 2 6 1\n0x4CA20261\n32-bits! (Not so bad…)\nQ: Create a random binary string and practice! 22\n\n\fLet's make a base-16 number system\nUsing base 16\n• A number represented by the digits\n• Has the value\n\n𝑑𝑛 𝑑𝑛−1 … 𝑑1 𝑑0\n\n𝑑𝑛 × 16𝑛 + 𝑑𝑛−1 × 16𝑛−1 + ⋯ + 𝑑1 × 161 + 𝑑0 × 160\n• Using 𝑛 digits we can represent 𝟏𝟔𝑛 different numbers\n• The smallest non-negative number representable with 𝑛 digits is 0\n• The largest number representable with 𝑛 digits is 16𝑛 − 1\n• Using 16 symbols: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F\n23\n\n\fWhy?\n\n1111 1111\n1 × 27 + 1 × 26 + 1 × 25 + 1 × 24 + 1 × 23 + 1 × 22 + 1 × 21 + 1 × 20\n\n1 × 23 + 1 × 22 + 1 × 21 + 1 × 20 =\n8+4+2+1=\n15\n1 × 23 + 1 × 22 + 1 × 21 + 1 × 20 × 24 + 15\nThis works with any\nbase that is a power\nof 2\n\n15 × 161 + 15 × 160\n\nE.g. Base 4=22\nSplit into groups of 2\nbits\n\nF\n\nF\n\nFactoring\n\n24 = 16\n24\n\n\fInteger Encoding\nCasting is Not Just a Witch or Wizard Thing\n\n25\n\n\fFinite numbers\n• In computers, numbers are finite.\n• Let's say our 4-digit display was counting up:\n9997, 9998, 9999…\n• What comes \"next\"?\n• What does this \"0000\" really mean?\n• It wrapped around.\n\n• This is overflow: the number you are trying\nto represent is too big to be represented.\n• Essentially, all arithmetic on the computer is modular arithmetic!\n• This causes a lot of software bugs.\n• https:\/\/en.wikipedia.org\/wiki\/Pac-Man#Level_256\n26\n\n\fNumber carrousel\n• Computers perform modulus arithmetic\n• Meaning: it goes around!\n• E.g. in a 4-bit computer\n\n15 0\n\n14 1111\n1110\n13 1101\n\n0000\n\n1001\n\n0001\n0010\n\n2\n\n0011\n\n12 1100\n1011\n11 1010\n10\n\n1\n\n0100\n0101\n0110\n0111\n\n3\n4\n5\n\n6\n\n0000\n8 7\n-0001\nwhat is 0 - 1?\n1111\n9\n\n1000\n\n27\n\n\fWhat about negative carrousels?\n• How much is 0 – 1?\n• -1\n\n-1 0\n\n-2 1111\n1110\n-3 1101\n\n0000\n\n1001\n\n0001\n0010\n\n2\n\n0011\n\n-4 1100\n1011\n-5 1010\n-6\n\n1\n\n0100\n0101\n0110\n1000\n\n0111\n\n-7 -8 7\n\n3\n4\n5\n\n6\nWhy did we\nstop here?\n\n28\n\n\fSigned Numbers (2’s Complement)\n• Representing negative numbers.\n• But it’s a little strange!\n\n• Hmm, it’s a little lopsided: -4 doesn’t have a valid positive number.\n\n100\n\n101\n\n110\n\n000\n010\n111\n001\n011\n\n-4 -3 -2 -1 0 +1 +2 +3\n2’s Complement\n\n• I can tell it’s negative if it starts with a 1 ☺\n• And if it’s positive, then I can clearly see how much it’s worth!\n\n• But how exactly… can we tell the value of a negative number?\n• We need to negate (flip the sign) negative numbers\n• But HOW????\n\n29\n\n\fTwo’s complement arithmetic\n• Negation\n\n-(3)\n\n0011\n\n-(-3)\n\n1101\n\nbit pattern for\npositive 3?\nbit pattern for\nnegative 3?\n\nflip!\n\nflip!\n\n1100\n\n0010\n\n1101\n\nAdd 1!\n\n0011\n\nAdd 1!\n\n• You don’t need to subtract!!\n• flip(k)+1 == flip(k-1)\n• If you ignore the carry! ☺\n\n30\n\n\fTwo's complement addition\n• the great thing is: you can add numbers of either sign without having to do\nanything special!\nto binary? 0111\n\n3\n+ 7\n10\n\nIgnore the carry\n\nbit pattern for\n-7… positive 7?\n\n0011\n3\n+0111 +-7\n1010 -4\n\n0011\n0011\n0111\n4\nflip!\n+1 +1001\nto decimal?\n1000\n1100 0100\nthis is negative, so\nwhat is it? flip!\n\nthe actual patterns of bits are the same.\nso how does the computer \"know\" whether it's\ndoing signed or unsigned addition?\n\n+1\n\n0011\n31\n\n\fSigned Numbers (2’s Complement)\n• Let’s look some examples:\n\n11010100➔-(00101011+1)➔-(43+1)=-44\n00100110 = 00100110 = 38\n\n00000000 = 00000000= 0\n11111111= -00000000=-(0+1) =-1\n• If the MSB is 1: Flip! Add one!\n• Otherwise: Do nothing! It’s the same!\n\n32\n\n\fSigned Numbers (2’s Complement)\n• What happens when we add zeros to a positive number:\n\n10100110 = ?\n00100110 = 38\n-(01011001+1) = ?\n0000000000100110 = 38?\n-01011010 = -90\n?\n• What happens when we add ones to a negative number:\n\n10100110 = -90\n?\n11111111110100110 =\n-00000000001011001 = -90\n?\n\n33\n\n\fCan I Get an Extension?\n• Sometimes you need to widen a number with fewer bits to\nmore\n• zero extension is easy: put 0s at the beginning.\n\n10012 ➔ to 8 bits ➔ 0000 10012\n• But there are also signed numbers… what about those?\n• The top bit (MSB) of signed numbers determines the sign (+\/-)\n\n• sign extension puts copies of the sign bit at the beginning\n\n10012 ➔ to 8 bits ➔ 1111 10012\n00102 ➔ to 8 bits ➔ 0000 00102\nQ: What happens when you sign extend the largest unsigned value? 34\n\n\fAbsolutely Bonkers\n\nQ: How many bits is a Java\n\n? What happened here? 35\n\n\fInteger Ranges\n• Recall:\n• The range of an unsigned integer is 0 to 2n – 1\n• Q: Why do we subtract 1?\n\n• What is the range of a 2’s complement number?\n• Consider the sign bit, how many negative integers?\n• Consider, now, the positive integers.\n• Remember 0.\n\n-2n-1 to 2n-1 – 1\nQ: What if you needed CS\/COE\na range\nwith\nfar more negatives than positives? 36\n0449 – Spring\n2019\/2020\n\n\fIntegers in Java C\n• C allows for variables to be declared as either signed or unsigned.\n\n• Remember: “signed” does not mean “negative” just that it can be negative.\n\n• An unsigned integer variable has a range from 0 to 2n – 1\n• And signed integers are usually 2’s complement: 2n-1 to 2n-1 – 1\n• Where “n” is determined by the variable’s size in bits.\n\n• Integer Types: (signed by default, their sizes are arbitrary!!)\n•\n•\n•\n•\n\n• Usually no strong reason to use anything other than (un)signed int.\nQ: What is the range of a\n\n? 37\n\n\fIntegers in C: Limits\n• Since sizes of integers are technically arbitrary…\n\n• They are usually based on the underlying architecture.\n\n• … C provides standard library constants defining the ranges.\n\n• https:\/\/pubs.opengroup.org\/onlinepubs\/009695399\/basedefs\/limits.h.html\n\nQ: Experiment\nusing\nCS\/COEwith\n0449 – Spring\n2019\/2020\n\nfor both. What is the result? 38\n\n\fCasting\n• C lets you move a value from an unsigned integer variable to a signed\ninteger variable. (and vice versa)\n• However, this is not always valid! Yet, it will do it anyway.\n• The binary value is the same, its interpretation is not!\n• This is called coercion, and this is a relatively simple case of it.\n\n• Since it ignores obvious invalid operations this is sometimes referred to as “weak”\ntyping.\n• The strong\/weak terminology has had very fragile definitions over the years and\nare arguably useless in our context. Let’s ignore them.\n\n• Moving values between different types is called casting\n• Which sounds magical and it sometimes is.\nQ: What is true of the result\nthe value -1 to an unsigned type? 39\nCS\/COE of\n0449casting\n– Spring 2019\/2020\n\n\fFractional Binary\n\n40\n\n\fFractional numbers\n• Up to this point we have been working with integer numbers.\n\n2019\n2 0 1 9.320\n\n• Unsigned and signed!\n\n• However, Real world numbers are… Real numbers. Like so:\n\n• That create new challenges!\n\n• Let’s start by taking a look at them.\n41\n\n\fJust a fraction of a number\n• The numbers we use are written positionally: the position of a digit\nwithin the number has a meaning.\n• What about when the numbers go over the decimal point?\n\n?\n2 0 1 9. 3 2 0\n\n1000s\n\n100s\n\n10s\n\n1s\n\n10ths 100ths 1000ths\n\n103\n\n102\n\n101\n\n100\n\n10-1\n\n10-2\n\n10-3\n\n42\n\n\fA fraction of a bit?\n• Binary is the same!\n• Just replace 10s with 2s.\n\n0 1 1 0 .1 1 0 1\n23\n8s\n\n22\n4s\n\n21\n2s\n\n20\n1s\n\n2-1\n2ths\n\n?\n\n2-2\n4ths\n\n2-3\n8ths\n\n2-4\n16ths\n\n43\n\n\fTo convert into decimal, just add stuff\n\n0 1 1 0 .1 1 0 1=\n23\n\n22\n\n21\n\n20\n0×8+\n1×4+\n1×2+\n0×1+\n1 × .5 +\n1 × .25 +\n0 × .125 +\n1 × .0625\n\n2-1\n\n2-2\n\n2-3\n\n2-4\n\n= 6.812510\n44\n\n\fFrom decimal to binary? Tricky?\n\n6.8125 10\n6÷210 = 3R0\n3÷210 = 1R1\n\n1 1 0.1101\n\n0.812510\nx\n2\n1.6250\n\nMSB\n\n0.625010\nx\n2\n1.2500\n0.250010\nx\n2\n0.5000\n0.500010\nx\n2\n1.0000\n\nLSB\n\n45\n\n\fSo, it’s easy right? Well…\n\nWhat about: 0.1 10\n\n0.110\nx 2\n0.2\n0.210\nx2\n0.4\n\n0.0001\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n46\n\n\fSo, it’s easy right? Well……\n\nWhat about: 0.1 10\n\n0.0001\n1001\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n47\n\n\fSo, it’s easy right? Well………\n\nWhat about: 0.1 10\n\n0.0001\n1001\n10\n0\n1\n...\n\n0.610\nx 2\n1.2\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n48\n\n\fHow much is it worth?\n\n•Well, it depends on where you stop!\n\n0.0001 2\n\n= 0.0625\n\n0.00011001 2\n\n= 0.0976…\n\n0.000110011001 2 = 0.0998…\n49\n\n\fLimited space!\n• How much should we store?\n• We have 32-bit registers, so 32-bits?\n• Let’s say we do!\n\n• How many bits are used to store the integer part?\n• How many bits are used to store the fractional part?\n\n• What are the tradeoffs?\n50\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":152,"segment": "unlabeled", "course": "cs0441", "lec": "lec13", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #13: Proof by Induction\n\nBased on materials developed by Dr. Adam Lee\n\n\fWe’ve learned a lot of proof methods…\nBasic proof methods\nl Direct proof, contradiction, contraposition, cases, …\n\nProof of quantified statements\nl Existential statements (i.e., ∃x P(x))\n➣ Finding a single example suffices\n\nl Universal statements (i.e., ∀x P(x)) can be harder to prove\n➣\n\n➣\n\nBottom line: We need new tools!\n\n\fMathematical induction lets us prove universally\nquantified statements!\nGoal: Prove ∀x∈N P(x).\n\nIntuition: If P(0) is true, then P(1) is\ntrue. If P(1) is true, then P(2) is\ntrue…\n\nProcedure:\n1. Prove P(0)\n2. Show that P(k) → P(k+1) for any arbitrary k\n3. Conclude that P(x) is true ∀x∈N\n\nP(0)\nFor arb. k, P(k) → P(k+1)\n∴∀x∈N P(x)\n\n\fAnalogy: Climbing a ladder\nProving P(0):\nl You can get on the first rung of the ladder\n\nProving P(k) → P(k+1):\nl If you are on the kth step, you can get to\nthe (k+1)th step\n\n∴ ∀x P(x)\nl You can get to any step on the ladder\n\n\fAnalogy: Playing with dominoes\nProving P(0):\nl The first domino falls\n\nProving P(k) → P(k+1):\nl If the kth domino falls, then the\n(k+1)th domino will fall\n\n∴ ∀x P(x)\nl All dominoes will fall!\n\n\fAll of your proofs should have the same overall\nstructure\nP(x) ≡ Define the property that you are trying to prove\nBase case: Prove the “first step onto the ladder.” Typically,\nbut not always, this means proving P(0) or P(1).\n\nInductive Hypothesis: Assume that P(k) is true for an arbitrary k\nInductive step: Show that P(k) → P(k + 1). That is, prove that once\nyou’re on one step, you can get to the next step. This\nis where many proofs will differ from one another.\n\nConclusion: Since you’ve proven the base case and\nP(k) → P(k + 1), the claim is true! ❏\n\n\fProve that\nP(n) ≡\nBase case: P(1): 1(1+1)\/2 = 1\n\n✔\n\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn 1+2+…+k = k(k+1)\/2\nn 1+2+…+k+(k+1) = k(k+1)\/2 + (k+1)\nn 1+2+…+k+(k+1) = k(k+1)\/2 + 2(k+1)\/2\nn 1+2+…+k+(k+1) = (k2 + 3k + 2)\/2\nn 1+2+…+k+(k+1) = (k+1)(k+2)\/2\n\nby I.H.\nk+1 to both sides\n\nfactoring\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fInduction cannot give us a formula to prove, but\ncan allow us to verify conjectures\nMathematical induction is not a tool for discovering new\ntheorems, but rather a powerful way to prove them\n\nExample: Make a conjecture about the sum of the first n odd\npositive numbers, then prove it.\nl 1=1\nl 1+3=4\nl 1+3+5=9\nl 1 + 3 + 5 + 7 = 16\nl 1 + 3 + 5 + 7 + 9 = 25\n\nThe sequence 1, 4, 9, 16, 25, … appears\nto be the sequence {n2}\n\nConjecture: The sum of the first n odd positive integers is n2\n\n\fProve that the sum of the first n positive odd\nintegers is n2\nP(n) ≡ The sum of the first n positive odd numbers is n2\nBase case: P(1): 1 = 1\n\n✔\n\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn 1+3+…+(2k-1) = k2\nn 1+3+…+(2k-1)+(2k+1) = k2+2k+1\nn 1+3+…+(2k-1)+(2k+1) = (k+1)2\n\nby I.H.\n2k+1 to both sides\nfactoring\n\nNote: The kth odd integer is 2k-1, the (k+1)th odd integer is\n2k+1\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fProve that the sum 1 + 2 + 22 + … + 2n = 2n+1 - 1 for all\nnonnegative integers n\nP(n) ≡\nBase case: P(0): 20 = 1\n\n✔\n\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn 1+2+…+2k = 2k+1-1\nn 1+2+…+2k+2k+1 = 2k+1-1+2k+1\nn 1+2+…+2k+2k+1 = 2k+1+2k+1-1\nn 1+2+…+2k+2k+1 = 2×2k+1-1\nn 1+2+…+2k+2k+1 = 2k+2-1\n\nby I.H.\n2k+1 to both sides\nassociative law\ndef’n of ×\ndef’n of exp.\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fWhy does mathematical induction work?\nThis follows from the well ordering axiom\nl i.e., Every set of positive integers has a least element\n\nWe can prove that mathematical induction is valid using\na proof by contradiction.\nl Assume that P(1) holds and P(k) → P(k+1), but ¬∀x P(x)\nl This means that the set S = {x | ¬P(x)} is nonempty\nl By well ordering, S has a least element m with ¬P(m)\nl Since m is the least element of S, P(m-1) is true\nl By P(k) → P(k+1), P(m-1) → P(m)\nl Since we have P(m) ∧ ¬P(m) this is a contradiction!\n\nResult: Mathematical induction is a valid proof method\n\n\fIn-class exercises\n$\n\n$%& − 𝑎\n𝑎𝑟\nif 𝑟 ≠ 1\nProblem 1: Prove that $ 𝑎𝑟 ! =\n𝑟−1\n!\"#\n$\n\n𝑛 3𝑛 − 1\nProblem 2: Prove that $ 3𝑗 − 2 =\n2\n!\"&\n\nHint: Be sure to\n1.\n2.\n3.\n4.\n5.\n\nDefine P(x)\nProve the base case\nMake an inductive hypothesis\nCarry out the inductive step\nDraw the final conclusion\n\n\fProve the formula for the sum of the first n\npositive squares\n$\n\nP(n) ≡ '\n\n𝑖% =\n\n!\"#\n\n𝑛 𝑛 + 1 2𝑛 + 1\n6\n\nBase case: P(1): 1% = #(#'#)(%'#)\n)\n\n✔\n\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn 1+4+9+…+k2 = k(k+1)(2k+1)\/6\nn 1+4+9+...+(k+1)2 = k(k+1)(2k+1)\/6 + (k+1)2\nn = k(k+1)(2k+1)\/6 + 6(k+1)2\/6\nn = (k+1)(2k2+k+6k+6)\/6 = (k+1)(2k2+7k+6)\/6\nn = (k+1)(k+2)(2k+3)\/6\nn = (k+1)((k+1)+1)(2(k+1)+1)\/6, ∴P(k+1)\n\nby I.H.\n(k+1)2 to both sides\ncommon denom.\nfactor k+1, mult.\nfactor\nproved for k+1\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fInduction can also be used to prove properties\nother than summations!\n\n≥<\n∈\n⊆\nInequalities\n\n≡\n| φ(p)\n\nDivisibility and results from\nnumber theory\n\n∪\n\nSet theory\nAlgorithms and data structures\n\n\fProve that 2n < n! for every positive integer n ≥ 4\nPrelude: The expression n! is called the factorial of n.\n\nDefinition: n! = n × (n-1) × … × 3 × 2 × 1\nExamples:\n\nNote how quickly the factorial of\nn “grows”\n\nl 4! = 4 × 3 × 2 × 1 = 24\nl 5! = 5 × 4 × 3 × 2 × 1 = 120\nl 6! = 6 × 5 × 4 × 3 × 2 × 1 = 720\nl 7! = 7 × 6 × 5 × 4 × 3 × 2 × 1 = 5,040\nl 8! = 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1 = 40,320\n\n\fProve that 2n < n! for every positive integer n ≥ 4\nP(n) ≡ 2n < n!\nBase case: P(4): 24 < 4!\n\n✔\n\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn 2k < k!\nn 2 × 2k < 2 × k!\nn 2k+1 < 2 × k!\nn 2k+1 < (k+1) × k!\nn 2k+1 < (k+1)!\n\nby I.H.\nmultiply by 2\ndef’n of exp.\nsince 2 < (k+1)\ndef’n of factorial\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fProve that n3 – n is divisible by 3 whenever n is a\npositive integer\nP(n) ≡ 3 | (n3 – n)\nBase case: P(1): 3 | 0\n\n✔\n\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn (k+1)3 – (k+1) = k3 + 3k2 + 3k + 1 – (k+1)\nn\n= k3 + 3k2 + 2k\nn\n= (k3 – k) + (3k2 + 3k)\nn\n= (k3 – k) + 3(k2 + k)\nn Note that 3 | (k3 – k) by the I.H. and 3 | 3(k2 + k) by definition,\nso 3 | [(k+1)3 – (k+1)]\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fIn-class exercises\nProblem 3: Prove that 𝑛' + 2𝑛 is divisible by 3 for any\npositive integer 𝑛\nProblem 4: Prove that 6$ − 1 is divisible by 5 for any\npositive integer 𝑛\n\nHint: Be sure to\n1.\n2.\n3.\n4.\n5.\n\nDefine P(x)\nProve the base case\nMake an inductive hypothesis\nCarry out the inductive step\nDraw the final conclusion\n\n\fProve that if S is a finite set with n elements, then S\nhas 2n subsets.\nP(n) ≡ Set S with cardinality n has 2n subsets\nBase case: P(0): ∅ has 20 = 1 subsets (i.e., ∅ ⊆ ∅) ✔\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn Let S be a set of size k\nn Assume without loss of generality that x ∉ S\nn Let T = S ∪ {x}, so |T| = k+1\nn ∀s⊆S (s ⊆ T) since T is a superset of S\nn Furthermore, ∀s⊆S (s ∪ {x} ⊆ T) since x ∈ T\nn Since S has 2k subsets by the I.H., T has 2×2k = 2k+1 subsets\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fFinal Thoughts\nn Mathematical induction lets us prove universally quantified\nstatements using this inference rule:\n\nP(0)\nP(k) → P(k+1)\n∴∀x∈N P(x)\nn Induction is useful for proving:\nl Summations\nl Inequalities\nl Claims about countable sets\nl Theorems from number theory\nl …\n\nn Next time: Strong induction and recursive definitions (Sections\n5.2 & 5.3)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":153,"segment": "unlabeled", "course": "cs0007", "lec": "lec11", "text":"CS 0007: Introduction to Java\nLecture 11\nNathan Ong\nUniversity of Pittsburgh\nOctober 13, 2016\n\n\fAnnoucements\n• October 17 – Fall Break\n• October 18 – Monday Schedule\n– Those who are signed up for Monday Lab\nmust attend\n– Those who are signed up for Tuesday Lab\nare not required to attend, but must\ncomplete the lab\n– No lecture\n\n\fAnnoucements\n• Project 0 is updated, please download\nthe new version\n• Rubric will be released very soon\n• This project must be done by yourself\n\n\fAnnoucements\n• Project 1 will be released while Project 0\nis going on just to get you to start\nthinking about it, but the due date will\nbe far into the future\n• There will only be two projects\n\n\fARRAYS, LOOPS, AND\nRECURSION\n\n\fA Sudden Need\n• I need a function that returns the first\n100 terms of the harmonic series.\n• Harmonic series: 1\n1 1\n\n n 1  2  3  ...\nn 1\n\n\fThe Dumb Way\npublic static void main(String[] args)\n{\ndouble term0 = 1.0;\ndouble term1 = 1.0\/2.0;\ndouble term2 = 1.0\/3.0;\n…\n}\n\n\fArrays\n• Arrays are ordered lists of things\ndepending on the type you specify\n• How do you make one?\n\n\fType[] name = new Type[size];\n\n\fint[] empty = new int[10];\n\n\fint[] list = {1,2,3,4};\n\n\fNow What?\n• I want the first element\n• list[0]\n• I want the last element\n• list[3]\n• I want the length\n• list.length == 4\n• All operations for int are allowed for an\nindividual element\n• list[0] + list[2] == list[3]\n\n\fHow do I get every element so I can\nprint it?\n• Reference every element\n• Many if statements!!!!!!\nYAAAAAAAAYYYY\n• No please don't\n• But what else can we do?\n• WILD KEYWORD APPEARS\n\n\fwhile(){…}\n• While a condition is true, run the block\n• While not at the end of the list, print out\nthe next element\nwhile(not at end of list)\n{\nSystem.out.println(next element);\n}\/\/end loop while(not at end of list)\n\n\fnot at end of list\n• How do we know we're at the end of the\nlist?\n• list.length\n• How do we know which one we're at?\n– We need to keep track\n\nint currElement = 0;\n\n– starts at 0\n– not equal to the end!\n\ncurrElement != list.length\n\n\fwhile(){…}\n• While not at the end of the list, print out the next\nelement\nint currElement = 0;\nwhile(currElement != list.length)\n{\nSystem.out.println(<next element>);\n}\/\/end loop while(currElement!=list.length)\n\n\fnext element\n• Do we really want to print out the next\nelement?\n• If we start at 0, and the list starts at 0, do\nwe?\n• No, we want it to print out the current\nelement and then move to the next one\n• How do we print out the current element?\nSystem.out.println(list[currElement]);\n\n• How do we move to the next one?\ncurrElement = currElement + 1;\n\n\fwhile(){…}\n• While not at the end of the list, print out the next\nelement\nint currElement = 0;\nwhile(currElement != list.length)\n{\nSystem.out.println(list[currElement]);\ncurrElement = currElement + 1;\n}\/\/end loop while(currElement!=list.length)\n\n\fTime to Optimize\n• currElement = currElement + 1;\n• This is really long. There must be a\nfaster way\n• There is! Use +=\n\n\f+=\n• Adds the right side to whatever was on the\nleft side\ncurrElement += 1;\nis equivalent to\ncurrElement = currElement + 1;\n• As it happens, people generally add one to\nmany things all the time, so there is a shortercut\n\n\f++\ncurrElement++;\nis equivalent to\ncurrElement = currElement + 1;\n• It is also common to subtract one from\nmany things all the time, so there is a\nsimilar shorter-cut\n\n\f-currElement––;\nis equivalent to\ncurrElement = currElement – 1;\n• Planning to add or subtract more than\none? Use += and –=\n\n\fMathematical Combination\nOperators\n+=, -=, *=, \/= (any numeric primitive)\n%=\n(int only)\n• Literally combines the operation and the\nassignment.\nvariable ◊= value  variable = variable ◊ value;\nint x = 5;\nx %= 3; \/\/x == ?\n\n\fwhile(){…}\nint currElement = 0;\nwhile(currElement != list.length)\n{\nSystem.out.println(list[currElement]);\ncurrElement++;\n}\/\/end loop while(currElement!\n=list.length)\n\n\fA Little Security\n• currElement != list.length\n• What happens if you accidentally added\nby three instead of one?\n• What happens if you accidentally\nchanged the value of currElement so\nit's past the list length?\n• Safer check:\ncurrElement < list.length\n\n\fwhile(){…}\nint currElement = 0;\nwhile(currElement < list.length)\n{\nSystem.out.println(list[currElement]);\ncurrElement++;\n}\/\/end loop\nwhile(currElement<list.length)\n\n\fSo Many Things We Have to\nRemember…WHYYYYYYYYYY\nint counter = start;\nwhile(condition)\n{\n\/\/Do stuff\ncounter increment\n}\/\/end loop while(condition)\n• There must be a better way\n• WILD KEYWORD APPEARS\n\n\ffor(;;){…}\n• The for loop allows us to put everything\ntogether for us!\n• for(counter = start; condition; counter\nincrement)\n• Now we don't have to worry about the\nplacement of the counter and\nincrementing it!\n\n\fEQUIVALENCE\nint counter = start;\nwhile(condition)\n{\n\/\/Do stuff\ncounter\nincrement\n}\/\/end loop\n\/\/while(condition)\n\nfor(int counter = start;\ncondition; counter\nincrement)\n{\n\/\/Do stuff\n}\/\/end loop\nfor(condition)\n\n\ffor(;;){…}\nfor(int currElement = 0; currElement <\nlist.length; currElement++)\n{\nSystem.out.println(list[currElement]);\n}\/\/end loop for(currElement <\nlist.length)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":154,"segment": "unlabeled", "course": "cs0447", "lec": "lec13", "text":"#13\n\nThe Interconnect,\nControl, and\nInstruction\nDecoding\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fInstruction Execution\n\n2\n\n\fPhases of instruction execution\n\nF\n\nD X M W\n\n1. Fetch (IF or F)\no use PC to get the next instruction from memory\n2. Decode (ID or D)\no look at the fetched instruction and set control signals\n3. Execute (EX or X)\no wait for data to flow through the datapath\n4. Memory Access (MEM or M)\no if it's a load or store, do that\n5. Write-back (WB or W)\no if there's a destination register, write the result to it\noften we can do multiple phases \"at the same time\"\n3\n\n\fWhich parts do what\n\nMemory\n\nControl\n\nPC\n\nRegister\nFile\n\nF\n\nD\n\nW\n\nALU\n\nX\n\nMemory\nagain\n\nM\n\nHow does lw work?\nlw t0, 12(s0)\n\n4\n\n\fA Thing about memory\n\n5\n\n\f☢️ Structural Hazards ☢️\n● how many RAMs does your computer have? one or two?\n● if we try to do lw t0, (s0) with one memory in a single cycle…\n\nPC\n\nInstruction\nAddress\n\nLoad word\naddress…?\n\nInstruction\nControl\n\nMemory\nLoaded\nword…??\nwhat about sw?!?\n\nwe can't really do this… memory hardware can't read\nfrom two addresses at the same time\n6\n\n\fVon Neumann vs Harvard\n● one way to solve this problem is to have two memories\nthis is a Harvard\nArchitecture\n\nInstruction\nMemory\n\nControl\n\nPC\n\nRegister\nFile\n\nALU\n\nData\nMemory\n\na Von Neumann Architecture has one memory for both things\n\"Von Neumann\" is 2 words for 1 memory…\n\"Harvard\" is 1 word for 2 memories…\n7\n\n\fMulti-cycle\n● a Von Neumann machine has one memory, but uses multiple\nclock cycles to execute each instruction\nCycle 1: Instruction\nAddress\n\nPC\n\nlw t0, (s0)\nInstruction\nControl\n\nMemory\nLoaded\nword\n\nCycle 2:\nLoad word\naddress\nmulti-cycle machines are by\nfar the most common today\n\nbut they're more complex…\n8\n\n\fThe Interconnect\n\n9\n\n\fGotta keep em separated interconnected\n● we've got pieces of a CPU, but they don't operate in isolation\n● we gotta hook em together. but which parts hook to which?\n● the instructions in the ISA tell you what has to connect to what.\nPC that\ncan branch\nand jump\n\nALU\n\nInstruction\nMemory\n\nRegister\nFile\n\nData\nMemory\n\n10\n\n\fSlowly coming together\n● if we look at all the different instructions we want to support, we'll\nstart to get an idea of what data goes where\n\nsub v0, t0, t1\nt0\n\ns0\n\nt1\n\nRegister\nFile\nt3\n\nv0 Register\n\nFile\n\nsw s0, 4(t3)\n\n-\n\nALU\n\nData\nData\nMemory\n\n+\n\n4\n\njal move_ball\naddress of\nmove_ball\n\nPC\n\n+ ra\n\n4\n\nALU?\n\nRegister\nFile\n\nALU\n\nAddress\n\nhow do we make all\nthese different things\nhappen with one set\nof hardware…?\n11\n\n\fPC to the left of me, ALU to the right, here I am\n● the interconnect lets the CPU parts combine in many ways\n● it's like the CPU's \"circulatory system\" – it moves data around\nPC\n\nInstruction\nMemory\n\njal\njr\n\nstores\n\nRegister\nFile\n\nli (immediate)\n\nloads\n\nData\nMemory\n\nadd, sub, etc.\n\naddi, ori etc.\n\nALU\n\nit's starting to take shape…\n\n12\n\n\fA little technique: an interconnect matrix\n● you can make a table to keep track of what things connect to what.\nDoes the data\nflow from\nhere…\nALU\n\n…to here?\nPC\n\nALU\nPC\n\nRegs\nIM\nDM\n\n√\n\n√\n√\n\nRegs\n\n√\n√\n√\n√\n\nIM\n\nDM\n\n√\n\nnow consider all the\ninstructions your CPU\nshould support, and mark\nthe cells accordingly.\n\nlw sw j beq\nadd, sub, and, or\njr li? jal\nany component (column) with\nmultiple things coming\ninto it will need a MUX.\n(huh? next slide.)\n\n13\n\n\fConjunction junction\n● the interconnect makes choices about which things go where\nonly one of these is written\nto the register file\n\nALU results\n\nso how do we choooooose\nwhich thing to write?\n\ndata from memory\n\nnow we have a select pin.\nthis is a control signal!\n\nRegister\nFile\n\ninstruction immediates\n\nsaved PC for jal\n\nthere will be several MUXes in the\ninterconnect, and each needs a control signal\n\n2\n\nRegDataSrc\n\nthe book calls this \"MemToReg“.\nBecause in its model the value is 1 when\nthe memory is read into a register\n\n14\n\n\fInterconnected (MIPS, not your project)\n● if we want to make a suuuuper simple version of MIPS, we can\nconnect the pieces together into a datapath like this\nData\n(this version doesn't\nRegWrite\nMemWrite\nData\nsupport jal, and …\nMemory\nbut that's fiiiine)\nrd\nrs\nrt\n\nAddress\n\nRegister\nFile\n\nALU\n\nRegDataSrc\nimm field\n\nALUSrc\n\nALUOp\n\nhow can we use this\nto implement add?\nsub? addi? lw? sw?\nli?\nbut now we need\nto, uh, control the\ncontrol signals.\n15\n\n\fThe Forgotten Phase:\nOperand Fetch\n\n16\n\n\fA little extra step\n● operand fetch is a phase of instruction execution you might see\n● it fetches the values to be operated on\n\nF\n\nD X M W\n\nit happens after the\ninstruction is decoded.\nwhere do values have to be for\nthe CPU to operate on them?\n\nin the registers…?\n\n17\n\n\fVestigial\n● in MIPS (and your project), operand fetch is super simple:\nthis is by design: load-store\narchitectures have very simple\noperand fetch phases.\n\nRegister\nFile\n\nALU\n\nimm field\n\nwhy? well…\n\nhere it is!\n\n18\n\n\fOperand Fetch in x86\n● as a CISC, x86 has some… crazy instructions.\n\ninc [eax + ecx*4 + 12]\nthis is an effective\naddress calculation.\n(the brackets mean \"access memory.\")\n\nthis is operand fetch.\nbe very glad you won't have\nto do this for your project.\n\nhere's what the CPU has to\ndo for this instruction:\n1. multiply ecx by 4\n2. add eax to that\n3. add 12 to that\n4. load a word from\nthat address\n5. add 1 to that value\n6. store that value back\ninto the address\n19\n\n\fThe Control\n\n20\n\n\fFeeling nervous\n● the control is what sets the write enables and selects to the\nappropriate values to make each instruction happen\n● it's like the CPU's brain and nervous system\n\nit does this by reading\nthe instructions.\n\nc'mon you\nlazy bums\nawwwww we\ndon't wannaaaa\n\nsub v0, t0, t1 👀Control\nRegister file, read t0 and t1, and write to\nv0. ALU, do subtraction. Interconnect,\nroute the data from the two registers into\nthe ALU and from the ALU into the register\nfile. Data memory, you get to take a break.\n\nALU\n\nRegister\nFile\n\nData\nMemory\n\nyissssss\n\n21\n\n\fTwo kinds of control signals\n● first there are the selects\no these go into the select pins of\nmuxes, demuxes, or decoders\no they can be any number of bits\n● then there are the write enables\no these tell registers and memory\nwhen to store data\no they're Booleans - 0 or 1\n● they often come in pairs!\no like RegWrite and RegDataSrc.\no they decide what to write and\nwhen to write it.\n\nRegWrite\nrd\n\n5\n\nRegister\nFile\n\nRegDataSrc\nMemWrite\n\nData\nMemory\n\n22\n\n\fGotta write it down\n● write enables are kind of the basis of \"things happening in a CPU\"\n● almost every instruction writes something somewhere!\n\nadd t0, t1, t2\nwrites to t0\n\nbeq s0, 10, end\n\nmight write to the PC\n\nsw s0, (t0)\nwrites to memory\n\njal func1\nwrites to the PC and ra!\n\nif an instruction doesn't write anything, it's a no-op (nop).\n(if an instruction does not change anything, did it ever happen?)\n\nwhat changes when a conditional branch isn't taken?\n\n23\n\n\fThe control hardware\n\n4\n\n+\n\n● we connected the datapath together; now for the control bits\n\nimmediate\n(jump target) PCSrc\n\nPC\n\nrt\n\nrd\n\nimmediate\n\naddress goes in… Instruction\n…instruction\nMemory\ncomes out.\n\nPCSrc\n\nrs\n\nControl\n\nRegWrite\nMemWrite\n\ninstruction\ngoes in…\n…control signals\ncome out.\nsomehow.\n\nALUSrc\nALUOp\n\nRegDataSrc\n\n24\n\n\fInstruction Decoding\n\n25\n\n\fPull 'n' peel\n● the first step is to split the encoded instruction up\n● but which instruction format is it? actually, it doesn't matter.\n\nR opcode\n31\n\n31\n\n26 25\n\nrs\n\n26 25\n\nI opcode\n31\n\n21 20\n\n16 15\n\nrt\n21 20\n\nrs\n\n0\n\n31-26\n\nshamt funct\n\n25-21\n\n0\n\n20-16\n\n11 10\n\nrd\n\n6 5\n\n16 15\n\nrt\n\nimmediate\n\n26 25\n\nJ opcode\n\n15-11\n0\n\ntarget\n\n\"do everything at once, but\n32\nuse only what you need.\" instruction\n\n10-6\n5-0\n15-0\n25-0\n\nopcode\nrs\nrt\nrd\nshamt\nfunct\nimmediate\ntarget\n26\n\n\fNo, really, it's fine, don't worry about it\n● suppose the encoded instruction was addi s0, s0, -1.\nop\n\nput it through\nthe splitter and…\naddi s0,s0,-1 32\n\n0x2210FFFF\n\n…out come a\nbunch of values.\n\n31\n\n26 25\n\n21 20\n\n16 15\n\n0\n\nrt\nimmediate\n0x08 opcode rs\nrs\naddi is an I-type instruction.\n0x10\nrt\n0x10\nopcode, rs, rt, and immediate\nrd\nwill be used.\n0x1F\nshamt\nthe rest are bogus and will be\n0x1F\nfunct\nignored. see? it's fiiiiiine\n0x3F\nimm\n0xFFFF\ntarget\n0x210FFFF\n\n27\n\n\fMaking the control work\n● the control is a boolean function that takes the instruction\nopcode as its input and outputs the control signals.\n● in other words, it's a big fat truth table.\nopcode\n\nPCSrc\n\nRegDataSrc\n\nRegWrite\n\nALUOp\n\n…\n\n000000\n000001\n000010\n000011\n000100\n000101\n…\n\n0\n0\n0\n1\n1\n0\n…\n\n00\n01\n00\n00\n11\n10\n…\n\n0\n1\n1\n0\n1\n1\n…\n\n000\n110\n010\n011\n000\n010\n…\n\n…\n…\n…\n…\n…\n…\n…\n\nThis is not the only way of\nmaking your control unit.\nit's time-consuming,\nconfusing, hard to debug,\nand hard to change.\nyou will go insane.\n\nThese are made up numbers. Please don't try to use them!\n\n28\n\n\fA more approachable approach\n● Here's a great use for a decoder: decoding. (huh.)\nopcode\n\n<r-type>\nexactly one of these will\nbe on at a time.\n<uhh random crap>\nj\njal\nnow it's just a matter of coming up with\nthe logic for each of the control signals.\nbeq\nbne\nblez\nfor that, it's good to focus on one\ncontrol signal at a time.\nbgtz\naddi\naddiu\nslti\nsltiu\n\n29\n\n\f<the sound a seal makes>\n● let's say we want to come up with the MemWrite control signal\n● which MIPS instructions write to memory?\nsw\nsh\n\nMemWrite\n\npretty straightforward, huh?\n\nsb\nwhat about multi-bit control signals,\nlike your ALU operation?\nthere are a few approaches…\n\n30\n\n\fThe brute-force approach: the MUXtipede\n● in this approach, you use enormous MUXes to select constants.\nopcode\nALUOp for opcode 0\nALUOp for opcode 1\nALUOp for opcode 2\nALUOp for opcode 3\nALUOp for opcode 4\nALUOp for opcode 5\nALUOp for opcode 6\nALUOp for opcode 7\nALUOp for opcode 8\nALUOp for opcode 9\nALUOp for opcode A\nALUOp for opcode B\n\nit… works, but it's hard to follow.\nit's hard to tell which constant\nis used for which instruction.\nit's also hard to add new\ninstructions.\nALUOp\nwe can make this a tiny\nbit MUCH MORE\nelegant!\n31\n\n\fPriority Encoders\n● a priority encoder is kind of the opposite of a decoder.\n● you give it several 1-bit inputs, and it tells you which one is 1.\n1\n\n0\n\n0\n\n1\n\n0\n\nPri\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n0\n\nPri\n\n2\n\n0\n1\n\nPri\n\nif none of the inputs is 1,\nthen it gives you X…\n1\n\n0\n0\n\n0\n\nPri\n\nX\n\n0\n\nPri\n\n3\n\nto avoid this, put a constant\n1 as the first input.\n32\n\n\fMulti-bit control signals\n● let's say we have these instructions, and these ALU operations.\n● for each input, ask: which instructions need this ALU operation?\n\nadd\naddi\nsub\nsubi\nand\nor\n\n0: &\n1: |\n2: +\n3: -\n\nALUOp 0 is the default,\nso and is handled.\n\n1\n\nwhich instruction(s)\nneed OR (1)?\n\nor\n\nwhat about + (2)?\nwhat about - (3)?\n\nadd\naddi\nsub\nsubi\n\nPri\n\nALUOp\n\nthink of it like an\nupside-down if-else-if…\n\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":155,"segment": "unlabeled", "course": "cs0007", "lec": "lec19", "text":"CS 0007: Introduction to Java\nLecture 19\nNathan Ong\nUniversity of Pittsburgh\nNovember 15, 2016\n\n\fMain Portions of OOP\n• Building a class\n• Determining inter-class relationships\n\n\fClass Components\n• Class-level\/Instance Variables\n• Methods\n• Constructor(s)\n\n\fInter-class Relationships\n• Building a single class is useless. It\nneeds to be used in context.\n• There are two main relationships\nbetween classes\n– Sub\/Super class\n– Ownership\n\n\fModeling a Problem\n“Joe’s Automotive Shop services foreign cars, and\nspecializes in servicing cars made by Mercedes,\nPorsche, and BMW. When a customer brings a car to\nthe shop, the manager gets the customer’s name,\naddress, and telephone number. Then the manager\ndetermines the make, model, and year of the car, and\ngives the customer a service quote. The service\nquote shows the estimated parts charges, estimated\nlabor charges, sales tax, and total estimated\ncharges.”\nSource: Starting Out with Java: From Control Structures\nthrough Objects by Tony Gaddis\n\n\fClass Overview\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\n\fThe Object Class\n• Let us examine the API for the highest\nclass.\n• toString() looks interesting. “It is\nrecommended that all subclasses\noverride this method.”\n\n\fWhat Is toString()?\n• It provides a String representation of\nan object\n• It should be descriptive, not just what\ntype it is, but what is contained within it.\n\n\fFunctional Responsibility\n• How do we determine which class(es)\nshould have which functions?\n• You need to ask: Who’s state is needed\nor changing?\n\n\fHard example\n• Using the automotive shop example, we\nhave four classes, Shop, Customer, Car,\nand Service Quote.\n• Who deals with car alterations?\n– Shop?\n– Mechanic?\n– Car?\n\n\fADVANCED INPUT AND\nSIMPLE EXCEPTION\nHANDLING\n\n\fScanners\n• Scanners actually have an\ninconveniently annoying flaw\n• Some of you may have already\nencountered it\n\n\fScanner Issue\nimport java.util.Scanner;\npublic class ScannerAttempt\n{\npublic static void main(String[] args)\n{\nScanner scan = new Scanner(System.in);\nSystem.out.println(\"Enter your age\");\nint age = scan.nextInt();\nSystem.out.println(\"Enter your name\");\nString name = scan.nextLine();\nSystem.out.println(name + \" is \" + age +\n\" years old.\");\n}\/\/end method main\n}\/\/End class ScannerAttempt\n\n\fI couldn’t\neven enter\nmy name\nbefore it\nfinished\n\n\fLong Story Short\n• Scanner has a buffering quirk. When\nscanning for a token (an item like\nboolean, int) that is not a line, the\nscanner reads up to the new line\ncharacter, but does not consume it.\n• When you call nextLine(), it sees the\nnew line, assumes it is done reading,\nand returns an empty String.\n\n\fWhat Should Happen Then?\nThere are two ways of dealing with it\n• Call nextLine() twice\n• Use the appropriate classes and parse\nthe input\n\n\fA Reminder\n• The act of receiving input is called\nreading\n• Relevant classes that do reading are\nReaders\n\n\fA Re-examination\n• System.in\n• An InputStream\n• If we examine the API, InputStreams\ncan only read bytes. Not very helpful.\n• What could we try?\n\n\fInputStreamReader\n• Now we can read characters, but having\nto combine all the characters is difficult\n• Why don’t we use the suggestion from\nthe API?\n\n\fBufferedReader\n• This thankfully allows us to read full\nlines of input\n• The input needs to be processed\ndepending on your needs\n• Let us first learn how to use\nBufferedReader correctly\n\n\fExample\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\npublic class BRExample\n{\npublic static void main(String[] args)\n{\nBufferedReader reader = new\nBufferedReader(new\nInputStreamReader(System.in));\nString line = reader.readLine();\nSystem.out.println(line +\n\" is what you wrote.\");\n}\/\/end method main\n}\/\/End class BRExample\n\n\f\fExceptions\n• An exception is an event meant to disrupt the\nflow of execution\n• To throw an exception is to see an exception\nbeing raised\n• To catch an exception is to acknowledge a\nthrown exception\n• You probably have already encountered some\nwhile debugging your code\n• We will focus on two of three types, which are\nmore common\n\n\fException Types\n• Checked\n\n– Exceptions that need to have a contingency plan\nshould the exception arise\n– These stop the compiler from running\n– Example: IOException\n\n• Unchecked\n\n– Exceptions that tend to indicate the program has\na flaw during its execution\n– These stop the program in the middle of\nexecution\n– Example: ArrayIndexOutOfBoundsException\n\n\fDealing with Exceptions\n• Checked\n– try, catch, finally\n\n• Unchecked\n– Fix your program code\n\n\fExample\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\n\npublic class BRExample\n{\npublic static void main(String[] args)\n{\nString line = \"\";\nSystem.out.println(\"Type something!\");\ntry\n{\nBufferedReader reader = new\nBufferedReader(new\nInputStreamReader(System.in));\nline = reader.readLine();\n}\ncatch (IOException e)\n{\ne.printStackTrace();\n}\nSystem.out.println(line + \" is what you wrote.\");\n}\/\/end method main\n}\/\/End class BRExample\n\n\fKeywords in Context\n• Everything inside the try block\nindicates the section of code that may\nthrow exceptions\n• Inside the catch’s parentheses is the\nexpected exception to process, and the\nname given to it (usually ‘e’)\n• Inside the catch block indicates the\nsection of code that should be executed\nif the exception is caught\n\n\fSome FAQ\n• How do I know when an exception is thrown?\n– Check the API for relevant methods\n\n• How do I know which exception is thrown?\n– Check the API\n\n• How much code should I surround with try?\n– Only as much as you need\n– Note that scope applies here as well\n\n• Can I catch multiple exceptions?\n– Yes, you can use several catch blocks\n– We’ll see an example soon\n\n\fConverting Strings into Useful\nPrimitives\n• Scanner gave us useful methods like\nnextInt() or nextDouble(). We can\ndo the same with BufferedReader, but\nnot directly.\n• We need to take another trip to the API,\nspecifically for the classes that back the\nprimitives.\n• Let’s look at Integer\n\n\fExample\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\npublic class Converter\n{\npublic static int convertToInt(String line)\n{\ntry\n{\nreturn Integer.parseInt(line);\n}\ncatch (NumberFormatException e)\n{\n\/\/??\n}\n}\/\/end method main\n}\/\/End class BRExample\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":156,"segment": "unlabeled", "course": "cs1502", "lec": "lec02_finite_automata_01","text":"Finite Automata 01\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFinite Automata\nSuppose you are asked to write a software to control an\nautomatic door as shown below:\nfront\npad\n\nrear\npad\n\ndoor\n\nAssume we have the following methods:\ngetFrontPad(): returns true if there is a person standing on\nthe front pad. Otherwise, it returns false.\ngetRearPad(): returns true if there is a person standing on\nthe rear pad. Otherwise, it returns false.\nopenDoor(): when called it will open the door.\ncloseDoor(): when called it will close the door.\n\nHow the write the program in Java?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFinite Automata\nProgram to control the automatic door:\npublic class DoorController {\npublic static void main(String[] args) {\nboolean isDoorOpen = false;\nwhile(true) {\nif(getFrontPad() && !getRearPad() && !isDoorOpen) {\nopenDoor();\nisDoorOpen = true;\n}\nif(!getFrontPad() && !getRearPad() && isDoorOpen) {\ncloseDoor();\nisDoorOpen = false;\n}\n}\n}\n}\n\nThe variable isDoorOpen of type boolean is used to record\nthe status of the door (1 bit of memory is required).\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFinite Automata\n\ngetFrontPad() and getRearPad() together acts as external\ninput to the program:\ngetFrontPad() getRearPad()\nInput\ntrue\ntrue\nBoth\ntrue\nfalse\nFront\nfalse\ntrue\nRear\nfalse\nfalse\nNeither\nWe can define the behavior of our program based on its input\nas well as the status of the door whether it is current open or\nclose\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fRepresentations\nThe program can be represented in two standard ways\nState Diagram:\nRear\n\nRear\n\nFront\nBoth\n\nOpen\n\nClosed\n\nBoth\n\nNeither\n\nNeither\n\nState Transition Table:\nNeither\nClosed Closed\nOpen\nClosed\n\nFront\n\nFront\nOpen\nOpen\n\nRear\nClosed\nOpen\n\nBoth\nClosed\nOpen\n\nBut how to represent these in a mathematical way?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFinite State Machine\nConsider the following finite state machine M1 :\n0\n\n1\n\n0\n\n1\n\nq0\n\nq1\n\nq2\n0, 1\n\nMachine M1 consists of:\nThree states: q0 , q1 , and q2\nThe start state q0 (arrow pointing to it from nowhere)\nAn accept state q1 (double circle)\nAll single circle states are called non-accept state\n\nArrows represent transition functions\nThe label 0, 1 represents two transitions\nq1\n\nq2\n0\n1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFinite State Machine\nConsider the following finite state machine M1 :\n0\n\n1\n\n0\n\n1\n\nq0\n\nq1\n\nq2\n0, 1\n\nWhen an input string is given to this machine, it returns\neither accept or reject.\n1101: accept\n1\n\n1\n\n0\n\n1\n\nq0 → q1 → q1 → q2 → q1\n\n(an accept state)\n\n0010: reject\n0\n\n0\n\n1\n\n0\n\nq0 → q0 → q0 → q1 → q2\n\n(a non-accept state)\n\n0100: accept\n0\n\n1\n\n0\n\n0\n\nq0 → q0 → q1 → q2 → q1\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n(an accept state)\n\nFinite Automata 01\n\n\fFinite State Machine\nConsider the following finite state machine M1 :\n0\n\n1\n\n0\n\n1\n\nq0\n\nq1\n\nq2\n0, 1\n\nCan we define the set of inputs that is accepted by the above\nmachine?\nM1 accepts any strings that end with a 1\nM1 also accepts a string that ends with a 0 but it needs to\nhave even number of 0s after the last 1\n\nThe set of all strings accepted by this machine is\n{x | x ends with a 1 and x is a string\nthat ends with an even number of 0s following the last 1}\nThe above set is called the language of the machine M1\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFinite-State Automaton\nA finite state machine M can be defined as five tuple\nM = (Q, Σ, δ, q0 , F )\nQ is a non-empty finite set of states\nM must have at least one state\n\nΣ is an alphabet (a finite set of symbols)\nδ : Q × Σ → Q is the transition functions\nWe generally use a table to represent δ\n\nq0 ∈ Q is the starting state\nA finite automata can only have exactly one start state\n\nF ⊆ Q is the set of accept states\nF can be ∅\nM can have no accept state (rejects all strings)\n|F | can be more than 1\nM has more than one accept states\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fExample: Machine M1\n0\n\n1\n\n0\n\n1\n\nq0\n\nq1\n\nq2\n0, 1\n\nM1 = (Q, Σ, δ, q0 , F )\nQ = {q0 , q1 , q2 }\nΣ = {0, 1}\nδ can be defined using the table below:\nδ\n0\n1\nq0 q0 q1\nq1 q2 q1\nq2 q1 q1\nq0 is the start state\nF = {q1 }\n\nThe state diagram and its formal definition are equivalent\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fFormal Definition of Machine M1\nM1 = ({q0 , q1 , q2 }, {0, 1}, δ, q0 , {q1 }) where δ is as follows:\nδ\n0 1\nq0 q0 q1\nq1 q2 q1\nq2 q1 q1\nThe above formal definition allows use to precisely answer\nquestions about M1 :\nIs 0101 is a valid input for this machine?\nYes. 0 ∈ {0, 1} and 1 ∈ {0, 1}.\n\nIs 01a0 is a valid input for this machine?\nNo. a 6∈ {0, 1}\n\nIs input 010 accepted by this machine?\n0\n\n1\n\n0\n\nNo. q0 → q0 → q1 → q2 and q2 6∈ {q1 }.\n\nIs input 101 accepted by this machine?\n1\n\n0\n\n1\n\nYes. q0 → q1 → q2 → q1 and q1 ∈ {q1 }.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fLanguage Recognized\n\nA string w = w1 w2 . . . wn is accepted by M if and only if\nafter processing each symbol wi of w, where 1 ≤ i ≤ n, M\nfinds itself in an accept state (a state belonging to F ).\nOtherwise, we say w is rejected by M .\nIf A is the set of all strings accepted by M , we say A is the\nlanguage of finite-state machine M , denoted by\nL(M ) = A\nWe say that M recognizes A\nA machine may accept several strings but it always recognizes\nonly one language.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fExample\n0\n\n1\n1\n\nq1\n\nq2\n0\n\nM2 = (Q, Σ, δ, start state, F )\nWhat is the formal definition of the above machine and the\nlanguage that it recognises?\nQ = {q1 , q2 }\nΣ = {0, 1}\nδ\n0 1\nq1 q1 q2\nq2 q1 q2\nThe start state is q1\nF = {q2 }\nL(M2 ) = {w | w ends in a 1}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fExample\n0\n\n1\n1\n\nq1\n\nq2\n0\n\nM3 = (Q, Σ, δ, start state, F )\nWhat is the formal definition of the above machine and the\nlanguage that it recognises?\nQ = {q1 , q2 }\nΣ = {0, 1}\nδ\n0 1\nq1 q1 q2\nq2 q1 q2\nStart state is q1\nF = {q1 }\nL(M2 ) = {w | w is the empty string ε or ends in a 0}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fExample\nQ = {s, q1 , q2 , r1 , r2 }\nΣ = {a, b}\na\n\nb\n\nr1\n\nq1\n\na\nb\nb\n\ns\n\na\n\nq2\n\na\n\nb\nb\n\nr2\n\nM4 = (Q, Σ, δ, start state, F )\n\na\n\nTransition Functions:\nδ\na b\ns q1 r1\nq1 q1 q2\nq2 q1 q2\nr1 r2 r1\nr2 r2 r1\nStart state is s\nF = {q1 , r1 }\n\nWhat is the formal definition of\nthe above machine and the language that it recognises?\n\nL(M2 ) =\n{w | w starts and ends\nwith the same symbol}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fExample\nQ = {q0 , q1 , q2 }\n\n0\n\nΣ = {0, 1, 2, <RESET>}\nq1\n\n<RESET>\n\n1\n2\n\n0\n\n2\n\n1\n2\n\nq0\n\n1\n\nq2\n\n0\n\n<RESET>\n<RESET>\n\nM5 = (Q, Σ, δ, start state, F )\nWhat is the formal definition of\nthe above machine and the language that it recognises?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTransition Functions:\nδ\n0 1 2 <RESET>\nq0 q0 q1 q2\nq0\nq1 q1 q2 q0\nq0\nq2 q2 q0 q1\nq0\nStart state is q0\nF = {q0 }\nL(M2 ) =\n{w | w is the empty string ε or\nends with <RESET> or\nsum of input is multiple\nof 3 after the last\n<RESET>}\nFinite Automata 01\n\n\fDesigning a Finite-State Machine\n\nA computation model simulates a set of algorithms\nDesigning a finite-state machine is the same as writing a\nprogram\nUse states to capture state-of-minds\nI just see a 1\nI just see two consecutive 0s\nI already saw 00 or 11\n\nDo not force yourself to use the least number of states\nNobody asks you to write a shortest possible program\nUnless you are asked to do so\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fDesigning Finite Automata\nSuppose the alphabet Σ is {0, 1}. Create a machine such that its\nlanguage is the set of all strings that contain either 11 or 00 as a\nsubstring.\nq1\n1\n\nq0\n\n1\n\n1\n\nq3\n\n0\n\n0\n\n0\n\nq2\n\nCommon mistakes:\nδ(q1 , 0) = q0\nδ(q2 , 1) = q0\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n0,1\n\n\fDesigning Finite Automata\nSuppose the alphabet Σ is {0, 1}. Create a machine such that its\nlanguage is the set of all strings that contain 011 as a substring.\n0\n\n1\n\n0\n\n1\n0\n\nqs\n\n1\n\nq0\n\nq01\n\nq011\n\n1\n\n0\n\nCommon mistakes:\nδ(q0 , 0) = qs\nδ(q01 , 0) = qs\n\nHint: Name of a state can be used to indicate a state-of-mind\nq01 means “I just see a 0 immediately followed by a 1”\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\n\fDesigning Finite Automata\nSuppose the alphabet Σ is {0, 1}. Create a machine such that its\nlanguage is the set of all strings that ends with 0110.\n1\n\n0\n1\n0\n\nq0\n\n1\n\nq1\n\nq2\n\n0\n\nq3\n\n0\n1\n\n0\n\nCommon mistakes:\nδ(q2 , 0) = q0\nδ(q4 , 0) = q0\nδ(q4 , 1) = q0\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 01\n\nq4\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":157,"segment": "unlabeled", "course": "cs0441", "lec": "lec16", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #16: Counting Basics\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nn Introduction to combinatorics\nn Product rule\nn Sum rule\n\n\fWhat is combinatorics?\nCombinatorics is the study of arrangements of discrete\nobjects.\nWe can think of this as a fancy\nword for “counting”\n\nMany applications throughout computer science:\nl Algorithm complexity analysis\nl Resource allocation & scheduling\nl Security analysis\nl…\n\nToday, we will learn the basics of counting. More\nadvanced topics will be covered in later lectures.\n\n\fA motivating example…\nTo access most computer systems, you need to login\nwith a user name and a password.\n\nSuppose that for a certain system\nl Passwords must contain either 6, 7, or 8 characters\nl Each character must be an uppercase letter or a digit\nl Every password must contain at least one digit\n\nHow many valid passwords are there?\n\n\fSolving these types of problems requires that we learn\nhow to count complex objects\nFortunately, we can solve many types of combinatorial\nproblems using two simple rules:\n\nThe product rule\n\nThe sum rule\n\n\fProduct rule applies when a counting problem\ncan be broken into multiple tasks\nThe Product Rule: Suppose a procedure can be broken\ninto a sequence t1, t2, …, tk of tasks. Further, let there\nbe n1, n2, …, nk ways to complete each task. Then there\nare n1× n2× …× nk ways to complete the procedure.\nTo apply the product rule, do the following:\n1. Identify each task t1, …, tk\n2. For each task ti, determine the ni, the number of possible\nways to complete ti\n3. Compute n1× n2× …× nk\n\nLet’s look at a few examples…\n\n\fAn example: Assigning offices\nExample: It is Sherif and Bill’s first day of work at Pitt.\nIf there are 10 unused offices in their department, how\nmany ways can Sherif and Bill be assigned an office?\n\nStep 1: Determine tasks\n1. Give Sherif an office\n2. Give Bill an office\n\nStep 2: Count possible completions\n1. Can give any one of 10 offices to Sherif\n2. Can give any one of the remaining 9 offices to Bill\n\nStep 3: Compute the product\nl\n\nSherif and Bill can be assigned offices in 10× 9 = 90 ways!\n\n\fAuditorium Seating\nExample: The chairs in an auditorium are to be labeled\nusing an upper case letter and a positive number not\nexceeding 100 (e.g., B23). What is the maximum\nnumber of seats that can be placed in the auditorium?\n\nSolution:\nl Task 1: Count the letters that can be used\n(26)\nl Task 2: Count the numbers that can be used\n(100)\nl So, the auditorium can hold 26× 100 = 2600 chairs.\n\n\fCounting Bit Strings\nExample: How many bit strings of length 5 are there?\nSolution:\nl Task 1: Choose first bit\nl Task 2: Choose second bit\nl Task 3: Choose third bit\nl Task 4: Choose fourth bit\nl Task 5: Choose fifth bit\n\n(2)\n(2)\n(2)\n(2)\n(2)\n\nl So, there are 2× 2× 2× 2× 2 = 25 = 32 bit strings of length 5\n\n\fCounting 1-to-1 Functions\nExample: How many 1-to-1 functions are there\nmapping a set A containing m elements to another set\nB containing n elements (assuming that m ≤ n)?\n\nSolution:\nl Task 1: Map first element of A to B\nl Task 2: Map second element of A to B\nl Task 3: Map third element of A to B\nl …\nl Task m: Map last element of A to B\n\n(n)\n(n-1)\n(n-2)\n(n-m+1)\n\nl So, there are a total of n ×(n-1) ×… ×(n-m+1) 1-to-1\nfunctions from A to B\n\n\fLicense Plates\nExample: Suppose that in some state, license plates\nconsist of three letters followed by three decimal\ndigits. How many valid license plates are there?\n\nA B C 1 2 3\n26 choices for each\n\n10 choices for each\n\nSolution: There are 263×103 = 17,576,000 possible\nvalid license plates\n\n\fIn-class exercises\nTop Hat\n\n\fThe sum rule applies when a single task can be\ncompleted using several different approaches\nThe Sum Rule: Suppose that a single task can be\ncompleted in either one of n1 ways, one of n2 ways, …,\nor one of nk ways. Then the task can be completed in n1\n+ n2 + … + nk different ways.\nNote: We can break the set of all possible solutions to\nthe problem into disjoint subsets. E.g., if we have k\n“classes” of solutions, then S = S1 ∪ S2 ∪ … ∪ Sk\nl |S| = |S1 ∪ S2 ∪ … ∪ Sk|\nl\n= |S1| + |S2| + … + |Sk|\nl\n= n1 + n2 + … + nk\n\nSince S1, …, SK are disjoint\n\n\fUniversity Committees\nExample: Suppose that either a CS professor or a CS\ngraduate student can be nominated to serve on the CS\nDay Committee. If there are 21 CS professors and 101\nCS graduate students, how many ways can this seat on\nthe committee be chosen?\n\nSolution:\nl Let\n➣ P be the set of professors\n➣ G be the set of graduate students\n➣ S be the solution set, with S = P ∪ G\n\nl Then there are |S| = |P ∪ G| = |P| + |G| = 21 + 101 = 122\nways to fill the empty seat on the committee.\n\n\fTravel Choices\nExample: Jane wants to travel from Pittsburgh to New\nYork City. If she flies, she can leave at any one of 12\ndeparture times. If she takes the bus, she can leave at\nany one of 6 departure times. If she takes the train, she\ncan leave at any one of 4 departure times. How many\ndifferent departure times can Jane choose from?\n\nSolution:\nl S = F ∪ B ∪ T, so\nl |S| = |F ∪ B ∪ T|\nl\nl\nl\n\n= |F| + |B| + |T|\n= 12 + 6 + 4\n= 22 departure times\n\n\fThe product and sum rules are kind of boring…\nMost interesting counting problems cannot\nbe solved using the product rule or the\nsum rule alone…\n\n… but many interesting problems can be\nsolved by combining these two approaches!\n\nLet’s revisit our password example…\n\n\fPasswords revisited…\nTo access most computer systems, you need to login\nwith a user name and a password.\nUse the product rule to count\npasswords of each possible\nlength!\n\nChoices: Sum rule!\n\nSuppose that for a certain system\nl Passwords must contain either 6, 7, or 8 characters\nl Each character must be an uppercase letter or a digit\nl Every password must contain at least one digit\n\nHow many valid passwords are there?\n\n\fFirst, we’ll apply the sum rule\nLet:\nl P6 = Set of passwords of length 6\nl P7 = Set of passwords of length 7\nl P8 = Set of passwords of length 8\nl S = P6 ∪ P7 ∪ P8\n\nNote: |S| = |P6| + |P7| + |P8|\nSince each element of P6, P7, and P8 is made up of\nindependent choices of letters and numbers, we can\napply the product rule to determine |P6|, |P7|, and\n|P8|\n\n\fRecall: a password must contain at least one number!\n\nObservation: To figure out the number of 6-character\npasswords containing at least one number, it is easier\nfor us to count all 6-character passwords and then\nsubtract away those passwords not containing a\nnumber.\nNote: there are\nl (26 + 10)6 = 366 6-character passwords\nl 266 6-character passwords not containing a digit\n\nSo, |P6| = 366 – 266 = 1,867,866,560\n\n\fWrapping it all up…\nWe can compute\nl |P6| = 366 – 266 = 1,867,866,560\nl |P7| = 367 – 267 = 70,332,353,920\nl |P8| = 368 – 268 = 2,612,282,842,880\n\nBy leveraging our earlier observation that |S| = |P6| + |P7|\n+ |P8|, we can conclude that there are\n2,684,483,063,360 valid passwords for our target\nsystem.\n\n\fIP Addresses\nAn IP address (in IPv4) is a 32-bit string that is used to\nidentify a computer that is connected to the Internet.\n\nThere are three categories of IP addresses that can be\nassigned to computers:\n1. Class A addresses consist of the prefix “0” followed by a 7bit network ID and a 24-bit host ID\n2. Class B addresses consist of the prefix “10” followed by a\n14-bit network ID and a 16-bit host ID\n3. Class C addresses consist of the prefix “110” followed by a\n21-bit network ID and an 8-bit host ID\n\n\fSo how many valid IP addresses are there?\nNote: IP addresses are subject to restrictions:\nl 1111111 cannot be used as the network ID of a Class A IP\nl Host IDs consisting of only 1s or only 0s cannot be used\n\nTo count IP addresses, we will use the sum rule and\nthe product rule. So S = SA ∪ SB ∪ SC, so |S| = |SA|\n+ |SB| + |SC|\nCompute SA:\nl 27 – 1 network IDs since 1111111 can’t be used\nl 224 – 2 host IDs for each network ID\nl Total of 2,130,706,178 Class A IP addresses\n\n\fSo how many valid IP addresses are there? (cont.)\nCompute SB:\nl 214 network IDs\nl 216 – 2 host IDs for each network ID\nl Total of 1,073,709,056 Class B IP addresses\n\nCompute SC:\nl 221 network IDs\nl 28 – 2 host IDs for each network ID\nl Total of 532,676,608 Class C IP addresses\n\nSince |S| = |SA| + |SB| + |SC|, there are\n3,737,091,842 IP addresses that can be assigned to\ncomputers connected to the Internet!\n\n\fIn-class exercises\nTop Hat\n\n\fFinal Thoughts\nn Combinatorics is just a fancy word for counting!\nn There are many uses of combinatorics throughout\ncomputer science\nn We can solve a variety of interesting problems using\nsimple rules like the product rule and the sum rule\nn Next time:\nl Inclusion\/Exclusion principle (Section 6.1)\nl The pigeonhole principle (Section 6.2)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":158,"segment": "self_training_1", "course": "cs0447", "lec": "lec0F", "text":"#F\n\nLatches\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\nLuis Oliveira\nOriginal slides - wilkie\n(with content borrowed from: Jarrett Billingsley and Bruce Childers)\n\nFall 2020\n\n1\n\n\fAnnouncements\n\n2\n\n\fThe Latch\n\n3\n\n\fWhat the heck\n● Time to blow your mind.\n● Let’s look at this circuit.\no Yes, you can do this.\n\nR\n\nS\n\nQ\n\nQ\n4\n\n\fWhat the actual heck\n● What is this in combinational logic??\n\no Q = R + (S + (R + (S + (R + (… oh gosh. Hmm\n● This makes no sense (from a combinational point of view)\n\nR\n\nS\n\nQ\n\nQ\n5\n\n\fS\/R Latch\n● The feedback behavior allows it to “store” a value.\n\no “1” on S will set Q to “1”, and “1” or R will reset Q to “0”\no It becomes stable at that value, hence, it is an S\/R Latch.\no It’s really heckin’ neat.\n\nR\n(reset)\n\nQ\n\n(set)\nS\n\nQ\n6\n\n\fS\/R Latch (animated)\n● Currently, the value is “0”, and we can change it to “1” by sending a “1” on S\n\nR\n(reset)\n\n0\n1\n\n1\n\n0 Q\n\nNOR\n\nStable Feedback\n\n(set)\nS 1\n\n0\n\n0\n\n0\n\n1 Q\n7\n\n\fS\/R Latch (animated)\n● Currently, the value is “1”, and we can maintain it by having R and S be “0”\n● If the value was “0”, this would also maintain that value.\n● This is the “latch” operation. This is how an S\/R Latch can “store” a value.\n\nR\n(reset)\n\n0\n0\n\n1\n\nQ\n\nNOR\n\nStable Feedback\n\n(set)\nS 0\n\n1\n\n1\n\n0\n\nQ\n8\n\n\fS\/R Latch (animated)\n● Currently, the value is “1”, and we can change it to “0” by sending a “1” on R\no And having S be “0”\n\nR 1\n(reset)\n\n0\n\n0\n\n1 Q\n\nNOR\n\nStable Feedback\n\n(set)\nS\n\n1\n0\n\n1\n\n0 Q\n9\n\n\fS\/R Latch (animated)\n● Hmm, let’s set R and S to “1” at the same time.\n● And then let’s set them both to “0” afterward. (That should be stable… right?)\n● Oh no. Oscillation. Q is… both… 0 and 1… ??? Kinda???\n\nR 0\n1\n(reset)\n\n1\n\n1\n0\n\nQ\n\nNOR\n\nStable Feedback\n(but illogical)\n\n(set)\nS 1\n0\n\n0\n\n0\n\n0\n1\n\nQ\n10\n\n\fS\/R Latch: The Whole Truth\n\ntable\n\n● The state of this logic depends on the prior state.\no Q here is the current value of Q\no Qnext will be the new value.\n● This is an example of sequential logic.\no On your own: You can build it out of NANDs\nas well. Try to come up with that.\n\nR\n(reset)\n\nQ\n\n0\n0\n1\noh no\n\n1\n0\n1\n\n(set)\nS\n\nQ\n\naaaa\n11\n\n\fLet’s look at some circuits\n● sr_latch.circ\n\n● Factorio latch\n0eNrtV8GOmzAQ\/ZVqzlCB2ZANUvsFPbXHqkIOTJKRjEHGREUR\/14bWjYb1iyhP\nWylPYTI2J558948AxfYiwYrRVJDcgHKSllD8v0CNR0lF\/aebiuEBEhjAR5IXthRjhnlqP\nysLPYkuS4VdB6QzPEnJGH3wwOUmjThEK0ftKlsij0qs2CMY\/NpLvV1IA+qsjZ7S2n\nTm3h+5EFr\/kKTwmzQqhTpHk\/8TGa1WXIgoVE5cJ9J6cbcGVMOK3yFOfTxGlt6eA\n1+SCMxsxhqGyu0l6NClNflUA4JM2tJZQ3pfmh3d503qZjNMTctOPi4eSo5JzVAGb\nK9QMDvmKmZy2kEfSBV63QxIUN5Q+29JpAEdlBUXPUwE\/hkNpWNrpp1Yas27el\nOD6osUpImDiQHLmrsHJRbkW4Ij24J955Nx8+nmW3FF4W73RhNhfSALQPB5kG\n4miJa2RTsTTXF5xVNMZrvn7TE1IPeYrO6NWbzGm8coj481VpwIXzBi2oqZjh\/ov1J\nPGq3QjouW30ieXxNPTPd1GhSidKeoVo1uJx6FwubEU2BOTWFj8JEU5T5VSnQyQ\ndbfQ7cyvPgABav81z4brl7zLFb6sjdXzjy9gTfzoNgjo7Y3tmqbGyIhVzF81yFgQPY4\n6q3JPYGX5J2y56Hu3XeZO\/vSPMd57ZfPO+ox1k7xsv8FQZ3PBHZ\/\/5A3PYsGL777\n6Xk6vPKA8H3aIDCV\/\/bhy9cZydz72w82ZezjYJwE8X213W\/ABNorVw=\n12\n\n\fLet’s look at some circuits\n● Factorio power latch\n0eNrtWM2OmzAQfpVqzlBh8o+0K\/XeQ+9VhRyYbCwZGxmTNop4gL5Hn6xPUh\nu6bBrCbyMlhz2EyMb+5ht\/M2ObE2x5jqliQkNwAhZJkUHw9QQZexGU2z59TBEC\nYBoTcEDQxLZijFiMyo1ksmWCaqmgcICJGH9AQIpvDqDQTDOs0MrGMRR5skVlBt\nQ41p6mQp8DOZDKzMyVwpo3eO7MgaP9MybMBK0kD7e4pwdmRpshO8Y1qh\nbeB6Z0bnpqk9UI9xOUaLl1nHjeOfnKjMDIcsgsFrEPhfG5M8y0DLGIqShnumzauU\nXhNPz1u9at6S75uKgcJoZJzFRFpIS55v5fzNC8i1lNecdUpsOxy1GpAcFmY1tJSlXJ\nMYBnM0PmOs1HYL4oRFHhpsewXOtwp2QSMmFwINhRnmHxH6vt\/PN61RTDAd\n9iVUS60fwLtPkwaWcTpfUfR9rfP39NENfqcztp\/W5p+7T0L7W8LtZ8lFjeHdLwLGVe\n9fIu5Hp6gExcdMu1bsjhDNTN6TZTR0E\/p0usZTfHtvxeTAsZ\/6FC5vnu+d2spi0RM\nR8j+XxY2i9r3xKMWZ64yA1lxSI3lRybKvpvaT\/NvcvY27QQW03bPO53LCDeTfaOW\n5ejvt3DG3cyWI+Dawu79aTSQd4rx5jUImRoZVmNBhpeeUgPWFuIbN7WLqGcu5\nwmaVs98lpuP6926lCYEAlUHPWeiZe+YDCv8wyNKS7tfUurHIcruWxZBJtPU8qg\/z\nhl8P5pMvLysxoWn4RMqmH++4G5b0tpq1LrMVVofU1Fg1x+qwnOPu04wOkWje\nPwRX5H9eEz1dHe9B5QZaVEq5lHFrOl\/RXFHz5GPCg=\n13\n\n\fKeeping Everything In Order\n\nThe Clock\n\n14\n\n\fPropagation Delay (Basics)\n● Ok. Q at t=0 is different than at t=1.\n● How long does it take for a change to occur?\no (How much time is really between t=0 and t=1)\n● This is bounded by propagation delay.\n\nR\n(reset)\n\nQ\n\n(set)\nS\n\nQ\n15\n\n\fPropagation Delay\n● Propagation delay is the time it takes for a signal to pass from the inputs\nto the outputs\n● During that delay, the outputs are invalid (they can fluctuate)\n● After that delay, the outputs are valid\n● If you try to use the output while it's invalid, things break\no stuff like 2 + 2 = 17??\n\n16\n\n\fThe Critical Path\n● The critical path is the path through a circuit that has the longest series of\nsequential operations\n● The longest propagation delay\no they depend on each other and can't be done in parallel!\n\n17\n\n\fS\/R Latch (animated)\n\nR\n\nS1\n\nS 0\nQ\n\n1\n\nQ 0\ntime\n\n0\n1\n\n1\n\n1\n\nQ\n\n0\n0\n\n0\n\n0\n\nQ\n\n1\n0\n\n1\n18\n\n\fPropagation Delay (Basics)\n● If we have a component after this S\/R latch that reacts to the data on Q… we\nneed it to synchronize.\no We need it to wait until the Q value is updated.\n● One method: something that periodically and predictably updates in an\ninterval that’s a little longer than the propagation delay.\n\nR\n(reset)\n\nQ\n\n(set)\nS\n\nQ\n19\n\n\fTick Tock\n● Sequential logic is based on time, and time is continuous\n● Trying to build sequential circuits without anything to keep\ncircuit\ntrack of time is… possible, but very very difficult\nsymbol\n● This is why we use a clock signal: it goes 0, 1, 0, 1, 0, 1…\no Oscillation… on purpose this time.\nEach period is called a clock cycle.\no We typically electrocute rocks to do this, as usual. (poor rocks )\n\n1\nHigh\n\n0\n\ntime\n\nLow\n\nwe can synchronize our circuits to a clock state:\nwhen it is high (1) or low (0)\n\n20\n\n\fAdding a Clock\n● We need to augment our latch to wait for the clock before updating the\nvalue.\n● We need to account for the clock signal and only transmit a “1” on R or S if\nand only if the clock is high (or low).\n● Which leads us to something like this maybe…\no But let’s refine it a bit… (btw, how complex is this? How many transistors?)\n\nR\n\nR\n\nQ\n\nC\nS\n\nS\n\nQ\n21\n\n\fD(ata) Latch\n● If we do something like this, we simplify our S\/R Latch into a nicer\nsynchronized latch called a D-Latch.\n● “C” is the clock. “D” is the data to latch when the clock is high.\n● Circuit only changes the value when the clock signal is 1…\no Latches when clock is 0!\n\nC\n\nD\n\nR\n\nS\n\nQ\n\nQ\n22\n\n\fD Latch\n● When clock is low and D is … don’t care!\no Nothing changes\n\nC\n\nD\n\nR\n\nS\n\nQ\n\nQ\n\n23\n\n\fD Latch\n● When clock is high and D is high\no It’s a set operation\n\nC\n\nD\n\nR\n\nS\n\nQ\n\nQ\n\n24\n\n\fD Latch\n● When clock is high and D is low\no It’s a reset operation\n\nC\n\nD\n\nR\n\nS\n\nQ\n\nQ\n\n25\n\n\fD Latch\n● We can abstract this away and start using this symbol:\n\nD\n\nQ\n\nC\n\nQ\n\n26\n\n\fD Latch\n● We often omit the “C” for the clock and use a triangle instead:\no Sometimes you’ll see a square instead. Logisim uses triangles.\n\nD\n\nQ\nD Latch\nQ\n\n27\n\n\fTick Tock: D Latch\n● This diagram shows the behavior of the system over time.\no This is “high” triggered.\no Note the propagation delay. And how Q depends on D AND clock.\n\nD\n\n1\n\n0\n\nC\n\n1\n\n0\n\nQ\n\n1\n0\n\ntime\n\n28\n\n\fProblems: Owner of a lonely (D) Latch\n● What if we don’t want to change the value of Q.\no This means we have to constantly recharge the value, that is “D” has to be\nwhat we want Q to be every tick, limiting the usefulness.\n● We need a way to enable or disable the update.\n\nC\n\nD\n\nQ\n\nQ\n29\n\n\fProblems: Owner of a lonely (D) Latch\n● We could simply add a signal that we usually keep “0” and only allow the\nlatch when that ‘write enable’ (W) signal is “1”\n\nW\nC\n\nQ\n\nD\n\nQ\n30\n\n\fLet’s look at some circuits\n● Weird behaviour when clock is used in logic!!\no Note: N-e-v-e-r, never ever, connect logic to a clock port.\n▪ It’s hard to predict the behavior of circuits!\n\n● d_latch.circ\n● broken_latch.circ\n\n31\n\n\fAnd The Flip-Flop\n\n32\n\n\fThe New Problem\n● Remember propagation delay? Pesky thing, that.\n● Clocks don’t always help.\no We sometimes need a clock cycle to compute a value\no …and then another clock cycle to compute the next thing.\no …but the next thing needs to be computing the CURRENT thing.\no …but we would overwrite that input… so it would compute something else\no …before it was done computing the first thing…\n● AHHHHH!!!!!\n\n33\n\n\fWaiting for Godata\n● We want to record an intent to store (latch) a value.\no That is, to delay the latch by around a cycle.\n● (But only actually do it at an idle moment)\no When do we have an idle moment in the latch???\n▪ When the clock is low!!\n● If we cascade two D-Latches, and cleverly handle the clock…\no We can create a register! (Specifically, a D Flip-Flop)\n▪ Yes, that’s actually what it is called. ☺\n\n● We will create a component that latches a value on the clock’s falling edge.\no You can also make a rising edge D Flip-Flop by inverting the clock signal.\n34\n\n\fThe D Flip-Flop\n● While the clock is “1” (high), D’ can be computed while Q remains unaffected.\no Q is being used, after all, by whatever component is after the flip-flop\no While D is not immediately known and is being computed by the\ncomponent before the flip-flop\n● Falling clock edge: value is copied from the first latch to the second.\no This handles data propagation within a sequential circuit.\n\nD\nC\n\nD\n\nQ\nD Latch\n\nQ\n\nD’\n\nD\nD Latch\n\nQ\n\nQ\n\nQ\n\nQ\n35\n\n\fTick Tock\n● In this example, we are using the clock edges\no The circuit only updates its output in the instant the clock changes!\n● During the remaining time, other circuits\ncan compute the values\n\nrising edge\n\n1\n\n0\n\ntime\n\nfalling edge\n\nwe can synchronize our circuits to a clock edge:\nwhen it changes between 0 and 1\n\n36\n\n\fTick Tock: Falling Edge D Flip-Flop\n● This diagram shows the behavior of the system over time.\no This is “falling edge” triggered.\no Note the propagation delay. And how Q depends on D && clock.\n\nD\n\n1\n\nA Flip-Flop doesn’t race “D”\n\n0\n\nC\n\n1\n0\n\nQ\n\n1\n0\n\nQ remains stable while clock\nremains high and low\n\ntime\n\n37\n\n\fThe D Flip-Flop… Abstracted\n● We can of course reduce the flip-flop… it looks the same as the D Latch.\no This is effectively a 1-bit Register!\no That is, it is a simple 1-bit volatile memory cell.\n\nD\n\nQ\nD Flip-Flop\n\nQ\n38\n\n\fLet’s look at some circuits\n● D_flip_flop.circ\n● broken_latch.circ … again\n\n39\n\n\fReal-world clocking issues\n\n40\n\n\fDetermining clock speed\n\nD\n0ns\n\nQ\nR\n\n2ns\n\nA\n1\n\nB\n\nS\n5ns\n\ntime\n\nR is\nclocked\n\nR’s Q\nbecomes\nvalid\n\nthe adder has\nfinished; clock\nR to store\n41\n\n\fDetermining clock speed\n● It takes 5ns for a signal to propagate through our circuit\n● How fast can we clock it?\no if the time between clocks is less than 5ns, we'll clock the register too\nearly (while the adder's outputs are invalid)\no if the time between clocks is more than 5ns, no big deal\n\n𝟏\n𝟗\n=\n𝟎.\n𝟐\n×\n𝟏𝟎\n𝐇𝐳\n−𝟗\n𝟓 × 𝟏𝟎 𝒔\n= 𝟐𝟎𝟎𝐌𝐇𝐳\n\n● The fastest we can clock a sequential circuit is the reciprocal of the\ncritical path's propagation delay\n\n42\n\n\fClock Skew\n● The clock signal itself isn't immune to propagation delay!\n\nCLK\n\nwatch the input as the\nIN\nclock pulse travels down\n12\n??? the wire to B.\nD Q\nD Q\n???\n12\nEN\nEN\nB\nA\n\n● This is a race condition: the data and clock are having a race, and\nthe outcome depends on who wins\no the winner could change based on temperature, power, etc!\n43\n\n\fSummary\n\n44\n\n\fThe S\/R Latch … Abstracted\n● We now know the S\/R Latch!\no Allows you to store a value (1\/0) but with a pit fall!\no It could turn into an oscillator if input an invalid combination\n▪ R=S=1 ➔ R=S=0\n\nR\n\nS\n\nS\/R latch\n\nQ\n\nQ\n\n45\n\n\fThe D Latch … Abstracted\n● We improved it by adding some input logic\no Creating the D Latch\no But it was transparent (active) during the high clock state\n\nD\n\nQ\nD Latch\n\nQ\n46\n\n\fThe D Flip-Flop… Abstracted\n● Lastly, we saw the heroic D Flip-Flop!\no This is effectively a 1-bit Register!\no That is, it is a simple 1-bit volatile memory cell.\n\nD\n\nQ\n\nQ\n\nD\nD Latch\n\nQ\n\nD Flip-Flop\n\nD\n\nQ\nD Latch\nQ\n\nQ\n47\n\n\fCircuits using Flip-Flops\n\n48\n\n\fCreating an Adder Circuit\n● Suppose we want to (for 1 bit):\no Have three 1-bit registers: A, B, C\no Compute: C = A + B\n● We would need:\no Three D Flip-flops (for A, B, and C)\no A 1-bit adder\no What is the circuit?\n\n49\n\n\fRegister-backed 1-bit Adder\nA Register\nD\n\nQ\n\nD Flip-Flop\nQ\nB Register\nD\n\n(assume clock is connected)\n\nQ\n\nA\n\nS\n\nHalf-Adder\nB\nC\n\nC Register\nQ\n\nD Flip-Flop\nQ\n\nD Flip-Flop\nQ\n50\n\n\fRegister-backed 1-bit Adder\n(assume clock is connected)\n\nD\n\nQ\n\nD Flip-Flop\nQ\n\nA Register\n\nB Register\nD\n\nQ\n\nD Flip-Flop\nQ\n\nA\n\nS\n\nHalf-Adder\nB\nCo\n● What is the difference here?\n●A=A+B\n● This is fairly conventional\nsequential logic, actually.\n\n51\n\n\f4-bit Counter\n● Another simple component we can now build is a counter.\n● This is a register that increments every clock tick.\no On the falling-edge, in this case. (assume clock is connected)\n0\n\nA\nCo\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD\n\nD Flip-Flop\n\n3rd bit\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\n2nd bit\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\n1st bit\n\nAdder\n\nS\n\nB\nCi\n\n1\n\nQ\n\nD Flip-Flop\n\n0th bit\n\n52\n\n\f4-bit Counter: Thoughts\n● Hmm, adding takes some time because it ripples\n● Hmm, how long does our clock cycle have to be?\no No shorter than the propagation delay.\no If you assume latches take 2ns and adders take 4ns…\n0\n\nA\nCo\n\nD\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\nAdder\n\nS\n\nB\nCi\n\n1\n\nQ\n\nD Flip-Flop\n\n53\n\n\f4-bit Counter: Clocking\n\n0ns\n2ns\n\n1\n\n0\n0\n\nA\nCo\n\nD\n\nD B\n\nAdder\n\nS\n\n0\n\nCi\n\nQ\n\nD Flip-Flop\n\n0\nD\n\nA\nCo\n\nD Latch\n\nQ\nAdder\n\nS\nQ\n\nB\n\n0\n\nD\n\nQ\n\nD Flip-Flop\n\nA\nCo\n\nCi\n\nD Flip-Flop\nD\n\n0\n\nQ\n\nD Latch\n\nD\nQ\n\n0\n\nQ\n\nB\n\nS\n\nCi\n\nCo\n\nQ\n\nQ\n\nD\n\nAdder\n\n1\n\nD Flip-Flop\n\nA\n\nAdder\n\nS\n\n0\n\nB\nCi\n\n1\n\nQ\n\nD Flip-Flop\n54\n\n\f4-bit Counter: Clocking\n1\n\n12ns\n10ns\n14ns\n18ns\n16ns\n20ns\n2ns\n8ns\n6ns\n\n0\n0\n\nA\n\n0\n\nCo\n\nD\n\nAdder\n\nS\n\n0\n\n0\n\nB\nCi\n\nQ\n\nD Flip-Flop\n\n0\n\n0\n\nA\nCo\n\nD\n\nAdder\n\nS\n\n0\n\n0\n\nB\nCi\n\nQ\n\nD Flip-Flop\n\n0\n\n0\n\nA\nCo\n\nD\n\nAdder\n\nS\n\n1\n\n0\n1\n\nB\nCi\n\nQ\n\nD Flip-Flop\n\n0\n\n1\n\nA\nCo\n\nD\n\nAdder\n\nS\n\n1\n0\n\n0\n\nB\nCi\n\n1\n\nQ\n\nD Flip-Flop\n55\n\n\f4-bit Counter: Circuit Delay\nSerialized\n0\n\nA\nCo\n\nD\n\nAdder\n\nS\n\n4ns + 4ns + 4ns + 4ns\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\nParallelized\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\n2ns + 2ns\n\nAdder\n\nS\n\nB\n\n0\n\nA\n\nCi\n\nCo\n\nQ\n\nD\n\nD Flip-Flop\n\nAdder\n\nS\n\nB\nCi\n\n1\n\nQ\n\nD Flip-Flop\n\n20ns\n\n56\n\n\f4-bit Counter: Delay Explanation\n● Values of output bits must all be stable.\no That is, can’t pulse clock until all 4 bits are computed\n● However, the adder is a ripple-carry: Each bit waits for previous.\no 4ns per adder\no 4-bit adder\no Thus: 4 * 4ns = 16ns for the 4-bit adder\n● Flip-Flops\no Must wait for 1st latch to stabilize Q (2ns in parallel)\no Must wait for 2nd latch to stabilize Q (2ns also in parallel)\no Thus: 2ns + 2ns = 4ns\n● Overall delay: 16ns + 4ns = 20ns. Clock pulse is 20ns.\n\n57\n\n\fDetermine Maximum Clock Speed\n● If this operation takes 20ns, this bounds our clock speed.\n● Our clock period must be 20ns.\no Which means our frequency is once every 20ns: 1s ÷ 20ns\no What is that frequency in Hz? (Hertz is “cycles per second”)\n\no This would be the maximum clock speed for our circuit to work.\n\n1𝑠\n= 50,000,000 𝐻𝑧 = 50 𝑀𝐻𝑧\n20𝑛𝑠\n\n58\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":159,"segment": "unlabeled", "course": "cs0441", "lec": "lec20", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #20: Discrete Probability\n\nBased on materials developed by Dr. Adam Lee\n\n\fThe study of probability is concerned with the\nlikelihood of events occurring\nLike combinatorics, the origins of probability theory\ncan be traced back to the study of gambling games\nStill a popular branch of mathematics with many\napplications:\nRisk Assessment\nSimulation\nGenetics\n\nAlgorithm Design\n\nGambling\n\n\fMany situations can be analyzed using a\nsimplified model of probability\nAssumptions:\n1. Finite number of possible outcomes\n2. Each outcome is equally likely\n\nCard games\nDice\n\nFlipping coins\n\nRoulette\n\nLotteries\n\n\fTerminology\nDefinitions:\nl An experiment is a procedure that yields one of a given set of\npossible outcomes\nl The sample space of an experiment is the set of possible outcomes\nl An event is a subset of the sample space\nl Given a finite sample space S of equally-likely outcomes, the\nprobability of an event E is p(E) = |E|\/|S|.\n\nExample:\nl Experiment: Roll a single 6-sided die one time\nl Sample space: {1, 2, 3, 4, 5, 6}\nl One possible event: Roll an even number ⇒ {2, 4, 6}\nl The probability of rolling an even number is\n|{2, 4, 6}| \/ |{1, 2, 3, 4, 5, 6}| = 3\/6 = 1\/2\n\n\fSolving these simplified finite probability\nproblems is “easy”\nI told you that combinatorics and\nprobability were related!\n\nStep 1: Identify and\ncount the sample space\n\n\/\n\nStep 3: Divide!\n\nStep 2: Count the size of\nthe desired event space\n\n\fWhen two dice are rolled, what is the probability that\nthe sum of the two numbers is seven?\nStep 1: Identify and count sample space\nl Sample space, S, is all possible pairs of numbers 1-6\nl Product rule tells us that |S| = 62 = 36\n\nStep 2: Count event space\nl (1, 6)\nl (2, 5)\nl (3, 4)\nl (4, 3)\nl (5, 2)\nl (6, 1)\n\n|E| = 6\n\nStep 3: Divide\nl Probability of rolling two dice that sum to 7 is p(E)\nl p(E) = |E|\/|S| = 6\/36 = 1\/6\n\n\fBalls and Bins\nExample: A bin contains 4 green balls and 5 red balls.\nWhat is the probability that a ball chosen from the bin\nis green?\n\nSolution:\nl 9 possible outcomes (balls)\nl 4 green balls, so |E| = 4\nl So p(E) = 4\/9 that a green\nball is chosen\n\n\fHit the lotto\nExample: Suppose a lottery gives a large prize to a person who\npicks 4 digits between 0-9 in the correct order, and a smaller\nprize if only three digits are matched. What is the probability of\nwinning the large prize? The small prize?\n\nSolution:\nGrand prize\nl S = possible lottery outcomes\nl |S| = 104 = 10,000\nl E = all 4 digits correct\nl |E| = 1\nl So p(E) = 1\/10,000 = 0.0001\n\nSmaller prize\nl S = possible lottery outcomes\nl |S| = 104 = 10,000\nl E = one digit incorrect\nl We can count |E| using the sum rule:\nl\nl\nl\nl\n\n9 ways to get 1st digit wrong OR\n9 ways to get 2nd digit wrong OR\n9 ways to get 3rd digit wrong OR\n9 ways to get 4th digit wrong\n\nl So |E| = 9 + 9 + 9 + 9 = 36\nl p(E) = 36\/10,000 = 0.0036\n\n\fMega Lotteries\nExample: Consider a lottery that awards a prize if a person can\ncorrectly choose a set of 6 numbers from the set of the first 40\npositive numbers. What is the probability of winning this lottery?\n\nSolution:\nl S = All sets of six numbers between 1 and 40\nl Note that order does not matter in this lottery\nl Thus, |S| = C(40, 6) = 40!\/(6!34!) = 3,838,380\nl Only one way to do this correctly, so |E| = 1\nl So p(E) = 1\/3,838,380 ≈ 0.00000026\n\n:(\n\nLesson: You stand a better chance at being struck by\nlightning than winning this lottery!\n\n\fFour of a Kind\nExample: What is the probability of getting “four of a\nkind” in a 5-card poker hand?\n\nSolution:\nl S = set of all possible poker hands\nl Recall |S| = C(52,5) = 2,598,960\nl E = all poker hands with 4 cards of the same type\nl To draw a four of a kind hand:\n➣ C(13, 1) ways to choose the type of card (2, 3, …, King, Ace)\n➣ C(4,4) = 1 way to choose all 4 cards of that type\n➣ C(48, 1) ways to choose the 5th card in the hand\n➣ So, |E| = C(13,1)C(4,4)C(48,1) = 13 ×48 = 624\nl p(E) = 624\/2,598,960 ≈ 0.00024\n\n\fA Full House\nExample: What is the probability of drawing a full house when\ndrawing a 5-card poker hand? (Reminder: A full house is three\ncards of one kind, and two cards of another kind.)\n\nSolution:\nl |S| = C(52,5) = 2,598,960\nl E = all hands containing a full house\nl To draw a full house:\n➣ Choose two types of cards (order matters)\n➣ Choose three cards of the first type\n➣ Choose two cards of the second type\n\nl So |E| = 13 ×12 ×4 ×6 = 3,744\nl p(E) = 3,744\/2,598,960 ≈ 0.0014\n\nP(13, 2) = 13 × 12 ways\nC(4, 3) = 4 ways\nC(4, 2) = 6 ways\n\n\fSampling with or without replacement makes a\ndifference!\nExample: Consider a bin containing balls labeled with the\nnumbers 1, 2, …, 50. How likely is the sequence 23, 4, 3, 12, 48\nto be drawn in order if a selected ball is not returned to the bin?\nWhat if selected balls are immediately returned to the bin?\n\nSolution:\nl Note: Since order is important, we need to consider 5-permutations\nl If balls are not returned to the bin, we have P(50, 5) = 50 ×49 ×48 ×47 ×\n46 = 254,251,200 ways to select 5 balls\nl If balls are returned, we have 505 = 312,500,000 ways to select 5 balls\nl Since there is only one way to select the sequence 23, 4, 3, 12, 48 in\norder, we have that\n➣ p(E) = 1\/254,251,200 if balls are not replaced\n➣ p(E) = 1\/312,500,000 if balls are replaced\n\n\fYes, calculating probabilities can be easy\n\nAnyone can divide two numbers!\nBut, Be careful when you:\nl Define the sets S and E\nl Count the cardinality of S and E\n\n\fIn-class exercises\nProblem 1: Consider a box with 3 green balls and 1 pink ball.\nWhat is the probability of drawing a pink ball? What is the\nprobability of drawing two green balls in two successive picks\n(without replacement)?\nProblem 2: In poker, a straight flush is a hand in which all 5\ncards are from the same suit and occur in order. For example, a\nhand containing the 3, 4, 5, 6, and 7 of hearts would be a straight\nflush, while the hand containing the 3, 4, 5, 7, and 8 of hearts\nwould not be. Note that a royal flush (10 through A) is not\nconsidered a straight flush (but A through 5 is). What is the\nprobability of drawing a straight flush in poker?\nProblem 3: A flush is a hand in which all five cards are of the\nsame suit, but do not form an ordered sequence. What is the\nprobability of drawing a flush in poker?\n\n\fWhat about events that are derived from other\nevents?\nRecall: An event E is a subset of the sample space S\n\nS\n\nDefinition: p(E) = 1 – p(E)\nProof:\nl Note that E = S – E, since S is universe of all possible outcomes\nl So, |E| = |S| - |E|\nl Thus, p(E) = |E|\/|S|\nby definition\nl\n= (|S| - |E|)\/|S|\nby substitution\nE\nl\n= 1 - |E|\/|S|\nsimplification\nl\n= 1 – p(E) ❏\nby definition\n\nWhy is this useful?\n\nE\n\n\fSometimes, counting |E| is hard!\nExample: A 10-bit sequence is randomly generated. What is the\nprobability that at least 1 bit is 0?\n\nSolution:\nl S = all 10-bit strings\nl |S| = 210\nl E = all 10-bit strings with at least 1 zero\nl E = all 10-bit strings with no zeros = {1111111111}\nl p(E) = 1 – p(E)\nl\n= 1 – 1\/210\nl\n= 1 – 1\/1024\nl\n= 1023\/1024\nSo the probability of a randomly generated 10-bit string\ncontaining at least one 0 is 1023\/1024.\n\n\fWe can also calculate the probability of the\nunion of two events\nDefinition: If E1 and E2 are two events in the sample space S,\nthen p(E1 ∪ E2) = p(E1) + p(E2) – p(E1 ∩ E2).\n\nS\nE1\n\nE2\n\nProof:\nl Recall: |E1 ∪ E2| = |E1| + |E2| - |E1 ∩ E2|\nl p(E1 ∪ E2) = |E1 ∪ E2|\/ |S|\nl\n= (|E1| + |E2| - |E1 ∩ E2|) \/ |S|\nl\n= |E1|\/|S| + |E2|\/|S| - |E1 ∩ E2|\/|S|\nl\n= p(E1) + p(E2) - p(E1 ∩ E2)\n\nWhy does this look\nfamiliar?\n\n\fDivisibility…\nExample: What is the probability that a positive integer not\nexceeding 100 is divisible by either 2 or 5?\n\nSolution:\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n\nLet E1 be the event that an integer is divisible by 2\nLet E2 be the event that an integer is divisible by 5\nE1 ∪ E2 is the event that an integer is divisible by 2 or 5\nE1 ∩ E2 is the event that an integer is divisible by 2 and 5\n|E1| = 50\n|E2| = 20\n|E1 ∩ E2| = 10\np(E1 ∪ E2) = p(E1) + p(E2) - p(E1 ∩ E2)\n= 50\/100 + 20\/100 – 10\/100\n= 1\/2 + 1\/5 - 1\/10\n= 3\/5\n\n\fNot all events are equally likely to occur…\n\nSporting events\n\nGames of strategy\n\nNature\n\nInvestments\n\n\fWe can model these types of real-life situations\nby relaxing our model of probability\nAs before, let S be our sample space. Unlike before,\nwe will allow S to be either finite or countable.\nWe will require that the following conditions hold:\n1. 0 ≤ p(s) ≤ 1 for each s ∈ S\n2.\n\n∑!∈# 𝑝 𝑠 = 1\n\nNo event can have a negative\nlikelihood of occurrence, or more than\na 100% chance of occurrence\n\nIn 100% of experiments,\none of the outcome occurs\n\nThe function p : S → [0,1] is called a probability\ndistribution\n\n\fSimple example: Fair and unfair coins\nExample: What probabilities should be assigned to outcomes\nheads (H) and tails (T) if a fair coin is flipped? What if the coin\nis biased so that heads is twice as likely to occur as tails?\nCase 1: Fair coins\nnEach outcome is equally likely\nnSo p(H) = 1\/2, p(T) = 1\/2\nnCheck:\nl 0 ≤ 1\/2 ≤ 1\nl 1\/2 + 1\/2 = 1\n\n✔\n\nCase 2: Biased coins\nnNote:\n1. p(H) = 2p(T)\n2. p(H) + p(T) = 1\n\nn2p(T) + p(T) = 1\nn3p(T) = 1\nnp(T) = 1\/3, p(H) = 2\/3\n\n\fAre the following probability distributions valid?\nWhy or why not?\nS = {1, 2, 3, 4} where\nl\nl\nl\nl\n\np(1) = 1\/3\np(2) = 1\/6\np(3) = 1\/6\np(4) = 1\/3\n\nS = {a, b, c}\nl p(a) = 3\/4\nl p(b) = 1\/4\nl p(c) = 0\n\n✔\n✔\n\nS = {1, 2, 3, 4} where\nl\nl\nl\nl\n\np(1) = 2\/3\np(2) = 1\/6\np(3) = -1\/6\np(4) = 1\/3\n\nS = {a, b, c}\nl p(a) = 1\/2\nl p(b) = 1\/4\nl p(c) = 0\n\n\fMore definitions\nDefinition: Suppose that S is a set with n elements.\nThe uniform distribution assigns the probability 1\/n to\neach element of S.\nThe distribution of fair coin\nflips is a uniform distribution!\n\nDefinition: The probability of an event E ⊆ S is the\nsum of the probabilities of the outcomes in E. That is:\n𝑝 𝐸 = %𝑝 𝑠\n$∈&\n\n\fLoaded dice\nExample: Suppose that a die is biased so that 3 appears twice as\noften as each other number, but that the other five outcomes are\nequally likely. What is the probability that an odd number\nappears when we roll this die?\n\nSolution:\nl\nl\nl\nl\n\np(1) + p(2) + p(3) + p(4) + p(5) + p(6) = 1\nNote that p(1) = p(2) = p(4) = p(5) = p(6) and p(3) = 2p(1)\nSo, p(1) + p(1) + 2p(1) + p(1) + p(1) + p(1) = 7p(1) = 1\nThus p(1) = p(2) = p(4) = p(5) = p(6) = 1\/7 and p(3) = 2\/7\n\nl Now, we want to find p(E), where E = {1, 3, 5}\nl p(E) = p(1) + p(3) + p(5)\nl\n= 1\/7 + 2\/7 + 1\/7\nl\n= 4\/7\n\n\fIn-class exercises\nConsider a die in which (i) 1, 3, and 4 are rolled with the same\nfrequency, (ii) 2 is rolled 3 times as often as 1, (iii) 5 is rolled 2\ntimes as often as 4, (iv) and 6 is rolled 4 times as often as 2.\nProblem 4: What is the probability distribution for this die?\nProblem 5: What is the probability of rolling a 1 or a 3?\nProblem 6: What is the probability of rolling an even number?\nAn odd number?\n\n\fFinal Thoughts\nn Probability allows us to analyze the likelihood of\nevents occurring\nn Today, we learned how to analyze events that are\nequally likely, as well as those that have non-equal\nprobabilities of occurrence\nn Next time:\nl More probability theory (Section 7.2)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":160,"segment": "unlabeled", "course": "cs0449", "lec": "lec03", "text":"2\n\nData\n\nRepresentation\nII\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fBit Manipulation\nFlippin’ Switches\n\nCS\/COE 0449 – Spring 2019\/2020\n\n2\n\n\fWhat are \"bitwise\" operations?\n• The \"numbers\" we use on computers aren't really numbers right?\n• It's often useful to treat them instead as a pattern of bits.\n• Bitwise operations treat a value as a pattern of bits.\n\n0\n1\n\n0\n\n0\n\n0\n3\n\n\fThe simplest operation: NOT (logical negation)\n• If the light is off, turn it on.\n\n• If the light is on, turn it off.\n\nA\n\nQ\n\n0\n\n1\n\n1\n\n0\n\n• We can summarize this in a truth table.\nഥ\n• We write NOT as ~A, or ¬A, or A\n• In C, the NOT operation is the “!” operator\n4\n\n\fApplying NOT to a whole bunch of bits\n• If we use the not instruction (~ in C), this is what happens:\n\n~ 0 0 1 1 1 0 1 0\n\n= 1 1 0 0 0 1 0 1\nwe did 8 independent NOT operations\nThat's it.\n\nonly 8 bits shown cause 32 bits on a slide is too much\n\n5\n\n\fLet's add some switches\n• There are two switches in a row connecting the light to the battery.\n• How do we make it light up?\n\n6\n\n\fAND (Logical product)\n• AND is a binary (two-operand) operation.\n• It can be written a number of ways:\n\nA&B\n\nA∧B\n\nA⋅B\n\nAB\n\n• If we use the and instruction (& in C):\n\n1 1 1 1 0 0 0 0\n& 0 0 1 1 1 0 1 0\n= 0 0 1 1 0 0 0 0\n\nA B Q\n0 0 0\n0 1 0\n1 0 0\n\n1 1 1\n\nwe did 8 independent AND operations\n7\n\n\f\"Switching\" things up ;))))))))))))))))))))))\n• NOW how can we make it light up?\n\n8\n\n\fOR (Logical sum…?)\n• we might say \"and\/or\" in English\n• it can be written a number of ways:\n\n• A|B\n\nA∨B\n\nA+B\n\n• if we use the or instruction (or | in C\/Java):\n\nA B Q\n0 0 0\n\n1 1 1 1 0 0 0 0\n| 0 0 1 1 1 0 1 0\n\n0 1 1\n\n= 1 1 1 1 1 0 1 0\n\n1 1 1\n\n1 0 1\n\nWe did 8 independent OR operations.\n9\n\n\fXOR (“Logical” difference?)\n• We might say \"or\" in English.\n• It can be written a number of ways:\n\nA^B\n\nA⊕B\n\n• If we use the xor instruction (^ in C):\n\n1 1 1 1 0 0 0 0\n| 0 0 1 1 1 0 1 0\n= 1 1 0 0 1 0 1 0\n\nA B Q\n0 0 0\n\n0 1 1\n1 0 1\n1 1 0\n\nWe did 8 independent XOR operations.\n10\n\n\fBit shifting\n• Besides AND, OR, and NOT, we can move bits around, too.\n\n1 1 0 0 1 1 1 1 if we shift these bits left\nby 1…\n\n1 1 0 0 1 1 1 1 0 we stick a 0 at the bottom\n1 1 0 0 1 1 1 1 0 0 again!\n1 1 0 0 1 1 1 1 0 0 0 AGAIN!\n1 1 0 0 1 1 1 1 0 0 0 0 AGAIN!!!!\n11\n\n\fLeft-shifting in C\/Java\n\n(animated)\n\n• C (and Java) use the << operator for left shift\n\nB = A << 4; \/\/ B = A shifted left 4 bits\nIf the bottom 4 bits of the result are now 0s…\n• …what happened to the top 4 bits?\n\n0011 0000 0000 1111 1100 1101 1100 1111\nthe bit bucket is not a real place\nit's a programmer joke ok\nBit Bucket\n\nin the UK they might say the “Bit Bin”\nbc that’s their word for trash\n\n12\n\n\f>_> >_> >_> ☺\n• We can shift right, too\n0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1\n0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1\n0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0\n\n● C\/Java use >>, this in MIPS is the srl (Shift Right Logical) instruction\n\nsee what I mean about 32 bits on a slide\n\nQ: What happens when we shift a negative number to the right?\n\n13\n\n\fShift Right (Logical)\n• We can shift right, too (srl in MIPS)\n\n1 1 0 0 1 1 1 1\n\nif we shift these bits right\nby 1…\n\n0 1 1 0 0 1 1 1 1\n\nwe stick a 0 at the top\n\n0 0 1 1 0 0 1 1 1\n\nagain!\n\n0 0 0 1 1 0 0 1 1\n\nAGAIN!\n\n0 0 0 0 1 1 0 0 1\n\nWait… what if this was a\nnegative number?\n14\n\n\fShift Right (Arithmetic)\n• We can shift right with sign-extension, too (MIPS: sra)\n\n1 1 0 0 1 1 1 1\n\nif we shift these bits right\nby 1…\n\n1 1 1 0 0 1 1 1 1\n\nwe copy the 1 at the top (or 0,\nif MSB was a 0)\n\n1 1 1 1 0 0 1 1 1\n\nagain!\n\n1 1 1 1 1 0 0 1 1\n\nAGAIN!\n\n1 1 1 1 1 1 0 0 1\n\nAGAIN!!!!!! (It’s still\nnegative!)\n15\n\n\fHuh… that's weird\n• Let's start with a value like 5 and shift left and see what happens:\n\nBinary\n101\n1010\n10100\n101000\n1010000\n\nDecimal\n5\n10\n20\n40\n80\n\nWhy is this happening\nWell uh... what if I gave you\n\n49018853\nHow do you multiply that by 10?\n\nby 100?\nby 100000?\nSomething very similar is\nhappening here\n16\n\n\fa << n == a * 2n\n• Shifting left by n is the same as multiplying by 2n\n• You probably learned this as \"moving the decimal point\"\n• And moving the decimal point right is like shifting the digits left\n\n• Shifting is fast and easy on most CPUs.\n• Way faster than multiplication in any case.\n• (It’s not a great reason to do it when you’re using C though)\n\n• Hey… if shifting left is the same as multiplying…\n\n17\n\n\fa >> n == a \/ 2n, ish\n• You got it\n• Shifting right by n is like dividing by 2n\n• sort of.\n\n• What's 1012 shifted right by 1?\n• 102, which is 2…\n\n• It's like doing integer (or flooring) division\n\n• Generally, compilers are smart enough that you just multiply\/divide\n• It’s confusing to shift just to optimize performance.\n• It’s good to not be clever until it is proven that you need to be.\n\n18\n\n\fC Bitwise Operations: Summary\nC code\n\nDescription\n\nMIPS instruction\n\nWhen x is signed (most of the time…):\n19\n\n\fFractional Encoding\nEvery Time I Teach Floats I Want Some Root Beer\n\n20\n\n\fFractional numbers\n• Up to this point we have been working with integer numbers.\n\n2019\n2 0 1 9.320\n\n• Unsigned and signed!\n\n• However, Real world numbers are… Real numbers. Like so:\n\n• That create new challenges!\n\n• Let’s start by taking a look at them.\n21\n\n\fJust a fraction of a number\n• The numbers we use are written positionally: the position of a digit within the\nnumber has a meaning.\n• What about when the numbers go over the decimal point?\n\n?\n2 0 1 9. 3 2 0\n\n1000s\n\n100s\n\n10s\n\n1s\n\n10ths 100ths 1000ths\n\n103\n\n102\n\n101\n\n100\n\n10-1\n\n10-2\n\n10-3\n\n22\n\n\fA fraction of a bit?\n• Binary is the same!\n• Just replace 10s with 2s.\n\n0 1 1 0 .1 1 0 1\n23\n8s\n\n22\n4s\n\n21\n2s\n\n20\n1s\n\n2-1\n2ths\n\n?\n\n2-2\n4ths\n\n2-3\n8ths\n\n2-4\n16ths\n\n23\n\n\fTo convert into decimal, just add stuff\n\n0 1 1 0 .1 1 0 1=\n23\n\n22\n\n21\n\n20\n0×8+\n1×4+\n1×2+\n0×1+\n1 × .5 +\n1 × .25 +\n0 × .125 +\n1 × .0625\n\n2-1\n\n2-2\n\n2-3\n\n2-4\n\n= 6.812510\n24\n\n\fFrom decimal to binary? Tricky?\n\n6.8125 10\n6÷210 = 3R0\n3÷210 = 1R1\n\n1 1 0.1101\n\n0.812510\nx\n2\n1.6250\n\nMSB\n\n0.625010\nx\n2\n1.2500\n0.250010\nx\n2\n0.5000\n0.500010\nx\n2\n1.0000\n\nLSB\n\n25\n\n\fSo, it’s easy right? Well…\n\nWhat about: 0.1 10\n\n0.110\nx 2\n0.2\n0.210\nx2\n0.4\n\n0.0001\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n26\n\n\fSo, it’s easy right? Well……\n\nWhat about: 0.1 10\n\n0.0001\n1001\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n27\n\n\fSo, it’s easy right? Well………\n\nWhat about: 0.1 10\n\n0.0001\n1001\n10\n0\n1\n...\n\n0.610\nx 2\n1.2\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n28\n\n\fWELL…\n0.00011001100110011001100110011001100110011001100110\n0110011001100110011001100110011001100110011001100110\n0110011100110011001100110011001100110011001100110011\n0100110011001100110011001100110011001100110011001100\n10\n1100110100110011001100110011001100110011001100110011\n0011001100110100110011001100110011001100110011001100\n1100110011001100110011001100110011001100110011001100110011001100110011001100110\n0110011001100110011001100110011001100110011001100110011001100110011001100110011\n0011001100110011001100110011001100110011001100110011001100110011001100110011001\n1001100110011001100110011001100110011001100110011001100110011001100110011001100\n1100110011001100110011001100110011001100110011001100110011001100110011001100110\n0110011001100110011001100110011001100110011001100110011001100110011001100110011\n0011001100110011001100110011001100110011001100110011001100110011001100110011001\n1001100110011001100110011001100110011001100110011001100110011001100110011001100\n1100110011001100110011001100110011001100110011001100110011001100110011001100110\n0110011001100110011001100110011001100110011001100110011001100110011001100110011\n0011001100110011001100110011001100110011001100110011001100110011001100110011001\n1001100110011001100110011001100110011001100110011001100110011001100110011001100\n1100110011001100110011001100110011001100110011001100110011001100110011001100110\n0110011001100110011001100110011001100110011001100110011001100110011001100110011\n0011001100110011001100110011001100110011001100110011001100110011001100110011001\n1001100110011001100110011001100110011001100110011001100110011001100110011001100\n1100110011001100110011001100110011001100110011001100110011001100110011001100110\n0110011001100110011001100110011001100110011001100110011001100110011001100110011\n0011001100110011001100110011001100110011001100110011001100110011001100110011001\n1001100110011001…\n\n0. 1\n\n=\n\nCS\/COE 0449 – Spring 2019\/2020\n\n29\n\n\fHow much is it worth?\n\n•Well, it depends on where you stop!\n\n0.0001 2\n\n= 0.0625\n\n0.00011001 2\n\n= 0.0976…\n\n0.000110011001 2 = 0.0998…\n30\n\n\fMind the point\n• In this representation we assume that the lowest n digits are the decimal places.\n\n$12.34\n+$10.81\n$23.15\nfp_add:\nadd v0, a0, a1\njr ra\n\n1234\n+1081\n2315\n\nthis is called fixed-point\nrepresentation\nAnd it’s a bitfield :D\n\nfp_mult:\nmult\na0, a1\nmflo\nt0\nsrl t0, t0, N_BITS_FRAC\nmfhi\nt1\nsll t1, t1, N_BITS_INT\nor v0, t0, t1\njr ra\n\n31\nhttps:\/\/www.youtube.com\/watch?v=GwmrIcn9Hw\n\n\fFixing the point\n• If we want to represent decimal places, one way of doing so is by assuming that\nthe lowest n digits are the decimal places.\n\n$12.34\n+$10.81\n$23.15\n\n1234\n+1081\n2315\n\nthis is called fixed-point\nrepresentation\n\n32\n\n\fA rising tide\n• Maybe half-and-half? 16.16 number looks like this:\n\n0011 0000 0101 1010.1000 0000 1111 1111\nbinary point\nthe largest (signed) value we can the smallest fraction we can\nrepresent is +32767.9999ish\nrepresent is 1\/65536\nWhat if we place the binary point to the left…\n\n0011.0000 0101 1010 1000 0000 1111 1111\n…we can get much higher accuracy near 0…\n\n…but if we place the binary point to the right…\n\n0011 0000 0101 1010 1000 0000.1111 1111\n…then we trade off accuracy for range further away from 0.\n\n33\n\n\fMove the point\n• What if we could float the point around?\n• Enter scientific notation: The number -0.0039 can be represented:\n\n-0.39\n-3.9\n\n× 10-2\n× 10-3\n\n• These both represent the same number, but we need to move the decimal point\naccording to the power of ten represented.\n• The bottom example is in normalized scientific notation.\n• There is only one non-zero digit to the left of the point.\n\n• Because the decimal point can be moved, we call this representation:\n\nFloating point\n\n34\n\n\fFloating-point number\nrepresentation\nSeven-five-four!\n\n35\n\n\fIEEE 754\n\n• Established in 1985, updated as recently as 2008.\n• Standard for floating-point representation and arithmetic that virtually every\nCPU now uses.\n• Floating-point representation is based around scientific notation:\n\n1348 = +1.348 × 10+3\n-0.0039 = -3.9\n× 10-3\n-1440000 = -1.44 × 10+6\nsign significand\n\nexponent\n36\n\n\fBinary Scientific Notation\n• Scientific notation works equally well in any other base!\n• (below uses base-10 exponents for clarity)\n\n+1001 0101 = +1.001 0101 × 2+7\n-0.001 010 = -1.010\n× 2-3\n-1001 0000 0000 0000 = -1.001\n× 2+15\nWhat do you notice\nabout the digit before\nthe binary point?\n\n(+\/-)1.f × 2exp\n\nf – fraction\n1.f – significand\nexp – exponent\n\n37\n\n\fIEEE 754 Single-precision\n• Known as float in C\/C++\/Java etc., 32-bit float format\n• 1 bit for sign, 8 bits for the exponent, 23 bits for the fraction\n\n• Tradeoff:\n▪ More accuracy = More fraction bits\n▪ More range = More exponent bits\n\n• Every design has tradeoffs ¯\\_(ツ)_\/¯\n▪ Welcome to Systems!\nillustration from user Stannered on Wikimedia Commons\n\n38\n\n\fIEEE 754 Single-precision\n• Known as float in C\/C++\/Java etc., 32-bit float format\n• 1 bit for sign, 8 bits for the exponent, 23 bits for the fraction\n\n• The fraction field only stores the digits after the binary point\n• The 1 before the binary point is implicit!\n▪ This is called normalized representation\n▪ In effect this gives us a 24-bit significand\n▪ The only number with a 0 before the binary point is 0!\n\n• The significand of floating-point numbers is in sign-magnitude!\n▪ Do you remember the downside(s)?\nillustration from user Stannered on Wikimedia Commons\n\n39\n\n\fThe exponent field\n• the exponent field is 8 bits, and can hold positive or negative exponents, but... it\ndoesn't use S-M, 1's, or 2's complement.\n• it uses something called biased notation.\n• biased representation = exponent + bias constant\n• single-precision floats use a bias constant of 127\n\nexp + 127 => Biased\n\n-127 + 127 => 0\n-10 + 127 => 117\n34 + 127 => 161\n\n● the exponent can range from -126 to +127 (1 to 254 biased)\no 0 and 255 are reserved!\n● why'd they do this?\no You can sort floats with integer comparisons!\n40\n\n\fBinary Scientific Notation (revisited)\n• Our previous numbers are actually\n\n+1.001 0101 × 2+7 = (-1)0 x 1.001 0101 × 2134-127\n-1.010\n× 2-3 = (-1)1 x 1.010\n× 2124-127\n-1.001\n× 2+15= (-1)1 x 1.001\n× 2142-127\n(-1)s x1.f × 2exp-127\n\ns – sign\nf – fraction\nexp – biased exponent\n41\n\n\fBinary Scientific Notation (revisited)\nbias = 127\n\n+1.001 0101 × 2+7\n\nsign = 0 (positive number!)\nBiased exponent = exp + 127 = 7 + 127 = 134\n= 10000110\nfraction = 0010101 (ignore the “1.”)\ns\n\nE\n\nf\n\n0 10000110 00101010000000000…000\n(-1)0 x 1.001 0101 × 2134-127\n\n42\n\n\fBinary Scientific Notation (revisited)\nbias = 127\n\n-1.010 × 2-3 =\n\nsign = 1 (negative number!)\nBiased exponent = exp + 127 = -3 + 127 = 124\n= 01111100\nfraction = 010 (ignore the “1.”)\ns\n\nE\n\nf\n\n1 01111100 01000000000000000…000\n(-1)1 x 1.010\n\n× 2124-127\n\n43\n\n\fEncoding an integer as a float\n• You have an integer, like 2471 = 0000 1001 1010 01112\n1.\n\nput it in scientific notation\n\n• 1.001 1010 01112 × 2+11\n2.\n\nget the exponent field by adding the bias constant\n\n• 11 + 127 = 138 = 100010102\n3.\n\ns\n\ncopy the bits after the binary point into the fraction field\n\nexponent\n\nfraction\n\n0 10001010 00110100111000000…000\npositive\n\nstart at the left side!\n\n44\n\n\fEncoding a number as a float\nYou have a number, like -12.5937510\n1. Convert to binary:\n\nInteger part: 11002 Fractional part: 0.100112\n\n2. Write it in scientific notation:\n\n1100.100112 x 20\n\n3. Normalize it:\n\n1.100100112 x 23\n\n4. Calculate biased exponent\n\ns\n\nexponent\n\n+3 + 127 = 13010 = 100000102\n\nfraction\n\n1 10000010 10010011000000000…000\n45\n\n\fwhile ( computers don’t do real math ) { … }\n\nQ: Consider and\/or review the IEEE 754 standard. What is happening here?\n\n46\n\n\fOther formats\n• The most common other format is double-precision (C\/C++\/Java double),\nwhich uses an 11-bit exponent and 52-bit fraction\n\n• GPUs have driven the creation of a half-precision\n16-bit floating-point format. it's adorable\nHow much is\nthe bias?\n\nHow much\nis the bias?\n\nboth illustrations from user Codekaizen on Wikimedia Commons\n\n47\n\n\fThis could be a whole unit itself...\n• Floating-point arithmetic is COMPLEX STUFF.\n• But it's not super useful to know unless you're either:\n• Doing lots of high-precision numerical programming, or\n• Implementing floating-point arithmetic yourself.\n\n• However...\n• It's good to have an understanding of why limitations exist.\n• It's good to have an appreciation of how complex this is... and how much better things are\nnow than they were in the 1970s and 1980s!\n• It’s good to know things do not behave as expected when using float and double!!\n\n48\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":161,"segment": "unlabeled", "course": "cs0447", "lec": "lec01", "text":"#0\n\nWelcome!\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce Childers, David\nWilkinson\n\nFall 2020\n\n\fWhat do I need to know now!\nThe classes will be recorded!\n● You will be able to access the videos online\no They are for your personal use only!\no Do not distribute them!\n\n● You don’t need to turn on your camera\no If you do, you may be recorded\n● You can ask questions via text!\no Chat is great for that. If I don’t stop and read your questions, ask them\nagain\no But feel free to interrupt me at any point.\n\n2\n\n\fWelcome!\n● My name is Luis (pronounced Loo-eesh, but I don’t really care ☺ )\n● I’m not from these parts as you can tell from my accent\no I come from Portugal\ni before e except after c…\nand this guy’s name!\n\n● course site: cs.pitt.edu\/~loliveira or\nluisfnqoliveira.github.io\/CS447\no all the stuff I talk about today is on the course site\n● email: loliveira@pitt.edu\n● office: 5421 SENSQ (haha – more like Zoom) (check site)\n● office Hours: TBD (check the site)\n3\n\n\fTextbook\n\n• Computer Organization & Design (Patterson\n& Hennessy)\n\n• Not mandatory!\n\no Get it if you really want\no No readings, no exercises\no May reference it once in a while\n\n• gets into a lot of detail\n\no fun read if you pick this stuff up easily\n\n4\n\n\fGrading\n● There will be no attendance\no But you should come to classes!\no Sometimes the slides alone may not be clear enough\n▪ Even from seasoned professors! (I know, I’ve been there!)\n● labs: 20%\no there'll be around 12? (~one a week)\no lowest 2 lab grades are dropped\n● 3 projects: 40%\no two in MIPS assembly language, one in Logisim\no 15% two highest grades, 10% lowest grade\n● 2 exams: 40%\no 1 midterms, 1 (semi-cumulative) final\no Lowest grade 15%; highest is 25%\n5\n\n\fExpectations\n● Religious absences are excused: contact me ASAP\n\n● Students with disabilities should contact the Office of Disability Resources\nand Services (DRS) if you haven’t already\no 216 William Pitt Union; 412-648-7890; TTY:412-383-7355\n● Please, no comments about sex, gender, race, ethnicity, religion, etc..\no Anywhere!\no Just be nice!\n● Cheating: Don’t!\no 0 on assignment first time,\no fail the course second time.\no Do not publish your work in public palaces (no github!)\no you can also talk about labs, but still no sharing stuff\n6\n\n\fDO NOT CHEAT!\n● If you're confused, don't cheat, ask me for help?\no Hot tips for not cheating:\n1. Don’t!\n2. Do not!\no You have LOTS of resources\n▪ Me! and the TAs\n▪ Undergraduate Helpdesk (CRC)\n● People can tell when you cheat\no It is usually quite obvious!\no So don’t do it, it’s not worth it!\no The university is quite strict about it.\n7\n\n\fTeaching\n● No questions are dumb!\n● Are you confused?\no Surprise, surprise, surprise! IT’S EXPECTED!\no You are learning a bunch of new stuff!\no So ASK QUESTIONS!\no DON’T STRUGGLE IN SILENCE ON YOUR PROJECTS\/LABS!!!!!!!\n● Regret #1 of my students:\no I should have started earlier ;)\n● Come to lectures synchronously if you can!\no You have access to me, we can interact\no You can ask questions, and get the answers promptly!\no Please… be interactive ☺\n8\n\n\fNo recitations this first week.\n● My suggestion!\no You take this link, and meet your fellow recitation mates!\nTopic: Recitation get together\nTime: Aug 20, 2020 03:25 PM Eastern Time (US and Canada)\nhttps:\/\/pitt.zoom.us\/j\/95693091291\nMeeting ID: 956 9309 1291\nPasscode: 447meet\no Or this link, and meet your fellow recitation mates!\nTopic: Recitation get together\nTime: Aug 21, 2020 12:00 PM Eastern Time (US and Canada)\nhttps:\/\/pitt.zoom.us\/j\/98887922292\nMeeting ID: 988 8792 2292\nPasscode: 447meet\no I won’t be there, so you can speak freely ☺\n9\n\n\fFLEXing @ Pitt\n\n10\n\n\fIDK you tell me\n● Let’s discuss some things:\no What were your experiences last semester with remote teaching?\n▪ Good\/bad\/ugly\no If the university authorizes, do you want to attend classes physically?\n▪ Why? (I really want to know your reasons!)\n▪ Note the question is not do you prefer\n(cause, ya know, we all preferred no CoViD pandemic)\n▪ My opinion: There is no benefit in coming that overweighs your health.\no Who has timezone restrictions?\n▪ How should I distribute my office hours?\no Anything else?\n\n11\n\n\fWhat I know!\no Lectures will be synchronous! And online! ALWAYS!\n▪ But video will be available!\n▪ If you can, attend synchronously!\no You can attend lectures asynchronously\n▪ Have a job? some scheduling problem? Idk?\no Recitations are ALL remote!\no IF we go back to the classroom\n▪ First lecture, I’ll be in the classroom, you will be at home. Why?\n– If technical issues come, I’ll just relocate to teach remotely\n– You can experience how it feels (teacher in classroom, students remote)\n\no EVERYONE WILL FOLLOW THE RULES (distance, mask, etc.!)\n▪ https:\/\/www.coronavirus.pitt.edu\/\n▪ NO EXCEPTION (class will be dismissed)\n\n12\n\n\fIntroduction and Context\n\n13\n\n\fGoals of this course\n● Why should I take it?\no Looking under the hood (car analogy)…\no How do you understand complex modern architectures?\n▪ Learning the basics (here)\n\n● Topics covered\no Data representation\no Assembly language\no Program execution\n● Learning important fundamental skills:\no Representing numbers in different bases (binary, hexadecimal, …)\no Logical operations: The binary revolution\no Logic design: Making circuits without knowing EE :sad:\no Programming an assembly language – FUN! (your mileage may vary)\n14\n\n\fWhere does this material fit in with CS and EE?\nComputational Theory\nAlgorithm Design\nApplications\nOperating Systems\nLogic Design\nElectrical Design\n\nPhysics\n\nmore concrete\n\nISA\n\nmore abstract\n\n● The “hardware-software interface”\no where CS and EE overlap\n● Each layer affects\/is affected by\nlayers above and below\n● ISA: Instruction Set Architecture\no Programmer's interface to\nthe computer hardware\n● logic design\no how we make 0’s and 1’s do stuff\no How do we build a CPU\n\n15\n\n\fMore goals\n● Let’s learn Assembly!\n● Let’s unlearn High-level languages (Java, C)\no Datatypes\/structures? Nope!\no Infinite number of variables? Nope!\n● “Learning assembly is like a car mechanic learning how an engine runs”\no Normal people don’t want to, but it’s fun to take things apart!!!\n▪ Also putting them back together and (hopefully) see them work!\no And we are not normal people!\n\n16\n\n\fFAQ\n● Maybe the answer for my question is in the next slide. Should I ask?\no Yes! I’ll let you know if that’s the case\n● I’m sure I should know this. I shouldn’t ask!\no NO! Probably you should not know that yet! So just ASK!\no NO! Maybe you should know, BUT you don’t… So ASK!\n● I’m embarrassed I don’t want to ask\no That’s fine! But don’t be! Others will have the same question! I assure you!\no But if you want, write the question down, send it to me privately!\no BUT ASK!!!! (later)\n● Should I ask?\no YES! Interrupt me at any time!!!\no Like now? INTERRUPT ME!!!\n\n17\n\n\fComputers\n\n18\n\n\fThey are old!\n\nThe Antikythera\nMechanism\n• Thousands of years old\n▪ Late second\/early first century BC\n▪ That’s like -100 ish\n\n• Used for astronomy\n▪ Eclipses\n▪ Astronomical positions\n\n19\n\n\fThey are not (ALL) war machines\n\nJacquard machine\n• Mechanical loom (1804)\n▪ Programmed using perforated cards\n▪ Used to produce complex patterns\n\nWoven in silk\nusing 24k punch\ncards!\n\n20\n\n\fThe pre-history of computers\n\nThe Differential\nEngine\n• Designed by Charles Babbage\n▪ 1792-1871\n\n• Ermmm… Designed… right!\n▪ It was intended as a programmable\ncalculator\n▪ A multipurpose calculator!\n\n21\n\n\fThe pre-history of computers\n● The Differential Engine\no Devised by J.H. Müller in the Hessian army (1784)\no Designed by Charles Babbage (1819-ish)\no Built at Science Museum library in London (1980s)\no Outputs to a table that can be used for printing\n▪ Copying was a source of error\n▪ It still is nowadays\n▪ So never copy results manually if you can avoid it\n\n22\n\n\fThe pre-history of computers\n\nThe Analytical\nEngine\n• Designed by Charles Babbage\n▪ YES! Designed… again!\n\n• Mechanical general-purpose\ncomputer\n▪ Which had many modern\ncharacteristics\n\nNo actual picture because… it was never built\n23\n\n\f“The Enchantress of Numbers” - the first programmer\n● Augusta Ada King, Countess of Lovelace\no Wrote algorithms for this computer →\n▪ Yeah, that one!\n▪ But they probably would have worked\no Translating a paper, she added notes\no A LOT of notes\n▪ More than the actual paper\no Including instructions on how to calculate\na number series\n▪ Note G\no Studied the relation between maths and music\n\n24\n\n\fPart of note G\n\nThe Analytical Engine has no pretensions\n(…) to originate anything. It can do\nwhatever we know how to order it to\nperform.\nIt can follow analysis; but it has no\npower of anticipating any analytical\nrelations or truths.\n25\n\n\fThe pre-history of computers\n(replica)\n\nHollerith Electric\nTabulating System\n• Census happen every 10 years\n▪ Hey, they just did!\n\n• It took people 8 years to count\nresponses (in 1880)\n▪ It would soon take more than 10!\n▪ 7,000 cards a day using this system\n\n• Company would become IBM\n▪ After a merge with others\n\nhttps:\/\/www.computerhistory.org\/revoluti\non\/punched-cards\/2\/2\/5\n\n26\n\n\fBubbles\n\n• Check what Bubbles has to say about it ☺\n▪ https:\/\/www.youtube.com\/watch?v=L7jAOcc9kBU\n27\n\n\fDriven by the need for complex calculations\nhttps:\/\/computerhistory.org\/blog\/first-steps-lectures-from-the-dawn-of● George Stibitz (Bell Labs)\ncomputing\/\no Day-job: Electrical engineer\no Model K – binary addition with relays (Boolean algebra)\no Complex Number Computer – used remotely via telegraph lines!!\no Art with Amiga (1990s) - http:\/\/stibitz.denison.edu\/art.html\n● Konrad Zuse (Germany)\no Day-job: Aircraft designer (civil engineer)\no World's first programmable computer\no Several computers used for military calculations\n● John Atanasoff (Iowa State)\no Day-job: Physics professor\no Built the ABC (Atanasoff-Berry Computer)\n▪ solved 30 equations in 30 unknowns\n\n28\n\n\fThe first “modern” computers\n\nENIAC (Electronic Numerical Integrator and Computer)\n• 1946 – ENIAC\n▪ University of Pennsylvania\n▪ Developed during WWII to calculate\nbalistic missile trajectories\n▪ Designed by :\n• John Mauchly\n• J. Presper Eckert\n\n▪ Joined by a huge team!\n▪ Modular and reconfigurable\n• Flipping switches and connecting cables\nU. S. Army Photo\n\n29\n\n\fENIAC (Electronic Numerical Integrator and Computer)\n● Some numbers:\no 18000 valves (tubes)\no 1500 relays\no 30 tons\no 175 kW\no 5000 additions \/ s\no 357 multiplications \/ s\no 40 divisions \/ s\no Programs \"hardwired\"\n\nU. S. Army Photo\n30\n\n\fThe first “modern” computers\n\nEDVAC (Electronic Discrete Variable Automatic Computer)\n• 1947 – EDVAC\n▪ University of Pennsylvania\n▪ The ENIAC team joined by John Von Neumann\n▪ A computer with a new concept:\n• \"Memory Stored Program\" – same as data\n\n▪ Became operational in 1951\n\nU. S. Army Photo\n\n31\n\n\fThe first “modern” computers\n\nEDSAC (Electronic Delay Storage Automatic Calculator)\n• 1949 – EDSAC\n▪ Cambridge University\n▪ Designed by Maurice Wilkes\n▪ Based on the first EDVAC draft\n• Not to be better, but to be used!\n• accessible and practical vs. push technology\n\n• Was completed before the EDVAC!\n\n▪ Used for scientific research\n• Chemistry, Medicine, Physics\nhttps:\/\/en.wikipedia.org\/wiki\/EDSAC\n\nhttps:\/\/www.tnmoc.org\/edsac\n\n32\n\n\fThe first “modern” computers\n\nUNIVAC (Universal Automatic Computer)\n• 1951 – UNIVAC\n▪ First commercial computer!\n• Sold 46! Units\n• Used to predict the 1952 presidential election\n\n▪ Used MERCURY!! memory (as did the EDSAC)\nDelay\nStorage\n\nhttps:\/\/en.wikipedia.org\/wiki\/Delay_line_memory\n\n33\n\n\fThen came the transistor\n\n• The symbol for a transistor\n▪ Photo taken in the university\nwhere I did my master degree\n\n• They were tiny\n▪ Didn’t get HOT!\n▪ Didn’t break as often\n\nhttps:\/\/en.wikipedia.org\/wiki\/Transistor\n\n34\n\n\fThen came the Integrated circuit\n\n2300 of these\nin there\n\n• Things became tiny\n▪ More transistors could be fitted\n▪ Cheaper circuits\n▪ More affordable\n\n35\n\n\fExtremely brief story of Intel CPUs\n● 1971 – Intel 4004\no 4-bit microprocessor\no with 2300! Transistors\n\n● 2004 – Pentium 4\no x86 32-bit\no 125 Million transistors\n● 2017 – Kaby Lake\no x86_64 64-bit\no >1000 Million! (undisclosed?)\n36\n\n\fMoore’s Law\n\n82944 processors\nfrom a supercomputer\nWhere used to simulate\n1s of human brain activity\nIn 40m\n\nIllustration:\nhttps:\/\/www.wired.com\/2013\/05\n\/neurologist-markam-humanbrain\/\n\n37\n\n\fAll different but all (mostly) the\nsame\n\n38\n\n\fClasses of computers: Embedded (Microcontrollers)\n● 8\/16-bit architectures are still common\no What does this mean?\n● As little as 32 BYTES of memory!\no almost always run one, built-in program\n● Focus on ultra-low power, cost, and size\no becoming more common every day\no “Internet of Things” (IoT)\no now your fridge can crash,\nyour TV can crash,\nyour dishwasher can crash,\neverything can crash!\n\n39\n\n\fSome are small and cheap\n\n• They run a single program\n▪ E.g your refrigerator\n\n• Are used by hobbyists\n▪ For small projects\n\n40\n\n\fClasses of computers: Consumer-grade (PC\/Mobile)\n● 1-8 cores, 32\/64-bit architectures, MB-GB of\nmemory, GB-TB of persistent storage\n● multitasking operating systems\n● similar capabilities, different design goals\n● focus is real-time user interaction:\nproductivity, media, browsing, games\n● Energy consumption is still relevant\no Mobile\n\n41\n\n\fSome are small, cheap, and powerful\n\n• The Raspberry Pi\n▪ Affordable, yet powerful\n▪ ~$35\n▪ Can be used for A LOT of projects\n• Home automation\n• Affordable PC\n• Great to learn how to program\non a budget\n\n42\n\n\fSome are small, expensive, and powerful\n\n• My Moto G3 \n▪ I sacrificed it for you!\n• (battery was bloated, it had to go)\n\n▪ Had to rip some parts :D\n▪ Mobility is important (~1 day)\n▪ Portability is important\n▪ But it runs beefy apps!\n\n43\n\n\fWe hold them in our laps (does anyone do that often?)\n\n• Power and mobility\n▪ Battery life is important\n• We want to fly with them \n\n▪ Weight is important\n▪ Run demanding programs!\n\n44\n\n\fWe have them at home\n\nMy computer\n\n• Desktop computers\n▪ Wide range of prices ($300 to +$5k)\n▪ Energy consumption not important\n• Beyond cost and heat generation\n\n▪ Performance\n• Games\n• Browsers!!\n• Word?\n\nThat’s a\nRaspberry Pi\n\n45\n\n\fClasses of computers: Servers and Mainframes\n● from high end desktops to much more powerful machines or groups of\nmachines\n● dozens of cores, 32GB+ of RAM, massive storage systems, very high-speed\nnetworking\n● focus on real-time data delivery - either from storage or after processing\n● redundancy and hot-swappability\n● goal is 100% uptime\n● power and cooling become huge concerns\n\n46\n\n\fWe have them in warehouses\n\n• Server on a “drawer” (rack)\n▪ Don’t have monitors\n• People don’t “use them” directly\n• Non-interactive\n\n▪ Crunch numbers and return results\n• Webpages\n• Remote storage (e.g., box)\n\n47\n\n\fClasses of computers: Supercomputers\n● cluster of hundreds to thousands of CPUs\n● focus on crunching ENORMOUS datasets non-interactively\n● science, research, design, and simulation\no and now, stock trading and \"cryptocurrencies\"…\n\n48\n\n\fAll looking pretty much the same\n\nThe Internet\n49\n\n\fGeneral computer organization\n\nPersistent\nStorage\n\nRegisters\n\nControl\n\nMemory\nDatapath\n\nProcessor\n\nOther\nPeripherals\n\nOther\nComputers\n\nInput\/Output\nDevices\n\nNetwork\n50\n\n\fAssembly\n\n51\n\n\fDifferent language levels\n\nx=x+1\nHigh-level language\nWritten by humans → Abstracted\nlw\n$t0, x\naddi $t0, $t0, 1\nsw\n$t0, x\n\nEasier to\nread\n\nCloser to the\nhardware\n\nAssembly language\nWritten by humans→ No abstraction\n\n10011101100110011001111101111001\n11011110110010111011101001111001\n10011100100110110001101111111011\n\nRandom numbers ☺\n\nMachine language\nSpoken by the CPU→ Binary\n\n52\n\n\fSo what exactly is assembly?\n\n• Assembly: Human-readable representation of machine code\n• Machine code: Machine code is what a CPU actually runs\n• Essentially it's the \"atoms\" that make up a program\n\no CPUs are actually pretty simple in concept\n▪ (oh wait, we still have a whole semester about it… huh)\n• Each CPU has its own machine language and therefore its own assembly\nlanguage\n• We’ll be using MIPS:\no Not that common (why then?)\no Yet, often found in surprising places. (Nintendo 64, PS1\/2, FPGAs)\no Very influential, and most common assembly looks like it.\n▪ ARM and RISC-V are similar-ish ISA\nseeing much more usage\n53\n\n\fDifferent language levels\nx=x+1\nHigh-level language\n→ Same for different CPUs\nlea x, %eax\nmov 0(%eax), %ecx\ninc\n%ecx\nmov %ecx, 0(%eax)\n\nlw\n$t0, x\naddi $t0, $t0, 1\nsw\n$t0, x\n\nAssembly language\n→ Different for different CPUs\n\n010011110111100100011\n011001011100111101100\nRandom numbers ☺\n101110011010110100111\n101111001000110110010\n1110\n\n1001110110011001100\n1111101111001110111\n1011001011101110100\nRandom numbers ☺\n1111001100111001001\n1011000110111111101\n1\n\nMachine language\n→ Different for different CPUs\n\n54\n\n\fis assembly language useful today?\n\n• Short answer: YES\n• Assembly is “fast”, so we should use it for everything!\n\n--- NO!!! --• No type-checking, no control structures, very few abstractions\no Probably you can be fairly fast using compiler optimizations\n--- Fairly impractical for large things --• Tied to a particular CPU.\no So, large programs have to be rewritten (usually) to work on new things.\n• Yet: good for specialized stuff.\no Critical paths and “boot” code in Kernels \/ Operating Systems\no HPC (simulators, supercomputer stuff)\no Real-time programs (video games; tho increasingly less \/ abstracted away)\no And…\n55\n\n\fEmbedded systems\n● You’d be amazed at how many consumer devices have CPUs.\n● Many are programmed largely\/entirely in assembly (or C)\n\n56\n\n\fand next time we'll start\n• Data representations\n• How does a CPU look at numbers, letters, etc.\n• Learn about number bases:\no Decimal (that’s what we use!)\no Binary\no Octal\no Hexadecimal\n\n57\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":162,"segment": "unlabeled", "course": "cs0449", "lec": "lec04", "text":"3\n\nIntroduction\nto C\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fOverview of C\nWhat You C is What You Get\n\n2\n\n\fC: The Universal Assembly Language\nC is not a “very high-level” language, nor\na “big” one, and is not specialized to any\nparticular area of application. But its\nabsence of restrictions and its generality\nmake it more convenient and effective\nfor many tasks than supposedly more\npowerful languages.\n— Kernighan and Ritchie\n\n• Allows writing programs to exploit\nunderlying features of the architecture\n– memory management, special\ninstructions, parallelism.\n3\n\n\fC: Relevance\n• From IEEE Spectrum:\n• https:\/\/spectrum.ieee.org\/static\/interacti\nve-the-top-programming-languages-2019\n\n• Still relatively popular…\n• Lots of legacy code.\n• Lots of embedded devices.\n• Python, Java, R, JS are all written in C.\n\n4\n\n\fTIOBE index\nTIOBE Programming Community index is an indicator of the popularity of programming languages\n\nhttps:\/\/www.tiobe.com\/tiobe-index\/\n\n\fCompilation\n• C is a compiled language.\n• Code is generally converted into machine code.\n• Java, by contrast, indirectly converts to machine code using a byte-code.\n• Python, by contrast to both, interprets the code.\n\n• The difference is in a trade-off about when and how to create a machine-level\nrepresentation of the source code.\n• A general C compiler will typically convert *.c source files into an intermediate *.o\nobject file. Then, it will link these together to form an executable.\n• Assembly is also part of this process, but it is done behind the scenes.\n• You can have gcc (a common C compiler) spit out the assembly if you want!\n\n6\n\n\fCompilation: Simple Overview – Step 1\n• The compiler takes source code (*.c\nfiles) and translates them into machine\ncode.\n\nhello.c\n\nhello.o\n\n• This file is called an “object file” and is\njust potentially one part of your overall\nproject.\n• The machine code is not quite an\nexecutable.\n• This object file is JUST representing the\ncode for that particular source file.\n• You may require extra stuff provided by\nthe system elsewhere.\n\n7\n\n\fCompilation: Simple Overview – Step 2\n• You may have multiple files.\n• They may reference each other.\n• For instance, one file may contain certain\ncommon functionality and then this is\ninvoked by your program elsewhere.\nhello.c\n\nhello.o\n\n• You break your project up into pieces\nsimilarly to your Java programs.\n• The compiler treats them\nindependently.\nutil.c\nCS\/COE 0449 – Spring 2019\/2020\n\nutil.o\n8\n\n\fCompilation: Simple Overview – Step 3\n• Then, each piece is\nmerged together to form\nthe executable.\n• This process is called\nlinking.\nhello.c\n\nhello.o\n\nutil.c\n\nutil.o\n\nhello\n\nstdio.o\nExternal Libraries\n\n• The name refers to how\nthe references to\nfunctions, etc, between\nfiles are now filled in.\n• Before this step… it is\nunclear where functions\nwill end up in the final\nexecutable.\n• Keep this in mind as we\nlook at memory and\npointers later!\n9\n\n\fIt's just a grinder.\n• In summary:\n\nhello.c\n\ncode goes in, sausage object\nfiles come out\n\nSome compilers output\nassembly and rely on an\nassembler to produce\nmachine code\nThese days, it's common\nfor the compiler itself to\nproduce machine code,\nor some kind of\nplatform-independent\nassembly code\n(typically: a bytecode)\n10\n\n\fCompilation vs. Interpretation\nC (compiled)\n\nPython (interpreted)\n\n• Compiler + Linker translates code\ninto machine code.\n\n• Interpreter is written in some\nlanguage (e.g. C) that is itself\ntranslated into machine code.\n\n• Machine code can be directly\nloaded by the OS and executed by\nthe hardware. Fast!!\n• New hardware targets require\nrecompilation in order to execute\non those new systems.\n\n• The Python source code is then\nexecuted as it is read by the\ninterpreter. Usually slower.\n\n• Very portable! No reliance on\nhardware beyond the interpreter.\n11\n\n\fCompilation vs. Virtual Targets (bytecode)\n• Java translates source to a “byte code” which is a made-up architecture, but it\nresembles machine code somewhat.\n• Technically, architectures could execute this byte code directly.\n• But these were never successful or practical.\n\n• Instead, a type of virtual machine simulates that pseudo-architecture. (interpretation)\n• Periodically, the fake byte code is translated into machine code.\n• This is a type of delayed compilation! Just-In-Time (JIT) compilation.\n\n• This is a compromise to either approach.\n• Surprisingly very competitive in speed.\n• I don’t think the JVM-style JIT is going away any time soon.\n\n12\n\n\fC vs. Java\nC (C99)\n\nJava\n\nType of\nLanguage\n\nFunction Oriented\n\nObject Oriented\n\nProgramming\nUnit\n\nFunction\n\nClass = Abstract Data Type\n\nCompilation\n\ngcc hello.c - creates machine\nlanguage code\n\njavac Hello.java - creates Java virtual machine\nlanguage bytecode\n\nExecution\n\na.out - loads and executes\nprogram\n\njava Hello - interprets bytecodes\n\nManual (malloc, free)\n\nAutomatic (garbage collection)\n\nhello, world\n\nStorage\n\nFrom http:\/\/www.cs.princeton.edu\/introcs\/faq\/c2java.html\n\n13\n\n\fC vs. Java\nC (C99)\nor\n\nComments\n\nJava\n\n… end of line\n\nor\n\n… end of line\n\nConstants\nPreprocessor Yes\n\nNo\n\nVariable\ndeclaration\n\nBefore you use it\n\nAt beginning of a block\n\nVariable\nnaming\nconventions\nAccessing a\nlibrary\nFrom http:\/\/www.cs.princeton.edu\/introcs\/faq\/c2java.html\n\n14\n\n\fHello World\n\n15\n\n\fC Dialects\n• You will see a lot of different styles of C in the world at large.\n• The syntax has changed very little.\n\n• There have been a few different standard revisions.\n• C89 – ANSI \/ ISO C\n•\n\n–\n\n–\n\n• C99 – Adds ‘complex’ numbers and single-line comments\n•\n\n–\n\n• C11 – Newer than 99 (laughs in Y2K bug) starts to standardize\nUnicode and threading libraries.\n•\n\n–\n\n• C18 – Minor refinement of C11. The current C standard.\n•\n\n–\n\n• We will more or less focus on the C99 standard in our course.\n• I’ll try to point out some newer things if they are relevant.\n16\n\n\fThe C Syntax\nNothing can be said to be certain, except death and C-like syntaxes.\n\n17\n\n\fThe C Pre-Processor\n• The C language is incredibly simplistic.\n• To add some constrained complexity, there is a macro language.\n• This code does not get translated to machine code, but to more code!\n\n18\n\n\fThe “main” function\n\n19\n\n\fDeclaring variables\n\n20\n\n\fCasting\n\n21\n\n\fInteger Sizes – Revisted: sizeof\n\n22\n\n\fInteger Sizes – Revisted\n\n23\n\n\fIntegers: Python vs. Java vs. C\nLanguage\nPython\nJava\nC\n\nsizeof(int)\n>=32 bits (plain ints), infinite (long ints)\n32 bits\nDepends on computer; 16 or 32 or 64\n\n• C:\n• integer type that target processor works with most efficiently\n• For modern C, this is generally a good-enough default choice.\n\n• Only guarantee:\n•\n≥\n• Also,\n>= 16 bits,\n• All could be 64 bits\n\n≥\n\n≥\n\n>= 32 bits\n\n• Impacts portability between architectures\n24\n\n\fConstants\n\n25\n\n\fEnumerations\n\n26\n\n\fOperators: Java stole ‘em from here\n\n27\n\n\fAugmented Operators\n\n–\n\n28\n\n\fExpressions: an expression of frustration!!\n\n• C often coerces (implicitly casts) integers when operating on them.\n• To remove ambiguity, expressions, such as\nmost accommodates that operation.\n\n, result in a type that\n\n• Specifically, C will coerce all inputs of binary operators to at least\nan\ntype.\n• You’ll find that “this is weird, but consistent” is C’s general motto\n\n29\n\n\fThe C Syntax: Control Flow\nOnce you C the program, you can BE the program.\n\n30\n\n\fControlling the flow: an intro to spaghetti\n\n31\n\n\fControlling the flow: Boolean Expressions\n• C does not have a Boolean type!\n• However, the C99 and newer standard library provides one in\n\n• The Boolean expressions are actually just an\n\ntype.\n\n• It is just the general, default type. Weird but consistent, yet again!\n\n32\n\n\fControlling the flow: Putting it Together\n•\n\nstatements therefore take an\n\nand not a Boolean, as an expression.\n\n• If the expression is it is considered false.\n• Otherwise, it is considered true.\n\n33\n\n\fThrowing us all for a loop\n\n• Most loops (while, do) work exactly like Java.\n• Except, of course, the expressions are\n\ntyped, like\n\nstatements.\n\n• For loops only come in the traditional variety:\n•\n• C89 does not allow variable declaration within:\n• ERROR:\n\n• However, C99 and newer does allow this. Please do it.\n\n• Loops have special statements that alter the flow:\n•\n•\n\nwill end the current iteration and start the next.\nwill exit the loop entirely.\n34\n\n\fLoop Refresher: While, Do-While, For Loops\n\n35\n\n\fTaking a break and switching it up\n• The\nproperly.\n\nstatement requires proper placement of\n\n• Starts at\nmatching expression and follows until it sees a\n• It will “fall through” other\nstatements if there is no\n\nto work\n.\nbetween them.\n\n• Sometimes fall through is used on purpose... but it’s a bug 99% of the time :\/\n36\n\n\fControl Flow: Summary\n• Conditional Blocks:\n\nNote: a\n\ncan be a { block }\n\n•\n•\n• The if statement can be chained:\n\n• Conventional Loops:\n•\n•\n37\n\n\fControl Flow: Summary\n• For Loops:\n\nNote: a\n\ncan be a { block }\n\n•\n•\n•\n\n• Switch:\n•\n\n•\n38\n\n\fWhat’s your function?\n• Familiar: Java is, once again, C-like\n• You declare the return type\nbefore the name.\n•\n\nis used when there is nothing\nreturned\n• It is also used to explicitly denote\nthere being no arguments.\n• You SHOULD specify\ninstead\nof having an empty list.\n\n• Functions must be declared\nbefore they can be used.\n• We will look at how we divide\nfunctions up between files soon!\n\n39\n\n\fThis is all the structure you get, kid\n• C gives us a very simple method of defining aggregate data types.\n• The struct keyword can combine several data types together:\n\n40\n\n\fI don’t like all that typing… So I’ll… typedef it\n• To avoid typing the full name “struct Song” we can create a Song type instead.\n• The typedef keyword defines new types.\n\n41\n\n\fI don’t like all that typing… So I’ll… typedef it\n• You can also do this with integer types, for instance to define bool:\n\n• And\n\ntypes, although it won’t complain if you mix\/match them:\n\n• Now, functions can better illustrate they take an enum value:\n• Though, it accepts any integer and, yikes, any enum value without complaint!\n\n42\n\n\fThat’s seriously all you get…\n• Unlike Java, C is not Object-Oriented and has no class instantiation.\n• That’s C++!\n\n43\n\n\fGarbage in, garbage out: initialization\n• As we saw earlier, variables don’t require initialization.\n• However, unlike Java, the variables do not have a default value.\n• Java will initialize integers to 0 if you do not specify.\n• C, on the other hand…\n\n• The default values for variables are undefined.\n• They could be anything.\n• The Operating System ultimately decides.\n• Generally, whatever memory is left over. Also known as “garbage.”\n\n• ALWAYS INITIALIZE YOUR VARIABLES\n44\n\n\fThe trouble is stacking up on us!\n\nQ: Hmm. Where is the value for ‘x’ coming from? Why?\n\n45\n\n\fWhere’s that data coming from??\n• Every variable and data in your program technically has a location in which it\nlives.\n• In the previous nonsense example, the “x” variable was sharing the same space\nas the “a” variable from the other function.\n• The section of incremental memory called the stack, in this specific case.\n• This is not defined behavior of the language, but rather the OS.\n\n• C does not impose many rules on how memory is laid out and used.\n• In fact, it gets right out of the way and lets you fall flat on your face.\n\n• Now, we will take a deeper dive into…\n\nMEMORY\n46\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":163,"segment": "unlabeled", "course": "cs0007", "lec": "lec13", "text":"CS 0007: Introduction to Java\nLecture 13\nNathan Ong\nUniversity of Pittsburgh\nOctober 25, 2016\n\n\fAnnouncements\n• Exam grades are posted on Courseweb\n• You can see your exam during office\nhours or make an appointment, but you\nmay not keep them\n• Project 0 due October 31, 2016 at 11:59\nPM via submission link on Courseweb\n• Project 1 released soon\n\n\fUsage\n• When do I use For Loops?\n– Iteration, or going through all of the\nelements of an array or other type of list.\n– When the number of times you need to\nloop is known, either explicitly with a value,\nor through a variable.\n\n• While loops are used otherwise\n\n\fMulti-dimension Arrays\n• Arrays don't just have to be lists\n• They can also be grids, cubes, hypercubes, …\n\n\fType[]…[] name = new Type[size1]…\n[sizeN];\n\n\fType[]…[] name = values…s;\n\n\fint[][] listList = {{1,2,3},\n{4,5,6},{7,8,9}};\n\n\fThings to Remember\n• In 2-D Arrays, referencing an element\nrequires TWO PAIRS of square brackets\n• To get the #1 from listList\n• listList[0][0]\n• To get the #4 from listList\n• listList[1][0]\n• Remember: element  listList[row][col]\n\n\fHow Do We Go Through a 2-D\nArray?\n• If it took one for loop to go though a 1D array, then…\n• It will take two for loops to go through a\n2-D array\n• It will take N for loops to go through an\nN-D array\n• If you don't have to go through\neverything, don't. It just wastes time\n\n\flistList[row][col]\n\n\fLet's Test It on listList\npublic class Test\n{\npublic static void main(String[] args)\n{\nint[][] list = {{1,2,3},{4,5,6},{7,8,9}};\nfor(int i = 0; i < list.length; i++)\n{\nfor(int j = 0; j < list[i].length; j++)\n{\nSystem.out.println(\"Counting...\nNow at \" + list[i][j]);\n}\/\/end inner-loop for(j < list[i].length)\n}\/\/end outer-loop for(i < list.length)\n}\/\/end method main\n}\/\/End class Test\n\nFor convenience sake, listList  list\nWhat is the output?\n\n\fGood Coding Practices\n• Whenever you make any variable or array,\nensure that you fill it with the appropriate\nvalues, or use some default non-important\nvalue.\n• Rule of thumb:\n– Numbers\/char  0(.0)\n– boolean  false\n– Objects  null\n– Arrays  Depends on type, but every element\nshould be set to something explicitly\n\n\fMeaning of null\n• Null indicates the variable refers to nothing\n• There is no actual object with the given variable name\n• As expected, you cannot do anything useful with a variable\nset to null\n• Checking for null (in an if-statement or other boolean\nexpression) uses ==\nScanner myScan = new Scanner(System.in);\nif(myScan == null)\n{\nSystem.err.println(\"Something wrong happened\");\n}\/\/clearly never executed\n\n\fRecursion\n• Recursion is the process of a function\n(or set of functions) that calls itself.\n• Recursion is really just a special name\nfor calling the same function within\nitself\n• Recursion ends at a base case.\n\n\fA Simple Example\n• I want to compute n! (factorial)\nn! = n * (n-1) * (n-2) * … * 1\n• I can easily do this with loops, but for\nthe sake of an easy example, we can\neasily break this down\n\n\fA Simple Example\n• I want to compute n! (factorial)\nn! = n * (n-1) * (n-2) * … * 1\n• Recursion is the breakdown of a\nproblem into smaller chunks, like\nMatryoshka dolls\n\nSource:\nhttps:\/\/upload.wikimedia.org\/wikip\nedia\/commons\/7\/71\/RussianMatroshka.jpg\n\n\fRecursive Factorial\n• How can we take the large problem and\nbreak it up into something smaller?\n• Can we build up a solution?\n• With factorial, it is easy to see how to\nbreak this down by this equivalence:\nn! = n * ((n-1) * (n-2) * … * 2 * 1)\n= n * (n-1)!\n• To compute n!, we can rely on (n-1)!\n\n\fHow Do We Put This In Code?\n• We already know by the definition of\nrecursion that we have to call the function\ninside of itself.\npublic static int factorial(int n)\n{\nfactorial(<something>);\nreturn <something>;\n}\/\/end function(int)\n\n\fRecursive Factorial\n• Let us use the knowledge that we know\nfrom the equivalence.\nn! = n * ((n-1) * (n-2) * … * 2 * 1)\n= n * (n-1)!\n• In order to return n!, we need to know\nthe value of (n-1)! and then multiply it\nwith n.\n\n\fHow Do We Put This In Code?\n• In order to return n!, we need to know\nthe value of (n-1)! and then multiply it\nwith n.\npublic static int factorial(int n)\n{\nfactorial(<something>);\nreturn <something>;\n}\/\/end function(int)\n\n\fHow Do We Put This In Code?\n• In order to return n!, we need to know\nthe value of (n-1)! and then multiply it\nwith n.\npublic static int factorial(int n)\n{\nfactorial(<n-1>);\nreturn <n*(n-1)!>;\n}\/\/end function(int)\n\n\fHow Do We Put This In Code?\n• In order to return n!, we need to know\nthe value of (n-1)! and then multiply it\nwith n.\npublic static int factorial(int n)\n{\nint prevFact = factorial(n1);\nreturn n*prevFact;\n}\/\/end function(int)\n\n\fHow Do We Put This In Code?\n• In order to return n!, we need to know\nthe value of (n-1)! and then multiply it\nwith n.\npublic static int factorial(int n)\n{\nreturn n*factorial(n-1);\n}\/\/end function(int)\n\n\fHow Do We Put This In Code?\n• If we just use this, we run into the\nproblem that this goes on forever.\n• When do we stop? When is the base\ncase?\n\npublic static int factorial(int n)\n{\nreturn n*factorial(n-1);\n}\/\/end function(int)\n\n\fHow Do We Put This In Code?\n• When n <= 1, n! is just 1\npublic static int factorial(int n)\n{\nif(n <= 1)\n{\nreturn 1;\n}\nreturn n*factorial(n-1);\n}\/\/end function(int)\n\n\fHow Do We Put This In Code?\npublic class FactorialTester\n{\n…(Function)\npublic static void main(String[] args)\n{\nScanner scanner = new Scanner(System.in);\nSystem.out.println(\"Enter an integer\");\nint n = scanner.nextInt();\nSystem.out.println(n + \"! = \" +\nfactorial(n));\n}\/\/end method main(String[])\n}\/\/End class FactorialTester\n\n\fOBJECT ORIENTED\nPROGRAMMING\n\n\fRevisiting the History of\nLanguages\n• Assembly – Low-level hardware based\ncode\n• Structured – Introduces subroutines (i.e.\nfunctions) and well-defined looping\nstructures\n• Object-Oriented – Introduces classes\nand a philosophy about programming\n\n\fIt’s In the Name\n• Object-Oriented Programming is as it\nsounds. Assume everything can be\nmodeled as an object.\n\n\fTerminology\n• A class is code that describes objects of\nthat type\n• An instance is a particular object of a\ngiven type\n• A field is a property of an object\n• A method is a function that is provided\nby the class for an object of the class\ntype\n\n\fStatic\n• Referenced by keyword static\n• Property or method is not dependent on\nthe individual instantiated object, but\nrather the class as a whole\n• The property\/method exists between\ninstances; any change made to static fields\nare reflected through all instances of the\nclass\n• Static methods can only manipulate static\nfields\n\n\fMain Portions of OOP\n• Building a class\n• Determining inter-class relationships\n– Subclasses\n– Superclasses\n– Ownership\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":164,"segment": "unlabeled", "course": "cs0449", "lec": "midterm_review", "text":"Intro to Systems\nSoftware\n\nMidterm Review\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n1\n\n\fTopics for the Midterm\n• Data Representation\n• 2’s complement, Floating point, Bitwise operations,\nEndianness\n\n• C Programming\n• Pointers, Arrays, Strings, Memory Management\n\n• Machine-level Representation (x86-64)\n• Assembly Basics, Control Flow, Calling Conventions, Buffer\nOverflow\n\n• Executables\n• Compilers, Assemblers, Linkers, Loaders\n2\n\n\fData representation\n\n\fEncoding Integers\nTwo’s Complement\n\nUnsigned\n\nB2U(X ) =\n\nw−1\n\n xi 2\n\nB2T (X ) = − xw−1 2\n\ni\n\nw−1\n\nSign Bit\n\n• Two’s Complement Examples (w = 5)\n10 =\n\n-10 =\n\n8\n\n4\n\n2\n\n1\n\n0\n\n1\n\n0\n\n1\n\n0\n\n-16\n\n8\n\n4\n\n2\n\n1\n\n1\n\n0\n\n1\n\n1\n\n0\n\n+  xi 2\ni=0\n\ni=0\n\n-16\n\nw−2\n\n8+2 = 10\n\n-16+4+2 = -10\n\ni\n\n\fUnsigned & Signed Numeric Values\nX\n0000\n0001\n0010\n0011\n0100\n0101\n0110\n0111\n1000\n1001\n1010\n1011\n1100\n1101\n1110\n1111\n\nB2U(X)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\nB2T(X)\n0\n1\n2\n3\n4\n5\n6\n7\n–8\n–7\n–6\n–5\n–4\n–3\n–2\n–1\n\n• Equivalence\n• Same encodings for\nnonnegative values\n\n• Uniqueness\n• Every bit pattern represents\nunique integer value\n• Each representable integer has\nunique bit encoding\n• Expression containing signed\nand unsigned int:\nint is cast to unsigned\n\n\fImportant: Numeric Ranges\n\n• Unsigned Values\n• UMin =\n\n• Two’s Complement Values\n• TMin\n=\n–2w–1\n\n0\n\n• 000…0\n\n• UMax\n\n=\n\n• 100…0\n\n• TMax\n\n2w – 1\n\n• 111…1\n\n=\n\n• 011…1\n\n• Minus 1\n• 111…1\nValues for W = 16\nUMax\nTMax\nTMin\n-1\n0\n\nDecimal\n65535\n32767\n-32768\n-1\n0\n\nHex\nFF FF\n7F FF\n80 00\nFF FF\n00 00\n\nBinary\n11111111 11111111\n01111111 11111111\n10000000 00000000\n11111111 11111111\n00000000 00000000\n\n2w–1 – 1\n\n\fThree “kinds” of floating point numbers\ns exp\n1\n\n00…00\n\ndenormalized\n\nfrac\ne-bits\n\nf-bits\n\nexp ≠ 0 and exp ≠ 11…11\n\nnormalized\n\n11…11\n\nspecial\n\n\f“Normalized” Values\n\n• When: exp ≠ 000…0 and exp ≠ 111…1\n• Exponent coded as a biased value: E = exp – Bias\n• exp: unsigned value of exp field\n• Bias = 2k-1 - 1, where k is number of exponent bits\n• Single precision: 127 (exp: 1…254, E: -126…127)\n• Double precision: 1023 (exp: 1…2046, E: -1022…1023)\n\n• Significand coded with implied leading 1: M = 1.xxx…x2\n• xxx…x: bits of frac field\n• Minimum when frac=000…0 (M = 1.0)\n• Maximum when frac=111…1 (M = 2.0 – ε)\n• Get extra leading bit for “free”\n\nv = (–1)s M 2E\n\n\fDenormalized Values\n\n• Condition: exp = 000…0\n• Exponent value: E = 1 – Bias (instead of exp – Bias) (why?)\n• Significand coded with implied leading 0: M = 0.xxx…x2\n• xxx…x: bits of frac\n\n• Cases\n• exp = 000…0, frac = 000…0\n• Represents zero value\n• Note distinct values: +0 and –0 (why?)\n\n• exp = 000…0, frac ≠ 000…0\n• Numbers closest to 0.0\n• Equispaced\n\nv = (–1)s M 2E\nE = 1 – Bias\n\n\fSpecial Values\n\n• Condition: exp = 111…1\n• Case: exp = 111…1, frac = 000…0\n• Represents value  (infinity)\n• Operation that overflows\n• Both positive and negative\n• E.g., 1.0\/0.0 = −1.0\/−0.0 = +, 1.0\/−0.0 = −\n\n• Case: exp = 111…1, frac ≠ 000…0\n• Not-a-Number (NaN)\n\n• Represents case when no numeric value can be determined\n• E.g., sqrt(–1),  − ,   0\n\n\fCasting floats to ints and vice versa\n• (int) floating_point_expression\n• Coerces and converts it to the nearest integer\n(C uses truncation)\ni = (int) (3.14159 * f);\n\n• (float) integer_expression\n• Converts integer to nearest floating point\nf = f + (float) i;\n\n\ffloat → int → float\nif (f == (float)((int) f)) {\nprintf(“true”);\n}\n• Does it always print “true”?\n• No, small floating point numbers (< 1) don’t have integer\nrepresentations\n\n• For other numbers, often will be rounding errors\n\n\fint → float → int\nif (i == (int)((float) i)) {\nprintf(“true”);\n}\n• Does it always print “true”?\n• No, many large values of integers don’t have exact floating\npoint representations\n\n• What about double?\n• Yes, significand is now 52 bits, which can hold all of 32-bit\ninteger, so will always print “true”\n\n\fBitwise Operations\n• Write a C expression that will yield a word consisting of the\nleast significant byte of x and the remaining bytes of y\n• For operands x = 0x89ABCDEF and y =\n0x76543210, this would give 0x765432EF\n\nHint: use masking and bit manipulation\n\n20\n\n\fBitwise Operations\n• Write a C expression that will yield a word consisting of the\nleast significant byte of x and the remaining bytes of y\n• For operands x = 0x89ABCDEF and y =\n0x76543210, this would give 0x765432EF\n\n1) How to extract the upper 3 bytes of x?\n\n(x & 0xFF)\n\n2) How to extract the lower 1 byte of y?\n\n(y & ~0xFF)\n\n3) How to combine them?\n\n(x & 0xFF) | (y & ~0xFF)\n\n21\n\n\fBitwise Operations\n• How to perform the operation below using shifts\/adders only?\n\na * 7\n(a << 3) - a\nlong m12(long x)\n{\nreturn x*12;\n}\n\nConverted to x86-64 by compiler:\nleaq (%rdi,%rdi,2), %rax\nsalq $2, %rax\n\n# t = x+2*x\n# return t<<2\n\n22\n\n\fBitwise Operations\n• Suppose we number the bytes in a w-bit word from 0 (least significant) to w\/8 – 1\n(most significant)\n• Write code for the following C function, which will return an unsigned value in\nwhich byte i of argument x has been replaced by byte b:\n\nunsigned replace_byte (unsigned x, int i, unsigned char b);\n\nreplace_byte(0x12345678, 2, 0xAB) --> 0x12AB5678\nreplace_byte(0x12345678, 0, 0xAB) --> 0x123456AB\n\n23\n\n\fBitwise Operations\n• Write code for the following C function, which will return an unsigned value in\nwhich byte i of argument x has been replaced by byte b:\n\nunsigned replace_byte (unsigned x, int i, unsigned char b) {\nint itimes8 = i << 3;\n\nunsigned mask = 0xFF << itimes8;\nreturn (x & ~mask) | (b << itimes8);\n}\n\n24\n\n\fByte Ordering\n• Example\n• Variable x has 4-byte value of 0x01234567\n• Address given by &x is 0x100\nBig Endian\n\n0x100 0x101 0x102 0x103\n\n01\n01\n\nLittle Endian\n\n23\n23\n\n45\n45\n\n67\n67\n\n0x100 0x101 0x102 0x103\n\n67\n67\n\n45\n45\n\n23\n23\n\n01\n01\n\n\fC Programming\n\n\fC Memory Layout Analysis\n\ns1[0] → ??\n\ns2[0] → ??\n\n27\n\n\fC Programming\n• What is wrong with the C code below?\nint* pi = malloc(314*sizeof(int));\nif(!raspberry) {\npi = malloc(1 *sizeof(int));\n}\nreturn pi;\nThere’s a memory leak if raspberry is false as the original value of pi will be\nunreachable\n28\n\n\fC programming\nConsider the following code snippet which allocates an array and\nsets the values:\n1\n2\n3\n4\n5\n6\n7\n8\n\nint main(int argc, char** argv) {\nint *a = (int*) malloc(213 * sizeof(int));\nfor (int i=0; i<213; i++) {\nif (a[i] == 0) a[i]=i;\nelse a[i]=-i;\n}\nreturn 0;\n}\n\nWhich lines have a problem and how can you fix it?\n\n\fC programming\n\n1\n2\n\n3\n4\n5\n6\n7\n8\n\nint main(int argc, char** argv) {\nint *a = (int*) malloc(213 * sizeof(int));\nif (a == NULL) return 0;\nfor (int i=0; i<213; i++) {\nif (a[i] == 0) a[i]=i;\nelse a[i]=-i;\n}\nreturn 0;\n}\n\nmalloc can fail!\n\n\fC programming\n1\n2\n3\n4\n5\n6\n7\n8\n\nint main(int argc, char** argv) {\nint *a = (int*) calloc(213, sizeof(int));\nif (a == NULL) return 0;\nfor (int i=0; i<213; i++) {\nif (a[i] == 0) a[i]=i;\nelse a[i]=-i;\n}\nreturn 0;\n}\n\nAllocated memory is not initialized!\n\n\fC programming\n1\n2\n3\n4\n5\n6\n\n7\n8\n\nint main(int argc, char** argv) {\nint *a = (int*) calloc(213, sizeof(int));\nif (a == NULL) return 0;\nfor (int i=0; i<213; i++) {\nif (a[i] == 0) a[i]=i;\nelse a[i]=-i;\n}\nfree(a);\nreturn 0;\n}\n\nAll allocated memory must be freed!\n\n\fC programming\nWhat lines make safe_int_malloc not so safe?\n1\n2\n3\n4\n5\n\nint *safe_int_malloc(int *pointer) {\npointer = malloc(sizeof(int));\nif (pointer == NULL) exit(-1);\nreturn &pointer;\n}\n\npointer is a local copy of the pointer! Modifying *pointer only changes the value\nwithin the scope of this function not outside\n\n\fC programming\nPassing in an int** let’s us change the value of int* pointer\n\n1\n2\n3\n4\n5\n\nint *safe_int_malloc(int **pointer) {\n*pointer = malloc(sizeof(int));\nif (pointer == NULL) exit(-1);\nreturn &pointer;\n}\n\nWhat’s STILL wrong?\n\n\fC programming\nThe address of something on the stack will be invalid after the\nfunction’s execution\n\n1\n2\n3\n4\n5\n\nint *safe_int_malloc(int **pointer) {\n*pointer = malloc(sizeof(int));\nif (pointer == NULL) exit(-1);\nreturn &pointer;\n}\n\n&pointer is a location on the stack in\nsafe_int_malloc’s frame!\n\n\fC programming\n\n1\n2\n3\n4\n5\n\nint **safe_int_malloc(int **pointer) {\n*pointer = malloc(sizeof(int));\nif (pointer == NULL) exit(-1);\nreturn pointer;\n}\n\n\fPointers\n• Pointer: stores address of some value in memory\n• Example:\n• Let us have a pointer a where int* a = 0x100\n\n*a = accesses value stored at location 0x100\na + i = 0x100 + sizeof(*a) * i\n• Dereferencing a NULL pointer or any other invalid memory\naddress causes segmentation fault\n\n\fx86 programming\n\n\fx86-64: Stack Manipulation\nWe execute:\n\nmov $0x449, %rax\npushq %rax\nFor each of the following instructions, determine if they will result in\nthe value 0x449 being placed in %rcx?\n\n1) mov (%rsp), %rcx\n2) mov 0x8(%rsp), %rcx\n3) mov %rsp, %rcx\n4) popq %rcx\n\n\fx86-64: Stack is memory\nWe execute:\nmov $0x449, %rax\npushq %rax\npopq %rax\nIf we now execute: mov -0x8(%rsp), %rcx\nwhat value is in %rcx?\n1) 0x0 \/ NULL\n2) Seg fault\n3) Unknown\n4) 0x449\n\n\fx86-64: Sometimes arguments are implicit\n* How many arguments does “rsr” take?\n* How many registers are changed before the function call?\n(Note, %sil is the lower 1 byte of %rsi)\n0x0400596 <+0>:\n0x040059a <+4>:\n0x040059c <+6>:\n0x04005a0 <+10>:\n0x04005a4 <+14>:\n0x04005a9 <+19>:\n0x04005ad <+23>:\n0x04005ae <+24>:\n0x04005b0 <+26>:\n\ncmp\nje\nsub\nsub\ncallq\nadd\nretq\nmov\nretq\n\n%sil,(%rdi,%rdx,1)\n0x4005ae <rsr+24>\n$0x8,%rsp\n$0x1,%rdx\n0x400596 <rsr>\n$0x8,%rsp\n%edx,%eax\n\n\fx86-64: Arguments can already be “correct”\nrsr does not modify s and t, so the arguments in those\nregisters are always correct\n\nint rsr(char* s, char t, size_t pos)\n{\nif (s[pos] == t) return pos;\nreturn rsr(s, t, pos - 1);\n}\n\n\fExample: Reverse-engineer Assembly to C\nmystery(long, long, long):\nleaq (%rdi,%rsi), %rax\naddq %rdx, %rax\ncmpq $3, %rdi\njg .L2\ncmpq %rdx, %rsi\njge .L3\nmovq %rdi, %rax\nimulq %rsi, %rax\nret\n.L3:\nmovq %rsi, %rax\nimulq %rdx, %rax\nret\n.L2:\ncmpq $10, %rdi\njle .L1\nmovq %rdi, %rax\nimulq %rdx, %rax\n.L1:\nret\n\nlong mystery(long x, long y, long z)\n{\nlong val = _______________;\nif (__________) {\nif (_________) {\nval = ___________;\n} else {\nval = ____________;\n\n}\n} else if (___________) {\nval = __________;\n}\n\nreturn ________;\n}\n\n43\n\n\fExample: increment\n\nlong increment(long *p, long val) {\nlong x = *p;\nlong y = x + val;\n*p = y;\nreturn x;\n}\n\nincrement:\nmovq\n(%rdi), %rax\naddq\n%rax, %rsi\nmovq\n%rsi, (%rdi)\nret\n44\n\nRegister\n\nUse(s)\n\n%rdi\n\n1st arg (p)\n\n%rsi\n\n2nd arg (val), y\n\n%rax\n\nx, return value\n\n\fProcedure Call Example (initial state)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\nInitial Stack Structure\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\n⟵%rsp\n\n• Return address on stack is the\naddress of instruction\nimmediately following the call to\n“call_incr”\n• Shown here as main, but could be\nanything)\n• Pushed onto stack by call\ncall_incr\n\n\fProcedure Call Example (step 1)\nStack Structure\n\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\n•••\n\nReturn addr <main+8>\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\nAllocate space\nfor local vars\n\n449\nUnused\n\n⟵old %rsp\n⟵%rsp+8\n⟵%rsp\n\n• Setup space for local variables\n• Only v1 needs space on the stack\n\n• Compiler allocated extra space\n• Often does this for a variety of\nreasons, including alignment\n\n\fProcedure Call Example (step 2)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\nStack Structure\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\n449\nUnused\n\nSet up parameters for call\nto increment\n\nAside: movl is used because 100 is a small positive value that fits\nin 32 bits. High order bits of rsi get set to zero automatically. It\ntakes one less byte to encode a movl than a movq.\n\n⟵%rsp+8\n⟵%rsp\n\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n100\n\n\fProcedure Call Example (step 3)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\nStack Structure\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\nincrement:\nmovq\n(%rdi), %rax\naddq\n%rax, %rsi\nmovq\n%rsi, (%rdi)\nret\n\n449\nUnused\nReturn addr <call_incr+?>\n\n• State while inside increment\n\n⟵%rsp\n\n• Return address on top of stack is address of\nthe addq instruction immediately following\ncall to increment\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n100\n\n%rax\n\n48\n\n\fProcedure Call Example (step 4)\nStack Structure\n\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\n549\nUnused\nReturn addr <call_incr+?>\n\n⟵%rsp\n\n• State while inside increment\n• After code in body has been executed\n\nincrement:\nmovq\n(%rdi), %rax # x = *p\naddq\n%rax, %rsi\n# y = x + 100\nmovq\n%rsi, (%rdi) # *p = y\nret\n\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n549\n\n%rax\n\n449\n\n49\n\n\fProcedure Call Example (step 5)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\nStack Structure\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\n549\n\n⟵%rsp+8\n\nUnused\n\n⟵%rsp\n\n• After returning from call to increment\n• Registers and memory have been modified and\nreturn address has been popped off stack\n\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n549\n\n%rax\n\n449\n\n\fProcedure Call Example (step 6)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\nStack Structure\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\n549\n\n⟵%rsp+8\n\nUnused\n\n⟵%rsp\n\nUpdate %rax to contain v1+v2\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n549\n\n%rax\n\n549+449\n\n\fProcedure Call Example (step 7)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\n\nStack Structure\n•••\nReturn addr <main+8>\n\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\n549\nUnused\n\n⟵%rsp\n⟵old %rsp\n\nDe-allocate space for local vars\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n549\n\n%rax\n\n998\n\n\fProcedure Call Example (step 8)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\nStack Structure\n•••\nReturn addr <main+8>\n\n⟵%rsp\n\n• State just before returning from\ncall to call_incr\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n549\n\n%rax\n\n998\n\n\fProcedure Call Example (step 9)\nlong call_incr() {\nlong v1 = 449;\nlong v2 = increment(&v1, 100);\nreturn v1 + v2;\n}\ncall_incr:\nsubq\n$16, %rsp\nmovq\n$449, 8(%rsp)\nmovl\n$100, %esi\nleaq\n8(%rsp), %rdi\ncall\nincrement\naddq\n8(%rsp), %rax\naddq\n$16, %rsp\nret\n\nFinal Stack Structure\n•••\n⟵%rsp\n\n• State immediately after returning from\ncall to call_incr\n• Return addr has been popped off stack\n• Control has returned to the instruction\nimmediately following the call to\ncall_incr (not shown here)\n\nRegister\n\nUse(s)\n\n%rdi\n\n&v1\n\n%rsi\n\n549\n\n%rax\n\n998\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":165,"segment": "self_training_1", "course": "cs0447", "lec": "lec09", "text":"#9\n\nBinary arithmetic:\nAddition\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n\n• nope\n\n2\n\n\fBinary addition\n\n3\n\n\fAdding in binary\n● It works the same way as you learned in school\no Except instead of carrying at 1010, you carry at… 102!\n▪ 1 + 1 = 102 (210)\n▪ 1 + 1 + 1 = 112 (310)\n● Let's try it. (what are these in decimal?)\n0 1 1 1\n\n1 1 0\n\n1011 0010\n+0010 1111\n1110 0001\n\n4\n\n\fFormalizing the algorithm\n● for each pair of bits starting at the LSB,\no add the two bits and the carry\no the low bit of the sum goes into the sum row\no the high bit of the sum is the carry for the next higher bit\n● this is the grade school algorithm\no cause it's how you learned to add in grade school\n\nBit\nBucket..?\n\n1 0 1 1 0 0 1 0\n+0 0 1 0 1 1 1 1\n5\n\n\fSigned Numbers\n\n6\n\n\fRemember this?\n● if you load a byte…\n\n31\n0\n00000000 00000000 00000000 00000000\n\n10010000\n\nIf the byte is signed… what should it become?\n\n31\n0\n11111111 11111111 11111111 10010000\nIf the byte is unsigned… what should it become?\n\n31\n0\n00000000 00000000 00000000 10010000\n\nlb does\n\nsign extension.\n\nlbu does\n\nzero extension.\n7\n\n\fArbitrariness… Again\n● What does this mean?\n\n10001010\nIs it signed?\n\n¯\\_(ツ)_\/¯\n\nIs it\nunsigned?\n\nWhat’s the\ndifference?\n8\n\n\fUnsigned integers – they are all positive!\n● We have numbers, they are all positive!\no On a number line:\n\n000\n\n010\n100\n110\n001\n011\n101\n111\n\n0 1 2 3 4 5 6 7\n\n● What if we want to have negative numbers?\n\n9\n\n\fIf you wanted to implement signed integers…\n● What would be the most intuitive way of doing it?\no You could have a \"sign bit\" where 0 means +, and 1 means ● This is sign-magnitude representation. on a number line:\n\n111\n\n110\n\n000\n010\n101\n001\n011\n\n-3 -2 -1 0 +1 +2 +3\n100\n\n● What about the pattern 100?\no negative sign, 0 magnitude\no NEGATIVE ZERO??\n● Arithmetic is a bit awkward\n\n10\n\n\fSign-magnitude arithmetic\n● Negation\n\n-(3)\n\n0011\n\n-(-3)\n\n1011\n\nbit pattern for\npositive 3?\n\nbit pattern for\nnegative 3?\n\n1011\n\nFlip MSB!\n\n0011\n\nFlip MSB!\n\n11\n\n\fSign-magnitude arithmetic\n● Negation\n\n-(3)\n\n0011\n\n1011\n\n-(-3)\n\n1011\n\n0011\n\n● (wrong) Addition\n\n11\n0011\n+0011\n0110\n\nto binary?\n\n3\n+ 3\n6\n\nbit pattern for\n-3… positive 3?\n\n3\n+-3\n0\n\n11\n0011\n+1011\n1011\n1110\n\n-6\n\nto decimal?\n\n● It’s not impossible\no But it is awkward\n\n12\n\n\fWell, what if…\n● We use a technique borrowed from old accounting practices and\nmechanical calculators, we can flip all the bits to negate a number\n● This is ones' complement. on a number line:\n● We have the same problem with 111\nas sign-magnitude had with 100\n\n100\n\n101\n\n000\n010\n110\n001\n011\n\n-3 -2 -1 0 +1 +2 +3\n● This does make arithmetic easier\n● But it is not used in modern computers.\n\n111\n\n13\n\n\fSign-magnitude arithmetic\n● Negation\n\n-(3)\n\n0011\n\n-(-3)\n\n1100\n\nbit pattern for\npositive 3?\nbit pattern for\nnegative 3?\n\nflip!\n\nflip!\n\n1100\n\n0011\n\n14\n\n\fSign-magnitude arithmetic\n● Negation\n\n● Addition\n\nto binary?\n\n3\n+ 2\n5\n\n-(3)\n\n0011\n\n1100\n\n-(-3)\n\n1100\n\n0011\n\n0010\n0011\n+0010\n0101\n\nAdd the\ncarry back in\n\nbit pattern for\n-2… positive 2?\n\n3\n+-2\n1\n\n1111\n0011\n0010\nflip!\n+1101\n1101\n0000???\n+0001\n1\n0001\n\nthis is positive, so don’t flip\n\nto decimal?\n\n15\n\n\fOne’s complement arithmetic\n● Do you really want to write code like\n\nif( (my_var == 0) OR (my_var == -0) )\n{\n…\n}\n\n● Just kidding… Compilers would handle that for you!\n\n16\n\n\fFinally, two's complement\n● First, we flip all the bits\no Just like with 1’s complement\n● Then add 1.\n● The number line looks a little bit stranger\no But there is only one 0, and it is 0!\n\n100\n\n101\n\n110\n\n000\n010\n111\n001\n011\n\n-4 -3 -2 -1 0 +1 +2 +3\n\n17\n\n\fFinally, two's complement\n● First, we flip all the bits\no Just like with 1’s complement\n● Then add 1.\n● The number line looks a little bit stranger\no But there is only one 0, and it is 0!\n\n100\n\n101\n\n110\n\n000\n010\n111\n001\n011\n\n-4 -3 -2 -1 0 +1 +2\n● It's lopsided: There is no +4!\no But arithmetic is easy!\no when someone says \"signed,\" 99% of the time they mean this\no When I say “signed”, I 100% of the time mean this\n\n+3\n\n18\n\n\fSign-magnitude arithmetic\n● Negation\n\n-(3)\n\n0011\n\n-(-3)\n\n1101\n\nbit pattern for\npositive 3?\nbit pattern for\nnegative 3?\n\nflip!\n\nflip!\n\n1100\n\n0010\n\n1101\n\nAdd 1!\n\n0011\n\nAdd 1!\n\n● You don’t need to subtract!!\no flip(k)+1 == flip(k-1)\n▪ If you ignore the carry! ☺\n\n19\n\n\fTwo's complement addition\n● the great thing is: you can add numbers of either sign without having to do\nIgnore the carry\nanything special!\nto binary? 0111\n\n3\n+ 7\n10\n\nbit pattern for\n-7… positive 7?\n\n0011\n3\n+0111 +-7\n1010 -4\n\n0011\n0011\n0111\n4\nflip!\n+1 +1001\nto decimal?\n1000\n1100 0100\nthis is negative, so\n\nwhat is it? flip!\nthe actual patterns of bits are the same.\nso how does the computer \"know\" whether it's\ndoing signed or unsigned addition?\n\n+1\n\n0011\n\n20\n\n\fIT DOESN'T\n21\n\n\fThis is how it looks\n● Interpreting bit patterns as (left) unsigned and (right) signed\n\n15 0\n\n1\n\n-1 0\n\n14 1111 0000 0001 2\n1110\n0010\n13 1101\n0011 3\n\n-2 1111\n1110\n-3 1101\n\n12 1100\n1011\n11 1010\n\n-4 1100\n1011\n-5 1010\n\n10\n\n1001\n\n9\n\n0100\n0101\n0110\n1000\n\n8\n\n0111\n\n7\n\n6\n\n4\n5\n\n-6\n\n1001\n\n0000\n\n1\n0001\n\n0010\n\n2\n\n0011\n0100\n0101\n\n0110\n1000\n\n0111\n\n-7 -8 7\n\n6\n\n3\n4\n5\n\n\fTwo's complement negation\n● (assuming 4 bits) What about??? -8?\no What happens if we negate -8?\n▪ i.e. -(-8)?\n\n-(-8)\n\n1000\n\nbit pattern for\nnegative 8?\n\nflip!\n\n0111\n\nWHAT DID YOU\nDO WRONG??????\n\n1000\n\nAdd 1!\n\n-8???\n-1 0\n\n-2 1111\n1110\n-3 1101\n\n0000\n\n-6\n\n0001\n0010\n\n2\n\n0011\n\n-4 1100\n1011\n-5 1010\n1001\n\n1\n\n0100\n0101\n0110\n\n1000\n\n0111\n\n-7 -8 7\n\n3\n4\n5\n\n6\n23\n\n\fAbsolutely Bonkers – Slide borrowed from wilkie (449)\npublic class AbsTest {\npublic static int abs(int x) {\nif (x < 0) {\nx = -x;\n}\nreturn x;\n}\npublic static void main(String[] args) {\nSystem.out.println(\nString.format(\"|%d| = %d\", Integer.MIN_VALUE, AbsTest.abs(Integer.MIN_VALUE))\n\n);\n}\n\n}\n\n\/\/ Outputs: |-2147483648| = -2147483648\nCS\/COE 0449 – Spring 2019\/2020\n\nQ: How many bits is a Java int? What happened here?\n\n24\n\n\fNeat properties\n● If we take a positive number and add zeros:\no 12010 = 0111 10002\no 0000 0000 0111 10002 = 12010\n● If we take a negative number and add ones:\no 1000 01112 = - 0111 10002 + 12\n= - 0111 10012 = -12110\n\no 1111 1111 1000 01112 = - 0000 0000 0111 10002+12\n= - 0000 0000 0111 10012\n= - 12110\n\n25\n\n\fA return to E X P A N D V A L U E\n● if you load a byte…\n\n31\n0\n00000000 00000000 00000000 00000000\n\n10010000\n\nIf the byte is signed… what should it become?\n\n31\n0\n11111111 11111111 11111111 10010000\nIf the byte is unsigned… what should it become?\n\n31\n0\n00000000 00000000 00000000 10010000\n\nlb does\n\nsign extension.\n\nlbu does\n\nzero extension.\n26\n\n\fOverflow\n\n27\n\n\fOverflow\n● In computers, numbers are finite.\n● Let's say our 4-digit display was counting up:\n9997, 9998, 9999…\n● What comes \"next\"?\no What does this \"0000\" really mean?\no It wrapped around.\n● This is overflow: the number you are trying\nto represent is too big to be represented.\n● Essentially, all arithmetic on the computer is modular arithmetic!\no This causes a lot of software bugs.\no https:\/\/en.wikipedia.org\/wiki\/Pac-Man#Level_256\n\n28\n\n\fNumber carrousel\n● Computers perform modulus arithmetic\no Meaning: it goes around!\no E.g. in a 4-bit computer\n15 0\n\n14 1111\n1110\n13 1101\n\n0000\n\n1001\n\n0001\n0010\n\n2\n\n0011\n\n12 1100\n1011\n11 1010\n10\n\n1\n\n0100\n0101\n0110\n0111\n\n3\n4\n5\n\n6\n\n0110\n8 7\n+0111\nwhat is 6 + 7? 1101\n9\n\n1000\n\n29\n\n\fUnsigned overflowing\n\n15 0\n\n14 1111\n1110\n13 1101\n\n0000\n\n1001\n\n9\n\n2\n0010\n0100\n0101\n\n0110\n1000\n\n8\n\nUhhhhh\n\n0001\n\n0011\n\n12 1100\n1011\n11 1010\n10\n\n1\n\n0111\n\n6\n\n7\n\nwhat is 14 + 7?\n\n3\n4\n5\n\n1110\n+0111\n10101\n\nIf the result is smaller\nthan either addend,\nthere is an overflow\n\n30\n\n\fSigned overflowing\n● It may also happen with signed numbers\n\nwhat is -6 - 7?\n1 0 0 0\n\nThis is OK!\n\n-1 0\n\n-2 1111\n1110\n-3 1101\n\n0000\n\nThe sign of\noverflow is the\nopposite sign\n\n-6\n\n1001\n\n0001\n0010\n\n0100\n0101\n0110\n1000\n\n4\n5\n\nwhat is 6 + 7?\n\n2\n\n0011\n\n-4 1100\n1011\n-5 1010\n\n1010\n+1001\n0011\n\n1\n\n0111\n\n-7 -8 7\n\n3\n\n6\n\nIf the result of adding two numbers with the\nsame sign results in the opposite sign,\nthere is overflow\n\n0 1 1 0\n\n0110\n+0111\n1101\n31\n\n\fSigned overflowing\n\nWhat about this?\nHow can we detect\nif operations with\ndifferent signs\noverflow?\n\n-1 0\n\n-2 1111\n1110\n-3 1101\n\n1001\n\n1\n0001\n0010\n\n2\n\n0011\n\n-4 1100\n1011\n-5 1010\n-6\n\nThis is impossible:\nMax positive = 7\n-1+7=6!\n\n0000\n\n0100\n0101\n0110\n1000\n\n0111\n\n-7 -8 7\n\n3\n4\n5\n\n6\n\n32\n\n\fHow many bits?\n● if you add two 2-digit decimal numbers, what's the\nlargest number you can get?\n● what about two 4-digit decimal numbers?\n● what about two 4-bit numbers?\n● what's the pattern of the number of digits?\no if you add two n-digit numbers in any base…\no the result will have at most n + 1 digits\n● that means if we add two 32-bit numbers…\no …we might get a 33-bit result!\no if we have more bits than we can store in our\nnumber, that's overflow.\n\n99 9999\n+99 +9999\n198 19998\n1111\n+1111\n11110\n\nQ: How many bits do you need to represent -16 in 1s’ complement?\nQ: How many bits do you need to represent -16 in 2’s complement?\n\n33\n\n\fHandling overflow\n● we could ignore it\no in MIPS: addu, subu\no this is usually a bad idea\n▪ your program is broken\no it's also the default in most languages, thanks C\n● we could fall on the floor - i.e. crash\no in MIPS add, sub\no can be handled and recovered from\no but more complex\n● we could store that 33rd bit somewhere else\n\n34\n\n\fMaybe the bit bucket is a real place…\n● many other architectures do this\no MIPS does not.\n● they have a \"carry bit\" register\no this can be checked by the program after an add\/sub\n● this is very useful for arbitrary precision arithmetic\no if you want to add 2048-bit numbers, chain many 32-bit additions\n0 1 1 1\n\n0 0 0\n\n0 0 1 1 1\n\n1 0 0 0 0\n\n0010 1001\n+0001 1100\n0100 0101\n\n0010\n+0001\n0100\n\n1001\n+1100\n0101\n\nadd8bit\n\nadd4bit\n\nadd4bit\n\n35\n\n\fMore Bonkers\npublic class Main {\npublic static void main(String []args) {\nint w = Integer.MAX_VALUE;\nif (w + 1 > w) {\nSystem.out.println(\"This does not happen.\\n\");\n}\nint z = w + 1;\nif (z > w) {\nSystem.out.println(\"This does not happen.\\n\");\n}\nSystem.out.println(\"The end.\\n\");\n}\n}\nCS\/COE 0449 – Spring 2019\/2020\n\n36\n\n\fI have a book\n\n37\n\n\fWhat’s the first page?\n\nHilarious,\nright?\n\n38\n\n\fThen…\n\nWhat follows, chapter\n1?\n(it’s not!)\n\nWhat number page\nnumber does it start at?\n(it’s not “2”!)\n\n39\n\n\fIs it zero?\n\nWhere does it end?\n(nope!)\n40\n\n\fYikes….\n\nWhat can possible come\nafter zero?\n(How much is 0-1?)\n\n41\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":166,"segment": "unlabeled", "course": "cs0007", "lec": "lec20", "text":"CS 0007: Introduction to Java\nLecture 20\nNathan Ong\nUniversity of Pittsburgh\nNovember 22, 2016\n\n\fLong Story Short\n• Scanner has a buffering quirk. When\nscanning for a token (an item like\nboolean, int) that is not a line, the\nscanner reads up to the new line\ncharacter, but does not consume it.\n• When you call nextLine(), it sees the\nnew line, assumes it is done reading,\nand returns an empty String.\n\n\fA Reminder\n• The act of receiving input is called\nreading\n• Relevant classes that do reading are\nReaders\n\n\fA Re-examination\n• System.in\n• An InputStream\n• If we examine the API, InputStreams\ncan only read bytes. Not very helpful.\n• What could we try?\n\n\fInputStreamReader\n• Now we can read characters, but having\nto combine all the characters is difficult\n• Why don’t we use the suggestion from\nthe API?\n\n\fBufferedReader\n• This thankfully allows us to read full\nlines of input\n• The input needs to be processed\ndepending on your needs\n• Let us first learn how to use\nBufferedReader correctly\n\n\fExceptions\n• An exception is an event meant to disrupt the\nflow of execution\n• To throw an exception is to see an exception\nbeing raised\n• To catch an exception is to acknowledge a\nthrown exception\n• You probably have already encountered some\nwhile debugging your code\n• We will focus on two of three types, which are\nmore common\n\n\fException Types\n• Checked\n\n– Exceptions that need to have a contingency plan\nshould the exception arise\n– These stop the compiler from running\n– Example: IOException\n\n• Unchecked\n\n– Exceptions that tend to indicate the program has\na flaw during its execution\n– These stop the program in the middle of\nexecution\n– Example: ArrayIndexOutOfBoundsException\n\n\fDealing with Exceptions\n• Checked\n– try, catch, finally\n\n• Unchecked\n– Fix your program code\n\n\fKeywords in Context\n• Everything inside the try block\nindicates the section of code that may\nthrow exceptions\n• Inside the catch’s parentheses is the\nexpected exception to process, and the\nname given to it (usually ‘e’)\n• Inside the catch block indicates the\nsection of code that should be executed\nif the exception is caught\n\n\fSome FAQ\n• How do I know when an exception is thrown?\n– Check the API for relevant methods\n\n• How do I know which exception is thrown?\n– Check the API\n\n• How much code should I surround with try?\n– Only as much as you need\n– Note that scope applies here as well\n\n• Can I catch multiple exceptions?\n– Yes, you can use several catch blocks\n– We’ll see an example soon\n\n\fExample\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.IOException;\n\npublic class BRExample\n{\npublic static void main(String[] args)\n{\nString line = \"\";\nSystem.out.println(\"Type something!\");\ntry\n{\nBufferedReader reader = new\nBufferedReader(new\nInputStreamReader(System.in));\nline = reader.readLine();\nreader.close();\n}\ncatch (IOException e)\n{\ne.printStackTrace();\n}\nSystem.out.println(line + \" is what you wrote.\");\n}\/\/end method main\n}\/\/End class BRExample\n\n\fConverting Strings into Useful\nPrimitives\n• Scanner gave us useful methods like\nnextInt() or nextDouble(). We can\ndo the same with BufferedReader, but\nnot directly.\n• We need to take another trip to the API,\nspecifically for the classes that back the\nprimitives.\n• Let’s look at Integer\n\n\fExample\npublic class Converter\n{\npublic static int convertToInt(String line)\n{\ntry\n{\nreturn Integer.parseInt(line);\n}\ncatch (NumberFormatException e)\n{\n\/\/??\n}\n}\/\/end method main\n}\/\/End class Converter\n\n\fExample\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.IOException;\n\npublic class BRExample2\n{\npublic static void main(String[] args)\n{\nint inputNum = -1;\nSystem.out.println(\"Type an integer!\");\ntry\n{\nBufferedReader reader = new\nBufferedReader(new\nInputStreamReader(System.in));\nString line = reader.readLine();\ninputNum = Converter.convertToInt(line);\nreader.close();\n}\ncatch (IOException e)\n{\ne.printStackTrace();\n}\nSystem.out.println(inputNum + \" + 1 = \" + (inputNum+1));\n}\/\/end method main\n}\/\/End class BRExample2\n\n\fA Note about Integer\n• Integer is a wrapper class around the\nprimitive int\n• All primitives have wrapper classes\n• Consult the API\n\n\fFile Input\/Output\n• Files are the biggest way we interact\nwith computer data.\n• Your Word documents, your game save\nfiles, your Java source code, all of them\nare stored as files.\n• We want to read them too!\n• Let’s check BufferedReader’s API for\nany clues.\n\n\fFile Input\/Output\n• Files are the biggest way we interact\nwith computer data.\n• Your Word documents, your game save\nfiles, your Java source code, all of them\nare stored as files.\n• We want to read them too!\n• Let’s check BufferedReader’s API for\nany clues.\n\n\fimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.IOException;\nimport java.io.FileNotFoundException;\n\npublic class BRWithFiles\n{\npublic static void main(String[] args)\n{\nint inputNum = -1;\nSystem.out.println(\"Type an integer!\");\ntry\n{\nBufferedReader reader = new BufferedReader(new\nFileReader(\"test.txt\"));\nString line = reader.readLine();\ninputNum = Converter.convertToInt(line);\nreader.close();\n}\ncatch (FileNotFoundException e)\n{\nSystem.err.println(\"The file was not found\");\n}\ncatch (IOException e)\n{\ne.printStackTrace();\n}\nSystem.out.println(\"The number in the file was: \" + inputNum);\n}\/\/end method main\n}\/\/End class BRWithFiles\n\n\fWriters and Writing\n• The act of providing output is called\nwriting\n• Relevant classes that do writing are\nWriters\n• How do I write to a File?\n\n\fimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.IOException;\npublic class InputAndOutput\n{\n…\npublic static String getInput(String prompt)\n{\nSystem.out.println(prompt);\nString input = null;\ntry\n{\nBufferedReader reader = new BufferedReader(new\nInputStreamReader(System.in));\ninput = reader.readLine();\nreader.close();\n}\ncatch (IOException e)\n{\ne.printStackTrace();\n}\nreturn input;\n}\/\/end method getInput(String)\n}\/\/End class InputAndOutput\n\n\fimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.IOException;\npublic class InputAndOutput\n{\n…\npublic static void writeOutput(String lines, String filename)\n{\ntry\n{\nFileWriter writer = new FileWriter(filename);\nwriter.write(lines);\nwriter.flush();\nwriter.close();\n}\ncatch (IOException e)\n{\ne.printStackTrace();\n}\n}\/\/end method writeOutput(String,String)\n}\/\/End class InputAndOutput\n\n\fExample\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.IOException;\npublic class InputAndOutput\n{\n…\npublic static void main(String[] args)\n{\nString filename = getInput(\"What\nfile would you like to\ncreate?\");\nString toStore = getInput(\"What\nwould you like to store in\nthe file?\");\nwriteOutput(filename,toStore);\n}\/\/end method main\n}\/\/End class InputAndOutput\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":167,"segment": "unlabeled", "course": "cs0007", "lec": "lec05", "text":"CS 0007: Introduction to Java\nLecture 5\nNathan Ong\nUniversity of Pittsburgh\nSeptember 15, 2016\n\n\fand the Application Programming Interface (API)\n\nSTRINGS\n\n\fStrings\n• A length of characters encased in\nquotation marks.\nString name = \"Nathan\";\n\n\fApplication Programming\nInterface\n• The Java “dictionary”\n• http:\/\/docs.oracle.com\/javase\/8\/docs\/\napi\/\n\n\fWhat Can I Do with It?\n• Look up relevant classes (e.g. String) for\ntheir relevant functions\/methods.\n\n\fAn Example\n• What is the 4th character of \"Nathan\"\nand how do I get it?\n• Check the API\n\n\fCode Example\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan\";\nSystem.out.println(name.charAt(4));\n}\/\/end method main\n}\/\/end class Name\n\nThere’s one slight issue\n\n\fCounting in Java\nThe first character in the series is at the\n0th index\n\n\fCode Example\npublic class Name\n{\npublic static void main(String[] args)\n{\nString name = \"Nathan\";\nSystem.out.println(name.charAt(3));\n}\/\/end method main\n}\/\/end class Name\n\nThe 4th character is at the 3rd index\n\n\fCode Example\n\n\fAnother Example\n• How do I get \"urge\" from \"hamburger\"?\n• (It’s a substring)\n• Check the API\n\n\fRe-Counting in Java\nThe first character in the substring is at\nthe 4th index\n\n\fRe-Counting in Java\nThe last character in the substring is at\nthe 7th index\n\n\fAnother Example\npublic class DriveThru\n{\npublic static void main(String[] args)\n{\nString fastFood = \"hamburger\";\nSystem.out.println(fastFood.substring(4,7));\n}\/\/end method main\n}\/\/end class DriveThru\n\nThere’s one slight issue\n“The substring begins at the specified beginIndex and\nextends to the character at index endIndex – 1”\nWhere can I find this information?\n\n\fRe-Counting in Java\nThe last character in the substring is at\nthe 7th index  endIndex – 1 = 7\n\n\fAnother Example\npublic class DriveThru\n{\npublic static void main(String[] args)\n{\nString fastFood = \"hamburger\";\nSystem.out.println(fastFood.substring(4,8));\n}\/\/end method main\n}\/\/end class DriveThru\n\n.\n\n\fAnother Example\n\n\fYou must learn to embrace the\nAPI\n\n\fMath Class\n\n\fRemember from before\n• The caret (^) does NOT indicate\nexponentiation\n• In other words, in Java, 2^3 does not\nequal 8\n• There is another way to do\nexponentiation; maybe it is in the Math\nclass?\n\n\fMath Example\npublic class MorePower\n{\npublic static void main(String[] args)\n{\ndouble base = 2.0;\ndouble exponent = 3.0;\nSystem.out.println(Math.pow(base,exponent));\n}\/\/end method main\n}\/\/end class MorePower\n\n.\n\n\fMath Example\n\n\fWhen to Use the Variable\/Class\nName\n• Does the “Modifier and Type” column of\nthe method say “static”?\n– Yes: Use Class name (typically uppercase\nfirst letter)\n– No: Use variable name (typically lowercase\nfirst letter)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":168,"segment": "self_training_1", "course": "cs0447", "lec": "lec14", "text":"#14\nCS 0447\nIntroduction to\nComputer Programming\n\nThe single-cycle CPU\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n● OMETs don’t forget them!\n\n2\n\n\fThe single-cycle machine\n● we’ve been talking about a single-cycle machine and building one for project 3\n● this means each instruction takes one clock cycle to execute\n\nadd t0, t1, t2\n\nsb t0, 4(s0)\n\nstore sum in t0\ndo the addition...\n\ncalculate the\naddress...\n\nstore value\nin memory\n\nIn this example each instruction ends on the rising edge of the clock\n\nsince that's when the registers store their values\n\n3\n\n\fA global view\n0\nM\nu\nx\nALU\nAdd result\n\n+\n\n4\n\n4\n\nimmediate\n(jump target) PCSrc\n\nPC\n\n31-26\n\nAdd\n\n25-21\n\nPC\nInstruction [31 26]\n\nInstruction 32\nMemory\n\nRead\naddress\n\nControl\n\n15-11\n10-6\n\nInstruction [25 21]\n\n5-0\n\nInstruction [20 16]\n\n15-0\n\nInstruction\n[31– 0]\nInstruction\nmemory\n\n20-16\n\n25-0\nM\n0\n\nInstruction [15 11]\n\nu\nx\n1\n\nopcode\n\nShift\nleft 2\n\nRegDst\nBranch\nrs\nMemRead\nMemtoReg\n\nPCSrc\n\nrt\n\nALUOp\nMemWrite\nrd\nALUSrc\nRegWrite\n\nData\n\nRegWrite\n\nshamt\n\nMemWrite\n\nData\nMemory\n\nRead\nfunct1\nregister\n\nRead\ndata 1\nRead\nimmediate\nregister\n2\nRegisters Read\nWrite\ndata 2\ntarget\nregister\n\nALU ALU\nRegister\n0\nresult\nM File\n\nWrite\ndata\n\nu\nx\n1\n\n\\\nInstruction [15 0]\n\n1\n\n16\n\nAddress\nZero\n\nALU\n\nWrite\ndata\n\nRegDataSrc\n32\n\nSign\nextend\n\nALU field\nimm\ncontrol\n\nRead\ndata\n\nAddress\n\nALUSrc\n\nData\nmemory\n\n1\nM\nu\nx\n0\n\nALUOp\n\nInstruction [5 0]\n\n4\n\n\fA global view\nHow big was an\ninstruction in MIPS?\n\n0\nM\nu\nx\nALU\nAdd result\n\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nPC-in\ninstruction-out\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nBit Fields being\ndecoded\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n5\n\n\fA global view\n0\n\nThe brain!\n\nM\nu\nx\nALU\nAdd result\n\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nWhy is this?\n\nZero\nALU ALU\nresult\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nInstruction [5 0]\n\nSign\nextend\n\nRead\ndata\n\nAddress\n\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nSeparate ALU\ncontrol, why?\n6\n\n\fA global view\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nImmediates are\nInstruction [5 0]\nsigned\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\n7\n\n\fA global view\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\n\nRecognise this?\n\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n8\n\n\fA global view\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nHandling those\nbranches!\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n9\n\n\fExecuting an r-type instruction\n\n10\n\n\fExecuting an r-type\n0\n\nThe instruction is read\nand PC is incremented\n\nM\nu\nx\nALU\nAdd result\n\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n11\n\n\fExecuting an r-type\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nTwo registers (rs, rt) are read, and the\ncontrol unit determines the signals\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n12\n\n\fExecuting an r-type\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nThe ALU performs the operation\naccording to the lower 6-bits of the\ninstruction (func)\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n13\n\n\fExecuting an r-type\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nThe result is written back to the\nregister file (rd)\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n14\n\n\fExecuting a lw instruction\n\n15\n\n\fExecuting an i-type : e.g., lw\n0\n\nThe instruction is read\nand PC is incremented\n\nM\nu\nx\nALU\nAdd result\n\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n16\n\n\fExecuting an i-type : e.g., lw\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nOne register (rs) is read, and the\ncontrol unit determines the signals\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n17\n\n\fExecuting an i-type : e.g., lw\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nThe ALU adds the register value with\nthe immediate (offset) extended to\n32-bits.\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n18\n\n\fExecuting an i-type : e.g., lw\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nThe ALU result is the address we want\nto read. The memory is read in that\naddress.\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n19\n\n\fExecuting an i-type : e.g., lw\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nThe memory data is stored in the\nregister (rt)\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n20\n\n\fExecuting a beq instruction\n\n21\n\n\fExecuting an i-type : e.g., beq\n0\n\nThe instruction is read\nand PC is incremented\n\nM\nu\nx\nALU\nAdd result\n\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n22\n\n\fExecuting an i-type : e.g., beq\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nTwo registers (rs, rt) are read, and the\ncontrol unit determines the signals. The\nimmediate is shifted and added to PC+4\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n23\n\n\fExecuting an i-type : e.g., beq\n0\nM\nu\nx\nALU\nAdd result\nAdd\n\nInstruction [31 26]\n\nControl\n\nInstruction [25 21]\nPC\n\nRead\naddress\n\nInstruction\nmemory\n\nInstruction [15 11]\n\nPCSrc\n\nALUOp\nMemWrite\nALUSrc\nRegWrite\nRead\nregister 1\n\nInstruction [20 16]\nInstruction\n[31– 0]\n\nShift\nleft 2\n\nRegDst\nBranch\nMemRead\nMemtoReg\n\n4\n\n1\n\n0\nM\nu\nx\n1\n\nRead\ndata 1\nRead\nregister 2\nRegisters Read\nWrite\ndata 2\nregister\n\n0\nM\nu\nx\n1\n\nWrite\ndata\n\nZero\nALU ALU\nresult\n\nAddress\n\nWrite\ndata\nInstruction [15 0]\n\nThe ALU subtracts the contents of\nboth registers. Depending on the\nresult, the value of PC is set.\n\n16\n\nSign\nextend\n\nRead\ndata\nData\nmemory\n\n1\nM\nu\nx\n0\n\n32\nALU\ncontrol\n\nInstruction [5 0]\n\n24\n\n\fExecuting a jump instruction\n\n25\n\n\fExtending the CPU\n\n26\n\n\fExecuting an j-type\n\nThe instruction is read and PC is\nincremented\n27\n\n\fExecuting an j-type\n\nThe new PC value is obtained by shifting\nthe target field of the instruction left by 2.\n28\n\n\fReal-world clocking issues\n\n29\n\n\fRemember the Critical Path?\n● the critical path is the path through a circuit that requires the longest series of\nsequential operations\no they depend on each other and can't be done in parallel!\n\n30\n\n\fHow fast can we clock it?\n● what's the thing that limits the clock speed?\no the critical path. in our case it happens to be...\no and here is the Achilles' heel of a single-cycle datapath:\n\nMemory is\nSLOW.\nA.\nF.\n\nrd\nrs\nrt\n\nData\nData\nMemory\n\nAddress\nRegister\nFile\n\nALU\n\nimm field\n\n31\n\n\fSingle-cycle CPU\nAny instruction executes during a single clock cycle\nLength of the clock cycle must accommodate the longest instruction\n● Faster instructions waste cycle time\n\nClock cycle\n\nlw t0,0(t1)\n\nadd t2,t2,t3\n\n0\n\nMem\n\nReg\n\n1\n\nMem\n\nReg\n\nMem\n\nReg\n\nReg\n\n32\n\n\fIt's bad.\n● typical access times for modern DDR4 RAM is 12-15 ns\no that's our single-cycle CPU's critical path time\n● the inverse of that is... 66-83 MHz\no YEESH\n● the lw and sw instructions are holding us back\n● all the other instructions are gonna be WAY faster\n\nadd\nor\n\nbeq\nlw\/sw\n\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":169,"segment": "unlabeled", "course": "cs0441", "lec": "lec22", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #22: Bayes’ Theorem\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nn Bayes’ Theorem\nl What do we compute conditional probabilities with\nincomplete information?\n\n\fConditional Probability\nDefinition: Let E and F be events with p(F) > 0. The conditional\nprobability of E given F, denoted p(E | F), is defined as:\n\n𝑝 𝐸∩𝐹\n𝑝 𝐸 𝐹 =\n𝑝 𝐹\n\nIntuition:\nl Think of the event F as reducing the sample space that can be considered\nl The numerator looks at the likelihood of the outcomes in E that overlap\nthose in F\nl The denominator accounts for the reduction in sample size indicated by our\nprior knowledge that F has occurred\n\n\fBayes’ Theorem\nBayes’ Theorem allows us to relate the conditional and marginal\nprobabilities of two random events.\n\n?\nIn English: Bayes’ Theorem will help us assess the probability\nthat an event occurred given only partial evidence.\nDoesn’t our formula for conditional probability do this already?\n\nWe can’t always use this\nformula directly…\n\n\fA Motivating Example\nSuppose that a certain drug test correctly identifies a person who\nuse the drug as testing positive 99% of the time, and will correctly\nidentify a non-user as testing negative 99% of the time. If a\ncompany suspects that 0.5% of its employees are users of the\ndrug, what is the probability that an employee that tests positive\nfor this drug is actually a user?\n\nQuestion: Can we use our simple\nconditional probability formula?\n𝑝 𝐸𝐹 =\n\nX is a user\n\n!(#∩%)\n!(%)\n\nX tested positive\n\n\fThe 1,000 foot view…\nIn situations like those on the last slide, Bayes’ theorem can help!\nEssentially, Bayes’ theorem will allow us to calculate P(E|F)\nassuming that we know (or can derive):\nl 𝑃 𝐸\nl 𝑃 𝐹𝐸\n&\nl 𝑃(𝐹|𝐸)\n\nProbability that X is a user\nTest success rate\nTest false positive rate\n\nProbability that X is a user of the\ndrug, given a positive test\n\nReturning to our earlier example:\nl Let E = “Person X is a user of the drug”\nl Let F = “Person X tested positive for the drug”\n\nIt sounds like Bayes’ Theorem could help in this case…\n\n\fNew Notation\nTo simplify expressions, we will use the notation EC to\ndenote the complementary event of E\nThat is:\n\nC\nE=E\n\n\fA Simple Example\nWe have two boxes. The first contains two green balls and seven\nred balls. The second contains four green balls and three red\nballs. Bob selects a ball by first choosing a box at random. He\nthen selects one of the balls from that box at random. If Bob has\nselected a red ball, what is the probability that he took it from\nthe first box?\n\n1\n\n2\n\n\fPicking the problem apart…\nFirst, let’s define a few events relevant to this problem:\nl Let E = Bob has chosen a red ball\nl By definition EC = Bob has chosen a green ball\nl Let F = Bob chose his ball from this first box\nl Therefore, FC = Bob chose his ball from the second box\n\nWe want to find the probability that Bob chose from the first box,\ngiven that he picked a red ball. That is, we want p(F|E).\nGoal: Given that p(F|E) = p(F ∩ E)\/p(E), use what we know to\nderive p(F ∩ E) and p(E).\n\n\fWhat do we know?\nWe have two boxes. The first contains two green balls and seven red balls.\nThe second contains four green balls and three red balls. Bob selects a ball\nby first choosing a box at random. He then selects one of the balls from that\nbox at random. If Bob has selected a red ball, what is the probability that he\ntook it from the first box?\nStatement: Bob selects a ball by first choosing a box at random\nl Bob is equally likely to choose the first box, or the second box\nl p(F) = p(FC) = 1\/2\n\nStatement: The first contains two green balls and seven red balls\nl The first box has nine balls, seven of which are red\nl p(E|F) = 7\/9\n\nStatement: The second contains four green balls and three red balls\nl The second box contains seven balls, three of which are red\nl p(E|FC) = 3\/7\n\n\fNow, for a little algebra…\nThe end goal: Compute p(F|E) = p(F ∩ E)\/p(E)\nNote that p(E|F) = p(E ∩ F)\/p(F)\nl If we multiply by p(F), we get p(E ∩ F) = p(E|F) p(F)\nl Further, we know that p(E|F) = 7\/9 and p(F) = 1\/2\nl So p(E ∩ F) = 7\/9 × 1\/2 = 7\/18\n\nRecall:\n•p(F) = p(FC) = 1\/2\n•p(E|F) = 7\/9\n•p(E|FC) = 3\/7\n\nSimilarly, p(E ∩ FC) = p(E|FC) p(FC) = 3\/7 × 1\/2 = 3\/14\nObservation: E = (E ∩ F) ∪ (E ∩ FC)\nl This means that p(E) = p(E ∩ F) + p(E ∩ FC)\nl\nl\nl\nl\n\n= 7\/18 + 3\/14\n= 49\/126 + 27\/126\n= 76\/126\n= 38\/63\n\n\fDenouement\nThe end goal: Compute p(F|E) = p(F ∩ E)\/p(E)\nSo, p(F|E) = (7\/18) \/ (38\/63) ≈ 0.645\n\nRecall:\n•p(F) = p(FC) = 1\/2\n•p(E|F) = 7\/9\n•p(E|FC) = 3\/7\n•p(E ∩ F) = 7\/18\n•p(E) = 38\/63\n\nHow did we get here?\n1.\n2.\n\nExtract what we could from the problem definition itself\nRearrange terms to derive p(F ∩ E) and p(E)\n\n3.\n\nUse our trusty definition of conditional probability to do the rest!\n\n\fThe reasoning that we used in the last problem\nessentially derives Bayes’ Theorem for us!\nBayes’ Theorem: Suppose that E and F are events from some\nsample space S such that p(E) ≠ 0 and p(F) ≠ 0. Then:\n𝑝 𝐸 𝐹 𝑝 𝐹\n𝑝 𝐹 𝐸 =\n𝑃 𝐸 𝐹 𝑝 𝐹 + 𝑝 𝐸 𝐹' 𝑝 𝐹'\n\nProof:\nl The definition of conditional probability says that\n➣ p(F|E) = p(F ∩ E)\/p(E)\n➣ p(E|F) = p(E ∩ F)\/p(F)\n\nl This means that\n➣ p(E ∩ F) = p(F|E)p(E)\n➣ p(E ∩ F) = p(E|F)p(F)\n\nl So p(F|E)p(E) = p(E|F)p(F)\nl Therefore, p(F|E) = p(E|F)p(F)\/p(E)\n\n\fProof (continued)\nNote: To finish, we must prove p(E) = p(E | F)p(F) + p(E | FC)p(FC)\nl Observe that E = E ∩ S\nl\n= E ∩ (F ∪ FC)\nl\n= (E ∩ F) ∪ (E ∩ FC)\nl Note also that (E ∩ F) and (E ∩ FC) are disjoint (i.e., no x can be in both F and FC)\nl This means that p(E) = p(E ∩ F) + p(E ∩ FC)\nl We already have shown that p(E ∩ F) = p(E|F)p(F)\nl Further, since p(E | FC) = p(E ∩ FC)\/p(FC), we have that p(E ∩ FC) = p(E|FC)p(FC)\nl So p(E) = p(E ∩ F) + p(E ∩ FC) = p(E|F)p(F) + p(E|FC)p(FC)\n\nPutting everything together, we get:\n𝑝 𝐸 𝐹 𝑝 𝐹\n𝑝 𝐹 𝐸 =\n𝑃 𝐸 𝐹 𝑝 𝐹 + 𝑝 𝐸 𝐹' 𝑝 𝐹'\n❏\n\n\fAnd why is this useful?\nIn a nutshell, Bayes’ Theorem is useful if you want to find p(F|E),\nbut you don’t know p(E ∩ F) or p(E).\n\n\fHere’s a general solution tactic\nStep 1: Identify the independent events that are being\ninvestigated. For example:\nl F = Bob chooses the first box, FC = Bob chooses the second box\nl E = Bob chooses a red ball, EC = Bob chooses a green ball\n\nStep 2: Record the probabilities identified in the problem\nstatement. For example:\nl p(F) = p(FC) = 1\/2\nl p(E|F) = 7\/9\nl p(E|FC) = 3\/7\n\nStep 3: Plug into Bayes’ formula and solve\n\n\fExample: Pants and Skirts\nSuppose there is a co-ed school having 60% boys and 40% girls as\nstudents. The girl students wear pants or skirts in equal numbers;\nthe boys all wear pants. An observer sees a (random) student\nfrom a distance; all they can see is that this student is wearing\npants. What is the probability this student is a girl?\nStep 1: Set up events\nl\nl\nl\nl\n\nE = X is wearing pants\nEC = X is wearing a skirt\nF = X is a girl\nFC = X is a boy\n\nStep 2: Extract probabilities from problem definition\nl\nl\nl\nl\n\np(F) = 0.4\np(FC) = 0.6\np(E|F) = p(EC|F) = 0.5\np(E|FC) = 1\n\n\fPants and Skirts (continued)\n𝑝 𝐹 𝐸 =\n\n𝑝 𝐸 𝐹 𝑝 𝐹\n𝑃 𝐸 𝐹 𝑝 𝐹 + 𝑝 𝐸 𝐹! 𝑝 𝐹!\n\nStep 3: Plug in to Bayes’ Theorem\n\nRecall:\n• p(F) = 0.4\n• p(FC) = 0.6\n• p(E|F) = p(EC|F) = 0.5\n• p(E|FC) = 1\n\nl p(F|E) = (0.5 × 0.4)\/(0.5 × 0.4 + 1 × 0.6)\nl\n= 0.2\/0.8\nl\n= 1\/4\n\nConclusion: There is a 25% chance that the person seen was a\ngirl, given that they were wearing pants.\n\n\fDrug screening, revisited\nSuppose that a certain drug test correctly identifies a person who\nuses the drug as testing positive 99% of the time, and will\ncorrectly identify a non-user as testing negative 99% of the time.\nIf a company suspects that 0.5% of its employees are users of the\ndrug, what is the probability that an employee that tests positive\nfor this drug is actually a user?\nStep 1: Set up events\nl\nl\nl\nl\n\nF = X is a user\nFC = X is not a user\nE = X tests positive for the drug\nEC = X tests negative for the drug\n\nStep 2: Extract probabilities from problem definition\nl p(F) = 0.005\nl p(FC) = 0.995\nl p(E|F) = 0.99\nl p(E|FC) = 0.01\n\n\fDrug screening (continued)\n𝑝 𝐹 𝐸 =\n\n𝑝 𝐸 𝐹 𝑝 𝐹\n𝑃 𝐸 𝐹 𝑝 𝐹 + 𝑝 𝐸 𝐹! 𝑝 𝐹!\n\nStep 3: Plug in to Bayes’ Theorem\n\nRecall:\n• p(F) = 0.005\n• p(FC) = 0.995\n• p(E|F) = 0.99\n• p(E|FC) = 0.01\n\nl p(F|E) = (0.99 × 0.005)\/(0.99 × 0.005 + 0.01 × 0.995)\nl\n= 0.3322\n\nConclusion: If an employee tests positive for the drug, there is\nonly a 33% chance that they are actually a user!\n\n\fIn-class exercises\nSuppose that 1 person in 100,000 has a particular rare disease. A\ndiagnostic test is correct 99% of the time when given to someone\nwith the disease, and is correct 99.5% of the time when given to\nsomeone without the disease.\nProblem 1: Calculate the probability that someone who tests\npositive for the disease actually has it.\nProblem 2: Calculate the probability that someone who tests\nnegative for the disease does not have the disease.\n\n\fApplication: Spam filtering\nDefinition: Spam is unsolicited bulk email\nI didn’t ask for it, I probably\ndon’t want it\n\nSent to lots of people…\n\nIn recent years, spam has become increasingly\nproblematic. For example, in 2015, spam accounted\nfor ~50% of all email messages sent.\nTo combat this problem, people have developed spam\nfilters based on Bayes’ theorem!\n\n\fHow does a Bayesian spam filter work?\nEssentially, these filters determine the probability that a message\nis spam, given that it contains certain keywords.\n𝑝 𝐸 𝐹 𝑝 𝐹\n𝑝 𝐹 𝐸 =\n𝑃 𝐸 𝐹 𝑝 𝐹 + 𝑝 𝐸 𝐹' 𝑝 𝐹'\nMessage is spam\n\nMessage contains\nquestionable keyword\n\nIn the above equation:\nl p(E|F) = Probability that our keyword occurs in spam messages\nl p(E|FC) = Probability that our keyword occurs in legitimate messages\nl p(F) = Probability that an arbitrary message is spam\nl p(FC) = Probability that an arbitrary message is legitimate\n\nQuestion: How do we derive these parameters?\n\n\fWe can learn these parameters by examining\nhistorical email traces\nImagine that we have a corpus of email messages…\nWe can ask a few intelligent questions to learn the parameters of our\nBayesian filter:\nl How many of these messages do we consider spam?\nl In the spam messages, how often does our keyword appear?\nl In the good messages, how often does our keyword appear?\n\np(F)\np(E|F)\np(E|FC)\n\nAside: This is what happens when you click the “mark as spam” button\nin your email client!\n\nGiven this information, we can apply Bayes’ theorem!\n\n\fFiltering spam using a single keyword\nSuppose that the keyword “Rolex” occurs in 250 of 2000 known spam\nmessages, and in 5 of 1000 known good messages. Estimate the\nprobability that an incoming message containing the word “Rolex” is\nspam, assuming that it is equally likely that an incoming message is\nspam or not spam. If our threshold for classifying a message as spam\nis 0.9, will we reject this message?\nStep 1: Define events\nl F = message is spam\nl FC = message is good\nl E = message contains the keyword “Rolex”\nl EC = message does not contain the keyword “Rolex”\n\nStep 2: Gather probabilities from the problem statement\nl p(F) = p(FC)= 0.5\nl p(E|F) = 250\/2000 = 0.125\nl p(E|FC) = 5\/1000 = 0.005\n\n\fSpam Rolexes (continued)\nRecall:\n• p(F) = p(FC) = 0.5\n• p(E|F) = 0.125\n• p(E|FC) = 0.005\n\nStep 3: Plug in to Bayes’ Theorem\nl p(F|E) = (0.125 × 0.5)\/(0.125 × 0.5 + 0.005 × 0.5)\nl\n= 0.125\/(0.125 + 0.005)\nl\n≈ 0.962\n\nConclusion: Since the probability that our message is spam given\nthat it contains the string “Rolex” is approximately 0.962 >\n0.9, we will discard the message.\n\n\fProblems with this simple filter\nHow would you choose a single keyword\/phrase to use?\nl “All natural”\nl “Nigeria”\nl “Click here”\nl …\n\nUsers get upset if false positives occur, i.e., if legitimate\nmessages are incorrectly classified as spam\nl When was the last time you checked your spam folder?\n\nHow can we fix this?\nl Choose keywords so p(spam | keyword) is very high or very low\nl Filter based on multiple keywords\n\n\fSpecifically, we want to develop a Bayesian filter that tells us\np(F | E1 ∩ E2)\nFirst, some assumptions\n1. Events E1 and E2 are independent\n2. The events E1|F and E2|F are independent\n3. p(F) = p(FC) = 0.5\n\nBy Bayes’ theorem\n\nNow, let’s derive formula for this p(F | E1 ∩ E2)\n𝑝 𝐹 𝐸( ∩ 𝐸) =\n=\n=\n\n𝑝 𝐸(\n\n𝑝 𝐸( ∩ 𝐸)\n\n𝑝 𝐸( ∩ 𝐸) ∣ 𝐹 𝑝 𝐹\n𝐹 𝑝 𝐹 + 𝑝 𝐸( ∩ 𝐸) 𝐹 ' 𝑝 𝐹 '\n\n𝑝(𝐸( ∩ 𝐸) ∣ 𝐹)\n𝑝 𝐸( ∩ 𝐸) 𝐹 + 𝑝 𝐸( ∩ 𝐸) 𝐹 '\n\nAssumption 3\nAssumptions 1 and 2\n\n𝑝 𝐸( 𝐹 𝑝(𝐸) ∣ 𝐹)\n𝐹 𝑝 𝐸) 𝐹 + 𝑝 𝐸( 𝐹 ' 𝑝 𝐸) 𝐹 '\n\n\fSpam filtering on two keywords\nSuppose that we train a Bayesian spam filter on a set of 2000 spam\nmessages and 1000 messages that are not spam. The word “stock”\nappears in 400 spam messages and 60 good messages, and the word\n“undervalued” appears in 200 spam messages and 25 good messages.\nEstimate the probability that a message containing the words “stock”\nand “undervalued” is spam. Will we reject this message if our spam\nthreshold is set at 0.9?\nStep 1: Set up events\nl F = message is spam, FC = message is good\nl E1 = message contains the word “stock”\nl E2 = message contains the word “undervalued”\n\nStep 2: Identify probabilities\nl P(E1|F) = 400\/2000 = 0.2\nl p(E1|FC) = 60\/1000 = 0.06\nl p(E2|F) = 200\/2000 = 0.1\nl p(E2|FC) = 25\/1000 = 0.025\n\n\fTwo keywords (continued)\n𝑝 𝐹 𝐸! ∩ 𝐸\" =\n\n𝑝 𝐸!\n\n𝑝 𝐸! 𝐹 𝑝(𝐸\" ∣ 𝐹)\n𝐹 𝑝 𝐸\" 𝐹 + 𝑝 𝐸! 𝐹 # 𝑝 𝐸\" 𝐹 #\n\nStep 3: Plug in to Bayes’ Theorem\n\nRecall:\n• p(E1|F) = 0.2\n• p(E1|FC) = 0.06\n• p(E2|F) = 0.1\n• p(E2|FC) = 0.025\n\nl p(F|E1 ∩ E2) = (0.2 × 0.1)\/(0.2 × 0.1 + 0.06 × 0.025)\nl\n= 0.02\/(0.02 + 0.0015)\nl\n≈ 0.9302\n\nConclusion: Since the probability that our message is spam\ngiven that it contains the strings “stock” and\n“undervalued” is ≈ 0.9302 > 0.9, we will reject this\nmessage.\n\n\fIn-class exercises\nProblem 3: A business records incoming emails for 1 week and\ncollects 1,000 spam messages and 400 non-spam messages. The\nword “opportunity” appears in 175 spam messages and 20 nonspam messages. Assuming this week's emails were typical\n(including the proportion of spam), should an incoming message\nbe labeled as spam if it contains the word “opportunity” and the\nthreshold for rejecting is 0.9?\nProblem 4: Suppose that a Bayesian spam filter is trained on a\nset of 10,000 spam messages and 5,000 messages that are not\nspam. The word “enhancement” appears in 1,500 spam messages\nand 20 non-spam messages, while the word “herbal” appears in\n800 spam messages and 200 non-spam messages. Estimate the\nprobability that a received message containing both the words\n“enhancement” and “herbal” is spam. Here, you may assume that\n50% of emails are spam.\n\n\fFinal Thoughts\nn Conditional probability is very useful\nn Bayes’ theorem\nl Helps us assess conditional probabilities\nl Has a range of important applications\n\nn Next time:\nl Expected values and variance (Section 7.4)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":170,"segment": "unlabeled", "course": "cs0447", "lec": "lec16", "text":"#16\nCS 0447\nIntroduction to\nComputer Programming\n\nPipelining\n\nLuís Oliveira\n\nFall 2020\n\n\fAnnouncements\n● OMETS!!!!!!!!11!!!!!!!!!1!!!1eleven!!!!\n\n2\n\n\fHow can we make the CPU\nmore efficient? …\n\nP\n\n3\n\n\fDoing the laundry\nLuis (me), Artur, Stephen, and Ray have one load of clothes to\no Wash\nWasher takes 60 minutes\n\no Dry\no Fold\n\nDryer takes 60 minutes\n\nFolding takes 60 minutes\n\nWhere we live, we can only do laundry Saturday from 9:00 to 18:00!\n4\n\n\fSequential laundry\n● We have four loads of laundry to do (Luis, Artur, Stephen, and Ray)\n\nFirst, I wash\n\n9:00\n\n10:00 11:00 12:00 13:00 14:00 15:00 16:00 17:00\n\nThen, I dry\n\nFinally, I fold\n\nIt took me 3:00, we\nstill have three\nloads remaining!\nYikes!\n\nIt’s 15:00, we still\nhave two loads to go\n\nIt’s 18:00, and Ray\ncannot do his laundry!\n5\n\n\fHow can we solve this?\n● Buy more machines!!!\n\n● Or…\n6\n\n\fPipelined laundry\n● We have four loads of laundry to do (Luis, Artur, Stephen, and Ray)\n9:00\n\n10:00 11:00 12:00 13:00 14:00 15:00 16:00 17:00\n\nBut did the time it\ntakes to wash the\nclothes change?\n\nThe washer is now\nfree!!\n\nWe can start the\nnext load!\n\nEveryone can do\ntheir laundry\n\nRinse and\nrepeat\n7\n\n\fUpgrading the multi-cycle CPU\nLet’s apply the same concept to a multi-cycle CPU!\nKeep the same clock\n● Reuse resources with …\n\nTime\n\n0\n\n1\n\nadd t2,t2,t3\n\nMem\n\nReg\n\nlw t0,0(t1)\n\n2\n\n3\n\n4\n\n5\n\nMem\n\nReg\n\n6\n\n7\n\nReg\n\nMem\n\nReg\n\n8\n\n\fLighting up the silicon (animated)\nExecuting instructions in a pipeline\nF\nD\n\nX\n\nM\n\nMemory\n\nControl\n\nsub\nadd\nsw\nRegister\nFile\n\nMemory\nagain\n\nW\nUses\nmemory\n\nUses decoder\/registers\n\nUses ALU\n\nUses\nmemory\n9\n\n\fPipeline vs Multi-cycle\nPipelining doesn’t help latency of a single instruction!\nIt helps throughput of the entire workload!\nDifferent tasks operating different resources\ncan execute simultaneously\nMore stages, more potential speedup (too many stages is not good!)\nTime\n\n0\n\n1\n\nadd t2,t2,t3\n\nMem\n\nReg\n\nlw t0,0(t1)\n\nMem\n\n2\n\n3\n\n4\n\n5\n\nMem\n\nReg\n\n6\n\n7\n\nReg\n\nReg\n\n10\n\n\fSo how did we improve performance?\n● Did we make any individual instruction faster?\no No, the add still took 4 cycles… the lw took 5 cycles\n● But the whole thing finished faster. right?\nYes, by overlapping the instructions,\nwe increased the throughput.\nIn any given clock cycle, we're\nnow doing more work.\nWith this we can get the CPI\ndown closer to 1.\n11\n\n\fThe average CPI\n● It’s the average number of Cycles Per Instruction\no For any program, we count the # of cycles\n▪ and divide by the # of instructions\nTime\n\n0\n\n1\n\nadd t0,t1,t2\n\nMem\n\nReg\n\nadd t3,t4,t5\n\nMem\n\n2\n\n3\n\nReg\n\n5\n\n6\n\n7\n\nCPI = 7 ÷ 4\n= 1.75\n\nReg\n\nReg\n\nMem\n\nReg\n\nWhat\nhappens when we have an\nadd s3,s4,s5\ninfinite number of instructions?\n\nMem\n\nadd s0,s1,s2\n\n4\n\nReg\n\nReg\n\nReg\n\n12\n\n\fPipeline (real-world) issues\n\n13\n\n\fInstructions are co-dependent \n● Sometimes, the next instruction cannot execute in the next cycle\no We call those pipeline hazards.\n● Hazards happen when for any reason an instruction is unable to advance\n(execute) in the pipeline\n● We’ll look at three types of hazards\no Structural hazards\no Data hazards\no Control hazards\n\n14\n\n\fStructural hazards\nAttempting to use the same resource two different ways simultaneously. E.g.:\n● You get home soaking wet and need to dry your clothes while someone is\nusing the dryer\nIn a CPU with a single memory:\n● Can we fetch a new instruction while reading a word from memory?\no NOPe: structural hazard\n\n15\n\n\fStructural hazards\n● Two instructions using the same hardware at the same time\nTime\n\n0\n\n1\n\nlw t0,0($0)\n\nMem\n\nReg\n\nlw t1,4($0)\n\nlw t2,8($0)\n\nlw t3,12($0)\n\nMem\n\n2\n\n3\n\n4\n\nMem\n\nReg\n\nMem\n\nReg\n\nMem\n\nReg\n\n6\n\n7\n\nReg\n\nMem\n\nReg\n\nMem\n\n5\n\nReg\n\nMem\n\nReg\n\n16\n\n\fStructural hazards – What can I do???\n● Structural hazards arise from lack of resources\n● So… We can eliminate the hazard by adding more resources!\no Add a second memory?\n▪ The Harvard architecture!\n● Another solution:\no Stall the instruction until the resource is available\nTime\n\nlw t3,12($0)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nMem\n\nReg\n\nMem\n\nReg\n17\n\n\fStructural hazards – What can I do???\n● You may need more than one stall!\nTime\n\n0\n\n1\n\nlw t0,0($0)\n\nMem\n\nReg\n\nlw t1,4($0)\n\nlw t2,8($0)\n\nlw t3,12($0)\n\nMem\n\n2\n\n3\n\n4\n\nMem\n\nReg\n\nMem\n\nReg\n\nMem\n\nReg\n\n5\n\n6\n\n7\n\nReg\n\nMem\n\nReg\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nNOP\nNOP\n\nNOP\nNOP\n\nNOP\n\nNOP\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nNOP\n\nMem\n\nReg\n\nMem\n\nReg\n\n18\n\n\fData hazards\nAttempting to use an item before it is ready. E.g.:\n● Only one sock of a pair is found during folding\n● It’s in the dryer! Folding has to wait!\n\nIn a CPU:\n● Instruction depends on result of prior instruction still in the pipeline\nadd s0, t0, t1\nsub t2, t2, s0\n\ns0 must be produced before it can\nbe used\n\n19\n\n\fData hazards – What can I do???\n● Are these common?\no Yup! You bet!\ni=i+1\narray[i]\n\n● Solution 1: Stall until value is written back to the register file\no Penalty is high with this solution.\nTime\n\n0\n\n1\n\nadd s0,t0,t1\n\nMem\n\nReg\n\nsub t2,t2,s0\n\nNOP\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nReg\n\nNOP\n\nMem\n\nReg\n\nReg\n20\n\n\fData hazards – What can I do???\n● Solution 2: What if we improve the register file?\n\nWrite to the register on\nthe falling edge\n\nTime\n\n0\n\n1\n\nadd s0,t0,t1\n\nMem\n\nReg\n\nsub t2,t2,s0\n\nNOP\n\n2\n\nRead register during\nthe second half\n\nRegister\nFile\n\n3\n\n4\n\n5\n\n6\n\n7\n\nW\nReg\n\nMem\n\nRegR\n\nReg\n21\n\n\fData hazards – What can I do???\n● Solution 3: Can we forward the ALU output?\no Add path from ALU output to one of its inputs\nForwarding: Passing the result from a\nlater stage to an earlier one\n\nTime\n\n0\n\n1\n\nadd s0,t0,t1\n\nMem\n\nReg\n\nsub t2,t2,s0\n\nMem\n\n2\n\n3\n\n4\n\nReg\n\nReg\n\nReg\n\n5\n\n6\n\n7\n\nThe value needed by the\nsub isn’t read from the reg\nfile - it comes directly from\nthe result output from\ndoing the add operation\n22\n\n\fControl hazards\nAttempting to make a decision before condition is evaluated\n● If the dirty clothes are not clean after washing!\n● Then I must wash them again\nIn a CPU:\n● Branches\nblt s0, s1, DONE\nadd t0, t1, t2\nor t0, t1, t2\nDONE:\nsub t0, t1, t2\n\nWhich path will the\nprogram take?\nWhich instruction do\nwe fetch next?\n\n23\n\n\fControl hazards\nAttempting to make a decision before condition is evaluated\n● If the dirty clothes are not clean after washing!\n● Then I must wash them again\nIn a CPU:\n● Branches\nif s0 < s1 goto DONE\nadd t0, t1, t2\nor t0, t1, t2\nDONE:\nsub t0, t1, t2\n\nWhich path will the\nprogram take?\nWhich instruction do\nwe fetch next?\n\n24\n\n\fControl hazards – What can I do???\n● We can stall… until the outcome is known!\nTime\n\n0\n\n1\n\nblt s0,s1,DONE\n\nMem\n\nReg\n\nsub t0,t1,t2\n\nNOP\n\n2\n\n3\n\n4\n\nNOP\n\nMem\n\nReg\n\n5\n\n6\n\n7\n\nReg\n\n● This is a bit wasteful! We really don’t like stalls! ☺\n● We want the pipeline always full and doing useful work!\n25\n\n\fControl hazards – What can I do???\n● Sooo… we can predict that the branch is never taken! (naïve)\nTime\n\n0\n\n1\n\nblt s0,s1,DONE\n\nMem\n\nReg\n\nadd t0,t1,t2\n\nMem\n\n2\n\nReg\n\n3\n\n4\n\n5\n\n6\n\n7\n\nReg\n\n● We attempt to execute the next sequential instruction!\n● It is a gamble! that the branch will never be taken.\n● But if we are right, there is no stall!!! ☺\n26\n\n\fControl hazards – What if we are wrong?????!!!\n● Ok, what if we are wrong???!!\nTime\n\n0\n\n1\n\nblt s0,s1,DONE\n\nMem\n\nReg\n\nadd t0,t1,t2\n\nor t0,t1,t2\n\nsub t0,t1,t2\n\nMem\n\n2\n\n3\n\n4\n\nReg\n\nNOP\n\nNOP\n\nMem\n\nNOP\n\nNOP\n\nMem\n\nReg\n\n5\n\n6\n\n7\n\nNOP\n\nReg\n\n● Just abort (stall the remaining steps) to fix it! Nothing was actually changed!\n● Read the correct instruction!\n\n27\n\n\fFun facts!\n● How often do you think a (less-naïve) branch predictor is correct?\nUsing 128 Bytes all these predictors\nhave an accuracy of >90%!!!\n\nMcFarling, Scott. Combining branch predictors. Vol. 49. Technical\nReport TN-36, Digital Western Research Laboratory, 1993.\n\n28\n\n\fWhat to know more?\n● CS 1541 – Introduction to Computer Architecture\no Learn more about hazards.\no Learn more about branch predictors.\no Learn about memory hierarchies.\no And more…\n\n29\n\n\fPerformance and\nThe Law of Diminishing Returns\n\n30\n\n\fDon't waste your time...\n● suppose you're trying to get better at time management\n● you got an app that lets you time how long you do stuff\nif you wanted to get\nmore free time by\nhalving the amount of\ntime it takes to do one\ntask, which task would\nyou choose?\n\nCommuting\n\nHygiene\n\nWatching\nYoutube\n\nMeals\n\nWorking\n31\n\n\fIf you cut Youtube by half...\n● look at all the extra free time you have!\nHygiene\nCommuting\n\nFree time!\n\nMeals\n\nWatching\nYoutube\n\nWorking\n\n32\n\n\fIf you cut commuting by half...\n● look at ... all the extra free time... you have.\nFree time!\nHygiene\nCommuting\n\nWatching\nYoutube\n\nMeals\n\nWorking\n\n33\n\n\fThe tale of two multipliers\n● The tale starts with a simple program\nli\n\n$1, 100\n\nlw\nlw\nmult\nmflo\nsw\naddi\nbne\n\n$2, A[i]\n$3, B[i]\n$3, $2\n$4\n$4, C[i]\n$1, $1, -1\n$1, $0, L0\n\nL0:\n; pseudo-code to load A[i]\n; pseudo-code to load B[i]\n\n; pseudo-code to store C[i]\n\n● How many times does the loop execute?\n\nRuns 100 times\n\n34\n\n\fThe tale of two multipliers\n● You measure how long it takes to execute.\no It took 102010ns\nli\n\n$1, 100\n\n10ns\n\nL0:\nlw\nlw\nmult\nmflo\nsw\naddi\nbne\n\nThat’s too long!\n\n$2, A[i]\n$3, B[i]\n$3, $2\n$4\n$4, C[i]\n$1, $1, -1\n$1, $0, L0\n\n10ns\n10ns\n960ns\n10ns\n10ns\n10ns\n10ns\n\nI need to improve this,\nwhat should I do?\n\nLet me check what is\ngoing on here!\n\n96000ns!!!\n𝑡𝑖𝑚𝑒 = 1 × 10𝑛𝑠 +\n6 × 100 × 10𝑛𝑠 +\n1 × 100 × 960𝑛𝑠 = 102010𝑛𝑠\n35\n\n\fThe tale of two multipliers\n● It seems this CPU implements a slow multiplier\no It needs to execute 3 distinct steps:\n(1) add, (2) shift left, and (3) shift right\no Multiplication takes 32-bit numbers\n▪ The ALU and the adder are 64-bits!\no The 64-bit addition takes 10ns\no Shifts also take 10ns\no The multiplication takes 96 steps → 3 × 32bits\n\nWhat if I used another\nmultiplier design?\n\n▪ total = 96 steps × 10𝑛𝑠 = 960𝑛𝑠\n\n36\n\n\fThe tale of two multipliers\n● I know, let’s use a Fast multiplier design I have\n1. It combines some registers\n2. And we can make it do the 3 steps simultaneously\n3. And the ALU only needs to be 32-bits!\n\no Assuming a linear relationship between bits and adder speed:\n▪ The 32-bit addition takes 5ns\no The multiplication takes 32 steps → 1(𝑐𝑜𝑚𝑏𝑖𝑛𝑒𝑑) × 32bits\n▪ total = 32 steps × 5𝑛𝑠 = 160𝑛𝑠\nCool! That’s a lot\nfaster!!\n\n37\n\n\fThe tale of two multipliers\n● Let’s calculate how much faster it is:\no Let’s calculate the speedup ratio:\n▪ The factor by which the new version is faster than the old one\n\nslow multiplier time 960𝑛𝑠\nspeedup =\n=\n=6×\n𝑓𝑎𝑠𝑡 𝑚𝑢𝑙𝑡𝑖𝑝𝑙𝑖𝑒𝑟 𝑡𝑖𝑚𝑒 160𝑛𝑠\nBut will the program\nimprove that much?\n\n38\n\n\fThe tale of two multipliers\n𝑡𝑖𝑚𝑒 = 1 × 10𝑛𝑠 +\n6 × 100 × 10𝑛𝑠 +\n1 × 100 × 160𝑛𝑠 = 22010𝑛𝑠\n\nslow program 102010𝑛𝑠\n𝐬𝐩𝐞𝐞𝐝𝐮𝐩 =\n=\n𝑓𝑎𝑠𝑡 𝑝𝑟𝑜𝑔𝑟𝑎𝑚\n22010𝑛𝑠\n= 𝟒. 𝟔𝟑 ×\n\nIt still an improvement!\nBut not 6x. Why?\n\n● The multiplier is only used once!\n\n39\n\n\fThe tale of two multipliers\n● What happens if we decrease the proportion of execution time?\nSuppose 101 instructions: 100 non-multiply, 1 multiply\n𝑡𝑖𝑚𝑒 𝑠𝑙𝑜𝑤 = 100 × 10𝑛𝑠 + 960𝑛𝑠 = 1960𝑛𝑠\n𝑡𝑖𝑚𝑒 𝑓𝑎𝑠𝑡 = 100 × 10𝑛𝑠 + 160𝑛𝑠 = 1160𝑛𝑠\n1960𝑛𝑠\n𝑠𝑝𝑒𝑒𝑑𝑢𝑝 =\n= 1.7 ×\n1160𝑛𝑠\nSuppose 1001 instructions: 1000 non-multiply, 1 multiply\n𝑡𝑖𝑚𝑒 𝑠𝑙𝑜𝑤 = 1000 × 10𝑛𝑠 + 960𝑛𝑠 = 10960𝑛𝑠\n𝑡𝑖𝑚𝑒 𝑓𝑎𝑠𝑡 = 1000 × 10𝑛𝑠 + 160𝑛𝑠 = 10160𝑛𝑠\n10960𝑛𝑠\n𝑠𝑝𝑒𝑒𝑑𝑢𝑝 =\n= 1.08 ×\n10160𝑛𝑠\n\n40\n\n\fDecreasing gains\n● 6 ×→ 4.63 ×→ 1.7 ×→ 1.08 ×\n\n● What happened?\no Proportion of time spent multiplying was not enough to have gains\n● Optimization is a balancing act.\no As you solve a bottleneck, a new one will appear.\no Improve things to a point, then there are diminishing returns!\n5s\n\n4s\n\n3s\n\n2s\n\n4s\n\n3s\n\n2s\n\n3s\n\n3s\n41\n\n\fWhat about pipelining?\n● How much faster (and why) is a pipelined implementation of MIPS?\n● As we saw last class, we compute the speedup for this:\n\nslow time\nspeedup =\n𝑓𝑎𝑠𝑡 𝑡𝑖𝑚𝑒\n● And how do we compute “time”?\nCPU time = 𝑛 × 𝐶𝑃𝐼 × 𝑡 seconds\n\n42\n\n\fAverage Instruction CPI\n● What is an “average instruction” CPI?\no Remember how we calculated the average CPI of a program?\n● Given a program, how many cycles does an instruction typically take?\no It depends on the program!\no How many instructions, and what types?\n▪ E.g.: all adds vs. all loads for multi-cycle implementation\n\n● The average instruction CPI is the average cycle count per instruction\n\n43\n\n\fInstruction Mix\n● Instruction mix: Is the % total instruction count (n) corresponding to each\ninstruction class\n● Program A: 100 adds, 100 subtracts, 50 loads, 25 stores, 50 branches, and 10\njumps. Total 335 instructions.\n\n● What is the mix?\nArithmetic\n\n(100+100) \/ 335 =\n\n0.597 =\n\n59.7%\n\nLoad\n\n50 \/ 335 =\n\n0.149 =\n\n14.9%\n\nStore\n\n25\/335 =\n\n0.075 =\n\n7.5%\n\nBranch\n\n50\/335 =\n\n0.149 =\n\n14.9%\n\nJump\n\n10\/335 =\n\n0.03 =\n\n3.0%\n\n44\n\n\fCPI – Multi-cycle\n● Given this mix, what is the Average Cycles Per Instruction (CPI)?\no E.g., with a multi-cycle CPU.\n● We compute the weighted average\nCPI= Σ𝑎𝑙𝑙 𝑐𝑙𝑎𝑠𝑠𝑒𝑠 𝑓𝑟𝑒𝑞 × 𝑐𝑦𝑐𝑙𝑒𝑠\nClass\n\nFrequency\n\nCycles\n\nContribution\n\nArithmetic\n\n59.7%\n\n4\n\n2.388\n\nLoad\n\n14.9%\n\n5\n\n0.745\n\nStore\n\n7.5%\n\n4\n\n0.3\n\nBranch\n\n14.9%\n\n3\n\n0.447\n\nJump\n\n3.0%\n\n3\n\n0.09\n\nTotal\n\n3.97 CPI\n45\n\n\fCPU time\n● And now we can calculate the CPU time\no Assuming a cycle length of 2ns\nCPU time = 335 × 3.97 × 2𝑛𝑠\n= 2660𝑛𝑠\nClass\n\nFrequency\n\nCycles\n\nContribution\n\nArithmetic\n\n59.7%\n\n4\n\n2.388\n\nLoad\n\n14.9%\n\n5\n\n0.745\n\nStore\n\n7.5%\n\n4\n\n0.3\n\nBranch\n\n14.9%\n\n3\n\n0.447\n\nJump\n\n3.0%\n\n3\n\n0.09\n\nTotal\n\n3.97 CPI\n46\n\n\fWhat about in the pipeline implementation?\n● In the best case, what is the CPI?\no How many instructions are we starting every clock cycle?\n● What about the typical case, what is the CPI?\no We have to consider hazards.\no Say, 20% of branches are predicted correctly.\no 60% of loads do not conflict with other memory accesses.\n\n● Assume the same program and clock cycle.\n\n47\n\n\fInstruction Mix – Pipeline\n● Instruction mix: Treat the delayed load and branch instructions as a separate\nclass\nClass\nArithmetic\n\nFrequency\n\nCycles\n\n59.7%\n\n1\n\n0.6*14.9%=8.94%\n\n1\n\nLoad – delay\n\n5.96%\n\n2\n\nStore\n\n7.5%\n\n1\n\n0.2*14.9%=2.98%\n\n1\n\n11.92%\n\n3\n\n3.0%\n\n1\n\nLoad – no delay\n\nBranch predicted\nBranch not predicted\nJump\n\n48\n\n\fCPI – Pipeline\n● We compute the weighted average\nCPI= Σ𝑎𝑙𝑙 𝑐𝑙𝑎𝑠𝑠𝑒𝑠 𝑓𝑟𝑒𝑞 × 𝑐𝑦𝑐𝑙𝑒𝑠\nClass\n\nCycles\n\nContribution\n\n59.7%\n\n1\n\n0.597\n\n0.6*14.9%=8.94%\n\n1\n\n0.0894\n\nLoad – delay\n\n5.96%\n\n2\n\n0.1192\n\nStore\n\n7.5%\n\n1\n\n.075\n\n0.2*14.9%=2.98%\n\n1\n\n.0298\n\n11.92%\n\n3\n\n0.3576\n\n3.0%\n\n1\n\n0.03\n\nArithmetic\nLoad – no delay\n\nBranch predicted\nBranch not predicted\nJump\n\nFrequency\n\nTotal\n\n1.30 CPI\n\n49\n\n\fThe speedup\n● Compute CPU execution time of pipelined implementation\no Every value except CPI is the same as in the multi-cycle\n▪ n – is a property of the program\n\nPipeline CPU time = 335 × 1.30 × 2𝑛𝑠\n= 871𝑛𝑠\n\n2660𝑛𝑠\nspeedup =\n= 3.05 ×\n871𝑛𝑠\n\n50\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":171,"segment": "unlabeled", "course": "cs0441", "lec": "lec07", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #7: Proof Techniques\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday's topics\nn Proof techniques\nl How can I prove an implication is true?\nl What forms can an informal proof take?\n\nn Proof strategies\nl Which proof techniques should I try?\nl How do I find a proof without trying every proof technique?\n\n\fMathematical theorems are often stated in the\nform of an implication\nExample: If x > y, where x and y are positive real\nnumbers, then x2 > y2.\nl ∀x,y [(x > 0) ∧ (y > 0) ∧ (x > y) → (x2 > y2)]\nl ∀x,y P(x,y) → Q(x,y)\n\nWe will discuss three applicable proof methods:\nl Direct proof\nl Proof by contraposition\nl Proof by contradiction\n\n\fDirect proof\nIn a direct proof, we prove p → q by showing that if p is\ntrue, then q must necessarily be true\n\nExample: Prove that if n is an odd integer, then n2 is\nan odd integer.\n\nProof:\nl Assume that n is odd. That is n = (2k + 1) for some integer k.\nl Note that n2 = (2k+1)2 = (4k2 + 4k + 1)\nl We can factor the above to get 2(2k2 + 2k) + 1\nl Since the above quantity is one more than even number, we\nknow that n2 is odd. ☐\n\n\fDirect proofs are not always the easiest way to\nprove a given conjecture.\nIn this case, we can try proof by contraposition\nHow does this work?\nl Recall that p → q ≡ ¬q → ¬p\nl Therefore, a proof of ¬q → ¬p is also a proof of p → q\n\nProof by contraposition is an indirect proof technique\nsince we don’t prove p → q directly.\nLet’s take a look at an example…\n\n\fProve: If n is an integer and 3n + 2 is odd, then\nn is odd.\nFirst, attempt a direct proof:\nl Assume that 3n + 2 is odd, thus 3n + 2 = 2k + 1 for some k\nl Can solve to find that n = (2k – 1)\/3\nWhere do we go from here?!?\n\nNow, try proof by contraposition:\nl Assume n is even, so n = 2k for some k\nl 3(2k) + 2 = 6k + 2 = 2(3k + 1)\nl So, 3n + 2 is also even.\nl Since we proved ¬“n is odd” → ¬“3n+2 is odd”, we can\nconclude that “3n + 2 is odd” → “n is odd” ☐\n\n\fProof by contradiction\nGiven a conditional p → q, the only way to reject this\nclaim is to prove that p ∧ ¬q is true.\nIn a proof by contradiction we:\n1. Assume that p ∧ ¬q is true\n2. Proceed with the proof\n3. If this assumption leads us to a contradiction, we can\nconclude that p → q is true\n\nLet’s revisit an earlier example…\n\n\fProve: If n is an integer and 3n + 2 is odd, then\nn is odd.\nProof:\nl Assume the negation, that 3n + 2 is odd, n is even (i.e., n = 2k)\nl 3n + 2 = 3(2k) + 2 = 6k + 2 = 2(3k + 1)\nl The above statement tells us that 3n + 2 is even, which is a\ncontradiction of our assumption that 3n + 2 is odd.\nl Therefore, we have shown that if 3n + 2 is odd, then n is also\nodd. ☐\n\nNote that the mechanics\/algebra is the same! But the line\nof argumentation is different\nWe can also use proof by contradiction in cases where\nwere the theorem to be proved is not of the form p → q\n\n\fProve: At least 10 of any 64 days fall on the\nsame day of the week\nProof:\nl Let p ≡ “At least 10 of any 64 days fall on the same day of\nthe week”\nl Assume ¬p is true, that is “At most 9 of any 64 days fall on\nthe same day of the week”\nl Since there are 7 days in a week, at most 7× 9 = 63 days\ncan be chosen\nl This is a contradiction of the fact that we chose 64 days\nl Therefore, we can conclude that at least 10 of any 64 days\nfall on the same day of the week. ☐\n\nThis proof is an example of the pigeonhole principle, which we will\nstudy during our combinatorics unit.\n\n\fIn-class exercises\nProblem 1: Prove the following claims\na) Use a direct proof to show that the square of an even\nnumber is an even number.\nb) Show that if m + n and n + p are even integers, then the\nsum m + p is also an even integer.\nc) Use proof by contraposition to show that if n is an integer\nand n3 + 5 is odd, then n is even.\n\n\fSadly, not all theorems are of the form p → q\nSometimes, we need to prove a theorem of the form:\np1 ∨ p2 ∨ … ∨ pn → q\n\nNote: p1 ∨ p2 ∨ … ∨ pn → q\n\nDistributive law\n\n≡ ¬(p1 ∨ p2 ∨ … ∨ pn) ∨ q\n≡ (¬p1 ∧ ¬p2 ∧ … ∧ ¬pn) ∨ q\n≡ (¬p1 ∨ q) ∧ (¬p2 ∨ q) ∧ … ∧ (¬pn ∨ q)\n≡ (p1 → q) ∧ (p2 →q) ∧ … ∧ (pn → q)\nSo, we might need to examine multiple cases!\n\n\fProve that n2 + 1 ≥ 2n where n is a positive\ninteger with 1 ≤ n ≤ 4\nProof:\nl n = 1: (1)2 + 1 = 2, 2(1) = 2, and 2 ≥ 2\nl n = 2: (2)2 + 1 = 5, 2(2) = 4, and 5 ≥ 4\nl n = 3: (3)2 + 1 = 10, 2(3) = 6, and 10 ≥ 6\nl n = 4: (4)2 + 1 = 17, 2(4) = 8, and 17 ≥ 8\n\nSince we have verified each case, we have shown that n2\n+ 1 ≥ 2n where n is a positive integer with 1 ≤ n ≤ 4. ☐\nWith only 4 cases to consider, exhaustive proof was a good\nchoice!\n\n\fSometimes, exhaustive proof isn’t an option, but we\nstill need to examine multiple possibilities\nExample: Prove the triangle inequality. That is, if x\nand y are real numbers, then |x| + |y| ≥ |x + y|.\nClearly, we can’t use exhaustive proof here since\nthere are infinitely many real numbers to consider.\nWe also can’t use a simple direct proof either, since\nour proof depends on the signs of x and y.\n\n\fExample: Prove that if x and y are real numbers, then\n|x| + |y| ≥ |x + y|.\nn … on the board!\n\n\fMaking mistakes when using proof by cases is all\ntoo easy!\nMistake 1: Proof by “a few cases” is not equivalent to\nproof by cases.\nThis is a “there exists” proof,\nnot a “for all” proof!\n\nExample: Prove that all odd numbers are prime.\n“Proof:”\nl Case (i): The number 1 is both odd and prime\nl Case (ii): The number 3 is both odd and prime\nl Case (iii): The number 5 is both odd and prime\nl Case (iv): The number 7 is both odd and prime\n\nThus, we have shown that odd numbers are prime. ☐\n\n\fMaking mistakes when using proof by cases is all\ntoo easy!\nMistake 2: Leaving out critical cases.\nExample: Prove that x2 > 0 for all integers x\n“Proof:”\nl Case (i): Assume that x < 0. Since the product of two\nnegative numbers is always positive, x2 > 0.\nl Case (ii): Assume that x > 0. Since the product of two\npositive numbers is always positive, x2 > 0.\n\nSince we have proven the claim for all cases, we can\nconclude that x2 > 0 for all integers x. ☐\nWhat about the case in which x = 0?\n\n\fSometimes we need to prove the existence of a\ngiven element\nThere are two ways to do this\nProve the claim by showing how to construct\nan example\n\nThe constructive approach\nShow that it is guaranteed that such an\nelement exists\n\nThe non-constructive approach\n\n\fA constructive existence proof\nProve: Show that there is a positive integer that can\nbe written as the sum of cubes of positive integers in\ntwo different ways.\n\nProof: 1729 = 103 + 93 = 123 + 13 ☐\nObviously, the claim has been proven because we have\nshown that a specific instance of the claim is valid.\nConstructive existence proofs are really just instances of\n“existential generalization.”\n\n\fA non-constructive existence proof\nProve: Show that there exist two irrational numbers x\nand y such that xy is rational.\n\nProof:\nl We know that √2 is irrational, so let x = √2\nl If √2√2 is rational, then we are done! (i.e., x = y = √2)\nl If √2√2 is irrational, then let x = √2√2 and y = √2, both of which\nare irrational\nl Now, xy = (√2√2)√2 = √22 = 2, which is rational (i.e., 2 = 2\/1) ☐\n\nNote: We don’t know whether √2√2 is rational or\nirrational. However, in either case, we can use it to\nconstruct a rational number.\n\n\fSometimes, existence is not enough and we\nneed to prove uniqueness\nThis process has two steps:\n1. Provide an existence proof\n2. Show that any other solution to the problem is equivalent to\nthe solution generated in step 1\n\nExample: Prove that if a and b are real numbers, then\nthere exists a unique real number r such that ar + b = 0\nExistence\n\nProof:\nl\nl\nl\n\nNote that r = -b\/a is a solution to this equality since\na(-b\/a) + b = -b + b = 0.\nAssume that as + b = 0, s ≠ r\nThen as = -b, so s = -b\/a = r, which is a contradiction\n\n☐\n\nUniqueness\n\n\fThe scientific process is not always\nstraightforward…\n\nConjecture\n\nGather evidence,\nprove lemmas\n\nProve\ntheorem\n\n\fProof strategies can help preserve your sanity\nProof strategies help us…\n\nOrganize our problem\nsolving approach\n\nEffectively use all of the\ntools at our disposal\n\nDevelop a coherent plan\nof attack\n\n\fTypes of proof strategy\nToday we’ll discuss four types of strategy:\n1.\n2.\n3.\n4.\n\nForward reasoning\nBackward reasoning\nSearching for counterexamples\nAdapting existing proofs\n\n\fSometimes forward reasoning doesn’t work\nIn these cases, it is often helpful to reason backwards,\nstarting with the goal that we want to prove.\n\nExample: Prove that given two distinct positive real\nnumbers x and y, the arithmetic mean of x and y is\nalways greater than the geometric mean of x and y.\n\n(x + y)\/2\n\n√(xy)\n\nSanity check: Let x=8 and y=4. (8+4)\/2 = 6. √(8×4) =\n√(32) ≅ 5.66. 6 > 5.66\n\n\fProve that (x+y)\/2 > √(xy) for all distinct pairs of\npositive real numbers x and y.\nProof:\n(x + y)\/2 > √(xy)\n(x + y)2\/4 > xy\n(x + y)2 > 4xy\nx2 + 2xy + y2 > 4xy\nx2 - 2xy + y2 > 0\n(x – y)2 > 0\nSince (x – y)2 > 0 whenever x ≠ y, the final inequality is\ntrue. Since all of these inequalities are equivalent,\nit follows that (x + y)\/2 > √(xy). ☐\n\n\fOther times, searching for a counterexample is helpful\n\nProof by counterexample is helpful if:\nl Proof attempts repeatedly fail\nl The conjecture to be proven looks “funny”\n\nExample: Prove that every positive integer is the sum of\ntwo squares.\nThis seems suspicious to me, since other factorizations\n(e.g., prime factorizations) can be complex.\n\nCounterexample:\n3 is not the sum of two squares, so the claim is false. ☐\n\n\fThese four proof strategies are just a start!\nA great tool for programmers AND logicians!\n\nWhen trying to prove a new conjecture, a good “meta\nstrategy” is to:\n1. If possible, try to reuse an existing proof (analogy!)\n2. If the conjecture looks fishy, check for a counterexample\n3. Attempt a “real” proof\na)\nb)\nc)\n\nApply the forward reasoning strategy\nOr, apply the backward reasoning strategy\nPossibly alternate between forward and backward reasoning\n\nUnfortunately, not every proof can be solved using this\nnice little meta strategy…\nIn fact, there are many, many proof strategies out there, and\nNONE of them can be guaranteed to find a proof!!!\n\n\fIn-class exercises\nProblem 2: Prove that there exists a positive integer\nthat is equal to the sum of all positive integers less\nthan it. Is your proof constructive or non-constructive?\nProblem 3: Prove that there is no positive integer n\nsuch that n2 + n3 = 100.\nProblem 4: Use proof by cases to show that\nmin(a, min(b,c)) = min(min(a,b),c) whenever a, b, and\nc are real numbers.\n\n\fFinal Thoughts\nn Proving theorems is not always straightforward\nn Having several proof strategies at your disposal will\nmake a huge difference in your success rate!\nn We are “done” with our intro to logic and proofs\nn Next lecture:\nl Intro to set theory\nl Please read sections 2.1 and 2.2\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":172,"segment": "unlabeled", "course": "cs0441", "lec": "lec14", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #14: Strong Induction\n\nBased on materials developed by Dr. Adam Lee\n\n\fRecall that mathematical induction let us prove\nuniversally quantified statements\nGoal: Prove ∀x∈N P(x).\n\nIntuition: If P(0) is true, then\nP(1) is true. If P(1) is true, then\nP(2) is true…\n\nProcedure:\n1. Prove P(0)\n2. Show that P(k) → P(k+1) for any arbitrary k\n3. Conclude that P(x) is true ∀x∈N\n\nP(0)\nP(k) → P(k+1)\n∴∀x∈N P(x)\n\n\fStrong mathematical induction is another flavor\nof induction\nGoal: Prove ∀x∈N P(x).\nProcedure:\n1. Prove P(0)\n2. Show that [P(0) ∧ P(1) ∧ … ∧ P(k) ] → P(k+1) for any\narbitrary k\n3. Conclude that P(x) is true ∀x∈N\n\nP(0)\n[P(0) ∧ P(1) ∧ … ∧ P(k) ] → P(k+1)\n∴∀x∈N P(x)\n\n\fSo what’s the big deal?\nRecall: In mathematical induction, our inductive\nhypothesis allows us to assume that P(k) is true and use\nthis knowledge to prove P(k+1)\nHowever, in strong induction, we can assume that P(0) ∧\nP(1) ∧ … ∧ P(k) is true before trying to prove P(k+1)\nFor certain types of proofs, this is much easier than\ntrying to prove P(k+1) from P(k) alone.\n\nFor example…\n\n\fShow that if n is an integer greater than 1, then n can\nbe written as the product of primes\nP(n) ≡ n can be written as a product of primes\nBase case: P(2): 2 = 21\n\n✔\n\nI.H.: Assume that P(2) ∧ … ∧ P(k) holds for an arbitrary k\nInductive step: We will now show that [P(2) ∧ … ∧ P(k)] → P(k+1)\nn Two cases to consider: k+1 prime and k+1 composite\nn If k+1 is prime, then we’re done\nn If k+1 is composite, then by definition, k+1 = ab\nn Since 2 ≤ a ≤ k and 2 ≤ b ≤ k, a and b can be written as\nproducts of primes by the I.H.\nn Thus, k+1 can be written as a product of primes\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by strong induction ❏\n\n\fIs strong induction somehow more powerful than\nmathematical induction?\nThe ability to assume P(0) ∧ P(1) ∧ … ∧ P(k) true before\nproving P(k+1) seems more powerful than just assuming\nP(k) is true\nPerhaps surprisingly, mathematical induction, strong\ninduction, and well ordering are all equivalent!\nThat is, a proof using one of these methods can always\nbe written using the other two methods\n\nThis may not be easy, though!\n\n\fProve that every amount of postage of 12 cents or more can be\nformed using just 4-cent and 5-cent stamps\nP(n) ≡ n cents of postage can be made using 4- and 5-cent stamps\nBase case: P(12): 3 4-cent stamps\nP(13): 1 5-cent stamp, 2 4-cent stamps\nP(14): 2 5-cent stamps, 1 4-cent stamp\nP(15): 3 5-cent stamps\n\n✔\n\nI.H.: Assume that P(12) ∧ … ∧ P(k) holds for an arbitrary integer k\nInductive step: We will now show that [P(12) ∧ … ∧ P(k)] → P(k+1)\nn By the I.H., we can make k-3 cents worth of postage using\nonly 4-cent and 5-cent stamps\nn By adding another 4-cent stamp, we end up with k+1 cents\nworth of postage\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by strong induction ❏\n\n\fProve that every amount of postage of 12 cents or more can be\nformed using just 4-cent and 5-cent stamps\nP(n) ≡ n cents of postage can be made using 4- and 5-cent stamps\nBase case: P(12): 3 4-cent stamps ✔\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) → P(k+1)\nn We can k cents using 4- and 5-cent stamps, by the I.H.\nn If at least one 4-cent stamp was used, remove that stamp and\nadd in a 5-cent stamp, thereby making k+1 cents of postage\nn If no 4-cent stamps were used, then only 5-cent stamps were\nused.\nn Since k > 12, at least 3 5-cent stamps were used.\nn Replace 3 5-cent stamps with 4 4-cent stamps, thereby making\nk+1 cents of postage\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction ❏\n\n\fSo when should we use strong induction?\nIf it is straightforward to prove P(k+1) from P(k) alone,\nuse mathematical induction\nP(0)\n\nP(1)\n\n…\n\nP(k-1)\n\nP(k)\n\nP(k+1)\n\nIf it would be easier to prove P(k+1) using one or more\nP(j) for 0 ≤ j < k, use strong induction\nP(0)\n\nP(1)\n\n…\n\nP(k-1)\n\nP(k)\n\n?\n\nP(k+1)\n\n\fIn-class exercises\nProblem 1: Use strong induction to prove that any\nwhole dollar amount greater than or equal to $4 can\nbe formed using only $2 and $5 bills.\n\n\fIn-class exercises\nProblem 2: Suppose you have a box of Maltesers\ncontaining 𝑛 candies, and you want to split it into 𝑛 piles\nof 1 candy each by repeatedly splitting a pile into two\nsmaller piles. Each time you split a pile, you multiply the\nnumber of candies in each of the two smaller piles you\nform, and add it to a running total. For example, if you\nsplit a pile of 13 candies into two piles of size 4 and 9,\nyou would add 4×9 = 36 to the total.\nShow that, no matter how you split the piles, the sum of\n! !\"#\nthe products computed at each step will equal\n$\n\n\fFinal Thoughts\nn Strong induction lets us prove universally quantified\nstatements using this inference rule:\nP(0)\n[P(0) ∧ P(1) ∧ … ∧ P(k) ] → P(k+1)\n∴∀x∈N P(x)\n\nn Although sometimes more convenient than mathematical\ninduction, strong induction is no more powerful\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":173,"segment": "unlabeled", "course": "cs0441", "lec": "lec08", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #8: Sets\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nIntroduction to set theory\nl What is a set?\nl Set notation\nl Basic set operations\n\n\fWhat is a set?\nDefinition: A set is an unordered collection of objects\nExamples:\n\nSets can contain items of\nmixed types\n\nl A = {1, 2, 3, 4}\nl B = {Cooper, Dougie, Mr. C}\nl C = {motorcycle, 3.14159, Socrates}\nl E = {{1, 2, 3}, {6, 7, 8}, {23, 42}}\n\nSets can contain other sets\n\nInformally: Sets are really just a precise way of\ngrouping a “bunch of stuff”\n\n\fA set is made up of elements\nDefinition: The objects making up a set are called\nelements of that set.\n\nExamples:\nl 3 is an element of {1, 2, 3}\nl Bob is an element of {Alice, Bob, Charlie, Daniel}\n\nWe can express the above examples in a more precise\nmanner as follows:\nl 3 ∈ {1, 2, 3}\nl Bob ∈ {Alice, Bob, Charlie, Daniel}\n\nQuestion: Is 5 ∈ {1, 2, 3, {4, 5}}?\n\nNO!\n\n\fThere are many different ways to describe a set\nExplicit enumeration:\nl A = {1, 2, 3, 4}\n\nUsing ellipses if the general pattern is obvious:\nl E = {2, 4, 6, …, 98}\n\nSet builder notation (aka, set comprehensions):\nl M = {y | y = 3k for some integer k}\nThe set M contains…\n… all elements y…\n… such that…\n… y = 3k for some integer k\n\n\fThere are a number of sets that are so important to\nmathematics that they get their own symbol\nN = {0, 1, 2, 3, …}\nZ = {…, -2, -1, 0, 1, 2, …}\nZ+ = {1, 2, …}\nQ = {p\/q | p,q ∈ Z, q≠0}\nR\n∅ = {}\n\nNatural numbers\nIntegers\nPositive integers\nRational numbers\nReal numbers\nEmpty set\n\nNote: This notation differs from book to book\nl Some authors write these sets as ℕ, ℤ, ℤ+, ℚ, and ℝ\n➣ I’ll do so in handwriting (“blackboard bold”)\n\nl Some authors do not include zero in the natural numbers\n➣ I like the above because it makes N≠Z+ (more expressive)\n\nBe careful when reading other books or researching on the\nWeb, as things may be slightly different!\n\n\fYou’ve actually been using sets implicitly all along!\n\nF(x,y) ≡ x and y are friends\nDomain: “All people”\n\nMathematics\nFunction min(int x, int y) : int\nif x < y then\nreturn x\nelse\nreturn y\nendif\nend function\n\nProgramming language\ndata types\n\n∀x ∃y F(x,y)\n\nDomains of propositional\nfunctions\n\n\fSet equality\nDefinition: Two sets are equal if and only if they\ncontain exactly the same elements.\n\nMathematically: A = B iff ∀x (x ∈ A ⇔ x ∈ B)\nExample: Are the following sets equal?\nl {1, 2, 3, 4} and {1, 2, 3, 4}\nl {1, 2, 3, 4} and {4, 1, 3, 2}\nl {a, b, c, d, e} and {a, a, c, b, e, d}\nl {a, e, i, o} and {a, e, i, o, u}\n\nYes!\nYes!\nYes!\nNo!\n\n\fWe can use Venn diagrams to graphically\nrepresent sets\nU is the “universe” of all elements\n\nU\n•a\n•e\n\n•i\n•o\n\nV\n•u\n\nThe set V of all vowels is contained within the\nuniverse of “all letters”\nSometimes, we add points for the elements of a set\n\n\fSets can be contained within one another\nDefinition: Some set A is a subset of another set B iff every element\nof A is an element in the set B. We denote this fact as A ⊆ B, and\ncall B a superset of A.\n\nGraphically:\n\nB\n\nU\nA\n\nMathematically: A ⊆ B iff ∀x (x ∈ A → x ∈ B)\nDefinition: We say that A is a proper subset of B iff A ⊆ B, but A ≠ B.\nWe denote this by A ⊂ B. More precisely:\nA ⊂ B iff ∀x (x ∈ A → x ∈ B) ∧ ∃y (y ∈ B ∧ y ∉ A)\n\n\fProperties of subsets\nProperty 1: For all sets S, we have that ∅ ⊆ S\n\nProof: The set ∅ contains no elements. So, trivially,\nevery element of the set ∅ is contained in any other\nset S. ❏\nProperty 2: For any set S, S ⊆ S.\nProperty 3: If S1 = S2, then S1 ⊆ S2 and S2 ⊆ S1.\n\n\fIn-class exercises\nProblem 1: Come up with two ways to represent each\nof the following sets:\nl The even integers\nl Negative integers between -1 and -10, inclusive\nl The positive integers\n\nProblem 2: Draw a Venn diagram representing the\nsets {1, 2, 3} and {3, 4, 5}.\nProblem 3: On Top Hat\n\n\fWe can create a new set by combining two or\nmore existing sets\nDefinition: The union of two sets A and B contains every element\nthat is either in A or in B. We denote the union of the sets A and\nB as A ∪ B.\n\nGraphically:\n\nB\n\nA\n\nU\n\nMathematically: A ∪ B = {x | x ∈ A ∨ x ∈ B}\nExample: {1, 2, 3} ∪ {6, 7, 8} = {1, 2, 3, 6, 7, 8}\n\n\fWe can take the union of any number of sets\nExample: A ∪ B ∪ C\nGraphically:\n\nB\n\nA\n\nU\n\nC\n\nIn general, we can express the union S1 ∪ S2 ∪ … ∪ Sn\nusing the following notation:\nn\n[\n\nSi\n\ni=1\n\nThis is just like summation\nnotation!\n\n\fSometimes we’re interested in the elements\nthat are in more than one set\nDefinition: The intersection of two sets A and B contains every\nelement that is in A and also in B. We denote the intersection of\nthe sets A and B as A ∩ B.\n\nGraphically:\n\nB\n\nA\n\nU\n\nMathematically: A ∩ B = {x | x ∈ A ∧ x ∈ B}\nExamples:\nl {1, 2, 3, 7, 8} ∩ {6, 7, 8} = {7, 8}\nl {1, 2, 3} ∩ {6, 7, 8} = ∅\n\nWe say that two sets A and B\nare disjoint if A ∩ B = ∅\n\n\fWe can take the intersection of any number of sets\n\nExample: A ∩ B ∩ C\nGraphically:\n\nB\n\nA\n\nU\n\nC\n\nAs with the union operation, we can express the\nintersection S1 ∩ S2 ∩ … ∩ Sn as:\nn\n\\\n\nSi\n\ni=1\n\n\fSet differences\nDefinition: The difference of two sets A and B, denoted by A – B,\ncontains every element that is in A, but not in B.\n\nGraphically:\n\nB\n\nA\n\nU\n\nMathematically: A - B = {x | x ∈ A ∧ x ∉ B}\nExample: {1, 2, 3, 4, 5} – {4, 5, 6, 7, 8} = {1, 2, 3}\nBe careful: Some authors use the notation A \\ B to denote the set\ndifference A – B.\n\n\fIf we have specified a universe U, we can determine\nthe complement of a set\nDefinition: The complement of a set A, denoted by A, contains\nevery element that is in U, but not in A.\n\nGraphically:\n\nA\n\nMathematically: A = {x | x ∈ U ∧ x ∉ A}\nExamples: Assume that U = {1, 2, …, 10}\nl {1, 2, 3, 4, 5} = {6, 7, 8, 9, 10}\nl {2, 4, 6, 8, 10} = {1, 3, 5, 7, 9}\n\nU\n\n\fCardinality is the measure of a set’s size\nDefinition: Let S be a set. If there are exactly n\nelements in S, where n is a nonnegative integer, then S\nis a finite set whose cardinality is n. The cardinality of S\nis denoted by |S|.\n\nExample: If S = {a, e, i, o, u}, then |S| = 5.\nUseful facts: If A and B are finite sets, then\nl |A ∪ B| = |A| + |B| – |A ∩ B|\nl |A – B| = |A| – |A ∩ B|\n\nAside: We’ll talk about the cardinality of infinite sets\nlater in the course.\n\n\fPower set\nDefinition: Given a set S, its power set is the set\ncontaining all subsets of S. We denote the power set\nof S as P(S).\n\nExamples:\nl P({1}) = {∅, {1}}\nl P({1, 2, 3}) = {∅, {1}, {2}, {3}, {1,2}, {1,3}, {2, 3}, {1, 2, 3}}\n\nNote:\nl The set ∅ is in the power set of any set S: ∀S(∅ ∈P(S))\nl The set S is in its own power set: ∀S(S∈P(S))\nl |P(S)| = 2|S|\nl Some authors use the notation 2S to represent the power\nset of S\n\n\fHow do we represent ordered collections?\nDefinition: The ordered n-tuple (a1, a2, …, an) is the ordered\ncollection that has a1 as its first element, a2 as its second\nelement, …, and an as its nth element.\n\nNote: (a1, a2, …, an) = (b1, b2, …, bn) iff ai = bi for i = 1, …, n.\nSpecial case: Ordered pairs of the form (x ∈ Z, y ∈ Z) are the\nbasis of the Cartesian plane!\nl (a, b) = (c, d) iff a = c and b = d\nl (a, b) = (b, a) iff a = b\n\nHow can we construct and describe ordered n-tuples?\n\n\fWe use the Cartesian product operator to\nconstruct ordered n-tuples\nDefinition: If A and B are sets, the Cartesian product of A\nand B, which is denoted A× B, is the set of all ordered\npairs (a, b) such that a ∈ A and b ∈ B.\n\nMathematically: A× B = {(a, b) | a ∈ A ∧ b ∈ B}\nExamples: Let A = {1, 2} and B = {y, z}\nl What is A× B?\nl B× A?\nl Are A× B and B× A equivalent?\n\n{(1, y), (1, z), (2, y), (2, z)}\n{(y, 1), (z, 1), (y, 2), (z, 2)}\n\nNO!!\n\n\fCartesian products can be made from more than\ntwo sets\nExample: Let\nl S = {x | x is enrolled in CS 441}\nl G = {x | x ∈ R ∧ 0 ≤ x ≤ 100}\nl Y = {freshman, sophomore, junior, senior}\n\nThe set S× Y× G consists of all possible (CS441\nstudent, year, grade) combinations.\nNote: My grades database is a subset of S× Y× G that\ndefines a relation between students in the class,\ntheir year at Pitt, and their grade!\nWe will study the properties of relations towards the\nend of this course.\n\n\fSets and Cartesian products can be used to\nrepresent trees and graphs\nDave\n\nCharlie\nAlice\n\nLet:\n\nElise\n\nBob\n\nNode\n\nBecky\n\nl N = All names\nl F = N× N\n\nEdge\n\nFrank\nSarah\n\nTommy\n\nA social network can be represented as a graph (V, E) in which\nthe set V denotes the people in the network and the set E\ndenotes the set of “friendship” links: (V, E) ∈ P(N)× P(F)\nIn the above network:\nl V = {Alice, Bob, …, Tommy} ⊆ N\nl E = {(Alice, Bob), (Alice, Dave), …, (Sarah, Tommy)} ⊆ N×N\n\n\fSet notation allows us to make quantified\nstatements more precise\nWe can use set notation to make the domain of a\nquantified statement explicit.\n\nExample: ∀x∈R (x2 ≥ 0)\nl The square of any real number is at least zero\n\nExample: ∀n∈Z ∃j,k∈Z [(3n+2 = 2j+1)→(n = 2k+1)]\nl If n is an integer and 3n + 2 is odd, then n is odd.\n\nNote: This notation is far less ambiguous than simply\nstating the domains of propositional functions. In the\nremainder of the course, we will use this notation\nwhenever possible.\n\n\fTruth sets describe when a predicate is true\nDefinition: Given a predicate P and its corresponding\ndomain D the truth set of P enumerates all elements in\nD that make the predicate P true.\n\nExamples: What are the truth sets of the following\npredicates, given that their domain is the set Z?\nl P(x) ≡ |x| = 1\nl Q(x) ≡ x2 > 0\nl R(x) ≡ x5 = 1049\n\n{-1, 1}\n{x | x∈Z±∧ x≠0} -or- Z–{0}\n∅\n\nNote:\nl ∀x P(x) is true iff the truth set of P is the entire domain D\nl ∃x P(x) is true iff the truth set of P is non-empty\n\n\fHow do computers represent and manipulate\nfinite sets?\nObservation: Representing sets as unordered\ncollections of elements (e.g., arrays of Java Object\ndata types) can be inefficient.\nAs a result, sets are usually represented using either\nhash maps or bitmaps.\nYou’ll learn about these in 1501, so today we’ll focus on bitmap\nrepresentations.\n\nThis is probably best explained through an example…\n\n\fPlaying with the set S={x | x∈N, x<10}\nTo represent a set as a bitmap, we must first agree on\nan ordering for the set. In the case of S, let’s use the\nnatural ordering of the numbers.\nNow, any subset of S can be represented using |S|=10\nbits. For example:\nl {1, 3, 5, 7, 9} = 0101 0101 01\nl {1, 1, 1, 4, 5} = 0100 1100 00\n\nWhat subsets of S do the following bitmaps represent?\nl 0101 1010 11\nl 1111 0000 10\n\n{1, 3, 4, 6, 8, 9}\n{0, 1, 2, 3, 8}\n\n\fSet operations can be carried out very\nefficiently as bitwise operations\nExample: {1, 3, 7} ∪ {2, 3, 8}\n0101 0001 00\n∨ 0011 0000 10\n0111 0001 10 = {1, 2, 3, 7, 8}\n\nExample: {1, 3, 7} ∩ {2, 3, 8}\n0101 0001 00\n∧ 0011 0000 10\n0001 0000 00 = {3}\nNote: These operations are much faster than searching\nthrough unordered lists!\n\n\fSet operations can be carried out very\nefficiently as bitwise operations\nExample: {1, 3, 7}\n¬0101 0001 00\n1010 1110 11 = {0, 2, 4, 5, 6, 8, 9}\nSince the set difference A – B can be written as A ∩ (A ∩ B),\nwe can calculate it as A ∧ ¬(A ∧ B).\n\nAlthough set difference is more complicated than the basic\noperations, it is still much faster to calculate set differences\nusing a bitmap approach as opposed to an unordered search.\n\n\fIn-class exercises\nProblem 4: Let A = {1, 2, 3, 4}, B = {3, 5, 7, 9}, and C\n= {7, 8, 9, 10}. Calculate the following:\nlA∩B\nlA∪B∪C\nlB∩C\nlA∩B∩C\n\nProblem 5: Come up with a bitmap representation of\nthe sets A = {a, c, d, f} and B = {a, b, c}. Use this to\ncalculate the following:\nlA∪B\nlA∩B\n\n\fFinal thoughts\nn Sets are one of the most basic data structures used\nin computer science\nn Today, we looked at:\nl How to define sets\nl Basic set operations\nl How computers represent sets\n\nn Next time:\nl Set identities (Section 2.2)\nl Functions (Section 2.3)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":174,"segment": "unlabeled", "course": "cs0007", "lec": "lec06", "text":"CS 0007: Introduction to Java\nLecture 6\nNathan Ong\nUniversity of Pittsburgh\nSeptember 20, 2016\n\n\fEven more API for your first Object\n\nSCANNERS\n\n\fScanner\n\n\fWhat is a Scanner?\n• Retrieves input\n• Can take input from a file, an input\nstream (console, network, etc.), or from\na String\n• Can read all primitives (except chars),\nand can read Strings\n• An object\n\n\fDeclaring Objects\n\nType name = expression;\n\n\fScanner name = new Scanner();\nWe will fill this in with something,\nbut what exactly goes here?\n\n\fDeclaring Objects\n• Any declaration of objects requires the\nkeyword “new”\n• The method that follows the keyword\nare called constructors\n• Where can I look for them?\n\n\fA Special InputStream\nSystem.in\n• The standard way to retrieve input (via\nthe console)\n• A “special” variable.\n• Is this static or non-static?\n\n\fScanner consoleInput =\nnew Scanner(System.in);\n\n\fWhat can I do with it?\n• I have a Scanner that can read input\nfrom the user via the console.\n• How do I actually use it?\n\n\fUsing Scanner\npublic class AddingMachine\n{\npublic static void main(String[] args)\n{\nScanner scan = new Scanner(System.in);\nSystem.out.println(\"Please enter a number:\");\ndouble firstNum = scan.nextDouble();\nSystem.out.println(\"Please enter a second\nnumber:\");\ndouble secondNum = scan.nextDouble();\ndouble sum = firstNum + secondNum;\nSystem.out.println(\"The sum of \" + firstNum +\n\" and \" + secondNum + \" equals \" + sum);\n}\/\/end method main\n}\/\/End class AddingMachine\n\n\fUsing Scanner\npublic class AddingMachine\n{\npublic static void main(String[] args)\n{\nScanner scan = new Scanner(System.in);\nSystem.out.println(\"Please enter a number:\");\ndouble firstNum = scan.nextDouble();\nSystem.out.println(\"Please enter a second\nnumber:\");\ndouble secondNum = scan.nextDouble();\ndouble sum = firstNum + secondNum;\nSystem.out.println(\"The sum of \" + firstNum +\n\" and \" + secondNum + \" equals \" + sum);\n}\/\/end method main\n}\/\/End class AddingMachine\n\n\fUsing Scanner\npublic class AddingMachine\n{\npublic static void main(String[] args)\n{\nScanner scan = new Scanner(System.in);\nSystem.out.println(\"Please enter a number:\");\ndouble firstNum = scan.nextDouble();\nSystem.out.println(\"Please enter a second\nnumber:\");\ndouble secondNum = scan.nextDouble();\ndouble sum = firstNum + secondNum;\nSystem.out.println(\"The sum of \" + firstNum +\n\" and \" + secondNum + \" equals \" + sum);\n}\/\/end method main\n}\/\/End class AddingMachine\n\n\fLet’s Compile\n\n???\n\n\fError?!?\n“Cannot find symbol”\n• This indicates that the name does not\nrefer to anything\n• Based on the console output, “Scanner”\ndoes not exist\n• Is there a way to figure out why?\n\n\fFrom the API\n\n\fError?!?\n• Rather than being a part of “java.lang,”\nwhich is already included by default, it is\npart of a different package\n• In order to include Scanner, we need to\ndo an import\n\n\fimport java.util.Scanner;\npublic class AddingMachine\n{\npublic static void main(String[] args)\n{\nScanner scan = new Scanner(System.in);\nSystem.out.println(\"Please enter a number:\");\ndouble firstNum = scan.nextDouble();\nSystem.out.println(\"Please enter a second\nnumber:\");\ndouble secondNum = scan.nextDouble();\ndouble sum = firstNum + secondNum;\nSystem.out.println(\"The sum of \" + firstNum +\n\" and \" + secondNum + \" equals \" + sum);\n}\/\/end method main\n}\/\/End class AddingMachine\n\n\fDon’t Forget to Import\n\n\fFUNCTIONS\n\n\fFunctions\n• A function is similar to its mathematical\ncounterpart\n• f(x) = x2, plug in 3, get 9\n• Contains several more parts\n\n\fJava Functions\n• Takes in zero or more parameters,\nprocesses them in the function body,\nand returns a result\n• Imagine going to BestBuyTM and telling\nthem you want your computer fixed.\nYou are telling them to run a fixing\nfunction, with your computer being a\nparameter. What you get back is your\nfixed computer.\n\n\fYou Already Have the Power!\n• You already know how to call functions!\nStatic:\nClassName.functionName(<parameters\n>);\nNon-static:\nobjectName.functionName(<parameter\ns>);\n• How do I make my own?\n\n\fFunction Components\n\n1.\n2.\n3.\nFunction 4.\nHeader 5.\n\nFunction\nBody 6.\n\nVisibility type (public\/protected\/private)\nstatic (For now, required)\nReturn Type\nfunctionName\nParentheses “()”\n–\na)\nb)\nc)\n\nParameters\nType1 parameterName1\nType2 parameterName2\n…\n\nCurly Brackets\/Braces “{}”\n–\n\nreturn a value\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":175,"segment": "unlabeled", "course": "cs0449", "lec": "lec09", "text":"8\n\nBuffer Overflow\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fReview: General Memory Layout\n• Stack\n• Local variables (procedure context)\n\n• Heap\n\n2N-1\n\nnot drawn to scale\n\nStack\n\n• Dynamically allocated as needed\n• malloc(), calloc(), new(), …\n\n• Statically allocated Data\n• Read\/Write: global variables\n• Read-only: string literals\n\nHeap\nStatic Data\n\n• Code\/Instructions\n• Executable machine instructions\n• Read-only\n\nLiterals\n0\n\nInstructions\n\n\fx86-64 Linux Memory Layout\n• Stack\n• Runtime stack (8MB limit)\n• e.g., local variables\n\n• Heap\n\n00007FFFFFFFFFFF\n(= 247–1)\n00007FFFF0000000\n\nnot drawn to scale\nShared\nLibraries\nStack\n\n8MB\n\n• Dynamically allocated as needed\n• When call malloc(), new()\n\n• Data\n• Statically allocated data\n• e.g., global\/static vars, string constants\n\n• Text \/ Shared Libraries\n\nHeap\n\n• Executable machine instructions\n• Read-only\nHex Address\n\n400000\n000000\n\nData\nText\n\n\fMemory Allocation Example\nnot drawn to scale\n\nchar big_array[1L<<24]; \/* 16 MB *\/\nchar huge_array[1L<<31]; \/* 2 GB *\/\n\nStack\n\nint global = 0;\nint useless() { return 0; }\nint main()\n{\nvoid *p1, *p2, *p3, *p4;\nint local = 0;\np1 = malloc(1L << 8); \/* 256 B *\/\np2 = malloc(1L << 8); \/* 256 B *\/\np3 = malloc(1L << 8); \/* 256 B *\/\np4 = malloc(1L << 8); \/* 256 B *\/\n\/* Some print statements ... *\/\n}\nWhere does everything go?\n\nOther stuff\nShared\nLibraries\n\nHeap\nData\nInstructions\n4\n\n\fMemory Allocation Example\nnot drawn to scale\n\nchar big_array[1L<<24]; \/* 16 MB *\/\nchar huge_array[1L<<31]; \/* 2 GB *\/\n\nStack\n\nint global = 0;\nint useless() { return 0; }\nint main()\n{\nvoid *p1, *p2, *p3, *p4;\nint local = 0;\np1 = malloc(1L << 8); \/* 256 B *\/\np2 = malloc(1L << 8); \/* 256 B *\/\np3 = malloc(1L << 8); \/* 256 B *\/\np4 = malloc(1L << 8); \/* 256 B *\/\n\/* Some print statements ... *\/\n}\nWhere does everything go?\n\nHeap stuff\nOther\nShared\nLibraries\nShared\nLibraries\n\nHeap\nData\nInstructions\n5\n\n\fReminder: x86-64\/Linux Stack Frame\nHigher Addresses\n\n• Caller’s Stack Frame\n• Arguments (if > 6 args) for this call\n\n• Current\/Callee Stack Frame\n• Return address\n• Pushed by call instruction\n\n• Old frame pointer (optional)\n• Saved register context\n(when reusing registers)\n• Local variables\n(if can’t be kept in registers)\n• “Argument build” area\n(If callee needs to call another function parameters for function about to call, if\nneeded)\n\nCaller\nFrame\n\nFrame pointer\n%rbp\n(Optional)\n\nArguments\n7+\nReturn Addr\nOld %rbp\n\nSaved\nRegisters\n+\nLocal\nVariables\n\nStack pointer\n%rsp\n\nArgument\nBuild\n(Optional)\n6 Lower Addresses\n\n\fRecall: Memory Referencing Bug Example\ntypedef struct {\nint a[2];\ndouble d;\n} struct_t;\ndouble fun(int i) {\nstruct_t s;\ns.d = 3.14;\ns.a[i] = 1073741824; \/* Possibly out of bounds *\/\nreturn s.d;\n}\n\nfun(0)\nfun(1)\nfun(2)\nfun(3)\nfun(4)\n\n->3.1400000000\n->3.1400000000\n->3.1399998665\n->2.0000006104\n->Segmentation fault\n\nResult is system specific\n7\n\n\fMemory Referencing Bug Example\nfun(0)\nfun(1)\nfun(2)\nfun(3)\nfun(4)\nfun(8)\n\ntypedef struct {\nint a[2];\ndouble d;\n} struct_t;\n\nMemory:\n\nstruct_t\n\n???\n\n8\n\nCritical State\n\n7\n\nCritical State\n\n6\n\nCritical State\n\n5\n\nCritical State\n\n4\n\nd7 ... d4\n\n3\n\nd3 ... d0\n\n2\n\na[1]\n\n1\n\na[0]\n\n0\n\n->3.1400000000\n->3.1400000000\n->3.1399998665\n->2.0000006104\n->Segmentation fault\n->3.1400000000\n\nLocation accessed by\nfun(i)\n\n8\n\n\fBuffer Overflow\n• Traditional Linux memory layout provide\nopportunities for malicious programs\n• Stack grows “backwards” in memory\n• Data and instructions both stored in the same memory\n\n• Recall that C does not check array bounds\n• Many Unix\/Linux\/C functions don’t check argument sizes\n• Allows overflowing (writing past the end) of buffers\n(arrays)\n\n9\n\n\fBuffer Overflow (cont.)\n• Buffer overflows on the stack can overwrite “interesting”\ndata\n• Attackers just choose the right inputs\n\n• Simplest form (sometimes called “stack smashing”)\n• Unchecked length on string input into bounded array causes\noverwriting of stack data\n• Try to change the return address of the current procedure\n\n• Why is this a big deal?\n• It is (was?) the #1 technical cause of security vulnerabilities\n• #1 overall cause is social engineering \/ user ignorance\n\n10\n\n\fString Library Code\n• Implementation of Unix function gets()\n\/* Get string from stdin *\/\nchar* gets(char* dest) {\nint c = getchar();\nchar* p = dest;\nwhile (c != EOF && c != '\\n') {\n*p++ = c;\nc = getchar();\n}\n*p = '\\0';\nreturn dest;\n}\n\npointer to start\nof an array\nsame as:\n*p = c;\np++;\n\nWhat could go wrong in this code?\n11\n\n\fString Library Code\n• Implementation of Unix function gets()\n\/* Get string from stdin *\/\nchar* gets(char* dest) {\nint c = getchar();\nchar* p = dest;\nwhile (c != EOF && c != '\\n') {\n*p++ = c;\nc = getchar();\n}\n*p = '\\0';\nreturn dest;\n}\n\n• No way to specify limit on number of characters to read\n• Similar problems with other Unix functions:\n• strcpy: Copies string of arbitrary length to a dst\n• scanf, fscanf, sscanf, when given %s specifier\n12\n\n\fVulnerable Buffer Code\n\/* Echo Line *\/\nvoid echo() {\nchar buf[8];\ngets(buf);\nputs(buf);\n}\n\nCode example to try on Thoth!\nhttps:\/\/bit.ly\/3eWwndQ\n\/* Way too small! *\/\n\n BTW, how big\nis big enough?\n\nvoid call_echo() {\necho();\n}\n\nunix> .\/buf-nsp\nEnter string: 12345678901234567890123\n12345678901234567890123\nunix> .\/buf-nsp\nEnter string: 123456789012345678901234\nSegmentation Fault\n\n\fBuffer Overflow Disassembly\n24 bytes (decimal)\n\necho:\n000000000040069c <echo>:\n40069c: 48 83 ec 18\n4006a0: 48 89 e7\n4006a3: e8 a5 ff ff ff\n4006a8: 48 89 e7\n4006ab: e8 50 fe ff ff\n4006b0: 48 83 c4 18\n4006b4: c3\n\n$0x18,%rsp\n%rsp,%rdi\n40064d <gets>\n%rsp,%rdi\n400500 <puts@plt>\n$0x18,%rsp\n\nreturn address\n\ncall_echo:\n4006b5:\n4006b9:\n4006be:\n4006c3:\n4006c7:\n\nsub\nmov\ncallq\nmov\ncallq\nadd\nretq\n\n48 83 ec 08\nb8 00 00 00 00\ne8 d9 ff ff ff\n48 83 c4 08\nc3\n\nsub\nmov\ncallq\nadd\nretq\n\n$0x8,%rsp\n$0x0,%eax\n40069c <echo>\n$0x8,%rsp\n\n\fBuffer Overflow Stack Example\nBefore call to gets\nStack Frame\nfor call_echo\n\nReturn Address\n(8 bytes)\n\n\/* Echo Line *\/\nvoid echo()\n{\nchar buf[4];\ngets(buf);\nputs(buf);\n}\n\n\/* Way too small! *\/\n\n20 bytes unused\n\n[3] [2] [1] [0] buf\n\n%rsp\n\necho:\nsubq $0x18, %rsp\nmovq %rsp, %rdi\ncall gets\n. . .\n15\n\n\fBuffer Overflow Stack Example\nBefore call to gets\nStack Frame\nfor call_echo\n\n00 Address\n00 00\n00\nReturn\n00 (8\n40bytes)\n06 c3\n\n20 bytes unused\n\n[3] [2] [1] [0] buf\n\nvoid echo()\n{\nchar buf[4];\ngets(buf);\n. . .\n}\n\necho:\nsubq $0x18, %rsp\nmovq %rsp, %rdi\ncall gets\n. . .\n\ncall_echo:\n. . .\n4006be:\n4006c3:\n. . .\n\ncallq\nadd\n\n4006cf <echo>\n$0x8,%rsp\n\n%rsp\n\n16\n\n\fBuffer Overflow Stack Example #1\nAfter call to gets\nStack Frame\nfor call_echo\n\n00\n00 Address\n00 00\nReturn\n00 (8\n40bytes)\n06 c3\n00 32 31 30\n39 38 37 36\n35\n34 unused\n33 32\n20 bytes\n31 30 39 38\n37 36 35 34\n33 32 31 30 buf\n\nvoid echo()\n{\nchar buf[4];\ngets(buf);\n. . .\n}\n\necho:\nsubq $0x18, %rsp\nmovq %rsp, %rdi\ncall gets\n. . .\n\ncall_echo:\n. . .\n4006be:\n4006c3:\n. . .\n\n%rsp\n\ncallq\nadd\n\n4006cf <echo>\n$0x8,%rsp\n\nunix>.\/bufdemo-nsp\nType a string:01234567890123456789012\n01234567890123456789012\n\n“01234567890123456789012\\0”\n\nOverflowed buffer, but did not corrupt state\n\n17\n\n\fBuffer Overflow Stack Example #2\nAfter call to gets\nStack Frame\nfor call_echo\n\n00\n00 Address\n00 00\nReturn\n00 (8\n40bytes)\n06 00\n33 32 31 30\n39 38 37 36\n35\n34 unused\n33 32\n20 bytes\n31 30 39 38\n37 36 35 34\n33 32 31 30 buf\n\nvoid echo()\n{\nchar buf[4];\ngets(buf);\n. . .\n}\n\necho:\nsubq $0x18, %rsp\nmovq %rsp, %rdi\ncall gets\n. . .\n\ncall_echo:\n. . .\n4006be:\n4006c3:\n. . .\n\n%rsp\n\ncallq\nadd\n\n4006cf <echo>\n$0x8,%rsp\n\nunix>.\/bufdemo-nsp\nType a string:012345678901234567890123\n012345678901234567890123\nSegmentation fault\n\nProgram “returned” to 0x0400600, and then crashed.\n\n18\n\n\fBuffer Overflow Example #2 Explained\nAfter return from echo\n0000000000400500 <deregister_tm_clones>:\nStack frame for\n400500: mov\n$0x60104f,%eax\ncall_echo\n400505: push\n%rbp\n⟵%rsp\n400506: sub\n$0x601048,%rax\n40050c: cmp\n$0xe,%rax\n00 00 00 00\n400510: mov\n%rsp,%rbp\n00 40 05 00\n400513: jbe\n400530\n34 33 32 31\n400515: mov\n$0x0,%eax\n40051a: test\n%rax,%rax\n30 39 38 37\n40051d: je\n400530\n36 35 34 33\n40051f: pop\n%rbp\n32 31 30 39\n400520: mov\n$0x601048,%edi\n400525: jmpq\n*%rax\n38 37 36 35\n400527: nopw\n0x0(%rax,%rax,1)\n34 33 32 31 buf\n40052e: nop\n400530: pop\n%rbp\n400531: retq\n“Returns” to unrelated code, but continues!\nEventually segfaults on retq of deregister_tm_clones.\n19\n\n\fMalicious Use of Buffer Overflow: Code Injection Attacks\nStack after call to gets()\nHigh Addresses\n\nvoid foo(){\nbar();\nA:...\n}\nint bar() {\nchar buf[64];\ngets(buf);\n...\nreturn ...;\n}\n\nfoo stack frame\nreturn address A\nAB\n(return address)\ndata written\nby gets()\nbuf starts here\n\nB\n\npad\n\nbar stack frame\n\nexploit\ncode\n\nLow Addresses\n\n• Input string contains byte representation of executable code\n• Overwrite return address A with address of buffer B\n• When bar() executes ret, will jump to exploit code\n\n20\n\n\fExploits Based on Buffer Overflows\n• Buffer overflow bugs can allow remote machines to execute arbitrary code on\nvictim machines\n• Distressingly common in real programs\n• Programmers keep making the same mistakes \n• Recent measures make these attacks much more difficult\n\n• Examples across the decades\n• Original “Internet worm” (1988)\n• Still happens!!\n• Heartbleed (2014, affected 17% of servers)\n• Cloudbleed (2017)\n\n• Fun: Nintendo hacks\n• Using glitches to rewrite code: https:\/\/www.youtube.com\/watch?v=TqK-2jUQBUY\n• FlappyBird in Mario: https:\/\/www.youtube.com\/watch?v=hB6eY73sLV0\n\n\fExample: the original Internet worm (1988)\n• Exploited a few vulnerabilities to spread\n• Early versions of the finger server (fingerd) used gets() to read the argument\nsent by the client:\n• finger droh@cs.cmu.edu\n\n• Worm attacked fingerd server with phony argument:\n• finger “exploit-code padding new-return-addr”\n• Exploit code: executed a root shell on the victim machine with a direct TCP connection to the\nattacker\n\n• Scanned for other machines to attack\n• Invaded ~6000 computers in hours (10% of the Internet)\n• see June 1989 article in Comm. of the ACM\n\n• The young author of the worm was prosecuted…\n22\n\n\fHeartbleed (2014)\n• Buffer over-read in OpenSSL\n• Open source security library\n• Bug in a small range of versions\n\n• “Heartbeat” packet\n• Specifies length of message\n• Server echoes it back\n• Library just “trusted” this length\n• Allowed attackers to read contents\nof memory anywhere they wanted\n\n• Est. 17% of Internet affected\n• “Catastrophic”\n• Github, Yahoo, Stack Overflow,\nAmazon AWS, ...\n\nBy FenixFeather - Own work, CC BY-SA 3.0,\nhttps:\/\/commons.wikimedia.org\/w\/index.php?curid=32276981\n\n23\n\n\fCrafting Smashing String\nint echo() {\nchar buf[4];\ngets(buf);\n...\nreturn ...;\n}\n\nStack Frame\nfor call_echo\n\n07 00\nFF\n00\n00 Address\n00\nReturn\n00\n40\n06\nFF (8\nFFbytes)\nAB c3\n80\n33 32 31 30\n39 38 37 36\n35\n34 unused\n33 32\n20 bytes\n31 30 39 38\n37 36 35 34\n33 32 31 30\nAttack String (Hex)\n\n%rsp\n\n24 bytes\n\nTarget Code\nvoid smash() {\nprintf(\"I've been smashed!\\n\");\nexit(0);\n}\n00000000004006c8 <smash>:\n4006c8:\n48 83 ec 08\n\n30 31 32 33 34 35 36 37 38 39 30 31 32 33 34 35 36 37 38 39 30 31 32 33\nc8 06 40 00 00 00 00 00\n\n\fSmashing String Effect\nStack Frame\nfor call_echo\n\n07 00\nFF\n00\n00 Address\n00\nReturn\n00\n40\n06\nFF (8\nFFbytes)\nAB c8\n80\n33 32 31 30\n39 38 37 36\n35\n34 unused\n33 32\n20 bytes\n31 30 39 38\n37 36 35 34\n33 32 31 30\nAttack String (Hex)\n\n%rsp\n\nTarget Code\nvoid smash() {\nprintf(\"I've been smashed!\\n\");\nexit(0);\n}\n00000000004006c8 <smash>:\n4006c8:\n48 83 ec 08\n\n30 31 32 33 34 35 36 37 38 39 30 31 32 33 34 35 36 37 38 39 30 31 32 33\nc8 06 40 00 00 00 00 00\n\n\fPerforming Stack Smash\nCode example to try on Thoth!\nhttps:\/\/bit.ly\/3eWwndQ\n\n• Put hex sequence in file smash-hex.txt\n• Use hexify program to convert hex digits to\ncharacters\n• Some of them are non-printing\n\n• Provide as input to vulnerable program\n$ cat smash-hex.txt\n30 31 32 33 34 35 36 37 38 39 30 31 32 33 34 35 36 37 38 39 30 31 32 33\nc8 06 40 00 00 00 00 00\n$ cat smash-hex.txt | .\/hexify | .\/bufdemo-nsp\nType a string:012345678901234567890123?@\nI've been smashed!\nvoid smash() {\nprintf(\"I've been smashed!\\n\");\nexit(0);\n}\n\n26\n\n\fCode Injection Attacks\nStack after call to gets()\nvoid P(){\nQ();\nA: ...\n}\n\nP stack frame\n\nReturn address A\n\nint Q() {\nchar buf[64];\ngets(buf);\n...\nreturn ...;\n}\n\nB\nABA\ndata written\nby gets()\nB\n\npad\n\nexploit\ncode\n\nQ stack frame\n\n• Input string contains byte representation of executable code\n• Overwrite return address A with address of buffer B\n• When Q executes ret, will jump to exploit code\n\n27\n\n\fHow Does The Attack Code Execute?\nrip\nvoid P(){\nQ();\n...\n}\n\nStack\nrsp\nrsp\n\nrsp\nShared\nLibraries\n\nret\n\nBB\nAA\npad\n\nret\nint Q() {\nchar buf[64];\ngets(buf); \/\/ A->B\n...\nreturn ...;\n}\n\n…\n\nrip\nrip\n\nexploit\ncode\n\nHeap\n\nrip\nrip\n\nData\nText\n28\n\n\fDealing with buffer overflow attacks\n\n1) Avoid overflow vulnerabilities\n2) Employ system-level protections\n3) Have compiler use “stack canaries”\n\n30\n\n\f1) Avoid Overflow Vulnerabilities in Code\n• Use library routines that limit string lengths\n• fgets instead of gets (2nd argument to fgets sets limit)\n• strncpy instead of strcpy\n• Don’t use scanf with %s conversion specification\n• Use fgets to read the string\n• Or use %ns where n is a suitable integer\n\/* Echo Line *\/\nvoid echo()\n{\nchar buf[8]; \/* Way too small! *\/\nfgets(buf, 8, stdin);\nputs(buf);\n}\n31\n\n\f2) System-Level Protections\nHigh Addresses\n\n• Randomized stack offsets\n• At start of program, allocate random amount of space on stack\n• Shifts stack addresses for entire program\n\nRandom\nallocation\nmain’s\nstack frame\n\n• Addresses will vary from one run to another\n\n• Makes it difficult for hacker to predict beginning of inserted code\n\nOther\nfunctions’\nstack frames\n\n• Example: Code from Slide 6 executed 5 times; address of variable\nlocal =\n•\n•\n•\n•\n•\n\n0x7ffd19d3f8ac\n0x7ffe8a462c2c\n0x7ffe927c905c\n0x7ffefd5c27dc\n0x7fffa0175afc\n\n• Stack repositioned each time program executes\n\nB?\npad\n\nB?\n\nexploit\ncode\nLow Addresses\n\n32\n\n\f2) System-Level Protections\nStack after call\nto gets()\n\n• Non-executable code segments\n\nfoo\nstack\nframe\n\n• In traditional x86, can mark region of\nmemory as either “read-only” or “writeable”\n• Can execute anything readable\n\n• x86-64 added explicit “execute” permission\n• Stack marked as non-executable\n• Do NOT execute code in Stack, Static Data, or\nHeap regions\n• Hardware support needed\n\nB\ndata written\nby gets()\nB\n\npad\nexploit\ncode\n\nbar\nstack\nframe\n\nAny attempt to execute this code will fail\n33\n\n\f3) Stack Canaries\n• Basic Idea: place special value (“canary”) on stack just beyond buffer\n• Secret value known only to compiler\n• “After” buffer but before return address\n• Check for corruption before exiting function\n\n• GCC implementation (now default)\n• -fstack-protector\n• Code back on Slide 14 (buf-nsp) compiled with\n–fno-stack-protector flag\n\nunix>.\/buf\nEnter string: 12345678\n12345678\n\nunix> .\/buf\nEnter string: 123456789\n*** stack smashing detected ***\n\n34\n\n\fProtected Buffer Disassembly (buf)\necho:\n400638:\n40063c:\n400645:\n40064a:\n...\n400656:\n400659:\n40065e:\n400661:\n400666:\n40066b:\n400674:\n400676:\n40067b:\n40067f:\n\nsub\n$0x18,%rsp\nmov\n%fs:0x28,%rax\nmov\n%rax,0x8(%rsp)\nxor\n%eax,%eax\n... call printf ...\nmov\n%rsp,%rdi\ncallq 400530 <gets@plt>\nmov\n%rsp,%rdi\ncallq 4004e0 <puts@plt>\nmov\n0x8(%rsp),%rax\nxor\n%fs:0x28,%rax\nje\n40067b <echo+0x43>\ncallq 4004f0 <__stack_chk_fail@plt>\nadd\n$0x18,%rsp\nretq\n\n\fSetting Up Canary\nBefore call to gets\nStack frame for\ncall_echo\nReturn address\n(8 bytes)\n\nCanary\n(8 bytes)\n[7] [6] [5] [4]\n\n\/* Echo Line *\/\nvoid echo()\n{\nchar buf[8]; \/* Way too small! *\/\ngets(buf);\nputs(buf);\n}\nSegment register\n(don’t worry about it)\necho:\n. . .\nmovq\n%fs:40, %rax\n# Get canary\nmovq\n%rax, 8(%rsp)\n# Place on stack\nxorl\n%eax, %eax\n# Erase canary\n. . .\n\n[3] [2] [1] [0] buf ⟵%rsp\n\n36\n\n\fChecking Canary\nAfter call to gets\nStack frame for\ncall_echo\nReturn address\n(8 bytes)\n\nCanary\n(8 bytes)\n\n00 37 36 35\n34 33 32 31\n\n\/* Echo Line *\/\nvoid echo()\n{\nchar buf[8];\ngets(buf);\nputs(buf);\n}\necho:\n. . .\nmovq\nxorq\nje\ncall\n.L6:\n\n\/* Way too small! *\/\n\n8(%rsp), %rax\n%fs:40, %rax\n.L2\n__stack_chk_fail\n. . .\n\n# retrieve from Stack\n# compare to canary\n# if same, OK\n# else, FAIL\n\nbuf ⟵%rsp\n\nInput: 1234567\n37\n\n\fSummary\n\n1) Avoid overflow vulnerabilities\n•\n\nUse library routines that limit string lengths\n\n2) Employ system-level protections\n•\n•\n\nRandomized Stack offsets\nCode on the Stack is not executable\n\n3) Have compiler use “stack canaries”\n\n38\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":176,"segment": "unlabeled", "course": "cs0449", "lec": "lec10", "text":"10\n\nHow\nPrograms\nAre Made\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fLinkers\nFilling in the blanks.\n\n2\n\n\fCompilation: Simple Overview – Step 1\n• The compiler takes source code (\nfiles) and\ntranslates them into machine code.\n\nhello.c\n\nhello.o\n\n• This file is called an “object file” and is just\npotentially one part of your overall project.\n\n• The machine code is not quite an executable.\n• This object file is JUST representing the code for that\nparticular source file.\n• You may require extra stuff provided by the system\nelsewhere.\n3\n\n\fCompilation: Simple Overview – Step 2\n• You may have multiple files.\n• They may reference each other.\n\nhello.c\n\nhello.o\n\n• For instance, one file may contain certain common\nfunctionality and then this is invoked by your program\nelsewhere.\n\n• You break your project up into pieces similarly to\nyour Java programs.\n• The compiler treats them independently.\nutil.c\nCS\/COE 0449 – Spring 2019\/2020\n\nutil.o\n4\n\n\fCompilation: Simple Overview – Step 3\n• Then, each piece is merged\ntogether to form the\nexecutable.\n• This process is done by a linker\nand is called linking.\nhello.c\n\nhello.o\n\nhello\n\n• The name refers to how the\nreferences to functions, etc,\nbetween files are now filled in.\n\nutil.c\nCS\/COE 0449 – Spring 2019\/2020\n\nutil.o\n\nstdio.o\nExternal Libraries\n\n• Before this step… it is unclear\nwhere functions will end up in\nthe final executable.\n5\n\n\fIt's just a grinder.\n• In summary:\n\nhello.c\n\ncode goes in, sausage object\nfiles come out\nThe executable is produced\nby a linker, which merges\ncode together.\n\nSome compilers output\nassembly and rely on an\nassembler to produce\nmachine code\nThese days, it's common\nfor the compiler itself to\nproduce machine code,\nor some kind of\nplatform-independent\nassembly code\n(typically: a bytecode)\n6\n\n\fCompiler\n• Input: Higher-level language code (e.g. C, Java)\n• foo.c\n\n• Output: Assembly language code (e.g. x86, ARM, MIPS)\n• foo.s\n\n• First there’s a preprocessor step to handle #directives\n• Macro substitution, plus other specialty directives\n• If curious\/interested: http:\/\/tigcc.ticalc.org\/doc\/cpp.html\n\n• Compiler optimizations\n• “Level” of optimization specified by capital ‘O’ flag (e.g. -Og, -O3)\n• Options: https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Optimize-Options.html\n7\n\n\fCompilers Are Non-Trivial\n• There’s a whole course about them – CS 1622\n• We won’t go into much detail in this course\n• For the very curious: http:\/\/www.sigbus.info\/how-i-wrote-a-self-hosting-ccompiler-in-40-days.html\n\n• Some examples of the task’s complexity:\n• Operator precedence: 2 + 3 * 4\n• Operator associativity: a = b = c;\n• Determining locally whether a program is valid\nif (a) { if (b) { … \/*long distance*\/ … } } }\/\/extra bracket\n\n8\n\n\fThe need for the linker\n• A compiler converts source code into machine code.\n• A linker merges pieces of machine code into an executable.\n• Why have a separate tool for creating executables?\n• Mixing different languages together (C, C++, Python, Rust, Go…)\n• Lot’s of complications we won’t get to here.\n• Assembly is the glue… all high-level languages have to get there.\n\n• Let’s us break large programs up into smaller pieces.\n• And we only have to recompile files that changed! (Faster)\n\n• Those small pieces can come from others. Code reuse!\n• We can share executable code among many running programs. (Shared Libraries)\n\n9\n\n\fWhat is inside that box?\n• To understand what linkers do, we\nneed to see what an executable is\nmade out of.\n(Spoilers: it is not just code\/data)\n• A Linux executable is defined by\nthe Executable and Linkable\nFormat (ELF) standard.\n• Used for\nfiles\n• And executables\n• And\n(shared objects; soon!)\n10\n\n\fObject File Format\n1. object file header: size and position of the other pieces of the object\nfile\n2. text segment: the machine code\n3. data segment: data in the source file (binary)\n4. relocation table: identifies lines of code that need to be “handled”\n5. symbol table: list of this file’s labels and data that can be referenced\n6. debugging information\n• A standard format is ELF (except MS)\n• http:\/\/www.skyfree.org\/linux\/references\/ELF_Format.pdf\n\n\fObject File Information Tables\n• Symbol Table holds list of “items” that may be used by other files\n• Non-local labels – function names for call\n• Static Data – variables & literals that might be accessed across files\n\n• Relocation Table holds list of “items” that this file needs the address of later\n(currently undetermined)\n• Any label or piece of static data referenced in an instruction in this file\n• Both internal and external\n\n• Each file has its own symbol and relocation tables\n\n12\n\n\fWhat the ELF ??\n• Contains all of the segments and data sections defining a program.\n• The ELF executable has roughly the following structure:\nOffset\n\nName\n\nDescription\n\nMagic Number\n\n4 bytes: A\n\nClass\n\n1 byte:\n\nif 32-bit, 0x2 if 64-bit\n\nData\n\n1 byte:\n\nif little-endian, 0x2 if big-endian\n\nVersion\n\n1 byte:\n\nfor the current version.\n\nABI\n\n1 byte:\n\nfor System V (our C ABI)\n\nMachine\n\n2 bytes:\n\nbyte followed by “\n\nis x86,\n\nis MIPS,\n\n” in ASCII\n\nis RISC-V, etc\n\n13\n\n\fWhat the ELF ??\n• The remaining fields indicate where certain sections start.\n• An ELF executable contains these sections:\n• Segment Headers (where .text, .data, .bss, etc, exist in the executable)\n• The initial data for each memory segment in the memory layout!\n• We will look at these again when we look at loading.\n\n• The Symbol Table\n• All of the “names” that may be referenced by other code.\n\n• Symbols can consist of:\n• Functions\n• Global variables\n• Special sections (special compiler or OS areas)\n\n• We will focus on function\/variable symbols.\n14\n\n\f– Viewing the symbol table\n• You can investigate the symbols that are part of any object file\nusing the\ncommand on Linux\/UNIX.\nC(\n\n)\n\nThis is a symbol. It has a location.\n\nHere it is! At 0x27 (39) bytes.\n\n15\n\n\f; Controlling the symbols\n• Remember the\n\nkeyword?\n\n• This forces any symbol to be local to the current file. That is, it can\nnot be referenced by an outside function.\n• This is because the symbol will not be included in the symbol table!\n• The linker will not be able to see it.\n\n• This is useful for avoiding name collisions, when two functions have\nthe same name.\n• This normally would make using multiple files and other people’s code\ntroublesome.\n• Using\nhelps because it will not pollute the symbol table.\n16\n\n\fControlled the symbols\n• You can investigate the impact of using\ncommand on Linux\/UNIX.\nC(\n\nby again using the\n\n)\nThis symbol has a location… but it can\nonly be referenced in this file.\n\nOur static function is now “LOCAL”\n\n17\n\n\f; when you used to be an intern\n• The other side of the coin is the\nkeyword.\n• This tells the linker that it should expect the symbol to be found\nelsewhere.\nC(\n\n)\n\nC(\n\n)\n\nHere it is!!\n\nThis symbol is… somewhere.\n\n18\n\n\fFinal thoughts of global variables\n• You should always avoid global variables.\n• However, if you are using them, make sure to liberally use\n• This will stop the names of variables from polluting the symbol table.\n• The use of\nis likely indicating a poor design.\n\n• This is also true for functions, too.\n• Generally declare them\nunless you need\nthem from within another file.\n• Helps make it clear what functions are important\nand which can be deleted or refactored.\n• (Much like private functions in classes)\n\n• Always initialize your global variables!\n\n19\n\n\fSeeing through the linker’s eyes\n• Which symbols are part of each file?\n• Which are local and which are global?\n• Which symbols are satisfied by the other file?\nC(\n\n)\n\nA local symbol.\n\nA global symbol.\n\nReferenced here.\n\nC(\n\n)\n\nThe linker references\n“main” when it compiles\nthe executable.\n\nWe need to tell the compiler\nthat we are linking to a symbol.\n\nA global symbol.\nLinker doesn’t see these\ntemporary variables.\n\nReferenced here.\n\n20\n\n\fSumming it up: Playing mad-libs\n\nfibonacci.c\n\nfibonacci.o\n\nfibonacci\n\n0x007c0e10\n???\n\nmain.c\n\n• The compiler hands\noff object files with\nblanks where\nreferenced symbols\nreside.\n• The linker’s job is to\nfill in those blanks\nwith the location of\nthe symbol in the\nfinal executable.\n\nmain.o\n21\n\n\fStatic Libraries (\n\nfiles)\n• If you want to share\nyour library with\nothers…\n\nutil.c\n\ntree.c\n\nutil.o\n\ntree.o\n\nmy-lib.a\n\n• Instead of creating an\nexecutable, you can\npackage together all of\nthe\nfiles into a\nsingle archive (\nfile)\n• You can use the\nprogram on Linux for\nthis.\n22\n\n\fCompilation: Simple Overview – Redux\n• We can use my-lib.a in\nplace of the object files\nwe need.\n\nhello.c\n\nutil.c\nCS\/COE 0449 – Spring 2019\/2020\n\nhello.o\n\nutil.o\n\nhello\n\nmy-lib.a\nExternal Libraries\n\n• The\nfile is just a\ncontainer for a set of\nobject files. Essentially,\nit is just a kind of zip\nfile of object files.\n• These object files get\ncopied into our\nexecutable… not very\nefficient! Hmm!\n23\n\n\fLoaders\nYou should always stretch before you run – OSes do this, too.\n\n24\n\n\fThe Operating System\n• How does your ELF executable actually\nrun?\n• There needs to be some system software\nto unpack the executable into memory.\n\n• That system software is a loader and it is\npart of an operating system.\n\n25\n\n\fMemory Segments – Deeper dive!\n• The ELF executable defines several segments:\n•\n•\n•\n•\n\n– The code segment (machine code)\n– The data segment (program data)\n– The read-only data segment (constants)\n– Uninitialized data segment (“zero” data)\n\n• The\nsegment is a special segment for all data\nthat starts as\nor\n.\n• (Its name is Block Started by Symbol which is a historic\nname.)\n• It is often an optimization: the executable does not need to\nstore a whole bunch of zeros.\n• Hmm… the operating system must then allocate a bunch of\nzeros. Is that fast?? (We’ll get there)\n\nKernel Memory\nstack\ncurrently unused but\navailable memory\n\nheap\n.bss\n.data\n\n.text\n26\n\n\fRunning a program\n1. Take the ELF executable.\n\nOur lie starts to unravel!\nWe have a kernel…\n\n• This defines each segment and where in memory\nit should go.\n\n2. Place the\nsegment into memory.\n3. Place the\nsegment into memory.\n4. Write the number of zeroes specified to the\nsegment.\n5. Allocate the stack and assign the stack\npointer (\n)\n6. Jump to the entry point address (the\nlocation of the\nsymbol)\n•\n\nwill call main after initializing the C\nruntime and the heap.\n\nKernel Memory\nstack\ncurrently unused but\navailable memory\n\nheap\n.bss\n.data\n\n.text\n27\n\n\fSome .bss BS I’ve dealt with…\n• Forgetting to zero the .bss segment is… very interesting.\n• If you write an OS, and forget this, then you get loops that don’t work write.\n• Because now variables that were equal to\nare now random garbage.\nC(\n\n)\n\nThis goes into the .bss because it is zero\n\nThis does not go into the .bss because it is not a symbol.\n\n28\n\n\fThat’s it???\n• Pretty much! However, let’s make it more flexible.\n• Our linking so far is static linking where all of the code goes into\nthe executable. Duplicate code from static libraries is copied in.\n• Not very space efficient. Duplicates code most programs are using! (libc)\n• What if we “shared” the code external to the executable?\n\n• For dynamic linking we will think about loading not just the\nexecutable, but library code as well. A shared library.\n• The OS loader must load the program into memory and also take on the\ntask of loading library code.\n• It then must do the “mad-libs” replacing references in the program to\npoint to where in memory the library code was loaded. Tricky!\n29\n\n\fDynamic Linking\nLinking but… yanno… animated.\n\n30\n\n\fCode that can be loaded… anywhere?\n• The main problem is this:\n• Programs generally need to assume where in\nmemory they live.\n\n• They refer to functions and data at particular\naddresses.\n• The linker decides where those are, but they are\nthen hard-coded in.\nWhere should this go??\n\n• We want to provide a single software library\nto multiple executables…\n• We can’t know ahead of time where that library\ncan go in memory since programs are different\nsizes… they might need multiple libraries… etc.\nCS\/COE 0449 – Spring 2019\/2020\n\nKernel Memory\nstack\nlibz.so .data\nlibz.so .text\n\n.bss\n.data\n\n.text\n31\n\n\fSolution: relocatable code\n• Let’s allow code to refer to functions and\/or data that may move.\n• Essentially, the operating system plays the mad-lib game.\n• The ELF executable has a list of “relocatable entries”\n• The OS goes through them and fills them in according to where the external\nsymbols are.\nLinking to the libz.so dynamic library\n\nC(\n\n)\n\nWe don’t know where this function ultimately is…\n\n32\n\n\fSolution: relocatable code\n• Let’s allow code to refer to functions and\/or data that may move.\n• Essentially, the operating system plays the mad-lib game.\n• The ELF executable has a list of “relocatable entries”\n• The OS goes through them and fills them in according to where the external symbols are.\nC(\n\n)\n\nx86-64 (\n\n)\n\n33\n\n\fSolution: relocatable code: Loading\n• When the OS loads this executable… it will have a relocation entry that tells\nit to overwrite at byte\nthe relative address of “compressBound”\n• With this extra step, the OS loader is also providing dynamic linking.\nC(\n\n)\n\nx86-64 (\n\n)\n\n0x114b + 0x5fe = 0x1749\n(\nis relative to\n)\nIn modern times, this makes use of a jump table\ncalled\nProcedure\nLinkage Table (PLT).\nCS\/COEa\n0449\n– Spring 2019\/2020\n\n34\n\n\fTaking a PIC, eating some PIE – Avoiding relocations\n• In order to allow code to be resident\nanywhere in memory, the compiler must\nemit machine code that always uses\nrelative addresses!\n\n• This is called position independent\ncode (or PIC).\n• When your entire executable is made\nout of PIC, it is a position independent\nexecutable (or PIE)\n•\n\nwill compile code this way when\nyou specify the\nflag.\n\nWho doesn’t like pie???\n\n• You generally need this when creating\ndynamic libraries.\n\n35\n\n\fRunning a program - Redux\n1. Take the ELF executable.\n2. Place and initially prepare the\nsegments into memory.\n5. Allocate the stack and assign the stack pointer\n(\n)\n6. Repeatably load each required shared library.\n6a. Place .text and .data in memory\n6b. Rewrite .text sections by looking at the\nrelocatable entries\n6c. Repeat for each library.\n7. Jump to the entry point address (the location of\nthe\nsymbol)\n•\n\nwill call main after initializing the C\nruntime and the heap.\n\nKernel Memory\nstack\nlibz.so .data\nlibz.so .text\n\nheap\n.bss\n.data\n\n.text\n36\n\n\fBeing lazy – Run-time loading\n• Having the OS load every library at\nthe start can delay the execution\nof a program.\n• What if your program rarely uses a\nlibrary?\n• What if you want to expand the\nprogram while it is running?\n\nC(\n\n)\n\nFunction pointers are\nvery messy.\n\n• Plugins are a good example.\n\n• We can make use of an OS service\nto dynamically load libraries.\n• On Linux we have the\nand\nsystem functions.\n• Look at the documentation online\nand refer to examples.\n\nPrints to the screen’s “error” buffer.\n\nUses the lazy-loaded function.\n\n37\n\n\fInvestigating dynamic libraries\n• If you would like to see what dynamic libraries a program uses, you\ncan use\nor the\ncommand.\n• Cannot see the\n\n\/\n\nlazy loaded libraries.\n\n•\n•\n\n38\n\n\fLinking, loading; static and dynamic… Whew!\n• Linking is when we merge multiple pieces of executable code into one\nlogical program.\n• We link at various times:\n• At compile-time: using our normal\n\nfiles and static libraries (\n\n)\n\n• At load-time: our OS reads and loads the executable and loads dynamic libraries\n(\n) at the same time, rewriting relocatable sections.\n• At run-time: our program uses system services (\nlibraries lazily.\n\n) to load dynamic\n39\n\n\fSoftware Licensing\nCombining code\n\n40\n\n\fSo derivative…\n• Software is generally built from existing software.\n• Only building bespoke programs would be impractical.\n\n• However, how do we negotiate such usage of software?\n• And how does this impact the design of systems?\n\n• Disclaimer: I am not a lawyer.\n• But… neither will you be one…\n• And yet, we often find the need to be.\n\n41\n\n\fUNIX\n• The UNIX system, which Linux is somewhat modeled after, was\noriginally from American Telephone & Telegraph (AT&T).\n• Hence lots of AT&T intellectual residue\n\n• Because of an anti-trust case from 1956, they entered a “consent\ndecree” with the government.\n• AT&T could not sell anything that does\nnot pertain to “common carrier\ncommunications services” (telephony)\n• Therefore, UNIX, their product, could\nnot be profitable\n• ANTI-TRUST WORKING?? WHAT A TIME!\n\n• Due to this, UNIX was shipped out\nmostly for a low cost; with source!\n• Hardware was the money maker.\n\n42\n\n\fThe bell tolls…\n• AT&T often flirted with what they could get away with, yet in 1983,\nthe U.S. government broke up the “Bells” that made up AT&T.\n• This freed AT&T from the decree.\n• And allowed them to commercialize UNIX.\n\n• The source code for UNIX became less\nand less available.\n▪ It came at a high cost… and not available to\nthe average user.\n▪ This motivated many engineers\/researchers\nto organize.\n\n43\n\n\fA GNU world…\n• Researchers, lulled into a world where systems software was opensource, felt disenfranchised by such new corporate policies.\n• Quite hard to research system design without easy access to the system.\n\n• Richard Stallman created the GNU\n(GNU is Not Unix) Project.\n• An effort to replace UNIX and other systems\nsoftware with community-built versions.\n• He creates the GNU C Compiler (gcc)\n• Organized around the Free Software\nFoundation (FSF)\n\n• He licenses the work under the GPL.\n• GNU Public License later GNU General\nPublic License\n44\n\n\fGPL: Free Software Movement\n• The GPL has evolved over time to legally enforce several things:\n• The work can be freely studied. (open source)\n• The work can be freely modified.\n• The work can be freely copied\/distributed with\/without changes.\n\n• These rules apply to all derivative versions. (All modifications)\n• Your modifications require you to distribute the source with your\nprogram.\n• Prevents others from adding substantive changes to divide userbases.\n• Known colloquially as a “copyleft” license or “viral” license.\n\n• This contract is enforced by U.S. and international copyright law.\n• By not using a software license, technically nobody can modify or\ndistribute your code!\n• Current copyright law lasts the life of the author + 70 years.\n• Yikes.\n\n45\n\n\fWhat is derivative?\n• Copyright interacts heavily with systems software.\n• When your program calls a system call, it executes\ncode written by somebody else.\n• Is this copyrighted? (Yes. Everything is.)\n• If it is GPL, does it make your program a derivative?\n• AHHHHH.\n\n• If your compiler, like GCC, is copyleft, is your\nprogram derivative?\n• Generally, no, but runtime code and C standard library\nare GPL.\n\n• Special exceptions must exist for these blurry lines\nbetween systems software and application\nsoftware.\n\nLinux is licensed\nunder the GPL\n46\n\n\fOperating Systems and Copyright Law\n• We’ve seen an executable loader in this lecture.\n• Part of the OS! Obviously derivative to the OS.\n\n• User programs are not considered “derivative” to the OS.\n• They aren’t operating systems… just enabled by them.\n• THANK GOODNESS.\n\n• Linux has a “system call” exception.\n• Use of system calls is never considered “derivative”\n• THANK GOODNESS.\n\n• But some other parts are left very very unclear.\n47\n\n\fThe Problem\n• What about device drivers??\n• Device drivers are small libraries the implement interactions with hardware.\n• They typically run in the operating system’s space.\n• Do they “derive” the OS? (They cannot co-exist… they extend…)\n\n• This is a difficult problem that plagues Linux…\n• They need to support proprietary hardware…\n• At least, not allow large companies software\/hardware exclusivity\n\n• So they then need to wedge in proprietary code…\n• They can’t because it needs to be under the GPL…\n• This is antagonistic to culture… the source cannot be read or modified.\n\n• Are we willing to wait 70-90 years for a device driver’s copyright to lapse?\n48\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":177,"segment": "unlabeled", "course": "cs0441", "lec": "lec18", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #18: Permutations and Combinations\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nn Permutations\nn Combinations\nn Binomial coefficients\n\n\fA permutation is an ordered arrangement of a\nset of objects\nS=\n\nσ(S) =\n\nNote: A permutation of some set is essentially\njust a shuffling of that set.\n\n\fSometimes we’re interested in counting the number of\nways that a given set can be arranged\nExample: Suppose that a photographer wants to take a picture of\nthree dogs. How many ways can the dogs be arranged?\n\nThere are six possible\narrangements of three\ndogs!\n\n\fCounting permutations\nIn general, we can use the product rule to count the\nnumber of permutations of a given set.\nGiven a set of n items, we have:\nl n ways to pick the 1st item in the permuted set\nl n-1 ways to pick the 2nd item in the permuted set\nl n-2 ways to pick the 3rd item in the permuted set\nl…\nl 1 way to choose the last item in the permuted set\n\nSo, for a set of size n, we have n × (n-1) × (n-2) × … × 1\n= n! ways to permute that set\n\n\fAnd the winner is…\nExample: Six friends run in a foot race. How many\npossible outcomes of the race are there, assuming that\nthere are no ties?\n\nSolution:\nl 6 ways to choose 1st place\nl 5 ways to choose 2nd place\nl 4 ways to chose 3rd place\nl 3 ways to choose 4th place\nl 2 ways to choose 5th place\nl 1 way to choose last place\nl So there are 6! = 720 possible outcomes\n\n\fFunctions and permutations\nLet S be some set. If f: S → S is a bijection, then f\ndescribes a permutation of the set S.\n\nExample: Let S = {1, 2, 3}, f(1) = 3, f(2) = 2, f(3) = 1\n\nS=\n\n1\n\n2\n\n3\n\nf(S) =\n\n3\n\n2\n\n1\n\n\fMore often than not, we’re only interested in\narranging a subset of a given set\nDefinition: An r-permutation is a permutation of some\nr elements of a set.\n\nExample: Let S = {Alice, Bob, Carol, Dave}. Then:\nl Dave, Bob is a 2-permutation of S\nl Carol, Alice, Bob is a 3-permutation of S\nl Bob, Dave is a 2-permutation of S\n\nRather than specifying a particular r-permutation of a\nset, we’re usually more interested in counting the\nnumber of r-permutations of a set\n\n\fCounting r-permutations\nDefinition: We denote the number of r-permutations of\na set of size n by P(n, r). By the product rule:\n\n𝑛!\n𝑃 𝑛, 𝑟 = 𝑛 × 𝑛 − 1 × … × 𝑛 − 𝑟 + 1 =\n𝑛−𝑟 !\n\nExample: In a foot race between six people, how many\nways can the gold, silver, and bronze medals be\nassigned, assuming that there are no ties?\n\nSolution:\nl P(6, 3) = 6!\/(6-3)! = 6!\/3! = 720\/6 = 120\nl So, 120 ways to assign medals\n\n\fThe traveling salesperson\nExample: A salesperson must visit 7 different cities.\nThe first and last cities of her route are specified by\nher boss, but she can choose the order of the other\nvisits. How many possible trips can she take?\n\nSolution:\nl Since first and last cities are fixed,\nwe must count the number of ways\nto permute the 5 remaining cities\nl So, there are 5! = 120 possible trips\nthat the salesperson can take.\n\n\fCounting strings\nExample: How many permutations of “ABCDEFG”\ncontain the substring “ABC”?\n\nSolution:\nl Observation: Treat “ABC” as one character\nl Now, how many ways are there to permute\n{ABC, D, E, F, G}?\nl |{ABC, D, E, F, G}| = 5\nl 5! = 120 permutations contain the substring “ABC”\n\n\fHow do we count unordered selections of\nobjects?\nExample: How many ways can we choose two objects\nfrom the set S =\n\nS1 =\n\n=\n\nS2 =\n\n=\n\nS3 =\n\n=\n\nConclusion: There are three 2-combinations of a set\nof size three.\n\n\fCounting r-combinations\nDefinition: Let C(n, r) denote the number of rcombinations of a set of size n. Then:\n𝑛!\n𝐶 𝑛, 𝑟 =\n𝑟! 𝑛 − 𝑟 !\n\nProof:\nl\n\nl\nl\nl\nl\nl\n\nThe r-permutations of the set can be formed by finding all\nof the r-combinations of the set and then permuting each\nr-combination in each possible way\nThere are 𝑃 𝑟, 𝑟 ways to permute each possible rcombination\nSo, 𝑃 𝑛, 𝑟 = 𝐶 𝑛, 𝑟 × 𝑃 𝑟, 𝑟\nThis means that 𝐶 𝑛, 𝑟 = 𝑃 𝑛, 𝑟 \/𝑃 𝑟, 𝑟\n= 𝑛!⁄ 𝑛 − 𝑟 ! ⁄𝑟!\n= 𝑛!⁄ 𝑟! 𝑛 − 𝑟 ! ❏\n\n\fAlternate notation\nNote 1: Sometimes, C(n, r) is read “n choose r”\n\nThis is intuitive, as C(n, r) specifies the number of\nways to choose r objects from a set of size n.\n\nNote 2: C(n, r) is often written as !\" . In this class, I\nwill use the notation C(n, r) exclusively.\n\n\fBe careful using the formula for C(n, r)\nUsing the formula for C(n, r) directly can result in\ndoing lots of multiplication…\nInstead, note that much of the denominator cancels\nout terms in the numerator. For example:\n\n\fPoker Hands\nExample: Given a standard 52-card deck, how many 5\ncard poker hands can be drawn?\n\nSolution:\nl C(52, 5) = 52!\/[5! 47!]\nl\n= (52 × 51 × 50 × 49 × 48)\/(5 × 4 × 3 × 2 × 1)\nl\n= 26 × 17 × 10 × 49 × 12\nl\n= 2,598,960 different hands\n\n\fChoosing participants\nExample: Say that a class consists of 30 students.\nHow many ways can 3 people be chosen from this class\nto participate in a survey?\n\nSolution:\nl Want to compute is “30 choose 3”\nl C(30, 3) = 30!\/[3! 27!]\nl\n= (30 × 29 × 28)\/(3 × 2)\nl\n= 10 × 29 × 14\nl\n\n= 4,060 ways to choose participants\n\n\fAn interesting observation…\nNote: Given a set of size n, choosing r elements is the\nsame as excluding n-r elements.\nThis means that C(n, r) = C(n, n-r).\n\nProof:\nl C(n, n-r) = n!\/[(n-r)! (n-(n-r))!]\nl\n= n!\/[(n-r)! r!]\nl\n= n!\/[r! (n-r)!]\nl\n= C(n, r)\n\nby simplification\nby the commutative property\nby definition ❏\n\n\fPermutations and combinations can be used in\nconjunction with the product and sum rules!\nExample: Suppose the CS department has 11 faculty\nmembers and the Math department has 9 faculty members.\nHow many ways can a committee consisting of 4 CS faculty\nmembers and 3 Math faculty members be chosen?\n\nSolution:\nl C(11, 4) ways to choose 4 CS profs\nl For each of these, there are C(9, 3) ways to choose 3 Math profs\nl So, we need to compute C(11, 4) × C(9, 3)\nl C(11, 4) × C(9, 3) = 11!\/(4! 7!) × 9!\/(3! 6!)\nl\n= 84 × 330\nl\n\n= 27,720 ways!\n\n\fIn-class exercises\nTop Hat\n\n\fCombinations can be helpful when examining\nbinomial expressions\nDefinition: A binomial is an expression involving the sum of\ntwo terms.\nl For example (x + y) is a binomial, as is (3 + j)4.\n\nr-Combinations are also called binomial coefficients, since\nthey occur as coefficients in the expansions of binomial\nexpressions.\n\nExample:\n(x + y)3 = x3 + 3x2y + 3xy2 + y3\n= C(3,0)x3y0 + C(3,1)x2y1 + C(3,2)x1y2 + C(3,3)x0y3\n\n\fThe Binomial Theorem\nThe Binomial Theorem: Let x and y be variables, and\nlet n be a non-negative integer. Then:\n!\n\n𝑥 + 𝑦 ! = \/ 𝐶 𝑛, 𝑗 𝑥 !&# 𝑦 #\n#$%\n\nExample: Compute (x + y)2.\nl C(2, 0) = 1\nl C(2, 1) = 2\nl C(2, 2) = 1\nl (x + y)2 = x2y0 + 2x1y1 + x0y2\nl\n= x2 + 2xy + y2\n\n✔\n\n\fA small example…\nQuestion: What is the coefficient of x12y13 in the\nexpansion of (x+y)25?\n\nSolution: By the binomial theorem, we know that this\nterm can be written as C(25,13)x12y13, so the\ncoefficient of x12y13 is C(25, 13) = 5,200,300.\n\n\fA slightly more complicated example…\nQuestion: What is the coefficient of x12y13 in the\nexpansion of (2x-3y)25?\n\nSolution:\nl Note that (2x-3y)25 = (2x + (-3)y)25\nl The binomial theorem tells us that:\n2𝑥 + −3𝑦\n\n!\"\n\n!\"\n\n= 1 𝐶 25, 𝑗 2𝑥 !\"&# −3𝑦 #\n#$%\n\nl Thus, the coefficient of x12y13 occurs when j = 13\nl C(25,13)(2x)25-13(-3y)13 = C(25,13)212(-3)13x12y13\nl So the coefficient = 25!\/(13! 12!) 212 (-3)13\nl\n= -[25!\/(13! 12!) 212 313]\n\n\fPascal’s Identity\nPascal’s identity: C(n+1, k) = C(n, k-1) + C(n, k)\n\nProof:\nl Let T be a set of n+1 elements\nl Let a ∈ T, and S = T — {a}\nl By definition, there are C(n+1, k) subsets of T containing k\nelements\nl A subset of T containing k elements either contains k elements\nof S, or the element a along with k-1 elements of S\nl There are C(n, k) subsets of S containing k elements, so there\nare C(n, k) subsets of T not containing a.\nl Further, there are C(n, k-1) subsets of S containing k-1\nelements, so there are C(n, k-1) subsets of T containing a.\nl Thus, C(n+1, k) = C(n, k-1) + C(n, k) ❏\n\n\fWe can use Pascal’s identity to define C(n, r)\nrecursively\nBasis step: For all n, we have that C(n, 0) = C(n, n) = 1\nRecursive step: C(n+1, k) = C(n, k-1) + C(n, k)\n\nExample: Compute C(3,2).\nl C(3,2) = C(2,1) + C(2,2)\nl\n= C(1,0) + C(1,1) + 1\nl\n=1+1+1\nl\n=3\nl\n= 3!\/(2!(3-2)!)\n\n✔\n\nUsing this definition, we can compute\nC(n, r) without using multiplication at\nall!\n\n\fIn-class exercises\nProblem 4: What is the coefficient of the term a7b8 in\n(a+b)15?\nProblem 5: Use the recursive definition of C(n, r) to\ncalculate C(5,3).\n\n\fFinal Thoughts\nn Permutations count the ways that we can shuffle a\nset (forming a sequence). r-Permutations count the\nnumber of ways that we can arrange r items from a\nset.\nn r-Combinations are useful when we want to count\n(unordered) subsets of a given set\nn Next time:\nl Generalized permutations and combinations\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":178,"segment": "unlabeled", "course": "cs0449", "lec": "lec07", "text":"7\n\nIntroduction\nto x86 asm\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fAssembly Refresher\nWhat is forgotten… is art.\n\n2\n\n\fWhat is “Assembly”\n• Assembly: Human-readable representation of machine code.\n• Machine code: what a computer actually runs.\n• The “atoms” that make up a program.\n• CPUs CAN actually be fairly simple in concept.\n\n• Each CPU chooses its own machine code (and therefore its own style\nof assembly language)\n• We used MIPS in CS 447.\n• A RISC processor.\n\n• We will compare that to x86 today!\n• A CISC processor.\n\n3\n\n\fWhat is “Assembly”\n• Involves very simple commands.\n• This command copies data from\none place to another.\n\n• Despite being called “move”, ugh!\n\n• Surprise! It’s actually shorthand\nfor a different set of instructions.\n\n• The processor can be made simpler.\n\n• This command gets transformed\ninto a numerical representation.\n\nCompute t0+0\nPut into “a0”\n\n• The processor then interprets the binary representation.\n• That’s essentially all a computer does!\n• CS 447 looks at this in much greater detail.\n\n4\n\n\fAssembly vs. Machine Language\n• Machine language instructions are the patterns of bits that a\nprocessor reads to know what to do\n• Assembly language (or \"asm\") is a human-readable (mostly), textual\nrepresentation of machine language.\nMIPS asm\n\nMIPS machine language\n\n5\n\n\fIs Assembly Useful?\n• Short answer: YES\n• Assembly is “fast”, so we should use it for everything!\n--- NO!!! --• No type-checking, no control structures, very few abstractions.\nFairly impractical for large things --• Tied to a particular CPU.\n\n---\n\n• So, large programs have to be rewritten (usually) to work on new things.\n\n• Yet: good for specialized stuff.\n• Critical paths and “boot” code in Kernels \/ Operating Systems\n• HPC (simulators, supercomputer stuff)\n• Real-time programs (video games; tho increasingly less \/ abstracted away)\n• And…\n\n6\n\n\fArchitecture Sits at the Hardware Interface\nSource code\n\nCompiler\n\nArchitecture\n\nApplications\/algorithms\n\nPerform optimizations,\ngenerate instructions\n\nInstruction set\n\nDifferent implementations\n\nIntel Pentium 4\n\nC Language\nProgram\nA\n\nHardware\n\nIntel Core 2\n\nGCC\n\nx86-64\n\nIntel Core i7\nAMD Opteron\n\nProgram\nB\n\nAMD Athlon\nClang\n\nYour\nprogram\n\nARMv8\n(AArch64\/A64)\n\nARM Cortex-A53\nApple A7\n\n\fPractical Applications of Assembly: Modification\n• Modifying programs after-the-fact. (Or reverse-engineering them)\n• Legal “gray-area,” \/ “confusing-mess” but generally modification\/reverse engineering is allowed.\nKinda? (Section 1201, US Code 17 § 108, etc)\n• Removing copy protection in order to preserve\/backup.\n• Librarians and preservationists and “pirates” alike may all use\/view\/write assembly for this!\n\nI’m not a lawyer\n• I know someone that patched (the freely distributed) Lost Vikings so it would avoid copy protection\nand use a different sound configuration (so I could run it in a browser emulator)\n\nx86 (NASM \/ Intel Syntax, MS-DOS)\n\n8\n\n\fPractical Applications of Assembly: Debugging\n\n• Programs written in C, etc are generally translated into assembly.\n• And then into machine code.\n\n• You can look at the machine code of programs and get an assembly\ncode listing.\n• And step through the program one instruction at a time.\n\n• When programs crash (sometimes programs you don’t have the code\nfor) you can look at the assembly code and assess.\n• Programs exist to help you (gdb, IDA Pro, radare, etc)\n\n• We will apply this knowledge (using gdb) in a future assignment!\n9\n\n\fBasics of x86 Assembly\nx86 really puts the… you know what… in Assembly\n\n10\n\n\fInstruction Set Architecture (ISA)\n• An ISA is the interface that a CPU presents to the programmer.\n• When we say \"architecture,\" this is what we mean.\n\n• The ISA defines:\n• What the CPU can do (add, subtract, call functions, etc.)\n• What registers it has (we'll get to those)\n• The machine language\n• That is, the bit patterns used to encode instructions.\n\n• The ISA does not define:\n• How to design the hardware!\n• …if there's any hardware at all (think of Java, etc: virtual\/hypothetical ISAs)\n\n11\n\n\fTypes of ISAs: RISC\n• RISC: \"Reduced Instruction Set Computer\"\n• ISA designed to make it easy to:\n• build the CPU hardware\n• make that hardware run fast\n• write compilers that make machine code\n\n• A small number of instructions.\n• Instructions are very simple\n• MIPS (and RISC-V) is very RISCy\n\n12\n\n\fTypes of ISAs: CISC\n• CISC: \"Complex Instruction Set Computer\"\n• ISA designed for humans to write asm.\n• From the days before compilers!\n\n• Lots of instructions and ways to use them\n• Complex (multi-step) instructions to shorten\nand simplify programs.\n• \"search a string for a character\"\n• \"copy memory blocks\"\n• \"check the bounds of an array access“\n\n• Without these, you’d just write your programs to use the\nsimpler instructions to build the complex behavior itself.\n• x86 is very CISCy\n13\n\n\fTypes of ISAs: Overview\n• CISC: Complex Instruction Set Computer (does a whole lot)\n• RISC: Reduced Instruction Set Computer (does enough)\n• Both: Equivalent!! (RISC programs might be longer)\n\n“Hackers” (1995) – Of course, they are talking about a Pentium x86 chip…\nwhich thanks to its backwards compatibility, is CISC. Oh well!\nThen again… x86 is so complex, modern designs translate the CISC instructions into RISC microcode on the fly… so it’s RISC?? It can get complicated.\n\n14\n\n\fx86\n• Descended from 16-bit 8086 CPU from 1978.\n• Extended to 32 bits, then 64.\n• Each version can run most programs from the previous version.\n• You can (mostly) run programs written in ‘78 on your brand new x86 CPU!\n\n• This ISA is complex!\n• 30 years of backwards-compatibility… yikes.\n• We won’t exhaustively go over it.\n• There are, however, many very common idioms\nand instructions.\n• We will focus on these.\n• And we will focus on READING x86, not writing it.\n\n15\n\n\fx86 Registers (general)\n• Like MIPS, there are a set of general-purpose registers.\n• There are 16; 64-bits in size and hold integer values in binary form.\n\n• Unlike MIPS, you can refer to parts of each register.\n• Called partial registers.\n\nDenoted\nby %\n\nStack\nPointer\n\n16-bit register names\n\n32-bit register names\n\n16\n\n\fgeneral purpose\n\n32-bit OLD Registers – 32 bits wide\n8 bits\n\n32 bits\n\n16 bits\n\n%eax\n\n%ax\n\n%ah\n\n%al\n\naccumulate\n\n%ecx\n\n%cx\n\n%ch\n\n%cl\n\ncounter\n\n%edx\n\n%dx\n\n%dh\n\n%dl\n\ndata\n\n%ebx\n\n%bx\n\n%bh\n\n%bl\n\nbase\n\n%esi\n\n%si\n\nsource index\n\n%edi\n\n%di\n\ndestination index\n\n%esp\n\n%sp\n\nstack pointer\n\n%ebp\n\n%bp\n\nbase pointer\n16-bit virtual registers\n(backwards compatibility)\n\nName Origin\n(mostly obsolete)\n\n\fx86 Registers (specialized)\n• There are also registers that you cannot directly interact with.\n• Like MIPS, x86 has a program counter (\n)\n• Also like MIPS, it cannot be read directly.\n\n• There is also a\nstatus register, which has information about the\nCPU state after an instruction is completed.\n• Stuff like a carry flag (CF) that denotes if an addition has a final carry.\n• Overflow detection (OF) denoting if an operation overflowed.\n\n• And some extra registers for vector math, floating point math, and for\nOS usage we won’t go over.\n18\n\n\fx86 Instruction Types\n• In MIPS, you had R-type, I-type and J-type instructions.\n• In x86 (CISC) you generally can have any instruction refer to data anywhere it is:\n• Registers, Immediates, Memory addresses, etc\n• Cannot refer to memory twice! (not possible:\nx86-64 (gas \/ AT&T syntax)\n\n)\n\nMIPS\n\nImmediates (prefixed by $)\nMemory load (within parens)\nMemory store\n\nDisplacement (can be -4, etc)\n\n19\n\n\fComplex Addressing\n• In MIPS, you would carefully craft the set of instructions necessary to\ninterface with an array. (RISC)\n• In x86, you can do a lot with just a single instruction. (CISC)\n•\n\n: Base + (Index Scalar) where Scalar must be 1, 2, 4 or 8\n• The fields are all optional; i.e.,\n\nx86-64 (gas \/ AT&T syntax)\n\ndoes just Index Scalar\n\nMIPS\n\n“Load Effective Address”\n\nLEA simply computes address (no memory access)\n\n20\n\n\fComplex Addressing: CISC Strikes Again!!\n• When we say you can do a lot with just a single instruction, we\nmean it!\n•\n: Base + (Index Scalar) where Scalar must be 1, 2, 4 or 8\n• What does the following do?\n•\n\nx86-64 (gas \/ AT&T syntax)\n\n“Load Effective Address” ???\n\nLEA simply computes address… it’s just very specific math.\n\n21\n\n\fx86 Instruction Qualifiers\n• In MIPS, you sometimes had instructions varying on bitsize.\n• In x86 (CISC) you can operate on any part of a register.\n• 64-bits, 32-bits, 16-bits… even 8-bit sections sometimes.\n\n• The assembler can assume usually, but explicit names also work:\nx86-64 (gas \/ AT&T syntax)\nThe assembler “figures it out”\n\nMIPS64\n\n“quad word” which is 64-bits.\n“long word” which is 32-bits. \nUgh. In x86 a “word” here is 16-bits\nCS\/COE 0449 – Spring 2019\/2020\n\n22\n\n\fHello World! (x86 vs. MIPS)\nx86-64 (gas \/ AT&T syntax)\n\nMIPS (MARS)\n\n23\n\n\fDoing some x86 maths\n• x86 and MIPS have, essentially, the same mathematical instructions.\nx86-64 (gas \/ AT&T syntax)\n\nMIPS\n\n24\n\n\fHowever, x86 lets you slice and dice\n• Each math instruction in x86 has variants based on the bitsize.\n•\n\n(64-bit),\n\n(32-bit),\n\n(16-bit),\n\nx86-64 (gas \/ AT&T syntax)\n\n(8-bit) (rest of field zero extended!!)\n\nMIPS\n\nArithmetic shift (sign extends)\n\nLogical shift (zero extends)\n\n8-bit register aliases are not commonly used\n\n25\n\n\fAssembly Interlude\nHere, we take a break, and look at some existing code.\n\n26\n\n\fWhy write assembly? When you can write C\n• You can take any of your C programs and emit the assembly.\n• The compiler can do this for you:\n\n• This will create a file called\n\nwhich looks… messy.\n\n• It has a ton of messy specific stuff wedged in there.\n• But you can generally pull apart some meaning from it.\n\n27\n\n\fLooking at C compilers…\n• The messy output of the gcc compilation to assembly:\nx86-64 (gas \/ AT&T syntax,\n\nmain hasn’t even shown up yet…\n\n)\n\nC\n\n28\n\n\fDisassembly – See how the sausage is made…\n• So, that’s not very useful. And often we don’t have the code!\n• How do we go backward?\n\n• You can take any compiled program and emit the assembly.\n• Many tools can help you do this (radare, objdump, gdb)\n\n• Using a tool called objdump (only disassembles code section):\n\n• This will create a file called\n\n.\n\n• You can glance at it and notice that it does not have names.\n• And labels are a bit, well, nonexistent.\n29\n\n\fAnd… here we are…\n• An objdump disassembly is slightly lacking context.\nx86-64 (gas \/ AT&T syntax,\n\n)\n\nC\n\nMachine code (in bytes)\nInstruction address\n30\n\n\fLooking deeper\n• Now we are starting to read the code… It does what we tell it to do!\nx86-64 (gas \/ AT&T syntax,\n\n)\nPreserves\n(caller activation frame)\nAllocates “ ” on stack ( from top)\nMove argument to\nCompares to and sets\nJumps if\nis 0 ( is positive)\nSets\nto\nResets caller activation frame\nReturns (return value is in\n)\n\nInstructions have varying size\nSo, the next instruction address\nis irregular. Compare with MIPS \/ RISC-V.\n\n31\n\n\fBrought to you by the letters: C ABI\n• The C Application Binary Interface (ABI) are assembly conventions\n• Like MIPS, certain registers are typically used for returns values, args, etc\n• It is not defined by the language, but rather the OS.\n• Windows and Linux (UNIX\/System V) have a different C ABI \n\n• In our x86-64 Linux C ABI, registers are used to pass arguments:\n•\n,\n,\n,\n,\n,\n(First, second, etc) (Like MIPS\n–\n)\n• Remaining arguments go on the stack.\n• Callee must preserve\n,\n,\n,\n,\n,\n(Like MIPS\n–\n• Return value:\n(overflows into\nfor 128-bits) (MIPS\n–\n)\n• Lots of other small things not worth going over.\n\n)\n\n• For reference: https:\/\/github.com\/hjl-tools\/x86-psABI\/wiki\/x86-64-psABI-1.0.pdf\n32\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":179,"segment": "unlabeled", "course": "cs0441", "lec": "lec06", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #6: Rules of Inference\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s topics\nn Rules of inference\nl Logical equivalences allowed us to rewrite and simplify\nsingle logical statements.\nl How do we deduce new information by combining\ninformation from (perhaps multiple) known truths?\n\n\fWhat have we learned? Where are we going?\nPredicate logic\n(refined representation)\n\nPropositional logic\n(representation)\n\nQuantifiers\n(generalization)\n\nInference and proof\n(deriving new knowledge!)\n\n\fWriting valid proofs is a subtle art\nThis is called\n“research”\nStep 1: Discover and\nformalize the property\nthat you wish to prove\n\nStep 2: Formalize the ground truths\n(axioms) that you will use to prove\nthis property\n\nSubtle, but not terribly\ndifficult\nStep 3: Show that the property in\nquestion follows from the truth of\nyour axioms\n\nThis is the hard part…\n\n\fWhat is science without jargon?\nA conjecture is a statement that is thought to be true.\nA proof is a valid argument that establishes the truth\nof a given statement (i.e., a conjecture)\nThe truth of the conclusion follows\nfrom the truth of the preceding\nstatements\n\nA sequence of statements ending\nwith a conclusion\n\nAfter a proof has been found for a given conjecture, it\nbecomes a theorem\n\n\fA tale of two proof techniques\nIn a formal proof, each step of\nthe proof clearly follows from\nthe postulates and axioms\nassumed in the conjecture.\nStatements that are assumed to be true\n\nIn an informal proof, one step in\nthe proof may consist of\nmultiple derivations, portions of\nthe proof may be skipped or\nassumed correct, and axioms\nmay not be explicitly stated.\n\n\fHow can we formalize an argument?\nConsider the following argument:\n“If you have an account, you can access the network”\n“You have an account”\nTherefore,\n“You can access the network”\nPremises\nConclusion\n\nThis argument seems valid, but how can we\ndemonstrate this formally??\n\n\fLet’s analyze the form of our argument\np\n\nq\n\n“If you have an account, then you can access the network”\n“You have an account”\nTherefore,\n“You can access the network”\nThis is called a “rule of\ninference”\n\np→q\np\n∴q\n\n\fRules of inference are logically valid ways to draw\nconclusions when constructing a formal proof\nThe previous rule is called modus ponens\nl Rule of inference: p → q\np\n∴q\nl Informally: Given an implication p → q, if we know that p\nis true, then q is also true\n\nBut why can we trust modus ponens?\nl Tautology: ((p → q) ∧ p) → q\nl Truth table: p\nq\np→q\nT\n\nT\n\nT\n\nT\n\nF\n\nF\n\nF\n\nT\n\nT\n\nF\n\nF\n\nT\n\nAny time that p→q and p\nare both true, q is also\ntrue!\n\n\fThere are lots of other rules of inference that\nwe can use!\nAddition\nl Tautology: p → (p ∨ q)\np\nl Rule of inference:\n∴p∨q\nl Example: “It is raining now, therefore it is raining now or\nit is snowing now.”\n\nSimplification\nl Tautology: p ∧ q → p\np∧q\nl Rule of inference:\n∴p\nl Example: “It is cold outside and it is snowing. Therefore, it\nis cold outside.”\n\n\fThere are lots of other rules of inference that\nwe can use!\nModus tollens\nl Tautology: [¬q ∧ (p → q)] → ¬p\nl Rule of inference: p → q\n¬q\n∴ ¬p\nl Example: “If I am hungry, then I will eat. I am not eating.\nTherefore, I am not hungry.”\n\nHypothetical syllogism\nl Tautology: [(p → q) ∧ (q → r)] → (p → r)\nl Rule of inference: (p → q)\n(q → r)\n∴ (p → r)\nl Example: “If I eat a big meal, then I feel full. If I feel full,\nthen I am happy. Therefore, if I eat a big meal, then I am\nhappy.”\n\n\fThere are lots of other rules of inference that\nwe can use!\nDisjunctive syllogism\nl Tautology: [¬p ∧ (p ∨ q)] → q\nl Rule of inference: p ∨ q\n¬p\n∴q\nl Example: “Either the heat is broken, or I have a fever.\nThe heat is not broken, therefore I have a fever.”\n\nConjunction\nl Tautology: [(p) ∧ (q)] → (p ∧ q)\nl Rule of inference: p\nq\n∴ (p ∧ q)\nl Example: “Jack is tall. Jack is skinny. Therefore, Jack is\ntall and skinny.”\n\n\fThere are lots of other rules of inference that\nwe can use!\nResolution\nl Tautology: [(p ∨ q) ∧ (¬p ∨ r)] → (q ∨ r)\nl Rule of inference: p ∨ q\n¬p ∨ r\n∴q∨r\nl Example: “If it is not raining, I will ride my bike. If it is\nraining, I will lift weights. Therefore, I will ride my bike or\nlift weights”\n\nSpecial cases:\n1. If r = q, we get\n\np∨q\n¬p ∨ q\n∴q\n\n2. If r = F, we get\n\np∨q\n¬p\n∴q\n\n\fIn-class exercises\nSee Top Hat\n\n\fWe can use rules of inference to build valid arguments\n\nIf it is raining, I will stay inside. If I am inside, Lisa\nwill stay home. If Lisa stays home and it is a Saturday,\nthen we will play video games. Today is Saturday. It\nis raining.\n\nLet:\nl r ≡ It is raining\nl i ≡ I am inside\nl l ≡ Lisa will stay home\nl p ≡ we will play video games\nl s ≡ it is Saturday\n\nHypotheses:\nl r→i\nl i→l\nl l∧s→p\nl s\nl r\n\n\fWe can use rules of inference to build valid arguments\nLet:\n\nHypotheses:\n\nl r ≡ It is raining\nl i ≡ I am inside\nl l ≡ Lisa will stay home\nl p ≡ we will play video games\nl s ≡ it is Saturday\n\nl r→i\nl i→l\nl l∧s→p\nl s\nl r\n\nStep:\n1. r → i\n2. i → l\n3. r → l\n4. r\n5. l\n6. s\n7. l ∧ s\n8. l ∧ s → p\n9. p\n\nhypothesis\nhypothesis\nhypothetical syllogism with 1 and 2\nhypothesis\nmodus ponens with 3 and 4\nhypothesis\nconjunction of 5 and 6\nhypothesis\nmodus ponens with 7 and 8\n\nI will play\nvideo games!\n\n\fWe also have rules of inference for statements\nwith quantifiers\nUniversal Instantiation\nl Intuition: If we know that P(x) is true for all x, then P(c) is\ntrue for a particular c\nl Rule of inference: ∀x P(x)\n∴ P(c)\n\nUniversal Generalization\nl Intuition: If we can show that P(c) is true for an arbitrary c,\nthen we can conclude that P(x) is true for any specific x\nl Rule of inference: P(c)\n∴ ∀xP(x)\nNote that “arbitrary” does not mean “randomly chosen.” It\nmeans that we cannot make any assumptions about c other than\nthe fact that it comes from the appropriate domain.\n\n\fWe also have rules of inference for statements\nwith quantifiers\nExistential Instantiation\nl Intuition: If we know that ∃x P(x) is true, then we know\nthat P(c) is true for some c\nl Rule of inference: ∃x P(x)\n∴ P(c)\nAgain, we cannot make assumptions about c other than the fact\nthat it exists and is from the appropriate domain.\n\nExistential Generalization\nl Intuition: If we can show that P(c) is true for a particular c,\nthen we can conclude that ∃x P(x) is true\nl Rule of inference: P(c)\n∴ ∃xP(x)\n\n\fHungry dogs redux\nGiven: Kody is one\nof my dogs\nM(Kody)\n\nGiven: All of my dogs like peanut butter\nM(x)\n\nP(x)\n\n1. ∀x [M(x) → P(x)]\n2. M(Kody)\n3. M(Kody) → P(Kody)\n4. P(Kody)\n\nhypothesis\nhypothesis\nuniversal instantiation from 1\nmodus ponens from 2 and 3\n\n\fReasoning about our class\nShow that the premises “A student in this class has not\nread the book” and “everyone in this class turned in\nHW1” imply the conclusion “Someone who turned in\nHW1 has not read the book.”\n\nLet:\nl C(x) ≡ x is in this class\nl B(x) ≡ x has read the book\nl T(x) ≡ x turned in HW1\n\nPremises:\nl ∃x [C(x) ∧ ¬B(x)]\nl ∀x [C(x) → T(x)]\n\n\fReasoning about our class\nLet:\nl C(x) ≡ x is in this class\nl B(x) ≡ x has read the book\nl T(x) ≡ x turned in HW1\n\nPremises:\nl ∃x [C(x) ∧ ¬B(x)]\nl ∀x [C(x) → T(x)]\n\nSteps:\n1. ∃x [C(x) ∧ ¬B(x)]\n2. C(a) ∧ ¬B(a)\n3. C(a)\n4. ∀x [C(x) → T(x)]\n5. C(a) → T(a)\n6. T(a)\n7. ¬B(a)\n8. T(a) ∧ ¬B(a)\n9. ∃x [T(x) ∧ ¬B(x)]\n\nhypothesis\nexistential instantiation from 1\nsimplification from 2\nhypothesis\nuniversal instantiation from 4\nmodus ponens from 5 and 3\nsimplification from 2\nconjunction of 6 and 7\nexistential generalization from 8\n\n\fIn-class exercises\nProblem 2: Show that the premises “Everyone in this\ndiscrete math class has taken a course in computer\nscience” and “Melissa is a student in this discrete\nmath class” lead to the conclusion “Melissa has taken\na course in computer science.”\n\n\fFinal Thoughts\nn Until today, we had look at representing different types of\nlogical statements\nn Rules of inference allow us to derive new results by reasoning\nabout known truths\nn Next time:\nl Proof techniques\nl Please read section 1.8\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":180,"segment": "self_training_1", "course": "cs0447", "lec": "lec0A", "text":"#A\nCS 0447\nIntroduction to\nComputer Programming\n\nFractions and\nFloating Point\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce Childers,\nDavid Wilkinson\n\nLuís Oliveira\n\nFall 2020\n\n\fAnnouncements\n● Don’t forget the points for discussing your project solution with your TA are expiring\n\n2\n\n\fFractional Binary\n\n3\n\n\fFractional numbers\n● Up to this point we have been working with integer numbers.\no Unsigned and signed!\n\n2019\n2 0 1 9.320\n\n● However, Real world numbers are… Real numbers. Like so:\n\n● That create new challenges!\no Let’s start by taking a look at them.\n\n4\n\n\fJust a fraction of a number\n● The numbers we use are written positionally: the position of a digit within the\nnumber has a meaning.\n● What about when the numbers go over the decimal point?\n\n?\n2 0 1 9. 3 2 0\n\n1000s\n\n100s\n\n10s\n\n1s\n\n10ths 100ths 1000ths\n\n103\n\n102\n\n101\n\n100\n\n10-1\n\n10-2\n\n10-3\n\n5\n\n\fA fraction of a bit?\n● Binary is the same!\n● Just replace 10s with 2s.\n\n0 1 1 0 .1 1 0 1\n23\n8s\n\n22\n4s\n\n21\n2s\n\n20\n1s\n\n2-1\n2ths\n\n?\n\n2-2\n4ths\n\n2-3\n8ths\n\n2-4\n16ths\n\n6\n\n\fTo convert into decimal, just add stuff\n\n0 1 1 0 .1 1 0 1=\n23\n\n22\n\n21\n\n20\n\n0×8+\n1×4+\n1×2+\n0×1+\n1 × .5 +\n1 × .25 +\n0 × .125 +\n1 × .0625\n\n2-1\n\n2-2\n\n2-3\n\n2-4\n\n= 6.812510\n\n7\n\n\fFrom decimal to binary? Tricky?\n\n6÷210 = 3R0\n\n6.8125 10\n\n3÷210 = 1R1\n\n1 1 0. 1101\n\n0.812510\nx\n2\n1.6250\n\nMSB\n\n0.625010\nx\n2\n1.2500\n0.250010\nx\n2\n0.5000\n0.500010\nx\n2\n1.0000\n\nLSB\n8\n\n\fSo, it’s easy right? Well…\n\nWhat about: 0.1 10\n\n0.110\nx 2\n0.2\n0.210\nx2\n0.4\n\n0. 0001\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n9\n\n\fSo, it’s easy right? Well……\n\nWhat about: 0.1 10\n\n0. 0001\n\n1001\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n10\n\n\fSo, it’s easy right? Well………\n\nWhat about: 0.1 10\n\n0. 0001\n1001\n10\n0\n1\n...\n\n0.610\nx 2\n1.2\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n11\n\n\fHow much is it worth?\n\n•Well, it depends on where you stop!\n\n0.0001 2\n\n= 0.0625\n\n0.00011001 2\n\n= 0.0976…\n\n0.000110011001 2 = 0.0998…\n12\n\n\fLimited space!\n● How much should we store?\no We have 32-bit registers, so 32-bits?\n▪ Let’s say we do!\n● How many bits are used to store the integer part?\n● How many bits are used to store the fractional part?\n\n● What are the tradeoffs?\n\n13\n\n\fA rising tide\n● Maybe half-and-half? 16.16 number looks like this:\n\n0011 0000 0101 1010.1000 0000 1111 1111\nbinary point\nthe largest (signed) value we\nthe smallest fraction we can\ncan represent is +32767.999\nrepresent is 1\/65536\nWhat if we place the binary point to the left…\n\n0011.0000 0101 1010 1000 0000 1111 1111\n…we can get much higher accuracy near 0…\n\n…but if we place the binary point to the right…\n\n0011 0000 0101 1010 1000 0000.1111 1111\n…then we trade off accuracy for range further away from 0.\n\n14\n\n\fMind the point\n● In this representation we assume that the lowest n digits are the decimal places.\n\n$12.34\n+$10.81\n$23.15\n\n1234\n+1081\n2315\n\nthis is called fixed-point\nrepresentation\nAnd it’s a bitfield :D\n\n15\n\n\fMove the point\n● What if we could float the point around?\no Enter scientific notation: The number -0.0039 can be represented:\n\n-0.39\n-3.9\n\n× 10-2\n× 10-3\n\n● These are both representing the same number, but we need to move the decimal\npoint according to the power of ten represented.\n\n● The bottom example is in normalized scientific notation.\no There is only one non-zero digit to the left of the point\n● Because the decimal point can be moved, we call this representation\n\nFloating point\n\n16\n\n\fFloating-point number\nrepresentation\n\n17\n\n\fThis could be a whole unit itself...\n● floating-point arithmetic is COMPLEX STUFF\n\n● However...\no it's good to have an understanding of why limitations exist\no it's good to have an appreciation of how complex this is... and how much better\nthings are now than they were in the 1970s and 1980s\no It’s good to know things do not behave as expected when using float and double!\n\n18\n\n\fBinary numbers using IEEE 754\n● est'd 1985, updated as recently as 2008\n● standard for floating-point representation and arithmetic that virtually every CPU\nnow uses\n● floating-point representation is based around scientific notation\n\n1348 = +1.348 × 10+3\n-0.0039 = -3.9\n× 10-3\n-1440000 = -1.44 × 10+6\nsign significand\n\nexponent\n\n19\n\n\fBinary Scientific Notation\n● scientific notation works equally well in any other base!\no (below uses base-10 exponents for clarity)\n\n+1001 0101 = +1.001 0101 × 2+7\n-0.001 010 = -1.010\n× 2-3\n-1001 0000 0000 0000 = -1.001\n× 2+15\nwhat do you notice about the digit\nbefore the binary point using\nnormalized representation?\n\n(-1)s x 1.f × 2exp s – sign\nf – fraction\n1.f – significand\nexp – exponent\n\n20\n\n\fIEEE 754 Single-precision\n● Known as float in C\/C++\/Java etc., 32-bit float format\n● 1 bit for sign, 8 bits for the exponent, 23 bits for the fraction\n\n● Tradeoff:\no More accuracy ➔ More fraction bits\no More range ➔ More exponent bits\n● Every design has tradeoffs ¯\\_(ツ)_\/¯\n\nillustration from user Stannered on Wikimedia Commons\n\n21\n\n\fIEEE 754 Single-precision\n● Known as float in C\/C++\/Java etc., 32-bit float format\n● 1 bit for sign, 8 bits for the exponent, 23 bits for the fraction\n\n● The fraction field only stores the digits after the binary point\n● The 1 before the binary point is implicit!\no This is called normalized representation\no In effect this gives us a 24-bit significand\n● The significand of floating-point numbers is in sign-magnitude!\no Do you remember the downside(s)?\n\nillustration from user Stannered on Wikimedia Commons\n\n22\n\n\fThe exponent field\n● the exponent field is 8 bits, and can hold positive or negative exponents, but... it\ndoesn't use S-M, 1's, or 2's complement.\n● it uses something called biased notation.\no biased representation = exponent + bias constant\no single-precision floats use a bias constant of 127\n\nexp + 127 => Biased\n\n-127 + 127 => 0\n-10 + 127 => 117\n34 + 127 => 161\n\n● the exponent can range from -126 to +127 (1 to 254 biased)\no 0 and 255 are reserved!\n● why'd they do this?\no You can sort floats with integer comparisons!\n23\n\n\fBinary Scientific Notation (revisited)\n● Our previous numbers are actually\n\nbias = 127\n\n+1.001 0101 × 2+7\n\nsign = 0 (positive number!)\nBiased exponent = exp + 127 = 7 + 127 = 134\n= 10000110\nfraction = 0010101 (ignore the “1.”)\ns\n\nE\n\nf\n\n0 10000110 00101010000000000…000\n(-1)0 x 1.001 0101 × 2134-127\n\n24\n\n\fBinary Scientific Notation (revisited)\n● Our previous numbers are actually\n\nbias = 127\n\n-1.010 × 2-3 =\n\nsign = 1 (negative number!)\nBiased exponent = exp + 127 = -3 + 127 = 124\n= 01111100\nfraction = 010 (ignore the “1.”)\ns\n\nE\n\nf\n\n1 01111100 01000000000000000…000\n(-1)1 x 1.010\n\n× 2124-127\n\n25\n\n\fBinary Scientific Notation (revisited)\n● Our previous numbers are actually\n\nbias = 127\n\n-1.001 × 2+15=\nsign = ?\nBiased exponent = ?\nfraction = ?\ns\n\nE\n\nf\n\n?\n\n?\n\n?\n26\n\n\fCheck it on C++ (there are online tools for this!)\n#include <iostream>\n#include<bitset>\nint main() {\n\/\/ This is the number from the previous slide\nfloat x {-0b1001000000000000};\n\/\/ C++ does not shift floats :(\n\/\/ This is C++-whispering: it allows me to shift the bits :)\nint num {*(int*)&x};\n\/\/ Extract the fields!\nstd::bitset <1> sign = (num >> 31) & 0x1 ;\nstd::bitset <8> biased_exp = (num >> 23) & 0xFF;\nstd::bitset <23> frac = (num >> 0) & 0x7FFFFF;\n\n}\n\n\/\/ Now let’s print\nstd::cout << \"sign: \" << sign << std::endl;\nstd::cout << \"biased exponent: \" << biased_exp << std::endl;\nstd::cout << \"frac: \" << frac << std::endl;\nreturn 0;\n\nTry it in: https:\/\/repl.it\/languages\/cpp\n\n27\n\n\fEncoding a number as a float\nYou have an number, like -12.5937510\n1. Convert it to binary.\nInteger part: 11002\nFractional part: 0.100112\n2. Write it in scientific notation:\n1100.100112 x 20\n3. Normalize it:\n1.100100112 x 23\n\n0.5937510\nx\n2\n1.18750\n\nMSB\n\n0.1875010\nx\n2\n0.37500\n\n0.3750010\nx\n2\n0.75000\n0.7500010\nx\n2\n1.50000\n0.5000010\nx\n2\n1.00000\n\nLSB\n\n28\n\n\fEncoding a number as a float\nYou have an number, like -12.5937510\n1. Convert it to binary.\nInteger part: 11002\nFractional part: 0.100112\n2. Write it in scientific notation:\n1100.100112 x 20\n3. Normalize it:\n1.100100112 x 23\n4. Calculate biased exponent\n+3 + 127 = 13010 = 100000102\n\n0xC1498000\n\ns\n\nexponent\n\nfraction\n\n1 10000010 10010011000000000…000\n29\n\n\fAdding floating point numbers\n\n1.11 × 20 + 1.00 × 2-2\n● Step 1 – Make both exponents the same\n\n1.11 × 20 + 0.01 × 20\n\n● Step 2 – Add the significands\n\n1.11 × 20 + 0.01 × 20 = 10.00 × 20\n\n● Step 3 – Normalize the result\n\n10.00 × 20 = 1.000 × 21\n30\n\n\fMultiply floating point numbers\n\n1.11 × 20 x 1.01 × 2-2\n● Step 1 – Add the exponents\n\n0 + (-2) = [0+127]+[-2+127] =\n[127] + [125] – 127 = [125] = -2\n● Step 2 – Multiply the significands\n\n1.11 x 1.01 = 10.0011\n● Step 3 – Normalize the result\n\n10.0011 × 2-2 = 1.00011 × 2-1\n31\n\n\fDivide floating point numbers\n\n1.001 × 20 \/ 1.1 × 2-2\n● Step 1 – Subtract the exponents\n\n0 - (-2) = [0+127]-[-2+127] =\n[127] - [125] + 127 = [129] = 2\n\n● Step 2 – Divide the significands\n\n1.001 \/ 1.1 = 0.11\n● Step 3 – Normalize the result\n\n0.11 × 22 = 1.1 × 21\n32\n\n\fOther formats\n● the most common other format is double-precision (C\/C++\/Java double), which uses\nan 11-bit exponent and 52-bit fraction\n\n● GPUs have driven the creation of a half-precision 16-bit floatingpoint format. it's adorable\n\n1023 bias\n15 bias\n\nboth illustrations from user Codekaizen on Wikimedia Commons\n\n33\n\n\fSpecial cases\n● IEEE 754 can represent data outside of the norm.\no Zero! How do you do that with normalized numbers?\no +\/- Infinity\no NaN (Not a number). E.g. when you divide zero by zero.\no Other denormalized number: Squeeze the most out of our bits!\n▪ E.g.: 0.00000000000000000000001 x 2-127\nSingle precision\n\nDouble precision\n\nMeaning\n\nExponent\n\nFraction\n\nExponent\n\nFraction\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n!=0\n\n0\n\n!=0\n\nNumber is denormalized\n\n255\n\n0\n\n2047\n\n0\n\nInfinity (sign-bit defines + or -)\n\n255\n\n!=0\n\n2047\n\n!=0\n\nNaN (Not a Number)\n34\n\n\fCheck out this cool thing in MARS\n● go to Tools > Floating Point Representation\n● Try it out!\n\n35\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":181,"segment": "unlabeled", "course": "cs0449", "lec": "lec13", "text":"13\n\nVirtual\nMemory\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fThe Virtual\nI just want to use this time to point out that “Virtual Reality” is an oxymoron.\n\n2\n\n\fOur protagonist’s journey so far…\n• Processes have an addressable memory space.\n▪ We call this an address space.\n\n• Now, we know it has some obvious things…\n▪ The code (.text)\n▪ The data (.data)\n▪ A stack and some available space that can be\nallocated as we need it.\n\n• We added the “Kernel” memory to our diagram.\n▪ This is the OS code and data.\n▪ For system processes to run, they need to be\nresident in memory as well.\n3\n\n\fRandom Access Memory\n• Memory is a\nphysical device.\n\nSense Line\n\n• It is a random\naccess device.\n▪ Allows the\nmachine to access\ndata at any point.\n\nCapacitor\nControl\nLine\n\n• As opposed to\n\nsequential access.\nDynamic RAM (DRAM)\n\n4\n\n\fLet’s get physical\n• Physical\naddressing is\nwhen hardware\nrelates a program\naddress directly to\nthe memory\nhardware.\n• The program\naddresses are the\nexact physical\nlocations of data.\nDynamic RAM (DRAM)\n\n5\n\n\fLet’s get physical\n• Physical addressing is\nuseful when you\nhave only a single,\nsimple process.\n• Embedded devices\n(small, specific uses)\n• Think of your\ntoaster… or a\nthermostat.\n\nMemory load\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n6\n\n\fThe problem\n• However, we don’t always have such simple cases.\n▪ A general-purpose system, like our phones and desktop machines, can run a\nvariety of programs.\n▪ We often have multitasking operating systems running many programs at the\nsame time.\n\n• When we have several processes, how to we manage memory?\n▪ How to present a consistent address space?\n▪ How to prevent other processes from interfering?\n▪ How to prevent address space fragmentation?\n\n• The solution, as usual: indirection: Virtual Memory.\n7\n\n\fVirtual vs. Reality\n• Just like “virtual reality,” we create a\nworld that resembles reality, but it\nreally is a facsimile.\n• We can provide a scheme, backed\nby hardware, that allows memory\naddresses to be seen by programs in\na specific place…\n\n• Yet, those addresses differ from the\nactual physical memory location.\n\nAngela Lansbury as Jessica Fletcher, Director Lee Smith\nMurder, She Wrote; “A Virtual Murder”\nUniversal Studios, Universal Television, 1993\n\n8\n\n\fConsistency, dear Watson.\n• When you write a program, do you write it deliberately for the\nmemory layout of your OS?\n▪ No!\n\n• The OS loads executables to specific places in memory.\n▪ The program expects data to be in a specific place.\n▪ The program expects memory to be “large enough.”\n\n• So, we will look at one strategy to define well-known stretches of\nmemory (virtually) that are mapped to physical memory (in reality.)\n\n9\n\n\fSegmentation\nIt’s not just a type of fault.\n\n10\n\n\fSegmentation\n• Segmentation is a virtual\nmemory system where spans\nof physical memory called a\nsegment are given a physical\nbase address.\n• The application refers to the\nvirtual address by its segment\nindex which is translated by\nhardware into the real address\nbehind-the-scenes.\n\n• Here, we have a segment table\nwhich defines two segments.\n▪ The first segment defines a range of\naddresses from\nto\n.\n▪ Second is\nto\nSegment Table\nIndex\n\nPhysical\nBase\n\nSize\n\n11\n\n\fAddress Translation\n• The Memory Management Unit\n(MMU) is a hardware component of\nyour CPU that translates virtual\naddresses to physical addresses.\n• Here, it translates based on the\nsegment table.\nAddress Space\n\nSegment Table\nIndex\n\nPhysical\nBase\n\nSize\n\nMMU\n\nCPU\n\nDynamic RAM (DRAM)\n\n12\n\n\fAddressing the Code Segment\n• The MMU translates\naddresses by looking\nup the segment index\nand adding the base to\nthe given offset.\nMMU\n\nMemory load\n\nSegment Table\nIndex\n\nPhysical\nBase\n\nSize\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n13\n\n\fAddressing the Data Segment\n• The same offset might\nrefer to a different\nphysical address\ndepending on the\nindex and the table.\nMMU\n\nMemory load\n\nSegment Table\nIndex\n\nPhysical\nBase\n\nSize\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n14\n\n\fAddressing… nothing\n• However, if the MMU\ncannot translate an\naddress, it will fault.\n• This is a segmentation\nfault.\n\nMMU\n\nMemory load\n\nSegment Table\nIndex\n\nPhysical\nBase\n\nSize\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n15\n\n\fAddressing… out of range\n• This is true even if the\naddress calculation results\nin an address that is\nallocated to another\nsegment.\n• Fault: It goes beyond the\nsize of the segment.\n\nMMU\n\nMemory load\n\nSegment Table\nIndex\n\nPhysical\nBase\n\nSize\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n16\n\n\fIt’s for your pwn protection\n• The lie only operates if processes cannot see each other.\n▪ It’s not just about address spaces not overlapping…\n▪ It is also for security purposes!\n\n• You don’t want your login process to be snooped on by another.\n\n• Yet, also, you don’t want your own program to do ridiculous things it\nshould not do!\n▪ Should your program be able to write to constant values?\n▪ Should your program be able to execute instructions in the\n\nsegment?\n\n• Virtual memory generally also has ways to arbitrate access.\n17\n\n\fAmending to add access control\n• The MMU can also arbitrate access\nto the segments by adding access\ncontrol to the segment table.\n• Here, a in the table denotes the\naction is allowed.\n▪\n▪\n\nWrites allowed.\nCan be executed.\n\nAddress Space\n\nSegment Table\nI\n\nPhysical\nBase\n\nSize\n\nW\n\nX\n\nMMU\n\nCPU\n\nDynamic RAM (DRAM)\n\n18\n\n\fWriting to the Code Segment? NO!!\n• When the MMU\ndecides if an action is\nallowed, it looks at the\naccess control bits in\nthe table.\nMMU\n\nMemory store\n\nSegment Table\nI\n\nPhysical\nBase\n\nSize\n\nW\n\nX\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n19\n\n\fExecuting the Data Segment? ABSOLUTELY NOT!!\n• This feature can be\nused to effectively\nprevent many buffer\noverflow attacks.\n• Here, you can’t\nexecute application\ndata.\n\nMMU\nInstruction Fetch\n\nSegment Table\nI\n\nPhysical\nBase\n\nSize\n\nW\n\nX\n\nAddress Space\n\nCPU\n\nDynamic RAM (DRAM)\n\n20\n\n\fA Problem Remains: Fragmentation\n• In a purely segmented system, you can map\nregions of physical memory.\n\n???\n\n• However, the segments of virtual memory\nare continuous in physical memory and\ncannot overlap other physical regions on the\nsystem.\n\n• We may run out of room as we run more\nprocesses…\n• … and as processes finish, they may leave\nawkward gaps in memory. (external\nfragmentation)\n21\n\n\fPaging\nThis won’t give you paper cuts... I don’t think.\n\n22\n\n\fMaking things… smaller.\n• So, segmentation helped us isolate\nprocesses by allowing a virtual address\nspace where large spans of memory\nwere mapped continuously to a\nphysical address range.\n• Since segments are large, managing\nthat space is difficult.\n• So why don’t we make the segments…\nsmall? And use more of them?\n\n• Welcome to the wonderful world of\npages!\n\nProcess’s Virtual\nAddress Space\n\n23\n\n\fPaging Mr. Herman… Mr. P. W. Herman…\n• Each segment is itself divided into\nsmaller pieces called a page.\n• This allows us to even interleave the\ndifferent pages that make up a section\nof memory.\n• Because of this interleaving and that\nevery page is the same exact size,\nremoving a page leaves room for\nexactly one page… no fragmentation.\n▪ At the cost of over-allocating, if we need\nless space than a single page.\n\nAllocating this:\n\nRemoving this\npage left room big\nenough for… a new\npage!\n\nProcess’s Virtual\nAddress Space\n\n24\n\n\fPage Tables\n• There are many strategies for\nmaintaining the metadata that maps\nvirtual addresses to physical\naddresses.\n\nCPU\n\nVirtual Address\nPage Index\n\nPage Offset\n\n• The first we will look at is the simple\npage table.\n• In this strategy, we will maintain a\ndata structure that maps virtual\naddresses to physical addresses.\n\nPhysical Address\nPage Index\n\nPage Offset\n\n25\n\n\fAddress Fields\nVirtual Address\n\n• First, you need to set a static\npage size.\n\nPage Index\n\nPage Offset\n\n▪ Every page is the same size.\nProcess Page Table\n\n• Part of the virtual address is the\noffset, which is retained when\nthe MMU translates the physical\naddress.\n\nValid\n\nWrite\n\nExecute\n\nPage Address\n\nIndex\n\n▪ This is determined by the page size.\nPhysical Address\n\n• The remainder is used to\ndetermine the entry in the table.\n\nPage Index\n\nPage Offset\n\n26\n\n\fIt’s not your fault…\nVirtual Address\n\n• If there is no entry for the given\npage or the entry isn’t valid…\n\nPage Index\n\nPage Offset\n\n▪ Or if an operation is not allowed.\nProcess Page Table\nValid\n\n• This signals a page fault.\n\nExecute\n\nPage Address\n\nIndex\n\n▪ Similar to a segmentation fault.\n▪ In fact, many OSes retain that term\nto this day, even when it is a page\nfault, technically.\n\n• This is a generic error that is\ntriggered by the MMU on such\ninvalid accesses.\n\nWrite\n\nDRAM\nCPU\n\nVirtual Address\n\nX MMU\n\n27\n\n\fProcess Isolation\n• To give each\nprocess its own\nvirtual address\nspace, each process\ngets its own page\ntable.\n\nPage Table\nAddress\nProcess Page Table\nValid\n\nWrite\n\nExecute\n\nPage Address\n\nIndex\n\n• The CPU keeps\ntrack of which page\ntable is active.\nDRAM\nCPU\n\nVirtual Address\n\nMMU\n\nPhysical Address\n\n28\n\n\fContext Switching: Getting to the root of it.\n• When an Operating\nSystem goes from one\nprocess to another, it\nperforms a context\nswitch.\n1. Store registers\n(including stack\npointer and program\ncounter) to memory.\n2. Determine next\nprocess to run.\n3. Load those registers\nfrom memory.\nSwitch address\nspace.\n4. Jump to old program\ncounter. Go!\n\nProcess A\n\nProcess B\n\nProcess C\n\nstack\n\nstack\n\nstack\n\n.bss\n.data\n.text\n\n.bss\n.data\n.text\n\n.bss\n.data\n.text\n\nCPU State A:\nRegisters\n,\n\nCPU State B:\nRegisters\n,\n\nCPU State C:\nRegisters\n,\n\nPage Table\nAddress A\n\nPage Table\nAddress B\n\nPage Table\nAddress C\n\nDRAM\nCPU\n\nVirtual Address\n\nMMU\n\nPhysical Address\n\n29\n\n\fAddressing the granularity issue\n• The table size has to do with how big\nyou make each page.\n▪ The bigger the page, the less entries you’ll\nneed for your process.\n▪ However, the more internal\nfragmentation if you do not need some of\nthat space!\n\n• For a page size of 𝐾\n▪ Page offset will have log 2 𝐾 bits.\n▪ Page index will be the remaining bits.\n\n• For 32-bit address spaces:\n▪ Assuming table entries are also 32-bits.\n\nVirtual Address\nPage Index\n\nPage Offset\n\n𝐾 = 216 𝐾𝑖𝐵 = 64𝐾𝑖𝐵\nMapping 2MiB takes 32 pages.\nPage table size: 216 ∗ 4𝐵 = 256𝐾𝑖𝐵\nPage Index\n\nPage Offset\n\n𝐾 = 212 𝐾𝑖𝐵 = 4𝐾𝑖𝐵\nMapping 2MiB takes 512 pages.\nPage table size: 220 ∗ 4𝐵 = 4𝑀𝑖𝐵\nPage Index\n\nPage Offset\n\n𝐾 = 28 𝐵 = 256𝐵\nMapping 2MiB takes 8192 pages.\nPage table size: 224 ∗ 4𝐵 = 64𝑀𝑖𝐵 30\n\n\fInverted Page Tables\n• There are many strategies for\nmaintaining the metadata that maps\nvirtual addresses to physical\naddresses.\n\nCPU\n\nVirtual Address\nPage Index\n\nPage Offset\n\n• Now we will look at the inverted page\ntable.\n• In this strategy, we switch things\naround: we have just one table for the\nwhole system and an entry for every\npossible physical page.\n\nPhysical Address\nPage Index\n\nPage Offset\n\n31\n\n\fAddress Fields\nVirtual Address\n\n• In this case, you have a single\ntable for the entire system.\n• When translating, you scan\nthe table to find an entry that\ncontains the page index.\n\nPage Tag\n\nPage Offset\n\nInverted Page Table\nPage Tag\n\nValid\n\nWrite\n\nExecute\n\nIndex\n\n▪ This may be intensive!\n\n• When you do, and it is valid,\nmake a note of the index of\nthe entry. That is the physical\npage index.\n\nPhysical Address\nPage Index\n\nPage Offset\n\n32\n\n\fProcess Isolation\n• Many processes exist, and\neach may use the same\nvirtual address.\n▪ And expect a different physical\npage!\n\nVirtual Address\nPage Tag\n\nProcess ID\n\nPage Offset\n\nInverted Page Table\nPage Tag\n\nProcess ID\n\nValid\n\nWrite\n\nExecute\n\nIndex\n\n• Since there is only one table\non the entire system, we\nhave to disambiguate.\n\n• Therefore, we also tag by the\nprocess identifier.\n\nPhysical Address\nPage Index\n\nPage Offset\n\n33\n\n\fWhat’s the size??\n• One nice feature of an inverted\npage table is the size is bound.\n\nInverted Page Table\nPage Tag\n\nProcess ID\n\nValid\n\nWrite\n\nExecute\n\n• Since an inverted page table has\nan entry for every possible\nphysical page…\n▪ You can simply allocate the table\nof a fixed size big enough to\nrepresent all of physical memory.\n\n• The size of the table is the\nproduct of the entry size and\nthe number of physical pages.\n\nIf the page size (K) is 4KiB and our\nsystem has 16GiB of RAM, how big\nis the inverted page table?\n234\n16𝐺𝑖𝐵 \/ 4𝐾𝑖𝐵 = 12 = 222 𝑝𝑎𝑔𝑒𝑠\n2\n222 𝑝𝑎𝑔𝑒𝑠 ∗ 32 𝑏𝑖𝑡𝑠 = 222 𝑝𝑎𝑔𝑒𝑠 ∗ 4𝐵\n224 𝐵 = 24 220 𝐵 = 16𝑀𝑖𝐵\n\n34\n\n\fTrade-offs. Trade-offs everywhere!\n• What is best? … Who even knows.\n• Inverted page tables are very space efficient since entries are ordered\nby physical page.\n▪ However, translations mean scanning the table for entries… a time-consuming\ntask. 𝑂(𝑛) (Can implement with a hashing function, see your OS course.)\n▪ Normal page tables are a constant time lookup, 𝑂 1\n\n• Since they are indexed by virtual address, normal page tables require\nordered virtual memory to be space efficient.\n▪ Gaps in virtual memory mean lots of page table entries going unused.\n▪ Perhaps we can solve this problem…\n35\n\n\fMulti-level Page Tables (Not a pyramid scheme)\n• Perhaps we can allow\ngaps in virtual memory\nif we use MORE\nINDIRECTION!\n\n32-bit Virtual Address (\nPage Table\nRoot Address\n\nSecond Index\n\nFirst Index\n\n)\nPage Offset\n\n2nd Level Page Table\n\n• The use of multiple\nlevels of indirection\ngives a lot of flexibility\nin defining the virtual\naddress space.\n• Each page table is the\nsize of a page. (\n)\n\n1st Level Page Table\n\nDRAM\n\n36\n\n\fIndirection times two\n• We split up the virtual\naddress into further\nindex fields.\n\n32-bit Virtual Address (\nPage Table\nRoot Address\n\nSecond Index\n\nFirst Index\n\n)\nPage Offset\n\n2nd Level Page Table\nIndex\n\n• The top-level index\nyields the real\nphysical address of\nthe page containing\nthe next page table.\n\nWrite\n\nExecute\n\n1st Level Page Table (Real\nIndex\n\n• This table is used to\ndetermine the referred\nphysical page.\n\nValid\n\nValid\n\nWrite\n\nExecute\n\nPage Table Address\n\n)\nPage Address\n\nDRAM\n\n37\n\n\fHome, home on the [memory] range\n• Each entry in the toplevel page table\nrepresents an entire\nrange of memory.\n\n32-bit Virtual Address (\nPage Table\nRoot Address\n\n• Here, the 2nd level index\nis\n. This represents all\nvirtual memory addresses\nwith the most significant\nbinary digits:\n\nFirst Index\n\nPage Offset\n\n2nd Level Page Table\nIndex\n\nValid\n\nWrite\n\nExecute\n\nPage Table Address\n\n1st Level Page Table (Real\nIndex\n\nMaps 4KiB virtual page starting at 0x00400000\nMaps 4KiB virtual page starting at 0x00401000\n...\nMaps 4KiB virtual page starting at 0x007ff000\n\nSecond Index\n\n)\n\nValid\n\nWrite\n\nExecute\n\n)\nPage Address\n\n38\n\n\fIt’s a sparse world, after all\n• By marking entries\ninvalid in the top-level\npage table, this\ninvalidates the entire\nmemory range.\n• Attempting to access\nsuch a virtual address\nwould immediately page\nfault.\n\n32-bit Virtual Address (\nPage Table\nRoot Address\n\nSecond Index\n\nFirst Index\n\n)\nPage Offset\n\n2nd Level Page Table\nIndex\n\nValid\n\nWrite\n\nExecute\n\nPage Table Address\n\nNo 1st level table.\n\nTherefore: all virtual addresses between\nand\nare not mapped (and are not referenceable.)\n\n39\n\n\fA got a sparsity jacket, but it was just the sleeves.\n32-bit Virtual Address\n\n• Given a 32-bit virtual address.\n\nSecond Index\n\n▪ And multi-level paging with two\nlevels, each index 10 bits.\n\n• What is the page size?\n▪ 32 − 10 − 10 = 12 𝑏𝑖𝑡𝑠 for offset\n▪ 212 𝐵 = 4𝐾𝑖𝐵 (4 Kibibytes)\n\nFirst Index\n\nPage Offset\n\n2nd Level Page Table\nIndex\n\nValid\n\nWrite\n\nExecute\n\nPage Table Address\n\n• Given the root page table here,\nand assuming unknown entries\nare invalid, what virtual address\nranges are potentially used?\n▪ Let’s find out…\n40\n\n\fContinuing: Filling in the blanks\n• Given the root page table here, and\nassuming unknown entries are invalid, what\nvirtual address ranges are potentially used?\n\nSecond Index\n\nFirst Index\n\nPage Offset\n\n2nd Level Page Table\n\nto\n\nMaps virtual pages from\n\n32-bit Virtual Address\n\nIndex\n\nValid\n\nWrite\n\nExecute\n\nPage Table Address\n\nto\nto\n\nMaps virtual pages from\n\nto\nto\n\nMaps virtual pages from\n\nto\n\n41\n\n\fBest of both worlds.\n• With multi-level page tables, we can\nrepresent large ranges of memory\nwith gaps, much like segments!\n\n32-bit Virtual Address\nSecond Index\n\n▪ All the while, we can satisfy each\nindividual page in this “segment” by\ninterleaving them throughout physical\nram. (flexibility, no external\nfragmentation)\n\n2nd Level Page Table\n\n1st Level Page Table\n\n▪ x86-64 uses a 4 level page table!\nto\nto\nto\n\nPage Offset\n\nPage Table\nRoot Address\n\n• Modern architectures often use\nmulti-level page tables.\nMaps virtual pages from\nMaps virtual pages from\nMaps virtual pages from\n\nFirst Index\n\nSegments?\n(kinda)\n\nDRAM\n\n42\n\n\fSummary\nSo, we have many complex processes running at the same time.\n• How to present a consistent address space?\n▪ Indirection using segments or page tables.\n▪ We translate virtual addresses to physical addresses.\n\n• How to prevent other processes from interfering?\n▪ We can mark segments or individual pages with access controls. (Read-only,\nnon-execute, etc.)\n\n• How to prevent address space fragmentation?\n▪ We give each process its own address space.\n▪ When we context switch, we switch address spaces.\n43\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":182,"segment": "self_training_1", "course": "cs0447", "lec": "lec08", "text":"#8\n\nBitwise Operations\nand Bitfields\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce Childers, David Wilkinson\nFall 2020\n\n\fAnnouncements\n\n2\n\n\fLogical, right?\nclass Main {\npublic static void main(String[] args) {\nboolean a = true;\nboolean b = true;\nSystem.out.println(a||b);\n}\n}\n\nclass Main {\npublic static void main(String[] args) {\nint a = 4;\nint b = 1;\nSystem.out.println( a|b );\n}\n}\n\n● What is the output?\n\ntrue\n\n● What is the output?\n\n5\n3\n\n\fWhat are \"bitwise\" operations?\n● The \"numbers\" we use on computers aren't really numbers right?\n● It's often useful to treat them instead as a pattern of bits\n● Bitwise operations treat a value as a pattern of bits\n\n0\n1\n\n0\n\n0\n\n0\n4\n\n\fThe simplest operation: NOT (logical negation)\n● If the light is off, turn it on\n● If the light is on, turn it off\n\nA\n\nQ\n\n0\n\n1\n\n1\n\n0\n\n● We can summarize this in a truth table\nഥ\n● We write NOT as ~A, or ¬A, or A\n\n5\n\n\fApplying NOT to a whole bunch of bits\n● if we use the not instruction (or ~ in C\/Java), this is what happens:\n\n~ 0 0 1 1 1 0 1 0\n\n= 1 1 0 0 0 1 0 1\nwe did 8 independent NOT operations\nthat's it it's super simple\nonly 8 bits shown cause 32 bits on a slide is too much\n\n6\n\n\fLet's add some switches\n● There are two switches in a row connecting the light to the battery\n● How do we make it light up?\n\n7\n\n\fAND (Logical product)\n● AND is a binary (two-operand) operation\n● it can be written a number of ways:\n\no A&B\n\nA∧B\n\nA⋅B\n\nAB\n\n● if we use the and instruction (or & in C\/Java):\n\nA B Q\n0 0 0\n\n1 1 1 1 0 0 0 0\n& 0 0 1 1 1 0 1 0\n\n0 1 0\n\n= 0 0 1 1 0 0 0 0\n\n1 1 1\n\n1 0 0\n\nwe did 8 independent AND operations\n8\n\n\f\"Switching\" things up\n● NOW how can we make it light up?\n\n9\n\n\fOR (Logical sum…?)\n● we might say \"and\/or\" in English\n● it can be written a number of ways:\n\no A|B\n\nA∨B\n\nA+B\n\n● if we use the or instruction (or | in C\/Java):\n\nA B Q\n0 0 0\n\n1 1 1 1 0 0 0 0\n| 0 0 1 1 1 0 1 0\n\n0 1 1\n\n= 1 1 1 1 1 0 1 0\n\n1 1 1\n\n1 0 1\n\nWe did 8 independent OR operations.\n10\n\n\flui, ori…\n● If I write li t0, 0xDEADBEEF in MIPS, the assembler turns it into:\n\nlui at, 0xDEAD\nori t0, at, 0xBEEF\nNever use at!!\nNEVER!!\n\n● at is used by the assembler, sooooo…\n● The reason it splits it up is that there's only enough space in each\ninstruction to fit half of 0xDEADBEEF\no As we’ve seen in the lab, each immediate is 16 bits long\no We'll learn about instruction encoding later\n● What the heck are these instructions doing tho\n\n11\n\n\fBy your powers combined…\n● lui means load upper immediate. it puts the immediate value into the\nupper 16 bits of the register, and zeroes out the rest\n\nlui at, 0xDEAD\n● then, ori does logical OR of at and its zero-extended immediate\n\nori t0, at, 0xBEEF\n11011110101011010000000000000000\n| 00000000000000001011111011101111\n\n11011110101011001011111011101111\n\nD\n\nE\n\nA\n\nD\n\nB\n\nE\n\nE\n\nF\n12\n\n\fBit shifting\n\n13\n\n\fBit shifting\n● besides AND, OR, and NOT, we can move bits around, too.\n1 1 0 0 1 1 1 1 if we shift these\nbits left by 1…\n\n1 1 0 0 1 1 1 1 0 we stick a 0 at the bottom\n1 1 0 0 1 1 1 1 0 0 again!\n\n1 1 0 0 1 1 1 1 0 0 0 AGAIN!\n1 1 0 0 1 1 1 1 0 0 0 0 AGAIN!!!!\n14\n\n\fLeft-shifting in C\/Java and MIPS (animated)\n● C and Java use the << operator for left shift\n\nB = A << 4; \/\/ B = A shifted left 4 bits\n\n● MIPS has the sll (Shift Left Logical) instruction\n\nsll t2, t0, 4 # t2 = t0 << 4\n● MIPS has the sllv (Shift Left Logical Variable) instruction\no No, registers are not variables!\n\nsllv t2, t0, t1 # t2 = t0 << t1\n\n● if the bottom 4 bits of the result are now 0s…\no …what happened to the top 4 bits?\n\n0011 0000 0000 1111 1100 1101 1100 1111\nBit\nBucket\n\nthe bit bucket is not a real place\nit's a programmer joke ok\n15\n\n\f<_< >_> <_<\n● we can shift right, too\n0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1\n0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1\n0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0\n\n● C\/Java use >>, MIPS uses srl (Shift Right Logical)\n\n16\n\n\fNumbers they are a-changing\n● let's start with a value like 5 and shift left and see what happens\n\nBinary\n101\n1010\n10100\n101000\n1010000\n\nDecimal\n5\n10\n20\n40\n80\n\nwhy is this happening\nwell uh... what if I gave you\n\n49018853\n\nhow do you multiply that by 10?\nby 100?\nby 100000?\n\nsomething very similar is\nhappening here\n\n17\n\n\fa << n == a * 2n\n● shifting left by n is the same as multiplying by 2n\no you probably learned this as \"moving the decimal point\"\n▪ and moving the decimal point right is like shifting the digits left\n● shifting is fast and easy on most CPUs\no way faster than multiplication in any case\n● hey… if shifting left is the same as multiplying…\n\n18\n\n\fa >> n == a \/ 2n, ish\n● You got it\n● Shifting right by n is like dividing by 2n\no sort of.\n● What’s 510 (01012) shifted right by 1?\no 102, which is 2…\n▪ It's like doing integer (or flooring) division\n▪ Which is a fancy way of saying we round to the smallest number\n\n● What if the number is signed?\n● What’s -310 (next class: 11012) shifted right by 1?\no 01102, which is 610???\n▪ Ohhhhh\n\n19\n\n\f<_< >_> <_<\n● We can do sign-extension on shifts, too!!\n1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1\n1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1\n1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0\n\n● MIPS uses sra (Shift Right Arithmetic)\n● What’s -310 (11012) shifted right by 1?\no 11102, which is -210\n▪ Why is this -2????? next class\n\n20\n\n\fMIPS ISA: Bitwise operations\n● Bitwise logical operators\n\nInstruction\n\nMeaning\n\nnot a, b\n\na = ~b\n\nor a, b, c\n\na = b|c\n\nori a, b, imm\n\na = b|imm\n\nand a, b, c\n\na = b&c\n\nandi a, b, imm\n\na = b&imm\n\n21\n\n\fMIPS ISA: Bitwise shifts\n● Bitwise logical operators\n\nInstruction\n\nMeaning\n\nsll a, b, imm\n\na = b<<imm\n\nsllv a, b, c\n\na = b<<c\n\nsrl a, b, imm\n\na = b>>imm (zero extension)\n\nsrlv a, b, c\n\na = b>>c\n\nsra a, b, imm\n\na = b>>imm (sign extension)\n\nsrav a, b, c\n\na = b>>c\n\n(zero extension)\n\n(sign extension)\n\n22\n\n\fBitfields\n\n23\n\n\fclicky clicky\n● In the LED Keypad plugin in MARS, input works like this:\ninput_get_keys returns a value in v0…\n\n0 0 0 0 0 0 0 0 0 0 0 0\nB R L D U\n24\n\n\fWhy do we do this??\n● It lets us cram several booleans into a single value!\n● This technique is known as bit flags\n\n1 0 1 0 0\nB R L D U\n\n0 1 0 1 0\nB R L D U\n25\n\n\fThe masters of meaning\n● well what if we wanted to store multiple integers in one value?\n\n15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\n1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1\n\ndecimal?\n\nred\n\ngreen\n\nblue\n\n23\n\n32\n\n19\n\nThat's this color, in RGB565.\n\n26\n\n\fThe masters of meaning\n● This bitfield has 3 fields: red, green, and blue\n\n15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\n1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1\nred\n\ngreen\n\nblue\n\nPosition 11\n\nPosition 5\n\nPosition 0\n\n27\n\n\fWhyyyyy???\n● It's smaller\no really, that's it\no but that's super important in a lot of cases\n● Smaller data…\no Takes up less space in memory\no Takes up less space in cache\n\n▪ extremely important thing in modern CPUs that we talk about in 1541\n\no Is faster to move between memory and the CPU\no Is faster to transfer across the internet and other networks\no It allows a MIPS instruction to contain references to multiple registers\n\n28\n\n\fI wanna turn the light on!!\n● I have a sequence of 0s. I wanna turn one of them into a 1.\n● what bitwise operation can I use to do that?\n\n0\n? 1\n1\n\n0\n0\n0\n\n0\n0\n0\n\n0\n0\n0\n\n29\n\n\fI wanna turn the light off!!\n● I wanna turn one of the 1s into a 0.\n● what bitwise operation can I use to do that?\n\n1\n? 1\n1\n\n0\n1\n0\n\n1\n0\n0\n\n1\n1\n1\n\n30\n\n\fTurning off the first three, leaving the others alone\n● more bits, but one of the same operations…\n\n1\n? 0\n0\n\n0\n0\n0\n\n1\n0\n0\n\n0\n1\n0\n\n1\n1\n1\n\n1\n1\n1\n\n31\n\n\fRemember this?\n\nlui at, 0xDEAD\nori t0, at, 0xBEEF\n11011110101011010000000000000000\n| 00000000000000001011111011101111\n\n11011110101011011011111011101111\n\nD\n\nE\n\nA\n\nD\n\nB\n\nE\n\nE\n\nF\n\n32\n\n\fHow can we assemble on of these bitfields?\n15 14\n\n13\n\n12\n\n11 10\n\nred\n\n1 0 1 1 1\n\n9\n\n8\n\n7\n\ngreen\n\n6\n\n5 4\n\n3\n\n2\n\n1\n\n0\n\nblue\n\n1 0 1 1 1\n1 0 0 0 0 0\n1 0 0 0 0 0\n1 0 0 1 1\n\n33\n\n\f…\n● hmm\n\n1 0 1 1 1\n\n0 0 0 0 0 0 0 0 0 0 0\n\n0 0 0 0 0\n0 0 0 0 0\n\n1 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 1 0 0 1 1\n\n1 0 1 1 1\n\n1 0 0 0 0 0 1 0 0 1 1\n\n34\n\n\fLeft-shifting and ORing\n● if you have the values of the fields\n● and you want to put them together into a bitfield\no shift each value left to the correct bit position\no OR the shifted values together\n● for RGB565,\no red is shifted left 11\no green is shifted left 5\no blue isn't shifted (shifted left 0…)\n\ncolor = (red << 11) | (green << 5) | blue;\n\n35\n\n\fGoing the other way\n● let's go from the bitfield to three separate values.\n\n15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\n1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1\nlet's say we somehow set all the non-red bits to 0.\n\n1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0\nwhat value is this?\nit's not 23, that's for sure.\nso how do we fix that?\n\n36\n\n\fIt's the exact opposite\n● we have to shift right to put the field at position 0\n\n15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\n1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1\nshift right by 11 and…\n\n0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1\ncool. what about green? shift right by…?\n\n0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0\nuh oh.\n\n37\n\n\fMasquerade\n● we need to get rid of (zero out) the bits that we don't care about\n● a mask is a specially-constructed value that has:\no 1s in the bits that we want to keep\no 0s in the bits that we want to discard\n● which bits do we want to keep? which do we want to discard?\n\n0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0\n&\n\n0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\nthis is the mask\n\n38\n\n\fComing up with the mask value\n● if you want to mask a 3 bit value, the mask is 1112\n● if you want to mask a 4 bit value, the mask is 11112\n● if you want to mask a 5 bit value, it's…?\n\nSize(n) Mask\n1112\n3\n11112\n4\n111112\n5\n\n2n\n8\n16\n32\n\nMask in\ndecimal\n7\n15\n31\n2n-1\n39\n\n\fRight-shifting and ANDing\n● to extract one or more fields from a bitfield:\no shift the bitfield right to put the desired field at bit position 0\no AND that with 2n-1, where n is the number of bits in the field\n● so for RGB565…\no the red and blue masks are 25-1 = 31 (or 0x1F)\no the green mask is 26-1 = 63 (or 0x3F)\n\nred = (color >> 11) & 0x1F;\ngreen = (color >> 5) & 0x3F;\nblue = color & 0x1F;\n\n40\n\n\fNOW it works\n● let's extract green\n\n15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\n1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1\nshift right by 5 and…\n\n0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0\nand then AND with 0x3F…\n\n0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n\n41\n\n\fCan't you AND then shift?\n● sure, but…\n\n15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\n1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1\nAND with 0x7E0 (!)…\n\n0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\nshift right by 5…\n\n0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\nwhere did I get 0x7E0??\nit's 0x3F << 5.\n\nI feel like that's uglier.\n\nExperience shows me it leads to more mistakes\nSo don’t do this: Keep the mask aligned to the right!!!\n42\n\n\fExercise!\n● This bitfield represents the following MIPS instruction:\naddi s0, zero, 0x1234\n\n3 30 29 28 27 2 2 24 23 22 2 2 19 18 17 1 1 14 13 12 11 10 9 8 7 6 5 4 3 2 1\n0\n1\n65\n10\n65\n00100000000100000001001000110100\nopcode\n\nrs\nIt’s addi!\n\nrt\nzero\n\nimm\nLoad into this\nregister\n\nThis number\n\n● Using these operations: <<, >>, &, |, ~ write expressions to extract the value\nof each field\no E.g.: ( field << 3 )|( ~0xFFFF )\n43\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":183,"segment": "unlabeled", "course": "cs0449", "lec": "lec15", "text":"15\n\nThreads\nand\n\nSynchronization\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fThreads\nStrings? Threads?? What are we building… a loom???\n\n2\n\n\fOur story so far…\n• We looked at how processes reproduce with\n▪ This gave us some type of concurrency.\n▪ It is process-level, so the OS is scheduling each task.\n\n• We saw some issues with concurrent programming.\n▪ Race conditions mean we have to much more carefully\nconsider our code.\n\n• This time…\n▪ We will look at other forms of concurrency.\n▪ Some new methods of coordinating the different subprograms.\n▪ And some new… dreaded… types of concurrency bugs.\n3\n\n\fThreads\n• Process-level concurrency with\npowerful, but inflexible.\n\nis\n\n▪ The OS schedules the task, incurring context\nswitching overhead.\n▪ The process memory is copied making it hard\nto share data among tasks.\n\n• A thread is a concurrency primitive that is\ninner-process.\n▪ The program itself schedules the task as part\nof the same process.\n▪ Process memory, therefore, is shared across all\nthreads.\n4\n\n\fRecall our friend Dolly…\n• The\n\nsystem call in action:\n\n▪ Copies the memory layout.\n▪ Copies the process state. (but gives it a unique ID)\n\nA\n\nCS\/COE 0449 – Spring 2019\/2020\n\nParent\n\nChild\n\nstack\n\nstack\n\n.bss\n.data\n.text\n\n.bss\n.data\n.text\n\nCPU State A:\nRegisters\n,\n\nCPU State B:\nRegisters\n,\n\nPID: 4356\n\nPID: 6567\n\nPage Table A\n\nPage Table B\n\nB\n\n5\n\n\fDolly learned a new trick!\n• However, with threads… we retain much of the address space.\n▪ Threads share code\/data\/etc, however they have their own stack and CPU state.\n▪ They execute in parallel with one another interacting directly with the same data.\nProcess\n\n.bss\n.data\n.text\n\nA\n\nstack\n\nstack\n\nstack\n\nCPU State A:\nRegisters\n,\n\nCPU State B:\nRegisters\n,\n\nCPU State B:\nRegisters\n,\n\nPID: 4356\nPage Table A\n\n6\n\n\f• The 2011 amendment to the C standard (C11) added a threading API.\n▪ However, we will still be looking at an older, more prevalent standard.\n\n• We will be reviewing the\n\nstandard.\n\n▪ The C11\nAPI is still very similar.\n▪ There are ports of the\ninterface to many OSes.\n▪ Lots of threading APIs in other language emulate it.\n\n• Still very useful to learn!\n\n7\n\n\fPOSIX\n• The “p” in pthread stands for the Portable\nOperating System Interface (POSIX).\n▪ This is a standard for creating OS abstractions.\n▪ Intended to lower the burden of porting applications.\n• Most OSes conform to most POSIX standards.\n• However, very few OSes fully implement POSIX.\n\n• POSIX standardizes threads, but also process\ncreation and the behavior of fork, file\nabstractions, and how data is shared among\nprocesses.\n▪ Many OS interfaces are POSIX interfaces and remain\n(mostly) true across different platforms.\n8\n\n\fCreating… hmm… no… weaving a thread\nC(\n\n)\n\n• Here is a basic threaded program.\n▪\n\nThis function runs in a thread\n\nHolds the thread ID.\n\nThe thread function\n\nSpring 2019\/2020\n\nFunction\nargument\n\nis within the main thread.\n\n• The\nfunction\ncreates a second thread, which\nruns alongside the main thread.\n▪ The first argument is the address of\na variable to hold the thread ID.\n▪ The NULL is where you can add\nsome flags, but the defaults are OK.\n▪ The thread is the function to use.\n▪ The last argument is passed to that\nfunction and generally an address.\n9\n\n\fThe race to the finish.\nC(\n\n)\n\nThis never happens!\n\n• However, when the process exits\nnormally, all threads are also\ncanceled, even if they haven’t\ncompleted.\n• In this run, the second thread\nnever prints its message.\n\nIn the end of the program, threads are also all exited, potentially prematurely.\n\n10\n\n\fBeing considerate\nC(\n\n)\n\n•\nThe “str” argument.\n\nwaits for the\ngiven thread to exit by thread ID.\n▪ The NULL is, again, optional flags.\n\n• Here, the main thread waits until\nthe thread function completes.\n▪ It prints out the string given by the\nargument.\nWaits…\nGuaranteed to happen only after\n\ncompletes.\n\n11\n\n\fSharing is caring\nC(\n\n)\n\nThread function increments\n\n• Unlike process-level concurrency\nusing\n, threads share\nmemory.\n• Each thread, here, shares access to\nthe same global variable\n.\n▪ When the main thread updates, the\nsecondary thread sees that value.\n\n• Threads share the same virtual\naddress space (and page table.)\nMain thread increments, too!\n\n▪ They only have their own stack and\nCPU state.\n12\n\n\fA problem returns with a vengeance\nC(\n\n)\n\nThread function might get\ninterrupted before the print\n\nThen main thread increments! :(\nRace Condition 13\n\n\fWhat happened???\nC(\n\n)\n\nThread function increments\n\n• Since the threads share memory,\naccess to a variable, such as this,\nmay require extra care.\n• When the main thread gets\ninterrupted just as it was printing\nthe value, the thread is scheduled.\n▪ The thread prints the value instead.\n▪ Then the main thread, when it\ncontinues, prints it again!\n\nMain thread increments, too!\n\n• If only we had a way to… align\nthem in time… what’s the word…\n\n14\n\n\fSynchronization\nYou can’t touch this! Stop! Hammer time!\n\n15\n\n\fA story about the railroad\n• Systems scientists have long been inspired\nby the real-world for insight on design.\n• The rail system requires a lot of attention to\ndetail to provide:\n▪ Orderly and timely scheduling of trains.\n▪ Shared use of a single resource: rail.\n▪ Coordination with trains and competing interests.\n\n• In order to provide this, trains make use of\nsignals and switching areas.\n▪ Trains wait while others pass, all agreeing on the\nnature of signals.\n▪ The signals are called semaphores.\n\n16\nPhoto by David Ingham\n\n\fThe seminal semaphore\n• A semaphore is a special counter used for synchronization.\n▪ Invented by Dutch systems scientist Edsger Dijkstra in the early 1960s.\n\n• The counter is a signed integer that often starts at zero or one.\n• Two defined operations:\n▪ Up (signal\/release); increments counter.\n▪ Down (wait\/acquire); decrements counter but waits if the counter is 0.\n\n• These operations often have different names or are abbreviated:\n▪ V (Based on Dutch vrijgave “to release”)\n▪ P (Based on Dutch passering “to pass”, based around railroad terminology)\n17\n\n\fSemaphores to prevent the derailing\nC(\n\n)\n\n•\n\ncreates a new semaphore.\n▪ The first argument is an address to a\nvariable that will hold the semaphore data.\n▪ The second argument, when 0, means that\nother threads can see the semaphore.\nNon-zero means other threads cannot\ninteract with the semaphore, which is a bit\nmore advanced.\n▪ The third argument is the initial value.\n\nCritical Section\n\n• Here it is 1.\n\n•\n\ndecrements the counter.\n▪ Waits to decrement if the counter is 0.\n\n•\n\nincrements the counter.\n▪ May release a thread waiting at\n\n18\n\n\fSemaphores to prevent the derailing\nC(\n\n)\n\n• When both threads hit\nat\nthe same time, only one continues.\n• When one sets the lock; other waits.\n\nCritical Section\n\n▪ The other thread relies on the first to\neventually release the lock using\n▪ When this happens, the other thread can go.\n\n• The lock\/unlock pattern creates a\ncritical section, a piece of code that has\nthe guarantee that only one task can\nenter at a time.\nSpring 2019\/2020\n\n▪ Here, the counter is guaranteed to update at\nthe same time as it is printed.\n19\n\n\fSemaphores to prevent the derailing\nC(\n\n)\n\nCritical Section\n\n20\n\n\fMutex… ew… don’t like the sound of that\n• As you can see, there is a common case.\n▪ Simple critical sections just need a counter that covers 0 and 1.\n\n• A mutex is a special Boolean used for synchronization.\n▪ It is short for “mutual exclusion,” a term for when two things can only have one\nresource at a time.\n\n• There are two defined operations:\n▪ lock \/ wait; only proceeds if the mutex is unlocked.\n▪ unlock \/ release; unlocks the mutex.\n\n• A mutex can be created using a semaphore.\n▪ It provides a subset of the capabilities of the more general semaphore.\n\n21\n\n\fA mutex to prevent the derailing\nC(\n\n)\n\n• Mutexes are useful for locking\nsingle resources.\n▪ It follows much the same pattern as\nsemaphores, and perhaps easier to\nunderstand.\n\nCritical Section\n\n•\n\n•\n\nmutex similarly to\n\ncreates the\n.\n\nand\ndo the\nlocking and unlocking, as\nexpected.\n22\n\n\fA mutex to prevent the derailing\nC(\n\n)\n\nCritical Section\n\n23\n\n\fStrategies\n• Semaphores and mutexes are both primitives to aid in concurrent\nprogramming.\n• We saw, here, another example of a race condition, a concurrency\nbug where the absence of guaranteed order can result in incorrect\nbehavior.\n▪ Namely, threads being interrupted in-between operations that need to happen\ntogether and racing another thread that will incorrectly use that intermediate\nvalue.\n\n• However, that’s not the only type of concurrency bug we can have!\n▪ Yay! \n24\n\n\fParallel Pitfalls\nThis is like that time when a bird pooped on me the same time I stepped in a very\nmuddy puddle.\n\n25\n\n\fEverybody loves resources\nResource contention:\n• Printer needs paper…\n• You need to buy some\npaper…\n• You need to print an order\nform for paper…\n• Printer needs paper…\n26\n\n\fTypically…\n• Metaphor: intersection.\n• The intersection is a shared\nresource, much like a device or\nthe CPU.\nIt is fine when two\ntasks can share a\nresource without\nconflict… they do\nnot need to\ncoordinate.\n\n• Multiplexing the intersection is\nimportant to avoid crashes.\n• When the streets aren’t busy,\ncars just make it safely across.\n27\n\n\fDeadlock: The traffic jam (it’s not very delicious)\n• However, in some\ncircumstances, several cars\nmay reach the intersection at\nthe same time.\n\nNot fine if tasks\nneed a common\nresource at the\nsame time without\nan agreed way to\nproceed.\n\n• If there is no previously\ndefined way to handle this,\nthey all wait for the others to\nget out of the way.\n▪ Forever.\n\n• Deadlock occurs when multiple\ntasks are waiting for each\nother, making no progress.\n\n28\n\n\fSynchronization solves deadlock\n• Deadlock is a bug that needs\nextra consideration to avoid.\n• In this case, you need some\nmethod of making only some\nof the cars (tasks) wait, while\nletting others go.\n\nWith synchronization,\n▪ Traffic light, perhaps\ntasks can agree which\nget to go next, and\nwhich have to wait. • Beyond defining order,\n\nsynchronization helps avoid\nthese types of logical errors.\n29\n\n\fStarved for attention…\n• Another issue, related to\ndeadlock, is starvation where\nthe system makes progress but\none task is perpetually delayed.\n\n• When some tasks have priority\nover resources, they may not\ngive them up for other tasks.\n\nWhen nobody yields\nto a particular task\n▪ Those tasks wait forever.\nand gives up a shared\nresource, that task\ncannot proceed!\n• Without a traffic light, you rely\n\non people being nice. :(\n\n30\n\n\fStarvation: a matter of fairness\n• This can happen in situations where “fairness” scheduling goes awry.\n• If you have a webserver, the OS might schedule that process whenever\nthere is some incoming requests.\n▪ What if you are getting a lot of traffic!\n▪ The OS might always schedule the webserver.\n▪ Important background tasks might not run!\n\n• Preventing starvation might be keeping track of how much time a\nprocess has a resource and how long it has waited in line.\n▪ Low-priority tasks start at the back of the line and move up the queue the longer\nthey wait… eventually cutting in front of high-priority tasks that start in the front.\n\n• That’s just one idea. Scheduling resources is a very difficult problem!\n\n31\n\n\fLivelock: The hallway problem\n• There is a narrow hallway.\n• Let’s say you have two very polite\npeople.\n• They walk toward each other… and try\nvery hard to get out of each other’s way.\n▪ They keep insisting the other go ahead of\nthem.\n\n• This is livelock, where two tasks are\nactively signaling the other to go and\nmaking no progress.\nSpring 2019\/2020\n\n32\n\n\fCareful design works around livelock\n• Livelock can be solved using a tiebreaking scheme.\n▪ Just find something comparable and unique\namong the tasks to create an arbitrary\npriority.\n\n• All threads have IDs, so one easy\nstrategy is to have the largest ID yield to\nthe smaller.\n▪ This also helps starvation since livelock is\nstarvation to the extreme: where everybody\nis starving.\n\n33\n\n\fDeadlock vs. Livelock\n• In deadlock, all tasks are waiting for a signal that will never happen.\n▪ They are inactively achieving nothing. (“ZZZZZZZZ”, “ZZZZZZZZ”)\n\n• In contrast, livelock occurs when each task signals the other, and they\nrespond by signaling back. (“No, you.” “No… you!”)\n▪ They are actively achieving nothing.\n\n• Detecting that your program has a deadlock or livelock is tricky.\n▪ When it does, it may only happen a small percentage of the time.\n▪ In your OS course, you will learn more about deadlock detection and resolution.\n\n34\n\n\fSolving things\n• Proper synchronization and planning can solve all these issues.\n▪ Deadlock: Avoid patterns of critical sections that depend on each other.\n▪ Livelock: Establish a tie-breaking mechanism (thread with smallest ID goes first!)\n▪ Yet, it takes a good deal of programming experience to handle them.\n\n• The wide prevalence of multiprocessing and multithreading capable\ncomputers in the hands of average consumers is changing\nprogramming.\n▪ New (and old) languages are being pushed for their better handling of\nconcurrency issues.\n▪ Best-practices and frameworks continue to adapt to avoid many of the pitfalls\nwe have discussed today.\n▪ Pay attention in your compilers and OS course to hone your own skill!\n35\n\n\fbasic API summary\n• Thread creation\n▪\n\n• Join threads (wait until complete)\n▪\n\n• Getting thread ID\n\nWaits for the given thread to end.\n\nReturns the thread ID of the current thread.\n\n▪\n\n• Thread destruction (explicit)\n▪\n▪\n\nAttempts to preemptively exit the given thread.\nEnds current thread and returns the provided value.\n\n36\n\n\fsynchronization API summary\n• Semaphores\n▪\n▪\nCreates a semaphore with the given initial value. (The second argument means it the semaphore\ndata is in shared memory. If non-zero, it can’t be seen by other threads.)\n\n▪\n▪\n\nDecrements counter unless it is 0 in which case it waits.\nIncrements counter.\n\n• Mutexes\n▪\n▪\n▪\n▪\nSpring 2019\/2020\n\nCreates a mutex (unlocked).\nWaits until it can lock the mutex.\nUnlocks the mutex.\n37\n\n\fSummary\n• Threads are a different way to provide concurrency in a program.\n▪ Unlike process-level concurrency, threads share memory within the process.\n\n• Synchronization primitives such as semaphores allow for creation of\ncritical sections; necessary for correct concurrent code.\n• Incorrect code may result in a new set of logical errors.\n▪ Race conditions – When execution order stochastically results in wrong behavior.\n▪ Deadlock – When resources are contended so much the program freezes.\n▪ Starvation – When a resource is greedily kept by a task, certain tasks freeze.\n▪ Livelock – Starvation happens at every task… they all actively yield to each other.\n\n38\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":184,"segment": "unlabeled", "course": "cs0449", "lec": "lec12", "text":"12\n\nHow Programs\nReproduce\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fCreating Processes\nForks: what you stick in things that are done… and sometimes a system call.\n\n2\n\n\fThis is a story about a system call…\n• We are focusing several system calls starting with\n• This system call copies the current process.\n▪ This creates a “child” process that is a duplicate of the\nmemory and state of its parent.\n\n• This can be a convenient way to gain concurrency.\n▪ Copy the process and run each copy …\n▪ … those copies now run at the same time.\n• This is the origin of the term “fork” … a logical split in a\nprogram where there are now multiple paths.\n\n▪ We will see this idea in action soon.\n3\n\n\fHere’s Dolly\n• The\n\nsystem call in action:\n\n▪ Copies the memory layout.\n▪ Copies the process state. (but gives it a unique ID)\n\nA\n\nParent\n\nChild\n\nstack\n\nstack\n\n.bss\n.data\n.text\n\n.bss\n.data\n.text\n\nCPU State A:\nRegisters\n,\n\nCPU State B:\nRegisters\n,\n\nPID: 4356\n\nPID: 6567\n\nB\n\n4\n\n\fHere’s Dolly’s ID tag\n• The\n\nsystem call in action:\n\n▪ Updates the child’s CPU state so that it returns . (An invalid\n▪ Updates parent’s CPU state to return the child’s process ID. (\n\nA\n\nParent\n\nChild\n\nstack\n\nstack\n\n.bss\n.data\n.text\n\n.bss\n.data\n.text\n\nCPU State A:\nRegisters\n\nCPU State B:\nRegisters\n\nPID: 4356\n\nEnsures each\nprocess can detect\nwhich it is\n\n)\n)\n\nB\n\nPID: 6567\n\n5\n\n\fA small fork example… a… salad fork? example??\n• There is only one process when\nis called.\n\nC\n\n• However, when\nis called, the\nsystem call returns “twice”\nThe x is copied,\nso it has different\nvalues in child\nand parent.\n\n▪ Once in the parent process\n▪ Once in the child process\n\n• This starts two concurrent executions\nwithin the same program.\n▪ Via two processes.\n\n• What does this print out?\n\n6\n\n\fChildren first… OR NOT\n• If the child process goes first…\n\nC\n\n▪ Then it will print the child text.\nChild\n\nParent\n\n• Then the scheduler schedules the\nparent process once more.\n▪ Then it will print the parent text.\n\n• However, that’s not the only possible\npattern.\n• If the parent process goes first...\n▪ Then it prints the parent text …\n▪ … followed by the child.\n\n7\n\n\fTwo roads diverged in a yellow wood, AND I TOOK BOTH (NOT SORRY)\nC\n\n• If I were to extend the code to make\nit loop infinitely...\n▪ The parent and child will constantly race\nto print out their respective text.\n\n8\n\n\fThe good, the bad, and the unpredictable\n• Adding concurrency to your program makes things… weird.\n▪ You cannot rely on the order processes will be scheduled.\n▪ Your operations will be asynchronous (not synchronized; no known order)\n\n• If you need to synchronize processes, you can do so with\n\n•\n\n.\n\nyields the process and returns only when a child process ends.\n▪ It returns when any child process exits.\n▪ Its return value is the pid of the child process that exited.\n▪ You can also use\nto specify a specific child process by its pid.\n\n9\n\n\fWaiting is such sweet sorrow… wait that’s not right\n• By using\nthe parent process only\ncontinues when the child process ends.\nChild\n\nParent\n\n• Therefore, the output order is now\nknown.\n\nAlways:\n\n▪ If the parent goes first…\n▪ It gets stuck at the\ncall.\n▪ Then the child goes until it hits\n▪\nends the process.\n▪ And then the parent continues.\n\n• Nice and well-known behavior!\n10\n\n\fNotes on\n• The\n\nsystem call ends the current process.\n\n▪ The given argument is the process return code also known as an exit code.\n▪ Normally your program yields an exit code at the end of\n▪ Exit ends your program exactly at the point of the call.\n▪ Therefore, it has its own means of giving the exit code.\n\n• However, we can have processes that are no\nlonger running…\n▪ Yet, not deallocated either.\n▪ The are not living…\n▪ And not dead!!\n\n11\n\n\fZombies\n• A terminated process still takes up space\n▪ All that process metadata sticks around\n▪ Until the parent tells the system it doesn’t need it\n\n• As long as the parent stays alive…\n▪ The corpse of the child process sticks around, too.\n\n• These are called zombie processes.\n▪ They are processes that still exist and have an ID\nyet do not run and are no longer scheduled.\nDancing Zombie from Plants vs. Zombies\nCopyright PopCap Games, a subsidiary of EA Games\n\n12\n\n\fThe night of the living dead\nC\n\n• If I added an infinite loop to the\nparent…\n▪ When the child ends…\n▪ And I list the active processes using the\ncommand.\n▪ I see a “defunct” child process. A ZOMBIE!\n\n13\n\n\fJust the normal kind of dead.\nC\n\n• However, if I added an infinite loop to\nthe child…\n▪ When the parent ends… the program\nends as well!\n▪ And I list the active processes using the\ncommand. I see only the child process\n\nNo zombies here!\nJust orphans…\n\n14\n\n\fHow to run a different program?\n• When you\nprocess.\n\na process, you are making an exact copy of that\n\n• However, maybe you want to create a process to run a different\nprogram altogether?\n▪ This is very useful… instead of using a software library\n▪ You could just run the existing program.\n\n• For this purpose, the\n\nfamily of system calls is used.\n\n▪ There are several different variations of exec calls…\n\n15\n\n\fInvoking the OS loader…\n• Using the\n\nC\n\nsystem call.\n\n▪ The call takes the path to an executable\n▪ And an array of strings for the arguments.\n• Sentinel: must end in a\n\n▪ The first argument to a C program is\nalways its own path!\n\nWe ran “\n\n”\n\nAdd then continued\nour own process.\n\n16\n\n\fHere’s Dolly’s brother Bobby. Bobby is a goat somehow. Don’t ask.\n• The\n\nsystem call in action:\n\n▪ Copies the memory layout. Copies the process state. (but gives it a unique ID)\n\n• The\n\nsystem call in action:\n\n▪ It’s a goat now.\n\nA\n\nParent\n\nChild\n\nstack\n\nnew\nstack\nstack\n\n.bss\n.data\n.text\n\nnew\n.bss\n.bss\nnew\n.data\n.data\nnew\n.text\n.text\n\nCPU State A:\nRegisters\n,\n\nCPU State B:\nRegisters\n,\n\nPID: 4356\n\nPID: 6567\n\nB\n\n17\n\n\fDifferent forms of\n• You can look up the many different styles of exec\n▪ Each one has a different way of calling it.\n\ncalled with an array of strings terminated with a\nsame, but can use specific environment variables\nsearches the system paths for the executable\ncombination of\nand\n\n•\n•\n•\n•\n\n• There are also\nfunctions that use function arguments instead\nof an array of strings.\n▪\n18\n\n\fThe common ancestor… and the orphan.\n• UNIX\/Linux has an interesting design: every application is a child process.\n▪ The root is the\ntask.\n▪ Your shell spawns child\nprocesses when you ask\nto run a command.\n▪ It uses\n\/\n!\n\ninit\n\nhttpd\n(daemon)\n\nOrphans get adopted by\nthe root process.\n\nHow to interact with this process??\n(signals…)\n\nBash\n(shell)\n\nGUI\n\n• When your own application\nspawns a process, the same thing\nhappens.\n\n.\/app\n\nchild\n\n▪ You use\n▪ If your app exits before the child…\n▪ The child is an orphan process. 19\n\n\fAn extreme attitude\n• How do we interact with orphaned processes?\n• How do we synchronize at a finer granularity?\n▪ Using\nis rather inflexible.\n▪ It can only detect that a child process ends using\n\nor via main\n\n• What if you want to synchronize smaller events…\n\n▪ The child process does something... The parent responds…\n▪ But, keep the child process running longer.\n\n• For this, we will need the parent and child to be able to communicate\nwith one another.\n20\n\n\fInter-Process Communication\nIPC … not to be confused with the other IPC\n\n21\n\n\fWhat that last slide said…\n• Passing data or messages from one process to another is called\ninter-process communication.\n• This is a broad OS topic as there are many ways to do this.\n▪ Shared memory (we will talk about this a bit later)\n▪ Message passing (we will talk about this NOW)\n• Simple messages (signals, this lecture)\n• More complex (pipes, semaphores, etc, soon)\n• Most complex (network sockets, we will look at this later)\n\n• Message passing is a fancy way of saying are using an API to send a\nsmall message to another process.\n▪ And also some means of listening for messages.\n22\n\n\fAll aboard the train metaphor\n• In UNIX\/Linux, tiny messages sent between processes are called\nsignals.\n• They are typically used to send messages about events from the\nsystem. Here are a few:\nNumber\n\nName\n\nDescription\n\nDefault Behavior\n\nInterruption – Somebody pressed CTRL+C\n\nTerminate\n\nKill – Somebody wants us gone… \n\nTerminate\n\nMemory Violation – Oops! Seg-fault\n\nTerminate\n\nChild exited – Child process ended\n\nIgnore\n\nA signal that you can use for any purpose\n\nIgnore\n\n23\n\n\fTalking to orphans\n• Recall the infinite looping child.\n• Orphans run in the background.\n• However, we can send a\nmessage ( ) to the process by its id.\n\nC\n\nThe parent ended.\n\nBut not the child.\nWe can send a\nusing the\n\nmessage\napplication..\n\nAnd the child is gone!\n\n24\n\n\fReceiving Signals\nC\n\n• The\nstandard function will\nset up your application to listen for a\nparticular signal.\n\n• This example hooks the empty\nfunction sigint_handler to override\nthe default behavior of the SIGINT\nsignal.\n• If you recall, that happens on a\nCTRL+C… which now does not\nterminate the foreground process!\n▪ Needs to be killed using\n\nnow.\n25\n\n\fWaiting for a signal…\n• Proper use of signals and waiting on\nthe values of variables to change can\ncreate synchronization.\nBoth processes set\nto 0 on\n1.\n\n.\n\nis initially\n2. Which causes\nthe child to wait…\n5. Until child signals it back\nafter printing its own message.\n3. Until the parent process signals it,\nafter it prints its message.\n\n6. Repeat… for both\n\n4. Afterward, the\nparent process waits\n\n26\n\n\fLet’s look at that again. (animated)\n1. Child waits\n2. Parent prints\nChild 0\n\n1. Sets its own wait variable\n2. Sends signal to child\n3. Waits\n\nParent 0\n\n3. Child prints\n1. Sets its wait variable\n2. Sends signal to parent\n3. Waits\n\n4. Parent prints\n1. Sets its own wait variable\n2. Sends signal to child\n3. Waits\n5.\n\nRepeat…\n\n27\n\n\fIf you are in a hurry... (animated)\n1. Child waits\n2. Parent prints\nChild 0\n\n1. Sets its own wait variable\n2. Sends signal to child\n3. Waits\n\nParent 0\n\n3. Child prints\n1. Sends signal to parent\n2. Sets its wait variable\n3. Waits\n\nLet’s Mess Things Up!!\n4. Parent prints\n\n1. Sets its own wait variable\n2. Sends signal to child\n3. Waits\n5.\n\nOH NO!!!\n\n28\n\n\fThe race is on!\n• When you have concurrent tasks, they may compete.\n• A bug in a concurrent program where the logic breaks if one process\nout-paces another is called a race condition.\n▪ That is, if the correctness requires a strict order, but that order is not\nguaranteed.\n\n• When you add synchronization you need to be careful that you ensure\nthat each synchronized section (called a critical section) is logically\nsound.\nWe know we won’t be interrupted\nbetween the while loop and the signal.\n(This is our critical section)\n\n29\n\n\fSummary\n• Today we learned the birds and bees of programs.\n▪ They start as processes (technically children of a shell or some root process)\n▪ They can spawn child processes using\n▪ They can load executables over top of them using\nsystem calls\n▪ And if one process ends before the other, we either get zombies or orphans.\n\n• We also learned about inter-process communication in the form of\nsignals.\n▪ These are tiny messages sent using the\nfunction; received via\n.\n▪ We can use them to synchronize events between processes.\n▪ However, if we aren’t careful, we may introduce a bug called a race condition.\n▪ This is when the program requires a logical order it cannot guarantee.\n30\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":185,"segment": "unlabeled", "course": "cs0441", "lec": "lec01", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #1: Course Introduction\n\nBased on materials developed by Dr. Adam Lee\n\n\fAdministrivia\nCS 441: Discrete Structures for Computer Science\nInstructor:\n\nTA:\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nOH: TBA, by appt.\n\nTBA\n\nhttp:\/\/cs.pitt.edu\/~bill\/441\n\n\fCourse meeting times\nn Lecture\nl B: Flex@Pitt, M\/W 5:00–6:15\nl C: Flex@Pitt, T\/H 4:30–5:45\n\nn Recitations\nl Wednesday or Friday (see website)\n\nn It is important to attend both lecture and your\nassigned recitation section!\nl (No recitations this week)\n\n\fGrading\nn Overall breakdown:\nl 30% Midterm exam\nl 30% Final exam\nl 20% Homework\nl 10% Recitation exercises\nl 10% Lecture participation (Top Hat)\nl 100%\n\n\fRecitation\nRecitation exercises\nl Covers the previous week’s material (W–T)\nl Assigned the day before recitation, due the day of your\nrecitation at 11:59 PM via Gradescope\nl Late submissions are not accepted—submit early and often\nl Two lowest exercise grades will be dropped\n\nRecitations are collaborative\nl You should try the exercises in advance of recitation\nl You’ll go over much of the solutions in recitations\nl Submit solutions that you understand—don’t just copy what\nyour TA writes or copy a friend’s submission\n\n\fHomework\nWeekly homework assignments\nl Assigned on Friday, due the next Tuesday at 11:59 PM\nl Late homework is not accepted—submit early and often\nl Two lowest homework grades will be dropped\n\nHomework may be discussed with others, but must be\nsolved and written up individually\nl Limit discussion to understanding problems and developing\nhigh-level solution tactics\nl Canvas is a good place to do this\n\nl Identify collaborators on your homework cover sheet\nl No online resources (except Canvas)\nl Failure to comply with this policy is a violation of academic\nintegrity (F in the course)\n\n\fPolicies\nn Check the web page and Canvas frequently. Announcements,\nhomework, and lecture slides will be posted there\nn We will drop your two lowest homework and recitation scores\nbefore computing your homework average—no excuses necessary!\nn Regrade requests accepted within 2 weeks of hand-back. Grade\nmay increase, decrease, or stay the same.\nn Other policies are on the web page\nl Accommodating students with disabilities\nl Religious observances\nl Etc.\n\n\fQuestions?\n\n\fCourse overview\nn What is discrete mathematics?\nn Why is a math course part of the computer science\ncurriculum?\nn Will I really ever use this stuff again?\nn How to succeed in this course\n\n\fWhat is discrete mathematics?\nn Discrete mathematics is the study of distinct objects or\nstructures and their relationships to one another\nn For example:\nl How many ways can a valid password be chosen?\nl Can traffic flow between two computers in a network?\nl How can we transform messages to hide their contents?\nl How do we parse a given sequence of commands?\n\nn By contrast, continuous mathematics (e.g., calculus)\nstudies objects and relationships that vary continuously\nl e.g., position, velocity, and acceleration of a projectile\n\n\fWhy study discrete math?\nReason 1: Computers do not process continuous data\n\nSampling and\ndiscretization\nAnalog (continuous) input\n\nDigital (discrete) output\n\n\fWhy study discrete math?\nReason 2: Computers aren’t actually all that smart, they are\njust deterministic functions that map discrete\ninputs to discrete outputs\nExample: Does a given string contain an odd number of 1s?\n1\nRead\n\n0\n\nNo\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\n1\n\n…\n\nYes\n1\n\n0\n\n\fWhy study discrete math?\nIn general: Discrete mathematics allows us to better understand\ncomputers and algorithms\n\nfunction fib(int n)\nif(n == 0 || n == 1)\nreturn 1;\nelse\nreturn fib(n-1) + fib(n-2);\n\nfunction fib(int n)\nint first = 0;\nint second = 1;\nint tmp;\nfor(i = 1 to n)\ntmp = first + second;\nfirst = second;\nsecond = tmp;\nend for\nreturn first;\n\n\fTentative Syllabus\nn Logic and proofs\nn Sets\nn Functions\nn Integers and modular arithmetic\nn Counting\nn Probability and expectation\nn Relations\n\nAre these topics really useful?\n\n\fLogic and proofs\ngrant(X, projector) :- role(X, presenter), located(X, 104)\nlocated(adam, 104)\nrole(adam, presenter)\n=> ?grant(adam, projector)\n=> true\n\nAutomated reasoning, AI, security\n\nexp()\n+\n*\nVerifying data structures\nand hardware\n\n3\n\n3.1415\n4\n\n2\n\nParsing\nexpressions\n\nfunction fib(int n)\nint first = 0;\nint second = 1;\nint tmp;\nfor(i = 1 to n)\ntmp = first + second;\nfirst = second;\nsecond = tmp;\nend for\nreturn first;\n\nAlgorithm and\nprotocol analysis\n\n\fSets\n… and give us a means of\nreasoning about the\nrelationships between objects\n\nSets define collections of\nobjects…\n\n\fFunctions\n\nHardware design\n\nTheory of computation\nComputer graphics\n\n\fIntegers and Modular Arithmetic\nATTACK AT DAWN\n0111 0101 0110 1011\n+ 0101 1001 1110 0001\n1100 1111 0100 1100\nBinary arithmetic and\nbitwise operations\n\n01 20 20 01 03 11 01 20 04 01 23 14\nC = P+ 6 (mod 26)\n\n06 25 25 06 09 16 06 26 10 06 03 20\n\nFYYFIPFZJFCU\nCryptography\n\n\fCounting\n\nHow many valid\npasswords exist for a\ngiven set of rules?\n\nHow many IP addresses can be\nassigned within a network\nsegment? Will we run out?\n\n\fProbability and Expectation\n\nSpam classification\n\nHardware, software, and\nnetwork simulation\n\nRisk assessment\n\n\fRelations\nName\n\nAge\n\nPhone\n\nAlice\n\n19\n\n555-1234\n\nDanielle\n\n33\n\n555-5353\n\nZach\n\n27\n\n555-3217\n\nCharlie\n\n21\n\n555-2335\n\nRelational databases\n\nRoute\nplanning\n\nDHTs, DNS,\nname services\n\n\fSyllabus, redux\nn Logic and proofs\nn Sets\nn Functions\nn Integers and modular arithmetic\nn Counting\nn Probability and expectation\nn Relations\n\nAre these topics really useful?\nYes\n\n\fMastering discrete mathematics requires practice!\n\nn Succeeding in this class requires practicing the skills\nthat we will acquire, thinking critically, and asking\nquestions\nn Keys to success:\nl Attend class and take notes\nl Do your homework\nl Work extra problems when you’re unsure\n➣Solutions to odd-numbered exercises provided in textbook\n\nl Go to your recitation every week\nl Take advantage of office hours\n\n\fFinal thoughts\nn Our goal is to prepare you to be stronger computer\nscientists by:\nl Exploring the formal underpinnings of computer science\nl Developing critical thinking skills\nl Articulating ties between theory and practice\n\nn Next: Propositional logic\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":186,"segment": "unlabeled", "course": "cs0007", "lec": "lec03", "text":"CS 0007: Introduction to Java\nLecture 3\nNathan Ong\nUniversity of Pittsburgh\nSeptember 8, 2016\n\n\fPrimitive Types\nNO, STRING IS NOT A PRIMITIVE\n\n\fI Don't Want to Do Strings Anymore.\nIsn't There Anything Else?\n• Strings are okay, but\nthey don't do much\nfor us.\n• What about\nrepresenting\nnumbers and other\nthings?\n• We must use the\nbuilding blocks of\nJava!\n\n• Primitive Values:\n–\n–\n–\n–\n–\n–\n–\n–\n\nboolean\nbyte\nchar\ndouble\nfloat\nint\nlong\nshort\n\n\fI Don't Want to Do Strings Anymore.\nIsn't There Anything Else?\n• Most useful\n–\n–\n–\n–\n\nboolean\nchar\ndouble\nint\n\n• Second tier\n– byte\n– long\n\n• Primitive Values:\n–\n–\n–\n–\n–\n–\n–\n–\n\nboolean\nbyte\nchar\ndouble\nfloat\nint\nlong\nshort\n\n\fboolean\n• Two values only:\n– true\n– false\n\n• How do we declare one?\n\n\fType name = value;\n\n\fboolean name = value;\n\n\fboolean yes = true;\n\n\fchar\n• Characters in single quotation marks\n• How do we declare one?\n\n\fType name = value;\n\n\fchar mander = 'z';\n\n\fdouble\n• Decimal numbers (not whole numbers)\n• When providing a value, it must have at\nleast one decimal place (2 should be\n2.0)\n• How do we declare one?\n\n\fdouble pi = 3.14;\n\n\fint\n• Integers (whole numbers)\n• How do we declare one?\n\n\fint zero = 0;\n\n\fbyte\n• Special tiny whole number\n• How do we declare one?\n\n\fbyte kiloB = (byte)127;\n\n\flong\n• Special larger whole number\n• How do we declare one?\n\n\flong timeAgo =\n123456789L;\n\n\fshort\n• Special smaller whole number\n• How do we declare one?\n\n\fshort y = (short)-4;\n\n\ffloat\n• Special smaller double\n• How do we declare one?\n\n\ffloat away = 3.5f;\n\n\fWhy is String not purple?\n• Why is String not a primitive value?\n• What is a String made up of?\n• Characters  char\n\n\fWARNING\n• char uses SINGLE QUOTATIONS\n• String uses DOUBLE QUOTATIONS\n• String can be empty! (\"\"  legal)\n• char CANNOT be empty! (''  illegal)\n• Concatenation: + with only chars has a\ndifferent behavior than with Strings\n\n\fOkay, Let's Do Some Stuff\nNow!\npublic class Name\n{\npublic static void main(String[] args)\n{\nString firstName = \"Nathan\";\nchar midInitial = 'R';\nString lastName = \"Ong\";\nint age = 19;\nboolean hasFacebook = true;\nSystem.out.println(firstName + \" \" + midInitial + \" \" + lastName +\n\", age: \" + age +\n\", has a Facebook: \" + hasFacebook);\n}\/\/end method main\n}\/\/End class Name\n\nWhat is the output?\n\n\fWait Whaaaaa?\n• How did he add all of those things\ntogether??? Is that even legal? Is he\ncheating? He must have hacked Java.\n• Concatenation with any String forces\nother values to use their String\nrepresentations!\n• The value 19 is now \"19\"!\n• The value true is now \"true\"!\n• So again, what is the output?\n\n\fHmm, no one really says \"has a Facebook:\ntrue\"\nNow what? How do we fix this?\n\n\fRange of Values\n•\n•\n•\n•\n•\n•\n•\n•\n\nboolean\n– false, true\nbyte – -2^7, 2^7-1\nchar – ASCII\ndouble– -∞, +∞; with limited precision, and NaN\n(Not a Number)\nfloat – -∞, +∞; with limited precision, and NaN\n(Not a Number)\nint\n– -2^31, 2^31-1\nlong – -2^63, 2^63-1\nshort – -2^15, 2^15-1\n\n\fThis is great and all, but I really want\nto be able to store more useful info\n• Feels limited if we have to stay with the\nType name = value; structure\n• And thus we can introduce a new\nstructure\n\n\fType name = expression;\n\n\fboolean\n• Two values only:\n– true\n– false\n\n• We can also assign the variable an\nexpression to be evaluated.\n\n\fboolean Operators\n&&\n||\n!\n\n– Logical AND\n– Logical OR\n– Logical NOT\n\nboolean needUmb = rain || cloud;\nParentheses can be used as a way to\ngroup\n\n\fTruth Tables\nA B\n\nA&&B\n\nT T\n\nT\n\nT F\n\nF\n\nF T\n\nF\n\nF F\n\nF\n\n\fTruth Tables\nA B\n\nA||B\n\nT T\n\nT\n\nT F\n\nT\n\nF T\n\nT\n\nF F\n\nF\n\n\fTruth Tables\nA\n\n!A\n\nT\n\nF\n\nF\n\nT\n\n\fOrder of Operators\n1.\n2.\n3.\n\n!– Logical NOT\n&&\n– Logical AND\n||\n– Logical OR\n\nboolean needUmb = rain || !sun;\n\n\fConditional Logic\n• “If … then …”\n• Does not have an operator in Java\n\nA B\n\nAB\n\nT T\n\nT\n\nT F\n\nF\n\nF T\n\nT\n\nF F\n\nT\n\n\fConditional Logic\n• We can use an equivalent statement to\nget the result we want.\n\nA B\n\nAB\n\nT T\n\nT\n\nT F\n\nF\n\nF T\n\nT\n\nF F\n\nT\n\n\fConditional Logic\n• We can use an equivalent statement to\nget the result we want.\n\nA B\n\nAB\n\nT T\n\nT\n\nT F\n\nF\n\nF T\n\nT\n\nF F\n\nT\n\n\fConditional Logic\n• We can use an equivalent statement to\nget the result we want.\n\nA B\n\n!A||B\n\nT T\n\nT\n\nT F\n\nF\n\nF T\n\nT\n\nF F\n\nT\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":187,"segment": "unlabeled", "course": "cs0441", "lec": "lec10", "text":"Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #10: Sequences and Summations\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday’s Topics\nSequences and Summations\nl Specifying and recognizing sequences\nl Summation notation\nl Closed forms of summations\nl Cardinality of infinite sets\n\n\fSequences are ordered lists of elements\nDefinition: A sequence is a function from a subset of\nthe set of integers to a set S. We use the notation an\nto denote the image of the integer n. an is called a\nterm of the sequence.\n\nExamples:\nl 1, 3, 5, 7, 9, 11\nl 1, 1\/2, 1\/3, 1\/4, 1\/5, …\n\nA sequence with 6 terms\nAn infinite sequence\n\nNote: The second example can be described as the\nsequence {an} where an = 1\/n\n\n\fWhat makes sequences so special?\nQuestion: Aren’t sequences just sets?\nAnswer: The elements of a sequence are members of a\nset, but a sequence is ordered, a set is not.\n\nQuestion: How are sequences different from ordered\nn-tuples?\nAnswer: An ordered n-tuple is ordered, but always\ncontains n elements. Sequences can be infinite!\n\n\fSome special sequences\nGeometric progressions are sequences of the form\n{arn} where a and r are real numbers\n\nExamples:\nl 1, 1\/2, 1\/4, 1\/8, 1\/16, …\nl 1, -1, 1, -1, 1, -1, …\n\na = 1, r = ½\na = 1, r = -1\n\nArithmetic progressions are sequences of the form\n{a + nd} where a and d are real numbers.\n\nExamples:\nl 2, 4, 6, 8, 10, …\nl -10, -15, -20, -25, …\n\na = 2, d = 2\na = -10, d = -5\n\n\fSometimes we need to figure out the formula for a\nsequence given only a few terms\nQuestions to ask yourself:\n1. Are there runs of the same value?\n2. Are terms obtained by multiplying the previous value by a\nparticular amount? (Possible geometric sequence)\n3. Are terms obtained by adding a particular amount to the\nprevious value? (Possible arithmetic sequence)\n4. Are terms obtained by combining previous terms in a\ncertain way?\n5. Are there cycles amongst terms?\n\n\fWhat are the formulas for these sequences?\nProblem 1: 1, 5, 9, 13, 17, …\nl Arithmetic sequence with a = 1, d = 4\n\nProblem 2: 1, 3, 9, 27, 81, …\nl Geometric sequence with a = 1, r = 3\n\nProblem 3: 2, 3, 3, 5, 5, 5, 7, 7, 7, 7, 11, 11, 11, 11, 11, …\nl Sequence in which the nth prime number is listed n times\n\nProblem 4: 1, 1, 2, 3, 5, 8, 13, 21, 34, …\nl Each term is the sum of the two previous terms\nThis is called the Fibonacci sequence.\n\n\fSometimes we want to find the sum of the terms\nin a sequence\nSummation notation lets us compactly\nrepresent the sum of terms am + am+1 + … + an\nUpper limit\n\nIndex of summation\n\nLower limit\n\nExample: ∑1≤i≤5 i2 = 1 + 4 + 9 + 16 + 25 = 55\n\n\fThe usual laws of arithmetic still apply\n$\n\n$\n\n$\n\n$\n\n! 𝑎𝑥! + 𝑏𝑦! − 𝑐𝑧! = 𝑎 ! 𝑥! + 𝑏 ! 𝑦! − 𝑐 ! 𝑧!\n!\"#\n\n!\"#\n\n!\"#\n\n!\"#\n\nConstant factors can be pulled out of the\nsummation\nA summation over a sum (or difference) can be split into a\nsum (or difference) of smaller summations\n\nExample:\nl ∑1≤j≤3 (4j + j2) = (4+1) + (8+4) + (12+9) = 38\nl 4∑1≤j≤3 j + ∑1≤j≤3 j2 = 4(1+2+3) + (1+4+9) = 38\n\n\fExample sums\nExample: Express the sum of the first 50 terms of the\nsequence 1\/n2 for n = 1, 2, 3, …\nAnswer:\n\nExample: What is the value of\nAnswer:\n\n\fWe can also compute the summation of the\nelements of some set\nExample: Compute\nAnswer: (0 + 2) + (2 + 2) + (4 + 2) + (6 + 2) = 20\n\nExample: Let f(x) = x3 + 1. Compute\nAnswer: f(1) + f(3) + f(5) + f(7) = 2 + 28 + 126 + 344 = 500\n\n\fSometimes it is helpful to shift the index of a\nsummation\nThis is particularly useful when combining two or more\nsummations. For example:\nLet j = k - 1\n\nNeed to add 1\nto each j\n\n\fSummations can be nested within one another\nOften, you’ll see this when analyzing nested loops\nwithin a program (i.e., CS 1501\/1502)\nExpand inner sum\n\nExample: Compute\nSolution:\nSimplify if possible\n\nExpand outer sum\n\n\fIn-class exercises\nOn Top Hat\n\n\fComputing the sum of a geometric series by\nhand is time consuming…\nWould you really want to calculate\n\nby hand?\n\nFortunately, we have a closed-form solution for\ncomputing the sum of a geometric series:\n\nSo,\n\nWhy?\n\n\fProof of geometric series closed form\n\n\fThere are other closed form summations that\nyou should know\nSum\n\nClosed Form\n\n\fWe can use the notion of sequences to analyze\nthe cardinality of infinite sets\nDefinition: Two sets A and B have the same cardinality\nif and only if there is a one-to-one correspondence (a\nbijection) from A to B.\n\nDefinition: A finite set or a set that has the same\ncardinality as the natural numbers (or the positive\nintegers) is called countable. A set that is not\ncountable is called uncountable.\nImplication: Any sequence {an} ranging over the natural\nnumbers is countable.\n\n\fShow that the set of even positive integers is countable\n\nProof #1 (Graphical): We have the following 1-to-1\ncorrespondence between the positive integers and the\neven positive integers:\n1 2 3 4 5 6 7 8 9 10 …\n2 4 6 8 10 12 14 16 18 20 …\nSo, the even positive integers are countable. ❏\n\nProof #2: We can define the even positive integers as\nthe sequence {2k} for all k ∈ Z+, so it has the same\ncardinality as Z+, and is thus countable. ❏\n\n\fIs the set of all rational numbers countable?\nPerhaps surprisingly, yes!\n\nThis yields the sequence 1\/1, 1\/2, 2\/1, 3\/1, 1\/3, …, so the\nset of rational numbers is countable. ❏\n\n\fIs the set of real numbers countable?\nNo, it is not. We can prove this using a proof method called\ndiagonalization, invented by Georg Cantor.\n\nProof: Assume that the set of real numbers is countable. Then\nthe subset of real numbers between 0 and 1 is also countable, by\ndefinition. This implies that the real numbers can be listed in\nsome order, say, r1, r2, r3 ….\nLet the decimal representation these numbers be:\nr1 = 0.d11d12d13d14…\nr2 = 0.d21d22d23d24…\nr3 = 0.d31d32d33d34…\n…\nWhere dij ∈ {0,1,2,3,4,5,6,7,8,9} ∀i,j\n\n\fProof (continued)\nNow, form a new decimal number r=0.d1d2d3… where di = 0 if dii =\n1, and di=1 otherwise.\nExample:\nr1 = 0.123456…\nr2 = 0.234524…\nr3 = 0.631234…\n…\nr = 0.010…\nNote that the ith decimal place of r differs from the ith decimal\nplace of each ri, by construction. Thus r is not included in the list\nof all real numbers between 0 and 1. This is a contradiction of\nthe assumption that all real numbers between 0 and 1 could be\nlisted. Thus, not all real numbers can be listed, and R is\nuncountable. ❏\n\n\fFinal thoughts\nn Sequences allow us to represent (potentially\ninfinite) ordered lists of elements\nn Summation notation is a compact representation for\nadding together the elements of a sequence\nn We can use sequences to help us compare the\ncardinality of infinite sets\nn Next time:\nl Integers and division (Section 4.1)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":188,"segment": "unlabeled", "course": "cs0447", "lec": "lec07", "text":"#7\nCS 0447\nIntroduction to\nComputer Programming\n\nFunctions and the\nStack\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nLuís Oliveira\n\nFall 2020\n\n\fWhat do I need to know now!\nThe classes will be recorded!\n● You will be able to access the videos online\no They are for your personal use only!\no Do not distribute them!\n● You don’t need to turn on your camera\no If you do, you may be recorded\n● You can ask questions via text!\no Chat is great for that. If I don’t stop and read your questions, ask them again\no But feel free to interrupt me at any point.\n\n2\n\n\fClass announcements\n\n• Project 1 is out soon!\no You have 3 weeks!\n\n• Don’t forget!\n\no In the website (next to the slides) there are code examples\no Those code examples have extra exercises!\no Go and do them!\n\n• Have you been going to recitation?\no You should!\n\n3\n\n\fCalling conventions\n\n4\n\n\fWhat's a calling convention?\n● It ensures our programs don't trip over their own feet\n● It's how machine-language functions call one another\no How arguments are passed\no How return values are returned\no How control flows into\/out of the function\no What contracts exist between the caller and the callee\n\nvoid fork() {\nknife();\n}\n\ncaller\n\nvoid knife() {\n...\n}\n\ncallee\n\n5\n\n\fThe program counter register\n● A program's instructions are in memory, so they have addresses\n● The PC (program counter) holds the address of the next instruction to run\no Is incremented by a word! Each instruction is a word\n\ntime\n\nPC\n0x8000\n0x8004\n\n0x8008\n0x800C\n0x8010\n0x8000\n\n? 0x8014\n\nAddress\ntop:\n\n0x8000 lw\n\nt0, (s0)\n\n0x8004 add t0, t0, 1\n0x8008 sw t0, (s0)\n0x800C add s0, s0, 4\n0x8010 blt s0, s1, top\nbtw what pattern do you notice\nabout these addresses?\n6\n\n\fMIPS ISA: conditional branch instructions\n● The conditional branch instructions we’ve seen last class\no Actually interact with pc\n\nInstruction\n\nMeaning\n\nbeq a, b, label\n\nif(a == b) { goto label }\n\nbne a, b, label\n\nif(a != b) { goto label }\n\nblt a, b, label\n\nif(a < b)\n\nble a, b, label\n\nif(a <= b) { goto label }\n\nbgt a, b, label\n\nif(a > b)\n\nbge a, b, label\n\nif(a >= b) { goto label }\n\n{ goto label }\n\n{ goto label }\n\n7\n\n\fThe flow of control\n● When the caller calls a function, where do we go?\n● When the callee's code is finished, where do we go?\n\nvoid fork() {\nknife();\nspoon++;\n}\ncaller\n\nvoid knife() {\nspork++;\nspatula--;\n}\ncallee\n\n8\n\n\fMIPS ISA: The jump and link instruction\n● We call functions with jal: jump and link\n\nvoid main() {\nfunc();\n}\n\nmain:\nlabel\njal func\n\n9\n\n\fMIPS ISA: The jump and link instruction\n● We call functions with jal: jump and link\n\nPC 0x8004\nWhat address should\nPC 0x8C30\ngo into PC next?\nWhen func returns,\n0x8008\nra\nwhere will we go?\n\n0x8000 li a0, 10\n0x8004 jal func\n0x8008 li v0, 10\n... ...\n\nfunc: 0x8C30 li v0, 4\nThis is what jal does:\nit jumps to a new location, and\n... ...\nmakes a link back to the old one\nin the ra (return address) register\nand this is ALL it does.\n10\n\n\fMIPS ISA: The jump register instruction\n● We return from functions with jr: jump to address in register\n\nvoid main() {\nfunc();\n}\n\nmain:\nlabel\njal func\nlabel\n\nvoid func() {\nreturn;\n}\n\nfunc:\njr ra\n\nreturn\n\n11\n\n\fMIPS ISA: The jump register instruction\n● We return from functions with jr: jump to address in register\n\nNow we're at the end PC 0x8C38\nof func. ra still has the\nproper return address ra 0x8008\n\njr ra copies ra into pc. PC 0x8008\nand this is ALL it does.\n\n0x8000 li\n\na0, 10\n\n0x8004 jal func\n0x8008 li v0, 10\n... ...\n\nfunc: 0x8C30 li v0, 4\n0x8C34 syscall\n0x8C38 jr ra\n12\n\n\fArguments and\nReturn Values\n\n13\n\n\fIt's pretty simple, remember register names!!\n● if we have a function in a higher level language…\n\nwe use particular registers\nv0\na0\na1\nto pass arguments and\nint gcd(int a, int b) {\nreturn values.\nwhile(a != b) {\nif(a > b)\na -= b;\nelse\nwe already know how to\nb -= a; return. How do syscalls do it?\n}\nfor this, just put the value you want\nreturn a;\nto return in v0 before jr ra.\n}\n14\n\n\fThe a and v registers\n● a0-a3 are the argument registers\n● v0-v1 are the return value registers\no This is just a convention, there's nothing special about them\n▪ Does that mean I can pass values in (e.g.) s-registers?\n– Yessssssss….????¬.¬\n▪ Will I lose any points in midterms\/labs\/projects if I do?\n– Yessssssss!!!!¬.¬\n● By convention! We never do that!\no ALWAYS pass arguments in a-registers\no ALWAYS return arguments in v-registers\n\n15\n\n\fTo call a function…\n● You put its arguments in the a registers before doing a jal\n● Once control is inside the callee…\no The arguments are just \"there\" in the a registers.\n▪ Cause they are.\n– They didn't go anywhere!\nadd_nums\n\n● Functions should be black boxes for the caller\no You don’t need any information about\nthe implementation\no You only need to know inputs and outputs!\no … and conventions\n\nAmazing function that adds two\nnumbers. You do not need to\nknow how it is implemented!!!\nInputs:\n1. Number to add\n2. Number to add\nOutputs:\n1. Numbers added together\n\n16\n\n\fLet’s call a function!\n● Let's make main do this:\n\nv0=add_nums(3, 8)\nprint(v0)\n\nli\na0, 3\nli\na1, 8\njal add_nums\nmove a0, v0\nli\nv0, 1\nsyscall Careful!\n\n● How do we set 3 and 8 as the arguments?\n● How do we call add_nums?\n● Afterwards, which register holds the sum?\n● So how can we print that value?\n● Why do syscalls put the number of the\nsyscall in v0?\no Well what do you get when you cross an elephant and a rhino?\n▪ Hell if I know ¯\\_(ツ)_\/¯\n\nIt’s a rhinophant\n\n17\n\n\fInput, output\n● Now, let's write the function:\n\nint add_nums(int x, int y) {return x + y;}\n● inside of our add_nums asm function…\no which register represents x?\no which register represents y?\no which register will hold the sum that we return?\n\nadd_nums:\nadd __, __, __\njr\n\nv0\nra\n\na0\n\na1\n\n18\n\n\fMore conventions:\nSaved and Unsaved registers\n\n19\n\n\fLet’s try something\n● Let's make a variable and a function to change it\n\n.data\ncounter: .word 0\n.text\nincrement:\nla t0, counter\nlw t1, (t0)\nadd t1, t1, 1\nsw t1, (t0)\njr ra\n\nthen we can call it\n\nmain:\njal increment\njal increment\njal increment\n\n20\n\n\fEverything's just fine, right?\n● let's write a loop that calls it ten times in a row\n● so we need a loop counter ('i' in a for loop)\n\nli t0, 0 # our counter\nloop_begin:\njal increment\nadd t0, t0, 1\nblt t0, 10, loop_begin\nloop_end:\n(this is a do-while loop)\n\nif we run this, it only\nincrements the\nvariable once.\nwhy? let's put a\nbreakpoint on blt\nand see what it sees.\n\n21\n\n\fScribbling on someone else's notes\n● both functions are trying to use t0 for different purposes\no but there's only ONE t0!\n● the increment function is in the clear\no the problem is actually the loop\n● this is one of the contracts between the caller and the callee…\n\na caller cannot depend on the t, a, or v registers\nto have the same values after a call as before it.\nor to put it another way, callees are\nallowed to trash those registers.\n\n22\n\n\fAnother piece of the calling convention puzzle\n● When you call a function, it's allowed to change some registers\n● But other registers must be left exactly as they were\n\nfunctions are\nrequired to put\nthese registers\nback the way they\nwere before they\nwere called.\n\nSaved\ns0-s7\nsp\nra*\n\nanyone can change\nUnsaved these. after you call a\nv0-v1 function, they might\na0-a3 have totally different\nvalues from before\nt0-t9 you called it.\n\n*ra is a little weird cause it's kinda \"out of\nsync\" with the other saved regs but you\nDO save and restore it like the others\n23\n\n\fWhenever you call a function…\n● after a jal, you have no idea what's in these registers.\n\n...\njal\n...\n\nincrement\nUnsaved\nv0-v1\na0-a3\nt0-t9\n\n24\n\n\fWhy it broke\n● if we look at this code again…\n\nt0 is our loop counter and\neverything's fiiiine.\n\nli t0, 0\nloop_begin:\nuh oh.\njal increment\nadd t0, t0, 1\nblt t0, 10, loop_begin WHAT IS IN t0 NOW??\nloop_end:\ninstead, this is a great place to use an s register.\n\n25\n\n\fUsing the convention\n● if we use an s register…\n\ns0 is our loop counter and\neverything's fiiiine.\n\nli s0, 0\nloop_begin:\nuh oh.\njal increment\nadd s0, s0, 1\noh whew, we used an s\nblt s0, 10, loop_begin register, it's fine.\nloop_end:\n\nbut s registers aren't magic. they don't do this automatically.\n\n26\n\n\fDon't step on each others' toes\n● let's track PC and ra as we run this code.\n\nPC\n\nra\n\n0x8000 0x0000\nAfter jal fork: 0x8020 0x8004\n\nAfter jal spoon: 0x8040\nAfter jr ra: 0x8024\n\n0x8024 fork:\n0x8024\n\n0x8000 jal fork\n0x8004 li v0, 10\n... ...\n0x8020 jal spoon\n0x8024 jr ra\n\n... ...\nAfter jr ra: 0x8024 0x8024\nspoon: 0x8040 jr ra\nAfter jr ra: 0x8024 0x8024\nAfter jr ra: 0x8024\n\n0x8024\n\nAfter jr ra: 0x8024\n\n0x8024\n27\n\n\fWhat's the deal?\n● There's only one return address register\n● If we call more than one level deep, things go horribly wrong\n● Could we put it in another register?\no Then what about three levels deep? four?\n▪ We just don't have enough registers…\n● So where do we put things when we don't have room in registers?\no Tip: NOT in other registers (obviously!)\n▪ So don’t give into the urge of doing it\no Put things in memory!\n\n28\n\n\f(yes, memory)\n\nThe Stack\n\n29\n\n\fOne busy desk\n● there's a tiny desk that three people have to share\n● person 1 is working at the desk. it's covered in their stuff.\n● person 2 interrupts them and needs to do some important work\n● what does person 2 do with the stuff?\no throw it in the trash?\n● they put it somewhere else.\n\nP1\n\nP1\n\nTrash\n\n30\n\n\fOne busy desk\n● there's a tiny desk that three people have to share\n● person 1 is working at the desk. it's covered in their stuff.\n● person 2 interrupts them and needs to do some important work\n● what does person 2 do with the stuff?\no throw it in the trash?\n● they put it somewhere else.\n● And once they are done\no They put it back.\n\nP1\n\nP1\n\nTrash\n\n31\n\n\fOne busy desk\n● now person 2 is interrupted by person 3.\n● when person 3 is done, person 2 will come back.\n● where do we put person 2's stuff?\no on top of the stack of stuff.\n● the desk is the registers.\n● the people are functions.\n● the stack of stuff is… the stack.\n\nP2\n\nP2\nP1\n\n32\n\n\fWhat's the stack?\n● it's an area of memory provided to your program by the OS\no when your program starts, it's already there\n● the stack holds information about function calls:\no the return address to the caller\no copies of registers that we want to change\no local variables that can't fit in registers\n● how do we access the stack?\no through the stack pointer (sp) register\no this register is initialized for you by the OS too\n\nStack\n\nMemory\n\nProgram\n33\n\n\fThe stack pointer (animated)\n● let's say sp starts at the address 0xF000\n● we want to push something on the stack\n● the first thing we'll do is move sp to the next available slot\n● clearly, that's the previous address\n...\no subtract 4 from sp\n0xF008\n● then, we can store something in\nthe memory that sp points to.\n0xF004\n\nsp\n\n...\n0x00000000\n\n0x00000000\n0xF000 0x00000000\n0xEFFC 0xC0DEBEEF\n0x00000000\n\n34\n\n\fDoing that in MIPS (animated)\n● say ra = 0xC0DEBEEF\n● first: move the stack pointer down (up?):\n\nsub sp, sp, 4\n\n... ...\n0xF008 0x00000000\n\n● then, store ra at the address that sp holds.\n\nsw\n\nra, (sp)\n\n● now the value in ra is saved on the\nstack, and we can get it back later.\no and we can store as many return\naddresses as we want!\n\nsp\n\n0xF004 0x00000000\n0xF000 0x00000000\n0xEFFC 0xC0DEBEEF\n0x00000000\n\n35\n\n\fGoing the other direction (animated)\n● now we wanna pop the value off the stack and put it back in ra\n● we do the same things, but in reverse\nra 0xC0DEBEEF\n0xABAD1DEA\n\nlw\n\nra, (sp)\n\n...\n...\n0xF008 0x00000000\n0xF004 0x00000000\n0xF000 0x00000000\n\n● then, we move the stack pointer…\nup? down? whatever\n\nadd sp, sp, 4\n\n● now we got back the old value of ra!\n● and sp is back where it was before!\n\nsp\n\n0xEFFC 0xC0DEBEEF\n\n36\n\n\fShortening the pushes and pops\n● the push and pop operations always look and work the same\n● since you'll be using them in most functions, we shortened em\n● if you write push ra or pop ra, it'll do these things for you!\n\npush ra\n\nsubi sp, sp, 4\nsw\nra, (sp)\n\npop\n\nlw\nra, (sp)\naddi sp, sp, 4\n\nra\n\nthese are pseudo-ops: fake instructions to shorten common tasks\nthese can be used with ANY register, not just ra!\n\n37\n\n\fToes = protected\nsp\n\n0x8004\nstuff\n\nPC\n\nra\n\n0x8000 0x0000\nAfter jal fork: 0x8020 0x8004\n\nThen we push ra on the stack!\nAfter jal spoon: 0x8040\n\n0x802C\n\nAfter spoon jr ra: 0x802C\n\n0x802C\n\nThen we pop ra off the stack!\n\nBefore fork jr ra: 0x8034\n\n0x8004\n\nAfter fork jr ra: 0x8004\n\n0x8004\n\n0x8000 jal\nfork:\n\nfork\n\n0x8020 push ra\n0x8028 jal spoon\n0x802C pop\n0x8034 jr\nspoon:\n\nra\nra\n\n0x8040 jr\n\nra\n\n38\n\n\fWriting a simple function\n● Function calling conventions follows a simple structure :\n\n1. Give it a name (label). spoon:\n2. Save ra to the stack.\n3. Do whatever.\n4. Load ra from the stack.\n5. Return!\n\npush ra\n\nyour code goes here\npop\njr\n\nra\nra\n\n● Push everything you need! Pop it back in reverse order at the end!\n\n39\n\n\fWhat about other registers?\n● Function calling conventions follows a simple structure :\n\n1. Give it a name (label). spoon:\n2a. Save ra to the stack.\n\n2b. Save s0 to the stack.\n3. Do whatever.\n4a. Load s0 from the stack.\n4b. Load ra from the stack.\n5. Return!\n\npush ra\npush s0\nyour code goes here\npop s0\npop ra\njr\nra\n\n● Push everything you need! Pop it back in reverse order at the end!\n40\n\n\fit's really simple\n● treat pushes and pops like the { braces } around a function\n\nspoon:\npushes come at the\npush ra # {\nbeginnings of functions\n# 800 instructions\n# so much stuff omg\npops come at the end\npop ra # }\njr\nra\nthat is it, seriously, don't\nmake it more complicated\n\nnever push or pop anywhere else please\n\n41\n\n\fThe s register contract\n● if you want to use an s register…\n● you must save and restore it, just like ra.\n\nmy_func:\npush ra\npush s0\n\nmoving the papers off the desk\n\ncode that uses s0! it's fine! we saved it!\n\npop\npop\njr\n\ns0\nra\nra\n\nputting the papers back the pops happen\nin reverse order!\n\n42\n\n\fOh, and…\n● You must always pop the same number of registers\nthat you push.\n● To make this simpler for yourself… make a label\nbefore the pops.\no then you can leave the function by\njumping\/branching there.\n● Remember: These are the { braces }\no So… only push in the top and pop in the\nbottom of the function!\n▪ Only!\n\nmy_func:\npush ra\npush s0\n...\nbge ...\nb exit_func\n...\nexit_func:\npop s0\npop ra\njr\nra\n43\n\n\fSumming it up: Terminology\n\nmyFunction:\npush ra\n\nActivation Frame\n\npush s0\n\nContains:\n⚫ Arguments (that\naren’t in registers)\n⚫ Saved Registers\n(ra, s0, etc)\n⚫ Local Variables\n\n# my code\npop s0\n\nFunction Prologue\n\npop ra\njr\n\nra\n\nFunction Epilogue\n\n0xffff\n\nStack\n\nMemory\nHeap\n\nProgram\n0x0000\n44\n\n\fSide Stacking\n(on your own)\n\n45\n\n\fSooooo…..\n● Why this mysterious behavior?\no “Allocating” on the stack (making room) has you subtract from its base address.\n● Let’s visit this from a different direction.\n● Let’s consider… the problem itself.\no And how we might solve it.\n\n46\n\n\fThe Problem\n● We have a program. It uses memory.\n● We don’t know exactly how much memory we need.\no It may depend on how long the program runs.\no Or the size of the data it is working on (arbitrarily specified by a human being,\nperhaps)\no Maybe our program responds to the available memory by choosing a different\nalgorithm when it has more or less.\n● Either way, a program does not have a static allocation of memory.\n● How do we allow a program to allocate memory on-demand?\n\n47\n\n\fOur Example: Video Editor\n● Let’s consider a video editing program.\no But thankfully ignore all of the actual video details!\n● Data is large, and the memory usage is relative to the size of our video.\n● We want memory to be continuous.\no Could you imagine if data were all broken up?\no Your program would be difficult to code if an array was broken up.\n▪ Our array addressing math would no longer be general and would\ncease to work\nMemory\nwell. (You’d have multiple array base addresses)\n\nProgram\n48\n\n\fAllocating Memory\n● You’ll learn a lot more about this in CS 449\no But it’s worth sequence breaking and talking about it now\n● We will maintain a section of memory: the heap.\no The heap is a section of memory used for dynamic memory.\no Dynamic memory is memory that is allocated during the runtime of a program and\nmay be reclaimed later.\n● When we allocate memory, we add\nit to the end of the heap.\no It’s like appending to an array.\no Look at it go!\n\n0x46f0\n\nMemory\n\n0x4100\n0x4000\n\nHeap\n\nProgram\n0x0000\n49\n\n\fRevisiting Functions: A Problem Arises\n● Now, consider functions.\n\n● When we call a function, we need to remember where we were.\no This is stored in the $ra register.\no But if we call a function twice, what happens to $ra?\n▪ It is overwritten, and our first value in $ra is lost.\n▪ This means after our second function is called, the first function will now be lost,\nand it will return to itself. (Refer to the previous slides)\n● What are our strategies for remembering ra?\n\n50\n\n\fRemembering RA\n● Bad Idea #1: Place it in another register\n\nmyFunction:\nmove t0, ra\n# overwrites ra!\njal myOtherFunction\n# it’s ok though:\nmove ra, t0\njr ra\n\nHowever:\n• What if myOtherFunction uses t0?\n• Ok, t0 isn’t preserved, so let’s use s0.\n• Wait… we need to preserve s0…\n• Where do we put that?? s1???\n• Wait… we need to preserve s1!!\n• We will run out of saved registers and\nwe cannot trust unsaved registers.\n(other functions may overwrite them)\n• Therefore, we need memory.\n51\n\n\fRemembering RA\n● We need memory. We have that heap thing.\n\n● So can’t we just allocate some on the heap?\n● Sure can. But it is Bad Idea #2.\n● What happens if that function allocates memory?\n● And then calls another function.\n● And then we return…\n● And return from the first function…\n● Leaving gaps in our memory!\n\nMemory\n\n0x4000\n\nHeap\n\nProgram\n0x0000\n52\n\n\fLet’s Design a Memory Layout (kinda)\n● Our video editing application wants to use large, continuous memory regions.\no Videos are big things! (Continuous memory makes things easier\/faster… future\ncourses will convince you.)\n● We have very few registers, and need to remember ra\no So, we need to place ra in memory to recall it before we jr ra\n● However, placing it with other program memory creates gaps\no This is very very trash!!\n\n● How do we solve this.\no Occam’s Razor to the rescue… and it will create a very weird situation.\no One that involves subtracting to allocate…\n\n53\n\n\fSolving our Problem: Step 1\n● How can we use memory, but not create gaps?\n\n● Good [rational] Idea: Maintain two dynamic memory\nsections.\n● We call our function.\n● What happens if that function allocates\nmemory?\n● And then calls another function.\n● And then we return…\n● And return from the first function… WHEW!\nNo gaps.\n● (Ok, but now we start editing a LARGE video…)\no Uh oh! We’ve lost our $ra\n\nStack\n0x8000\n\nMemory\n\n0x4000\n\nHeap\n\nProgram\n0x0000\n54\n\n\fSolving our Problem: Step 2\n● Good [weird] Idea: Maintain two dynamic memory sections. One of which starts at the\nhighest memory address. Allocate via subtraction (append to bottom)\n0xfffc\n● We call our function.\n0xfff0\nStack\n● What happens if that function allocates memory?\n● And then calls another function.\n● And then we return…\n● And return from the first function… No gaps.\n● As for our large memory case…\n● It’s fine! (only problem: running out of memory)\no But, my goodness, you have a bigger problem, then.\n\nMemory\n\n0x4000\n\nHeap\n\nProgram\n0x0000\n55\n\n\fSolving our Problem: Step 2\n● Good [weird] Idea: Maintain two dynamic memory sections. One of which starts at the\nhighest memory address. Allocate via subtraction (append to bottom)\n0xfffc\n● We call our function. (subtract $sp, store)\n0xfff0\nStack\n● What happens if that function allocates memory?\n● And then calls another function. (sub, store)\n● And then we return… (load, add to $sp)\n● And return from the first function… (load, add $sp)\n● Refer to the previous slides on the Stack with this knowledge in your Memory\nmind.\n\n0x4000\n\nHeap\n\nProgram\n0x0000\n56\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":189,"segment": "unlabeled", "course": "cs0447", "lec": "lec12", "text":"#12\n\nControlling\nthe PC\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n● If you had a negative grade in the project\no You should talk to me\n\n2\n\n\fAssemblers and Compilers\nHow the machine-code sausage is made\n\n3\n\n\fWhat is machine code?\n\n4\n\n\fSomething denser\n● text is human-oriented and informationally… sparse.\n● instead, we encode each instruction as a bitfield.\no this encoding is specified by the ISA.\nMIPS has three\ninstruction formats.\n\nR opcode\n31\n\n26 25\n\nI opcode\n31\n\nrs\n\n26 25\n\nJ opcode\n31\n\n21 20\n\n16 15\n\nrt\n21 20\n\nrs\n\nthe opcode (and funct) field\nidentifies which instruction it is.\n11 10\n\nrd\n\n6 5\n\nshamt funct\n\n16 15\n\nrt\n\n0\n\nadd rd,rs,rt\nsll rd,rs,shamt\n\n0\n\nbeq rs,rt,offset\n\nimmediate\n\n26 25\n\n0\n\ntarget\n\njal target\n5\n\n\fHow does it… do the thing?\n● well this is mostly next lecture, but…\nthe fields are used\nas the control\nsignals to the CPU's\ncomponents.\n\nRegister File\nWE\n\nadd t0, t1, t2\n\nALU\n\nrd\n\nrs\n\nrt\n\n1\n\n\"add\"\n\ngets encoded as…\n31\n\n26 25\n\n21 20\n\n16 15\n\n11 10\n\n6 5\n\n0\n\n000000 01001 01010 01000 00000 100000\n6\n\n\fHow It's Made\n\n7\n\n\fSo we know the assembler\nli\n\ntop:\n\ns0, 0\n\nAddress\n\nInstruction\n\n0x00400000\n\n0x24100000\n\n0x00400004\n\n0x00102021\n\nmove a0, s0\nassembler!\n0x00400008\njal print_int\n0x0040000C\naddi s0, s0, 1\n0x00400010\nblt s0, 5, top\n0x00400014\nli\nv0, 10\nbut, there's clearly\n0x00400018\nsyscall\na little more\n0x0040001C\nprint_int:\ngoing on under\n0x00400020\nli v0, 1\nthe hood…\nsyscall\n0x00400024\njr ra\n0x00400028\n\n0x0C100008\n0x22100001\n0x2A010005\n0x1420FFFB\n0x2402000A\n0x0000000C\n0x24020001\n0x0000000C\n0x03E00008\n8\n\n\fHow it works\n● An assembler is a pretty simple program\n● See an instruction, output its encoding\n\naddi s0, s0, 1\n\nsplut\n\nopcode\n\nrs\n\nrt\n\nimm\n\n8\n\n16\n\n16\n\n1\n\n0x22100001\nBut what about\nlabels and the\ndata segment\n\n9\n\n\fHow it actually works (animated)\n.data\nLabels\n.text\nx: .word 0xDEADBEEF\n0: 24100000\nx:\n.data:0\ny: .word 5\n4: 00102021\ny:\n.data:4\nz: .word 0x12345678\nz:\n.data:8\n8: 0c000008\n0c000000\n.text\ntop: .text:4\nC: 22100001\nprint_int:\nli\ns0, 0\n10: 2A010005\n.text:20\n14: 1420FFFB\ntop:\n18: 2402000A\nmove a0, s0\n1C: 0000000C\njal print_int\n20: 24020001\naddi s0, s0, 1\nFixups\n24: 0000000C\nblt s0, 5, top\n28: 03E00008\n8:\nprint_int\nli\nv0, 10\nsyscall\nprint_int:\nli v0, 1\nsyscall\nthen, run through the fixups!\njr ra\n\n.data\n0: DEADBEEF\n4: 00000005\n8: 12345678\n\n10\n\n\fYum yum\n● if a label doesn't exist, it's an error.\n● now we have machine code!\n● it's packaged up into a casing: an object file\n● then the object files are linked\n● and then you get an executable program\no this is CS0449 stuff!\n\n11\n\n\fWhat about compilers?\n● ahahaha oh they're a lot more complicated\nint main(int argc, char** argv) {\nif(argc < 2)\nfatal(\"gimme arguments\");\nelse {\nTokens\n...\nKEYWORD(\"int\"),\n}\nID(\"main\"),\n}\nLPAREN,\n\nAST (Abstract Syntax Tree)\nFunction\nret_type: int\nname: \"main\"\nargs: [\n{type: int, name: \"argc\"},\n{type: ptr(ptr(char)),\nname: \"argv\"}\n]\n\nif\n\nKEYWORD(\"int\"),...\n<\nargc\n\n…\n2\nfatal\n\ncall\n\"gimme arguments\"\n\n12\n\n\fIt's just a grinder.\n● all that really matters:\nsome compilers output\nassembly and rely on an\nassembler to produce\nmachine code\n\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\/\\\n\nhello.c\n\ncode goes in, sausage object\nfiles come out\n\nthese days, it's common\nfor the compiler itself to\nproduce machine code,\nor some kind of\nplatform-independent\nassembly code\n\n13\n\n\fJumps and Branches\n\n14\n\n\fMaybe you never noticed…\n● the control flow instructions are divided into two groups.\n\njumps make execution go\nto one specific place\n\nbranches make execution\ngo to one of two places\n\nj end\n\nbne s1, t0, top\n\nbut.. why?\nwell, notice the operands of each.\n\n15\n\n\fA matter of practicality\n● Each jump or branch has a target: where it goes to\n● We'd like to be able to encode any target address…\n● But we have a fixed number of bits to encode our instructions.\nthink about the cases\nwhere jumps are used.\n\nnow think about the cases\nwhere branches are used.\n\nhow far away is a jump\ntarget likely to be?\n\nhow far away is a branch\ntarget likely to be?\n16\n\n\fAbsolute versus Relative\n● we say that jumps are absolute and branches are relative.\n\ntop:\nmove a0, s0\njal print_int\nadd s0, s0, 1\nblt s0, 5, top\n...\n\njumps just set the\nPC to a new value.\n\nPC = 0x800400B0\n\nPC += (-16)\nbranches either add an offset\nto the PC or do nothing.\n\njumps need a long address, but branches only need a\nsmall offset. so we can fit them into J and I instructions!\n\n17\n\n\fMore bang for your buck\n● every MIPS instruction is 4 bytes\n● what's memory alignment again?\nAddress in hex\n\nand in binary\n\n0x78000000\n0x78000004\n\n0111 1000 0000 0000 0000 0000 0000 0000\n0111 1000 0000 0000 0000 0000 0000 0100\n\n0x78000008\n0x7800000C\n\n0111 1000 0000 0000 0000 0000 0000 1000\n0111 1000 0000 0000 0000 0000 0000 1100\nwhat do you notice about these low 2 bits?\nin binary, multiples of 4 always end in 00\n\nsince every instruction's address ends in 00, do we need to store it?\n18\n\n\fInstruction Fetching\nWhat do we do next, boss?\n\n19\n\n\fRemember this?\n● what order do these instructions run?\n\nmost instructions\nchange the PC to the\nprint_int:\nnext address\nli v0, 1\n\nli\ns0, 0\ntop:\nsyscall\nmove a0, s0\ncontrol flow\njr ra\njal print_int\ninstructions can\naddi s0, s0, 1\nchange the PC\nblt s0, 5, top\nto a constant…\nli\nv0, 10\n…or the value from a register…\nsyscall\n…or one of two choices,\nconditionally\n\n20\n\n\fForwarrrrrrrrd MARCH\n\n+\n\n● moving ahead by 1 instruction each cycle is easy enough\no This is a FSD, the next state is the current state + 1 :D\n\nsize of one instruction\n\n00100004\nPC\n\nhow big are instructions in MIPS?\n\n21\n\n\fArbitrary locatiooooon MARCH\n\n4\n\n+\n\n● jumps (j, jal, jr) put a constant value into the PC\no we call this the jump target.\n● well now we have two choices of where to go. how do we choose?\n\njump target\nPC Source\n\n00100004\nPC\n\nPC Source (PCSrc for short) is a control signal.\nmany control signals are just MUX selectors\n22\n\n\fMIPS jump targets\n● in MIPS, j and jal use the J-type instruction format:\n\n31\n\n26 25\n\n0\n\nopcode\n\ntarget\n00100004\n\nthis is 26 bits…\n…but the PC is 32 bits.\n\nWHAT DO??\n\n23\n\n\fDo we really need a full 32-bit address (no)\n● we don't need to store the lower 2 bits because of alignment.\n● most programs are nowhere near big enough to need 32-bit addrs.\n● so in MIPS, jumps only change the lower 28 bits of the PC.\nhere's a j.\n\n31\n\n0\n\n26 25\n\n000010\n\n0x243C007\n<< 2\n\n0x90F001C\n\nwhat does this mean if the thing\nyou're jumping to is too far away?\n\n78000008\n790F001C\nPC\nput that into the\nlow 28 bits of the PC\n24\n\n\fDo we really need a full 32-bit address (no) (cntd.)\n● If a jump instruction is in address\n\nYXXXXXXX\n● It can reach from address:\n\n31\n\n26 25\n\n000010\n● To address:\n\n31\n\nY0000000\n0x0000000\n<< 2\n0x0000000\n\n26 25\n\n000010\n\n0\n\n0\n\nYFFFFFFC\n0x3FFFFFF\n<< 2\n0xFFFFFFC\n\n25\n\n\fIf jumping REALLY far, far away…\n● What if we want to jump too far?\nbeq t0, zero, a_label_far_far_away\n\nj\n\na_label_far_far_away\n\n● There is one instruction that can jump into a 32-bit address\no What is that?\n▪ How big is register ra?\njr\n\nra\n\n26\n\n\fIf jumping REALLY far, far away…\nj\n\na_label_far_far_away\n\nla\njr\nbeq\n\nt0, a_label_far_far_away\nt0\n\nt0, zero, a_label_far_far_away\n\nbne t0, zero, _skip_jump\nla\nt0, a_label_far_far_away\njr\nt0\n_skip_jump:\n\n27\n\n\fIf jumping REALLY far, far away…\njal\n\na_label_far_far_away\n\nla\njalr\n\nt0, a_label_far_far_away\nt0\n\n28\n\n\fRelative branches\n● think about a number line.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nyou\nare\nhere\n\n8\n\nif you want to get here, what\ndo you have to add to 10?\nwhat's the pattern?\n\n9 10 11 12 13 14 15 16 17\n\nhow about here?\n\ndestination - source\n\n29\n\n\fMIPS branch offsets\n● In MIPS the PC points to the next instruction to run.\n● let's say we're running the beq here. it's at address 00…\n\n00: beq a0, 10, else\n…but the PC is here. PC 04: li v0, 0\n08: b\nend\nwe want to get to address 0C. else: 0C: li v0, 1\nend: 10: ...\nhow do we get there?\n\nPC += 8\nthe branch offset for this beq is:\ntarget – (branch address + 4) = 12 – (0 + 4) = 8\n30\n\n\fEncoding it\n● Since the branch's immediate is only 16 bits…\n\n31\n\n26 25\n\n4\n\n21 20\n\n4\n\n16 15\n\n1\n\n0\n\n0x0002\n\n0x0008 >> 2\nif the branch offset is negative like 0xFFFFFFE8, no big deal –\nchop off the top 16 bits. 0xFFE8 is still a negative number.\n\n31\n\n\fThe number stored is the number of instructions\n● To go to else, from the updated value of PC: Jump 2 instructions down\n00: beq a0, 10, else\nPC\n04: li v0, 0\n08: b\nend\nelse: 0C: li v0, 1\nend: 10: ...\n\nthe branch offset for this beq\nis:\ntarget – (branch address + 4)\n=\n12 – (0 + 4) = 8\n\n31\n\n26 25\n\n4\n\n21 20\n\n4\n\n16 15\n\n1\n\n0\n\n0x0002\n\n0x0008 >> 2\n\n32\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":190,"segment": "unlabeled", "course": "cs0007", "lec": "lec09", "text":"CS 0007: Introduction to Java\nLecture 9\nNathan Ong\nUniversity of Pittsburgh\nSeptember 29, 2016\n\n\fIf Statements\n• English usage?\n• “If it will rain today, then I will bring my\numbrella.”\n• Rain today would indicate that I have\nbrought my umbrella\n• Cause and effect relationship: when a\ncondition is met, a behavior is elicited.\n\n\fSkeleton Statement\nif(<boolean condition>)\n{\n…\n}\n…\n\n\fBoolean Condition\n• Must evaluate to true in order for the\nstatements within the if-block to\nexecute.\n• Evaluating to false will skip the block.\n\n\fJava Example\n\/* checkWeather returns true if\nit\n* will rain today.*\/\nboolean rainToday = checkWeather();\nif(rainToday)\n{\nbringUmbrella();\n}\ngoToSchool();\n\n\fif()\n• if hasFacebook is true, then print \"has a\nFacebook\"\nif(hasFacebook)\n{\nSystem.out.println(\"has a\nFacebook\");\n}\/\/end block if(hasFacebook == true)\n• But how do I represent when hasFacebook is\nfalse?\n\n\felse if()\n• Otherwise, if hasFacebook is false, then\nprint \"does not have a Facebook\"\nelse if(!hasFacebook)\n{\nSystem.out.println(\"does not\nhave a Facebook\");\n}\/\/end block else if(hasFacebook\n\/\/ == false)\n\n\fif()\nif(hasFacebook)\n{\nSystem.out.println(\"has a Facebook\");\n}\/\/end block if(hasFacebook)\nelse if(!hasFacebook)\n{\nSystem.out.println(\"does not have a\nFacebook\");\n}\/\/end block else if(!hasFacebook)\n\nStill kinda wordy…\n\n\felse if(!hasFacebook)\n• How many values can hasFacebook\nhave?\n• How many values can a boolean have?\n• Why do we even need to check for !\nhasFacebook when we know if it is not\ntrue, it should do that portion of code?\n•  else\n\n\felse\n• When all else fails, execute everything\ninside this block.\n• All boolean expressions from the\nprevious if and else if blocks must\nall evaluate to false for this block to\nexecute.\n• In the hasFacebook example, because\nwe know it can only hold two values, if it\nis not true, then it must be false.\n\n\felse\nif(hasFacebook)\n{\nSystem.out.println(\"has a Facebook\");\n}\/\/end block if(hasFacebook)\nelse\n{\nSystem.out.println(\"does not have a\nFacebook\");\n}\/\/end block else\n\nSTILL kinda wordy…\n\n\fSystem.out.println(\"has a\nFacebook\");\n• Both lines use \"a Facebook\"\n• How can we use this to our advantage?\n• Why do we even want to do this?\n\n\fOptimization\n• Eventually, we get down all of that code\ninto something very short\n• Shorter code generally (in all the\nprogramming you will be doing) means a\nfaster program and is easier to read\n• if statements take time to evaluate\n• If you had five million of them, your\nprogram would be painfully slow. Reduce\nthe number of if statements, if you can.\n\n\fReducing Common Code\nif(hasFacebook)\n{\nSystem.out.print(\"has\");\n}\/\/end block if(hasFacebook)\nelse\n{\nSystem.out.print(\"does not have\");\n}\/\/end block else\nSystem.out.println(\" a Facebook\");\n\nThis is overkill for this example, but may prove useful in\nlater situations. I can still make this even shorter.\n\n\f?:\n• Simple choice operator\n(boolean expression) ? true return : false\nreturn;\n• Only use this in very simple if cases\n\n\f?:\n(hasFacebook ? \"has\" : \"does not\nhave\") + \" a Facebook\");\n• Now we can get it all in one line!\n\n\fAdding It in\npublic class Name\n{\npublic static void main(String[] args)\n{\nString firstName = \"Nathan\";\nchar midInitial = 'R';\nString lastName = \"Ong\";\nint age = 19;\nboolean hasFacebook = true;\nSystem.out.println(firstName + \" \" + midInitial +\n\" \" +\nlastName + \", age: \" + age + \", \" +\n(hasFacebook ? \"has\" :\n\"does not have\") +\n\" a Facebook\");\n}\/\/end method main\n}\/\/End class Name\n\nWhat is the output?\n\n\fWas it really worth it?\nHow messy was that code?\n\n\fTrade-Offs\n• Readability vs. Length\n• Who even wants to read a four-line oneliner?\n• Instead you should somehow tell the\nprogrammer what you mean (also so\nyou don't forget)\n• Remember the strange green thing at\nthe end?\n\n\fComment Reminders\npublic class Name\n{\npublic static void main(String[] args)\n{\nString firstName = \"Nathan\";\nchar midInitial = 'R';\nString lastName = \"Ong\";\nint age = 19;\nboolean hasFacebook = true;\nSystem.out.println(firstName + \" \" + midInitial +\n\" \" +\nlastName + \", age: \" + age + \", \" +\n(hasFacebook ? \"has\" :\n\"does not have\") +\n\" a Facebook\");\n}\/\/end method main\n}\/\/End class Name\n\n\fif() recap\n\/\/always run this code\nif(condition1 is true)\n{\n\/\/run this code\n}\nelse if(condition2 is true)\n{\n\/\/run this code\n}\nelse if(condition3 is true)\n{\n…\n}\n…\nelse \/\/if conditions 1…n were not satisfied\n{\n\/\/run this code\n}\n\/\/always run this code\n\n\fRevisiting Scope\n• Scope works by curly brackets.\n• Curly brackets surround all newly\ndeclared information, keeping it hidden\nfrom code outside of the curly brackets.\n\n\fScope\nConfidential\nSecret\nTop Secret\n\n\fScope\npublic class ScopeExample\npublic static void main\n\npublic static void func\n\nif\n\nif\nif\n\nelse if\n\nDarker shades can see lighter shades,\nbut not vice versa\n\n\fScope in Code\npublic class Name\n{\npublic static void main(String[] args)\n{\nint x = 0;\n\/\/x == 0, y is undefined\nif(x == 0)\n{\nint y = 10;\nx = 15;\n\/\/x == 15, y == 10\n}\n\/\/x == 15, y is undefined\n}\/\/end method main\n}\/\/End class Name\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":191,"segment": "unlabeled", "course": "cs0007", "lec": "lec15", "text":"CS 0007: Introduction to Java\nLecture 15\nNathan Ong\nUniversity of Pittsburgh\nNovember 1, 2016\n\n\fMain Portions of OOP\n• Building a class\n• Determining inter-class relationships\n– Subclasses\n– Superclasses\n– Ownership\n\n\fBuilding Classes\nThere are several parts of a class, much like\nthere are several parts of an object\n• Properties\n– Static: Class-level variables\n– Non-static: Instance variables\n\n• Methods (originally functions)\n– Static: Class methods\n– Non-static: Instance methods\n– Special: Constructor\n\n\fProperties\n• Inherent things (objects\/primitives) describing the\nclass or instance\n• Placed within the class, but outside any method\n• Class-level variables\n– Static\n– Usually constants\n– Declared and assigned outside any object\n\n• Instance variables\n– Non-static\n– Declared by each object construction, which allows them\nto contain different values\n\n\fClass-level Variables\n• Constants (final keyword)\n– Usually refers to limits or specific values\n– Signified in ALL_CAPS_WITH_UNDERSCORES\n– Variable cannot be changed\n– Usually visible to anyone (public keyword)\npublic static final double PI = 3.14;\nint radius = 5;\ndouble area = Math.PI*radius*radius;\n\n\fClass-level Variables\n• Non-Constants\n– Usually performs bookkeeping (e.g. how\nmany objects did you instantiate?)\n– Usually visible only to the class (private\nkeyword)\n\nprivate static int nextID = 0;\n\n\fInstance Variables\n• Variables that are object instance\ndependent\n• All cars have a color, but not all cars are\nred\n• Possibly constant (final keyword)\n– Must be assigned at construction\n– Cannot change value for the lifetime of the\nobject\n\n\fExample\npublic class Car\n{\nprivate final int idNum;\nprivate Color color;\nprivate String\nlicensePlate;\n…\n}\n\n\fHow to Determine?\n• Depends on your needs and\nrequirements of the scenario\n• A good rule of thumb: Restrict\neverything as much as possible. If a\nvariable does not need to change, make\nit final to prevent possible accidents.\nIf a variable does not need to be directly\nvisible to everyone, make it private.\n\n\fMethods\n• Functions that alter the object or operate\nusing the properties of the object\n• This allows further control over how the\nobject should be used, essentially\nproviding guidelines to programmers on\ncorrect usage\n• Three main types\n– Static: Class methods\n– Non-static: Instance methods\n– Special: Constructor\n\n\fConstructors\n• Essentially a method initializing the\nobject\n• Calling this special method requires the\nkeyword new\n• We have seen this before!!!\n• But how do I make one?\n\n\fConstructor\n\n1.\n2.\n3.\nFunction 4.\nHeader 5.\n\nFunction\nBody 6.\n\nVisibility type (public\/protected\/private)\nstatic (For now, required)\nReturn Type\nfunctionName Same name as the class\nParentheses “()”\n–\na)\nb)\nc)\n\nParameters\nType1 parameterName1\nType2 parameterName2\n…\n\nCurly Brackets\/Braces “{}”\n–\n\nreturn a value\n\n\fCar Example\npublic class Car\n{\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic Car(Color color)\n{\n\/\/initialize instance variables\n}\/\/end constructor(Color)\n…\n}\/\/End class Car\n\n\fInstance Variable\nManipulation\n• To reference non-static objectdependent variables, use the keyword\nthis followed by the dot operator.\n• The keyword can distinguish these\nobject-dependent variables and\nparameters.\n• It can be used regularly, with no need to\ndeclare them.\n\n\fCar Example\npublic class Car\n{\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic Car(Color color)\n{\nthis.color = color;\n…\n}\/\/end constructor(Color)\n…\n}\/\/End class Car\n\n\fControl Question\n• Should we allow the function caller (e.g.\nrandom user) to make the car’s ID\nnumber? Probably not.\n• We want the ID number to be unique.\n• Simple solution: just count up by one\nevery time a new car is made.\n• We can do this via a class-level variable.\n\n\fCar Example\npublic class Car\n{\nprivate static int nextIDNum = 0;\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic Car(Color color)\n{\nthis.color = color;\nthis.idNum = Car.nextIDNum;\nCar.nextIDNum++;\n…\n}\/\/end constructor(Color)\n…\n}\/\/End class Car\n\n\fCar Example\npublic Car(Color color)\n{\n…\nthis.idNum = Car.nextIDNum;\nCar.nextIDNum++;\n…\n}\/\/end constructor(Color)\nBecause nextIDNum is static, and was initialized to 0, the\nfirst car has an ID number of 0. Then the static value is\nincreased by 1. The next time we construct a new car,\nits ID number will be 1.\n\n\fMethod Overloading\n• Remember function overloading?\n• This can apply to constructors as well!\n\n\fOverloading Constructors\npublic class Car\n{\n…\npublic Car(Color color)\n{\nthis.color = color;\nthis.idNum = Car.nextIDNum;\nCar.nextIDNum++;\n…\n}\/\/end constructor(Color)\n\/\/default color is Color.RED\npublic Car()\n{\nthis(Color.RED);\n}\/\/end constructor()\n…\n}\/\/End class Car\n\nReferencing\nother\nconstructors\ncan be done\nwith the\n\n\fOverloading Constructors\n• If you will call another constructor, then\nthat is the only line of code that can be\npresent in the constructor\n\n\fFunctional Functions\n• The most common functions are getters\nand setters\n– Getters retrieve the values of instance\nvariables and return them\n– Setters alter the values of the instance\nvariables\n\n• Why use these functions rather than\nsetting the variable to be public?\n• Control!\n\n\fCar Example\npublic class Car\n{\nprivate static int nextIDNum = 0;\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic int getIDNum()\n{\nreturn this.idNum;\n}\/\/end function()\n…\n}\/\/End class Car\n\n\fCar Example\npublic class Car\n{\nprivate static int nextIDNum = 0;\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic void setColor(Color color)\n{\nthis.color = color;\n}\/\/end function(Color)\n…\n}\/\/End class Car\n\n\fWhy Bother?\n• Having these methods ensure you have\nfull control over the instance variables\n• Should you decide to change how the\nvariable is accessed or altered, you only\nneed to change the method.\n• You only add them when you need\nthem!\n\n\fRecap\nA class needs these components:\n• Class-level\/Instance Variables\n• Methods\n• Constructor(s)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":192,"segment": "unlabeled", "course": "cs0007", "lec": "lec14", "text":"CS 0007: Introduction to Java\nLecture 14\nNathan Ong\nUniversity of Pittsburgh\nOctober 27, 2016\n\n\fOBJECT ORIENTED\nPROGRAMMING\n\n\fRevisiting the History of\nLanguages\n• Assembly – Low-level hardware based\ncode\n• Structured – Introduces subroutines (i.e.\nfunctions) and well-defined looping\nstructures\n• Object-Oriented – Introduces classes\nand a philosophy about programming\n\n\fIt’s In the Name\n• Object-Oriented Programming is as it\nsounds. Assume everything can be\nmodeled as an object.\n\n\fTerminology\n• A class is code that describes objects of\nthat type\n• An instance is a particular object of a\ngiven type\n• A field is a property of an object\n• A method is a function that is provided\nby the class\n\n\fStatic\n• Referenced by keyword static\n• Property or method is not dependent on\nthe individual instantiated object, but\nrather the class as a whole\n• The property\/method exists between\ninstances; any change made to static fields\nare reflected through all instances of the\nclass\n• Static methods can only manipulate static\nfields\n\n\fMain Portions of OOP\n• Building a class\n• Determining inter-class relationships\n– Subclasses\n– Superclasses\n– Ownership\n\n\fBuilding Classes\nThere are several parts of a class, much like\nthere are several parts of an object\n• Properties\n– Static: Class-level variables\n– Non-static: Instance variables\n\n• Methods (originally functions)\n– Static: Class methods\n– Non-static: Instance methods\n– Special: Constructor\n\n\fProperties\n• Inherent things (objects\/primitives) describing the\nclass or instance\n• Placed within the class, but outside any method\n• Class-level variables\n– Static\n– Usually constants\n– Declared and assigned outside any object\n\n• Instance variables\n– Non-static\n– Declared by each object construction, which allows them\nto contain different values\n\n\fClass-level Variables\n• Constants (final keyword)\n– Usually refers to limits or specific values\n– Signified in ALL_CAPS_WITH_UNDERSCORES\n– Variable cannot be changed\n– Usually visible to anyone (public keyword)\npublic static final double PI = 3.14;\nint radius = 5;\ndouble area = Math.PI*radius*radius;\n\n\fClass-level Variables\n• Non-Constants\n– Usually performs bookkeeping (e.g. how\nmany objects did you instantiate?)\n– Usually visible only to the class (private\nkeyword)\n\nprivate static int nextID = 0;\n\n\fInstance Variables\n• Variables that are object instance\ndependent\n• All cars have a color, but not all cars are\nred\n• Possibly constant (final keyword)\n– Must be assigned at construction\n– Cannot change value for the lifetime of the\nobject\n\n\fExample\npublic class Car\n{\nprivate final int idNum;\nprivate Color color;\nprivate String\nlicensePlate;\n…\n}\n\n\fHow to Determine?\n• Depends on your needs and\nrequirements of the scenario\n• A good rule of thumb: Restrict\neverything as much as possible. If a\nvariable does not need to change, make\nit final to prevent possible accidents.\nIf a variable does not need to be directly\nvisible to everyone, make it private.\n\n\fRemember\n\nSource: http:\/\/www.craveonline.com\/images\/stories\/2011\/Film\/Captain%20Planet.jpg\n\n\fMethods\n• Functions that alter the object or operate\nusing the properties of the object\n• This allows further control over how the\nobject should be used, essentially\nproviding guidelines to programmers on\ncorrect usage\n• Three main types\n– Static: Class methods\n– Non-static: Instance methods\n– Special: Constructor\n\n\fConstructors\n• Essentially a method initializing the\nobject\n• Calling this special method requires the\nkeyword new\n• We have seen this before!!!\n• But how do I make one?\n\n\fFunction Review\n\n1.\n2.\n3.\nFunction 4.\nHeader 5.\n\nFunction\nBody 6.\n\nVisibility type (public\/protected\/private)\nstatic (For now, required)\nReturn Type\nfunctionName\nParentheses “()”\n–\na)\nb)\nc)\n\nParameters\nType1 parameterName1\nType2 parameterName2\n…\n\nCurly Brackets\/Braces “{}”\n–\n\nreturn a value\n\n\fConstructor\n\n1.\n2.\n3.\nFunction 4.\nHeader 5.\n\nFunction\nBody 6.\n\nVisibility type (public\/protected\/private)\nstatic (For now, required)\nReturn Type\nfunctionName Same name as the class\nParentheses “()”\n–\na)\nb)\nc)\n\nParameters\nType1 parameterName1\nType2 parameterName2\n…\n\nCurly Brackets\/Braces “{}”\n–\n\nreturn a value\n\n\fCar Example\npublic class Car\n{\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic Car(Color color)\n{\n\/\/initialize instance variables\n}\/\/end constructor(Color)\n…\n}\/\/End class Car\n\n\fInstance Variable\nManipulation\n• To reference non-static objectdependent variables, use the keyword\nthis followed by the dot operator.\n• The keyword can distinguish these\nobject-dependent variables and\nparameters.\n• It can be used regularly, with no need to\ndeclare them.\n\n\fCar Example\npublic class Car\n{\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic Car(Color color)\n{\nthis.color = color;\n…\n}\/\/end constructor(Color)\n…\n}\/\/End class Car\n\n\fControl Question\n• Should we allow the function caller (e.g.\nrandom user) to make the car’s ID\nnumber? Probably not.\n• We want the ID number to be unique.\n• Simple solution: just count up by one\nevery time a new car is made.\n• We can do this via a class-level variable.\n\n\fCar Example\npublic class Car\n{\nprivate static int nextIDNum = 0;\nprivate final int idNum;\nprivate Color color;\nprivate String licensePlate;\npublic Car(Color color)\n{\nthis.color = color;\nthis.idNum = Car.nextIDNum;\nCar.nextIDNum++;\n…\n}\/\/end constructor(Color)\n…\n}\/\/End class Car\n\n\fCar Example\npublic Car(Color color)\n{\n…\nthis.idNum = Car.nextIDNum;\nCar.nextIDNum++;\n…\n}\/\/end constructor(Color)\nBecause nextIDNum is static, and was initialized to 0, the\nfirst car has an ID number of 0. Then the static value is\nincreased by 1. The next time we construct a new car,\nits ID number will be 1.\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":193,"segment": "self_training_1", "course": "cs0447", "lec": "lec04", "text":"#4\n\nMemory and\nAddresses\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce\nChilders, David Wilkinson\n\nCS 0447\nIntroduction to\nComputer Programming\n\nLuís Oliveira\n\nFall 2020\n\n\fClass announcements\n● Add\/Drop period ends ??!\n● Everyone joined slack? Links will\/have? Expired\n● Repeat after me:\no Store copies from the CPU to memory\no Load copies from memory to CPU\n\n2\n\n\fVariables, Loads, Stores\n\n3\n\n\fMemory addresses\n● Everything in memory has an address\no the position in memory where it begins\n▪ where its first byte is\no this applies to variables, functions, objects, arrays etc.\n● A super important concept:\n\nevery piece of data really has two parts:\nan address and a value\n\n● If you want to put a variable in memory…\no first you need to figure out what address to put it in\no this extremely tedious task is handled by assemblers\n▪ whew\n\n4\n\n\fPutting a variable in memory\n● we can declare a global variable like this:\n\n.data\nx: .word 4\nname\n\ntype\n\ninitial value\n\n● the Java\/C equivalent would be static int x = 4;\n● .data says \"I'm gonna declare variables\"\no you can declare as many as you want!\no to go back to writing code, use .text\n● if we assemble this little program and make sure Tools > Show Labels\nWindow is checked, what do you see?\no the assembler gave the variable that address\no it'll do that for every variable\n5\n\n\fLoad-store architectures\n● In some architectures, many instructions can access memory\no x86-64: add [rsp-8], rcx\n▪ adds the contents of rcx to the value at address rsp-8\n● In a load-store architecture, all memory accesses are done with two kinds of\ninstructions: loads and stores (like in MIPS)\nloads copy data from memory\ninto CPU registers\n\nlw\nRegisters\n\nMemory\n\nsw\n\nstores copy data from CPU\nregisters into memory\n6\n\n\fOperating on variables in memory (animated)\n● we want to increment a variable that is in memory\no where do values have to be for the CPU to operate on them?\no what do we want the overall outcome to be?\n● so, what three steps are needed to increment that variable?\n1. load the value from memory into a register\n2. add 1 to the value in the register\n3. store the value back into memory\n● every variable access works like this!!!\no HLLs just hide this from you\n\n5\n4\n\n5\n4\n\nx\n\n7\n\n\fAccessing memory in MIPS\n\n8\n\n\fMIPS ISA: load and store instructions for words\n● you can load and store entire 32-bit words with lw and sw\n● the instructions look like this (variable names not important):\n\nlw t1, x # loads from variable x into t1\nsw t1, x # stores from t1 into variable x\n● Ermm… In MIPS, stores are written with the destination on the right. !?\no well, you can remember it with this diagram…\no the memory is \"on the right\" for both\nlw\nloads and stores\nRegisters\n\nMemory\n\nsw\n9\n\n\fMIPS ISA: load and store instructions for words\n● You can also load the 32-bit address of a variable with la\n\nla t1, x # loads the ADDRESS of x into t1\nt1  will now contain 4: The address of variable x\n\n● And then use that address to access memory, e.g.:\n\nla t2, 0(t1) # the contents of x into t2\nx\n\nAddr\n\nVal\n\n0\n\n04\n\n1\n\n00\n\n2\n\n00\n\n3\n\n00\n\n4\n\nDE\n\n5\n\nC0\n\n6\n\nEF\n\n7\n\nBE\n\n8\n\n6C\n\n9\n\n34\n\nA\n\n00\n\nB\n\n01\n\n10\n\n\fRead, modify, write\n● you now know enough to increment x!\n● But first, lets look at some assembly\n● first we load x into a register\n● then…\n● and then…\n\nlw t0, x\nadd t0, t0, 1\nsw t0, x\n\n● let's see what values are in t0 and memory after this program runs\n\n11\n\n\fIt really is that simple\n● variables in asm aren't THAT scary\n● please don't be afraid of them\n● you just gotta remember to store if you wanna change em\n\n12\n\n\fQuestions?\n\n• Just in case I prepared some for you:\no Does load word (lw) put or get data from memory?\no I already know the word is the most “comfortable” size for the CPU, but are\nthey the only size it can work with?\n\n13\n\n\fSmaller values\n\n14\n\n\fSmaller numeric\n● MIPS also understands smaller and tiny datatypes\n\n.data\nx: .word 4\ny: .half 4\nz: .byte 4\n\n=>\n=>\n=>\n\n0x00000004\n0x0004\n0x04\n\n15\n\n\fMIPS ISA: loading and storing 8\/16-bit values\n● to load\/store bytes, we use lb\/sb\n● to load\/store 16-bit (half-word) values, we use lh\/sh\n● these look and work just like lw\/sw, like:\n\nlb t0, tiny # loads a byte into t0\nsb t0, tiny # stores a byte into tiny\n\no …or DO THEY?!?!?!?\n● how big are registers?\no what should go in those extra 16\/24 bits then?\n▪ ???\n\n16\n\n\fcan I get an extension?\n\n… no\n\n● sometimes you need to widen a number with fewer bits to more\n● zero extension is easy: put 0s at the beginning.\n\n10012 ➔ to 8 bits ➔ 0000 10012\n\n● but there are also signed numbers which we didn't talk about yet\no the top bit (MSB) of signed numbers is the sign (+\/-)\n● sign extension puts copies of the sign bit at the beginning\n\n10012 ➔ to 8 bits ➔ 1111 10012\n00102 ➔ to 8 bits ➔ 0000 00102\n\no like spreading peanut butter\n▪ we'll learn about why this is important later in the course\n\n17\n\n\fEXPAND VALUE\n● if you load a byte…\n\n31\n0\n00000000 00000000 00000000 00000000\n\n10010000\n\nIf the byte is signed… what should it become?\n\n31\n0\n11111111 11111111 11111111 10010000\nIf the byte is unsigned… what should it become?\n\n31\n0\n00000000 00000000 00000000 10010000\n\nlb does\n\nsign extension.\n\nlbu does\n\nzero extension.\n18\n\n\fHow does the CPU know whether it's signed or unsigned\n➔ Everything’s a number\n➔ Everything's in binary (and hex is convenient shorthand)\n➔ Numbers may not be numbers\n➔ So, how does the computer know a number is a number?\no How does it know that a number is signed?\no How does it know how to add two numbers?\no How does it know how to manipulate strings?\no How does it know if one pattern of bits is a string or a number or a video\nor a program or a file or an icon or\n\n19\n\n\fIT DOESN'T\n20\n\n\fHow does the CPU know whether it's signed or unsigned\n\n• Do YOU think the CPU knows this?\n\no no\n▪ it doesn't\n– you have to use the right instruction.\n• It’s particularly easy to mess this up\no lbu is usually what you want for byte variables but lb is one character\nshorter and just looks so nice and consistent…\no But don’t!\n\n21\n\n\fTruncation\n● If we go the other way, the upper part of the value is cut off.\n\nsh\n31\n0 11111111 00000100\n10100010 00001110 11111111 00000100\n● The sign issue doesn't exist when storing, cause we're going from a larger\nnumber of bits to a smaller number\no therefore, there are no sbu\/shu instructions\n\n22\n\n\fMemory\n\n23\n\n\fWhat is the memory?\n\n• The system memory is a piece of temporary storage hardware\n\no it's smaller and faster (more expensive!) than the persistent storage.\n▪ maybe in the future it won't be temporary\n▪ the line between system memory and persistent storage will fade away…\n• It's where the programs and data that the computer is currently executing\nand using reside\no all the variables, all the functions, all the open files etc.\no the CPU can only run programs from system memory!\n\n24\n\n\fBytes, bytes, bytes\n● The memory is a big one-dimensional array of bytes\n● What do these bytes mean?\no ¯\\_(ツ)_\/¯\n● Every byte value has an address\no This is its \"array index\"\no Addresses start at 0, like arrays in C\/Java\n▪ Gee wonder where they got the idea\n▪ Addresses are the offset from the beginning!\n● When each byte has its own address, we call it a byteaddressable machine\no not many non-byte-addressable machines these days\n\nAddr\n\nVal\n\n0\n\n00\n\n1\n\n30\n\n2\n\n04\n\n3\n\n00\n\n4\n\nDE\n\n5\n\nC0\n\n6\n\nEF\n\n7\n\nBE\n\n8\n\n6C\n\n9\n\n34\n\nA\n\n00\n\nB\n\n01\n\nC\n\n02\n\n25\n\n\fHow much memory?\n● Each address refers to one byte. if your addresses are n bits long…\nhow many bytes can your memory have?\no 2n B\n● machines with 32-bit addresses can access 232 B = 4GiB of memory\no with 64-bit addresses… 16EiB\n● Remember:\no kibi, Mebi, Gibi, Tebi, Pebi, Exbi are powers of 2\n▪ kiB = 210, MiB = 220, GiB = 230 etc.\no kilo, mega, giga, tera, peta, exa are ostensibly powers of 10\n▪ kB = 103, MB = 106, GB = 109 etc.\n\n26\n\n\fWords, words, words\n● For most things, we want to use words\no The \"comfortable\" integer size for the CPU\no On this version of MIPS, it's 32b (4B)\n● But our memory only holds bytes…\n● Combine multiple bytes into larger values\no The CPU can handle this for us\no But importantly, the data is still just bytes\n● When we talk about values bigger than a byte…\no The address is the address of their first byte\n▪ The byte at the smallest address\no So what are the addresses of the three words here?\n\nAddr\n\nVal\n\n0\n\n00\n\n1\n\n30\n\n2\n\n04\n\n3\n\n00\n\n4\n\nDE\n\n5\n\nC0\n\n6\n\nEF\n\n7\n\nBE\n\n8\n\n6C\n\n9\n\n34\n\nA\n\n00\n\nB\n\n01\n\nC\n\n02\n\n27\n\n\fEndianness\n\n28\n\n\fA matter of perspective\n● let's say there's a word at address 4… made of 4 bytes\n● wh…what word do those 4 bytes represent?\n\n…is it\n0xDEC0EFBE?\n\nAddr\n\nVal\n\n...\n\n...\n\n7\n\nDE\n\n6\n\nC0\n\n5\n\nEF\n\n4\n\nBE\n\n...\n\n...\n\n…is it\n0xBEEFC0DE?\n\n29\n\n\fEndianness\n\n● when interpreting a sequence of bytes as larger values, endianness\nis the rule used to decide what order to put the bytes in\n\nlittle-endian means 0\nthe “LITTLE address“ DE\ncontains the END-byte\n\n0xBEEFC0DE\n\n1 2 3 big-endian means the\n“BIG address\"\nC0 EF BE\n\ncontains the END-byte\n\n0xDEC0EFBE\n\nnothing to do with value of bytes, only order\n\n30\n\n\fWhich is better: little or big?\n● it doesn't matter.* as long as you're consistent, it's fine\n● for political reasons, most computers today are little-endian\n● but endianness pops up whenever you have sequences of bytes:\no like in files\no or networks\no or hardware buses\no or… memory!\n● which one is MIPS?\no it's bi-endian, meaning it can be configured to work either way\no but MARS uses the endianness of the computer it's running on\n▪ so little-endian for virtually everyone\n– cause x86\n– Apple sillycone will use a bi-endien architecture: ARM architecture\n(x86)\n\n*big endian is better\n\n31\n\n\fWhat DOESN'T endianness affect?\n× the arrangement of the bits within a byte\no it just changes meaning of order of the bytes\n▪ note the bytes are still DE, C0 etc.\n× 1-byte values, arrays of bytes, ASCII strings…\no single bytes don’t care about endianness at all\n× the ordering of bytes inside the CPU\no there's no need for e.g. \"big-endian\" arithmetic\no the CPU works with whole words\n● endianness only affects moving\/splitting data:\no larger than single bytes\no between the CPU and memory\no or between multiple computers\n\n0xBEEFC0DE\n0xED0CFEEB\n0xDEC0EFBE\n\nl l e H o\nH e l l o\n\n32\n\n\fSummary\n\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":194,"segment": "unlabeled", "course": "cs0449", "lec": "lec14", "text":"14\n\nThe\nMemory\n\nHierarchy\n\nCS\/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fThis is a Pyramid Scheme\nBut this knowledge is a safe investment.\n\n2\n\n\fWanting Moore and Moore\nProcessors and memory work together but improve at different rates.\nMemory was initially faster than CPU, but its innovation was slower.\n\nThroughput\n\nInnovation starts to slow\n\nThe CPU overtakes\nmemory performance\n\n3\n\n\fCost of DRAM\/Disk in 2020\n• 8GiB\n\n$35 - 70\n\n• 1TiB\n\n$50 – 80\n\n• 16GiB\n\n$70 - 100\n\n• 8TiB\n\n$200 - $300\n\n• 32GiB\n\n$140 - 300\n\n• 16TiB\n\n$400 - 500\n\n4\n\n\fThe memory hierarchy\nRegisters\nFaster,\nDenser\nExpensive\n\nL1 Cache\n\nCheaper,\nSlower,\nLarger\n\n(DRAM) Main Memory\n\nL2 Cache\n\nLocal Disk\nDistributed Storage\n(Don’t forget it!) Tape\n\n5\n\n\fThe hierarchy of speed\n\nFaster,\nDenser\nExpensive\n\nCheaper,\nSlower,\nLarger\n\nA “cache” is used to hold\nuseful data closer than\nmain memory to\nimprove speed.\n\nRegisters\nL1 Cache\n\nL2 Cache\nDRAM is simply\ntoo slow\n(DRAM) Main Memory\nLocal Disk\nDistributed Storage\n(Don’t forget it!) Tape\n\n6\n\n\fMemory Hierarchy: Core 2 Duo\nNot drawn to scale\n\nSRAM\n\nDRAM\n\nStatic Random Access Memory\n\nDynamic Random Access Memory\n\n~4 MB\nL2\nunified\ncache\n\nL1\nI-cache\n\n~8 GB\n\n~500 GB\n\nMain\nMemory\n\nDisk\n\n32 KB\nCPU\n\nReg\n\nThroughput: 16 B\/cycle\nLatency: 3 cycles\n\nL1\nD-cache\n8 B\/cycle\n\n2 B\/cycle\n\n1 B\/30 cycles\n\n14 cycles\n\n100 cycles\n\nmillions\n\nMiss Penalty\n(latency)\n33x\n\nMiss Penalty\n(latency)\n10,000x\n\n\fMemory Caching\nCache: Another thing us teachers could really use more of.\n\n8\n\n\fExperiment: Scientific Maths\n\nSpring 2019\/2020\n\n9\n\n\fPractical Performance\n• Caching is necessary for the\nutility of computers.\n▪ The CPU\/Memory gap increases\n(The Memory Wall)\n\n• In order to actually use these fast\nCPUs, we need to improve the\napparent speed of RAM.\n\n“That’s a nice CPU you have there… it’d be\nterrible if something were to happen to it.”\n\n▪ Programs use memory a whole lot.\n▪ The bottleneck would grind\nperformance to the point where\nCPUs cannot improve.\n10\n\n\fThe problem: data is faaaaar away\n• Let’s say you want to read a book.\n• You check it out of the library.\n▪ You have to go there.\n▪ Find the book.\n▪ Maybe take the bus back.\n• Wait in traffic.\n\n• Now it sits on your desk.\n▪ As long as it is near you, it’s easy to access the\ninformation.\n▪ Yet, if you need another book…\n• You would take the book ALL THE WAY back!\n(bare with me)\n\n11\n\n\fCaching: Keeping things close\n• Let’s say you want to read a book.\n▪ It’s not on your bookshelf.\n\n• So, still have to check it out of the library.\n▪ You gotta go there. Find the book. Etc.\n▪ Take the bus back.\n\n• Now it sits on your desk.\n▪ As long as it is near you, it is easy to access the\ninformation.\n▪ When we need another book… we put it aside.\n• Maybe a bookshelf.\n\n▪ The next time we need it, it will be nearby.\n12\n\n\fThe metaphorical cache\n• The bookshelf is a cache.\n▪ It holds information that you might want later.\n\n• It is [much] smaller than a library, but faster\nto retrieve things.\n• However, it is small. Placing a new book on\nthe shelf may require taking an old book off.\n\n13\n\n\fMemory cache (CPU)\n• RAM is the library. It is far away and getting\nstuff from there is slow.\n• To better handle the performance gap\nbetween the CPU and memory we add a\nsmaller, fast memory near the CPU.\n• This is the CPU cache.\n\n14\n\n\fData, the journey\n• When data is requested, the goal is to read\na word into a CPU register.\n• The CPU first contacts the cache and asks\nif it has a copy.\n▪ If it does… that is a cache hit, and, well, that was\neasy. Just copy that value into the register.\n\n• If it does not, this is a cache miss.\n▪ It will then contact the next component in the\nmemory hierarchy. (RAM)\n\n• Ram copies the value to cache, and the\ncache copies the value to the register.\n\n15\n\n\fMissing the mark\n• When the CPU requests memory in an\nempty cache, the data obviously won’t be\navailable locally.\n• This is a compulsory miss, a “miss” due to\nthe first access of a block of data.\n▪ Also known as a “cold miss.”\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n8\n\n• These are, as they suggest, completely\nunavoidable.\n▪ They always incur the high penalty associated\nwith a memory read.\n\n8\n\n16\n\n\fHitting the target\n• When the CPU requests memory that\nhappens to already be in the cache, the\ndata is read locally (quickly).\n• This is a cache hit.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n▪ Your best-case scenario.\n\n• These avoid having to communicate at all\nwith memory.\n▪ No penalty taken for reading\/writing to\nmemory.\n▪ Very cheap in terms of time.\n\n8\n\n8\n\n17\n\n\fA cache half full…\n• As the CPU requests memory, the cache\nwill fill to satisfy each compulsory miss.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n• When it fills up completely, it will have no\nfurther room for the next miss.\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n• On a miss, it requests the data from\nmemory.\n\n4\n\n6\n\nC\n\n1\n\n8\nE\n\n▪ Yet, where does it go?? We must remove one.\n\n• This is a capacity miss. The memory\nrequirements of the program are larger\nthan the cache.\n\n8\n\n18\n\n\fLooking closer…\n• It is difficult to know what block of data\nto omit from this cache on such a miss.\n▪ However we can exploit the common locality\npatterns of programs to improve our cache.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n• There is temporal locality: accessed data\nis likely to be used again in near future.\n\nA\n\nB\n\nC\n\nD\n\nE\n\n4\n\n6\n\nC\n\n1\n\n8\n\n▪ This is what caches generally capture.\n\n• However, spatial locality is also likely:\ndata is often grouped together.\n▪ When we access a struct field, we will often\naccess another which is nearby in memory.\n\n8\n\n19\n\n\fExploring space…\n• We would like to keep data that is\nadjacent in memory in the cache, together,\nat the same time.\n• To do this, we “hash” the address. This is\nused to determine the cache slot.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n▪ Just a fancy way to say: we divide the address\nby the cache size and use the remainder.\n\n• Every 0th block, 1st block, 2nd block, etc.\n• The 5th block (in this example) goes to the\nslot, the 6th goes to the slot, and so on.\n8\n\n20\n\n\fDirect and to the point…\n• Let’s read addresses 3, 4, 5, 6, and 7 (in\nthat order) from memory.\n• Reading address 8 next incurs a capacity\nmiss, but it evicts the address that is\nfurthest away from the others.\n▪ This type of cache is good for programs that\nread through data sequentially.\n▪ That is because such programs will always\nremove the least recently used block on a miss,\nas shown here.\n\n• Because every address has a specific cache\nslot, this is called a direct-mapped cache.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n5\n\n6\n\n7\n\n3\n8\n\n4\n\n8\n\n21\n\n\fMissing your connection…\n• Let’s consider an antagonist pattern.\n▪ What is the worst case for this cache?\n\n• If we read every 5th address in our memory\nin order, we would overwhelm our directmapped cache.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n▪ Let’s access 0, 5, A in that order.\n\n• Accessing address 0 is a compulsory miss.\n\nA\n5\n0\n\n▪ Address 5, however, is a miss.\n▪ But our cache isn’t full!!\n\n• A miss that occurs even though your cache\ncould fit the block is called a conflict miss.\n\nA\n\n22\n\n\fHow big is that block?\n• Spatial locality is SO prevalent that it\nmakes a whole lot of sense to pull more\ndata than is requested.\n▪ If we request a word (8 bytes) from memory,\nand we have a cache miss, let’s pull 8 words at\na time (64 bytes).\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n64 bytes\n\n• Therefore, the blocks visualized to the\nright can have a size, called the block size.\n▪ The bigger the block, the better spatial locality\nwill become.\n▪ However, the more time it takes to copy from\nmemory and the higher penalty if you throw it\naway on a miss!\n\n8 bytes\n\n23\n\n\fBlock size helps locality\n• When we request an address from our\ncache, we are requesting the block that\ncontains that address.\n▪ Here, Block 0 contains byte addresses 0x00\nthrough 0x39. Block 1 is 0x40 to 0x79, etc.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n64 bytes\n\n• Let’s request 64-bit words in order starting\nat address 0x40 (Block 1)\n▪ There are 8 words in each cache block.\n▪ Therefore, we have only one compulsory miss.\n▪ And then we have 7 cache hits!!\n\n• If we request the ninth word, we will be at\naddress 0x80 (and a compulsory miss.)\n\n1\n\n2\n\n24\n\n\fOnce again… A Tale of Two C… um… programs\n\nAllocates matrices.\n(Array of arrays)\nCopies one matrix to another.\n(data itself is uninitialized.)\n\nSpring 2019\/2020\n\nIterates through column.\n(Other code goes through row)\n\n25\n\n\fOnce again… A Tale of Two C… um… programs\n• We will simplify by looking at a 4x4 matrix.\n▪ We want to get the addresses being used to\nsee the access pattern. (Goes across row)\n\n26\n\n\fOnce again… A Tale of Two C… um… programs\n• We will simplify by looking at a 4x4 matrix.\n▪ Notice the different type of access pattern.\n▪ (Goes down the column)\n\n27\n\n\fThe Antagonist\n• One program reads words sequentially in\nmemory (good spatial locality)\n▪ The other reads each word as far apart as\npossible! (worst spatial locality)\n\n• Let’s look at making the matrices much\nlarger! Let’s make each row span 256 Bytes.\n(4 blocks, which is the size of our cache!)\n\nThis cache\nitem holds\n,\n,\n, etc\n\n0\n\n1\n\n2\n\n3\n\nCache size: 256B\n64 bytes per block.\n\n28\n\n\fCache Performance\n• Recall that caches make computers practical.\n\n▪ Why? Well…\n▪ Our “slow” program effectively did not use cache, and it was 10 times slower.\n\n• Simply: Caches offer much faster accesses than DRAM.\n▪ Perhaps 100s of times faster.\n\n• Consider the math:\n▪ miss rate (MR): Fraction of memory accesses not in cache.\n▪ hit rate (HR): Fraction of memory accesses found in cache. ( H𝑅 = 1 − 𝑀𝑅 )\n▪ hit time (HT): Time it takes to read a block from cache to CPU. (Best case)\n▪ miss penalty (MP): Time it takes to read from main memory to cache.\n29\n\n\fCache Performance\n• Recall that caches make computers practical.\n• Consider the math:\n▪ miss rate (MR): Fraction of memory accesses not in cache.\n▪ hit rate (HR): Fraction of memory accesses found in cache. ( H𝑅 = 1 − 𝑀𝑅 )\n▪ hit time (HT): Time it takes to read a block from cache to CPU. (Best case)\n▪ miss penalty (MP): Time it takes to read from main memory to cache.\n\n• Average Memory Access Time (AMAT): The time it takes, on average,\nto perform a memory request, considering the cache performance.\n▪ 𝐴𝑀𝐴𝑇 = 𝐻𝑇 + 𝑀𝑅 × 𝑀𝑃\n\n• Assuming a HT of 1 clock cycle and a MP of 100 clock cycles…\n▪ A HR of 97%: 𝐴𝑀𝐴𝑇 = 1 + 0.03 × 100 = 4 𝑐𝑦𝑐𝑙𝑒𝑠\n▪ A HR of 99%: 𝐴𝑀𝐴𝑇 = 1 + 0.01 × 100 = 2 𝑐𝑦𝑐𝑙𝑒𝑠\n▪ A hit-rate jump from just 97% to 99% doubles memory performance. Wow.\n\n30\n\n\fCache Layout Summary\n• We have seen two types of cache layouts.\n▪ A freeform cache: blocks go wherever. ¯\\_(ツ)_\/¯\n• Also called a fully associative cache.\n\n▪ A direct-mapped cache: blocks go into slots.\n\n• They have their own trade-offs, and as\nusual…\n▪ We can have a hybrid approach!\n\n• Here, each cache slot has multiple bins.\n▪ You only need to evict when you fill up the bins.\nBest of both worlds!\n▪ Which do you evict? (Hmm… difficult choice.)\n31\n\n\fAssociativity\n• With an associative cache, the address\ndetermines the slot.\n▪ Much like a direct-mapped cache.\n▪ However, the slot has a number of bins.\n▪ Any bin in the slot is viable for a block.\n▪ The number of bins is the number of “ways”\n• A direct-mapped cache is a 1-way cache.\n\n• When the cache determines if the block is in\nthe cache already…\n▪ It determines the slot.\n▪ It scans every bin for a block tagged with that\nexact address.\n▪ Therefore, the cache performance degrades as you\nincrease the number of ways.\n\n32\n\n\fAnother way of viewing it\nAll have the same size, so…\n\nOn a fully associative cache all\nblocks belong to the same set\n\nOn a direct-mapped cache all\nblocks belong to a different set\n\nOn a n-way associative cache n\nblocks belong to the same set\n\n33\n\n\fMapping\nWT F…Cache!?\n\n\fSooo… exactly what can go where?\n• Given a memory address, in which set does it go?\n• How many sets are there?\n• Let’s start by defining a cache size\n• 32KiB\n\n64B\n\n32𝐾𝑖𝐵\n215\nI need 64𝐵 = 26 =\n29 = 512 lines\n\n• How many cache lines do you need?\n• Well it depends on the size of each cache line\n• # Cache line? # Cache row? # Block? # bins?\n\n• Let’s use: 64B!\n\n35\n\n\fSooo… exactly what can where?\n64B\n\n• Let’s start by defining a cache size\n• 32KiB\n\n• How many cache rows do you need?\n• Well it depends on the size of each row\n• Let’s use: 64B!\n\n• Decrease the size\n\n32𝐾𝑖𝐵\n215\nI need 64𝐵 = 26 =\n29 = 512 rows\n\n• +rows, + (but faster) memory access, -locality\n\n• Increase the size\n• -rows, -(but slower) memory access, +locality\n\n36\n\n\fHow many sets\n• The number of sets depends on the associativity\n• Remember we have a fixed amount of rows!\n\n• For fully associative (easy) we have 1 set ☺\n• For n-way associative cache we need some maths:\n• n-way associative means we divide the lines in groups of n-elements\n0\n\n…\n\ns-1\nSo how many sets?\nEach column (set)\nhas n rows\n\nStill 512 rows!\n\ns=\n\n# rows 512\n=\n𝑛\n𝑛\n\n37\n\n\fHow many sets\n• If we apply this to a 4-way associative cache\n\n0\n\n…\n\n127\nSo how many sets?\nEach column (set)\nhas 4 rows\n\nStill 512 rows!\n\n# rows 512\ns=\n=\n𝑛\n4\n= 128 𝑠𝑒𝑡𝑠\n\n38\n\n\fAddress Manipulation\n\nRequest from CPU:\nAccess PT:\n\n𝑛-bit virtual address\nVirtual Page Number\n\nPage offset\n\nTRANSLATION\n𝑚-bit physical\naddress:\nSplit to access\ncache:\n\nPhysical Page Number\nCache Tag\n\nPage offset\n\nSet Index\n\nOffset\n\n\fUsing our example\n• On our example:\n• Cache size: 32KiB\n• Block size: 64B = 26\n• Associativity: 4-way\n• Number of sets: 128 = 27\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nPhysical Page Number\n\nPage offset\n\nCache Tag\n\nSet Index\n\nOffset\n\n12 bits\n\n7 bits\n\n6 bits\n\nTAG\n\nSet\n\nvalid\n\nData\n\n0xFFF\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\nOnly showing 2 blocks per set (slide space)\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nHit!\n\n111111111111000000\n\n0000000\n\n0xFFF\n\n0\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nMiss!\n\n000000000000000000\n\n0000000\n\n0x000\n\n0\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nHit!\n\n000100100011000000\n\n1000000\n\n0x123\n\n1\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nMiss!\n\n101010101010010101\n\n0000000\n\n0xAAA\n\n42\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fOffset\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\nHit!\n\n63\n\n62\n\n61\n\n…\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n0xF5\n\n0x32\n\n0x45\n\n…\n\n0xFF\n\n0xFE\n\n0x00\n\n0x68\n\n0x67\n\n0x65\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\n111111111111000000\n\n0000010\n\n0xFFF\n\n0\n\n2\n\n12 bits\n\n7 bits\n\n6 bits\n\n45\n\n\fSummary\n• The notion of storing data is a complicated one.\n▪ Different technologies have different strengths (and costs)\n▪ Often trade-off between:\n• fast \/ small, expensive\n• slow \/ big, cheap\n\n▪ Hardware designs attempt to accommodate a variety of technologies.\n• Often using fast\/small memories to act as a “cache” for slower ones.\n\n• Caches can be arranged in several ways:\n▪ Blocks go anywhere (fully-associative)\n▪ Blocks go in particular slots (direct-mapped \/ 1-way associative)\n▪ Hybrid: Blocks go to particular slots… but then can go in any bin in that slot.\n\n• Caches attempt to exploit temporal and spatial locality of programs.\n▪ And even a slight improvement to hit rate can dramatically improve overall\nperformance of a program!\n\n46\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":195,"segment": "unlabeled", "course": "cs0449", "lec": "final_review", "text":"Intro to Systems Software\n\nCS 449\n\nFinal Review\n\nBryant and O’Hallaron, Computer Systems: A Programmer’s Perspective, Third Edition\n\n1\n\n\fTopics\n\n• Buffer overflow\n• Linking (high level understanding)\n• Cache memories\n• Virtual Memory\n• Memory Allocation\n• Processes\n\n2\n\n\fJust for fun #1: What does this code print?\n#include <stdio.h>\n#include <stddef.h>\nint foo(int x, int y) {\nstruct s {\nchar a[x];\nchar b[y];\nchar c;\n};\nreturn offsetof(struct s , c);\n}\nint main(void)\n{\nprintf(\"%d\\n\", foo(2,3));\n}\n\nOnly works with “gcc” J\n\n\fJust for fun #2: What does this code print?\n#include <stdio.h>\n#include <stddef.h>\nint bar(int x, int y) {\nstruct s {\nchar a[x][y];\n};\nreturn sizeof(struct s);\n}\nint main(void)\n{\nprintf(\"%d\\n\", bar(2,3));\n}\n\nOnly works with “gcc” J\n\n\fBuffer Overflow\n\n\fStack Discipline\n\n• This kind of problem covers a wide range of\ntopics, such as stack frames, string\nrepresentations, ASCII code, and byte\nordering\n• It demonstrates the dangers of out-of-bounds\nmemory references and the basic ideas behind\nbuffer overflow\n\n\fReference\n\n• Read: CSAPP3e 3.10.3 (“Out-of-Bounds\nMemory References and Buffer Overflow”)\n• Exercise: CSAPP practice problem 3.46\n\n\fPractice problem 3.46\n• Below is a (low-quality) implementation of a function that\nreads a line from standard input, copies the string to newly\nallocated storage, and returns a pointer to the result\n\/* This is very low-quality code. It is intended to illustrate\nbad programming practices. See Practice Problem 3.46. *\/\nchar *get_line()\n{\nchar buf[4];\nchar *result;\ngets(buf);\nresult = malloc(strlen(buf));\nstrcpy(result, buf);\nreturn result;\n}\n\n\fPractice problem 3.46\n• Disassembly up through call to gets\n\n1\n2\n3\n4\n5\n\nchar *get_line()\n0000000000400720 <get_line>:\n400720:53\npush\n%rbx\n400721:48 83 ec 10\nsub\n$0x10,%rsp\nSee diagram stack at this point\n400725:48 89 e7\nmov\n%rsp,%rdi\n400728:e8 73 ff ff ff\ncallq 4006a0 <gets>\nModify diagram to show stack contents at this point\n\n\fPractice problem 3.46\nConsider the following scenario. Procedure get_line is called with\nthe return address equal to 0x400776 and register %rbx equal\nto 0x0123456789ABCDEF. You type in the string:\n\n0123456789012345678901234\nThe program terminates with a segmentation fault. You run GDB and\ndetermine that the error occurs during the execution of\nthe ret instruction of get_line.\n\n\fPractice problem 3.46\n• Question A: Fill in the diagram that follows, indicating as much as you\ncan about the stack just after executing the instruction at line 3 in\nthe disassembly. Label the quantities stored on the stack (e.g., \"Return\naddress\") on the right, and their hexadecimal values (if known) within\nthe box. Each box represents 8 bytes. Indicate the position of %rsp.\nRecall that the ASCII codes for characters 0–9 are 0x30–0x39.\n\n\fPractice problem 3.46\n• Question A: Fill in the diagram that follows, indicating as much as you\ncan about the stack just after executing the instruction at line 3 in\nthe disassembly. Label the quantities stored on the stack (e.g., \"Return\naddress\") on the right, and their hexadecimal values (if known) within\nthe box. Each box represents 8 bytes. Indicate the position of %rsp.\nRecall that the ASCII codes for characters 0–9 are 0x30–0x39.\n\nStack after line 3:\n\n\fPractice problem 3.46\n• Question B: Modify your diagram to show the effect of\nthe call to gets (line 5)\n\n\fPractice problem 3.46\n• Question B: Modify your diagram to show the effect of\nthe call to gets (line 5)\nStack after line 5:\n\n\fPractice problem 3.46\n• Question C: To what address does the program\nattempt to return?\nThe program is attempting to return to address 0x040034.\nThe low-order 2 bytes were overwritten by the code for\ncharacter ‘4’ and the terminating null character.\n\n\fPractice problem 3.46\n• Question D: What register(s) have corrupted value(s)\nwhen get_line returns?\n\nThe saved value of register %rbx was set to 0x3332313039383736.\nThis value will be loaded into the register before get_line returns.\n\n\fPractice problem 3.46\n• Question E: Besides the potential for buffer overflow,\nwhat two other things are wrong with the code for\nfunction get_line?\n\nThe call to malloc should have had strlen(buf)+1 as its argument, and\nthe code should also check that the returned value is not equal to NULL.\n\n\fCache Memories\n\n18\n\n\fPractice Cache Problem\n• We have a 64 KiB address space\n• The cache is a 1 KiB, direct-mapped cache using 256-byte blocks\nwith write-back and write-allocate policies\n• write-allocate: memory is read after a cache-miss-on-write\n• write-back: dirty data is written back to memory on eviction\na) Calculate the TIO address breakdown:\n\n\fPractice Cache Problem\n• We have a 64 KiB address space\n• The cache is a 1 KiB, direct-mapped cache using 256-byte blocks\nwith write-back and write-allocate policies\n• write-allocate: memory is read after a cache-miss-on-write\n• write-back: dirty data is written back to memory on eviction\na) Calculate the TIO address breakdown:\n\n\fPractice Cache Problem\nb) During some part of a running program, the cache’s\nmanagement bits are as shown below.\n\nFour options for the next two memory accesses are\ngiven (R = read, W = write). Circle the option that results\nin data from the cache being written to memory\n\n\fPractice Cache Problem\n\n(1) R 0x4C00, W 0x5C00\n\n(3) W 0x2300, R 0x0F00\n\n(2) W 0x5500, W 0x7A00\n\n(4) R 0x3000, R 0x3000\n\n\fPractice Cache Problem\n\n(1) R 0x4C00, W 0x5C00\n\n(3) W 0x2300, R 0x0F00\n\nR 0b0100 1100… , W 0b0101 1100…\n\nW 0b0010 0011… , R 0000 1111…\n\n(2) W 0x5500, W 0x7A00\n\n(4) R 0x3000, R 0x3000\n\nW 0b0101 0101… , W 0b0111 1010…\n\nR 0b0011 0000… , R 0011 0000…\n\n\fPractice Cache Problem\n\n(1) R 0x4C00, W 0x5C00\n\n(3) W 0x2300, R 0x0F00\n\nR 0b0100 1100… , W 0b0101 1100…\n\nW 0b0010 0011… , R 0000 1111…\n\nThe read evicts line 0, but the dirty bit was not set so\nnothing is written (also, line 0 was initially invalid). The\nwrite overwrites line 0 again but since the cache is writeback nothing is written to memory.\n\nThe write evicts line 3 which was invalid and not\ndirty, so nothing is written to memory. The read,\nhowever, also maps to line 3 so it must write the\nvalue changed in the write back to memory before\nit can update the cache.\n\n(2) W 0x5500, W 0x7A00\n\n(4) R 0x3000, R 0x3000\n\nW 0b0101 0101… , W 0b0111 1010…\n\nR 0b0011 0000… , R 0011 0000…\n\nThe first write doesn’t evict anything because the tags match.\nThe second write evicts the old data, but the dirty bit was not\nset so the old data doesn’t need to be written back to memory.\n\nLine 0 is initially not dirty (and invalid) so nothing is\nwritten back to memory from either of these reads\n(which both read from the same line).\n\n\fImplementing LRU (Least Recently Used)\nBlock\n\nTag\n\nA\nE\nB\nC\nD\nBlock requests:\n\nValid LRU\n3\n0\n2\n\nß least recent used\n\n1\n0\n2\n1\n3\n2\n3\n\nE\n\nD\n\nHow large (in bits) should the LRU counter be on\nan N-way set-associative cache?\n25\n\n\fVirtual Memory\n\n\fEnd-to-End Address Translation\n\n• Consider the following memory system:\n\n– The memory is byte addressable\n– Virtual addresses are 14 bits wide (n = 14)\n– Physical addresses are 12 bits wide (m = 12)\n– The page size is 64 Bytes (P = 64)\n– The L1 d-cache is physically addressed and direct\nmapped, with a 4-Byte line size and 16 total sets\n\n\fVirtual and physical addresses\n\n• 14-bit virtual addresses (n = 14)\n• 12-bit physical addresses (m = 12)\n• 64-byte pages (P = 64)\n\n\fSnapshot of the page table\n\n\fSnapshot of the L1 d-cache\n\n4B-block\n\n\fPractice Problem 9.4 (CSAPP:3e)\n\n• Show how the example memory system\ntranslates a virtual address into a physical\naddress and accesses the cache\n• Given virtual address: 0x03d7\nWhat is the virtual address format?\n0 0\n\n0 0 1\n\n1\n\n1 1\n\n0 1\n\n0 1 1\n\n1\n\n\fAddress Translation\n\n0x0f\nN\n0x0d\n\n0x03d7\n\n0 | 0 | 0 | 0| 1 | 1 | 1 | 1 | 0 | 1 | 0 | 1 | 1 | 1\n0x0f\n\n0x17\n\n32\n\n\fPhysical Memory Access\n\n0x03\n0x05\n0x0d\nY\n0x1d\n\n0x0d\n\n0x03d7\n\n0x05\n\n0x03\n\n0 | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 1\n0x0d\n\n0x17\n\n\fMemory Allocation\n\n\fDynamic storage allocation\n• Consider an allocator that uses an list. The layout of each\nallocated AND free memory block is as follows:\n• Each memory block, either allocated or free, has a size.\n• The size doesn’t include the header.\n• The size is negative if the block is free, positive otherwise\n• It’s a 32-bit machine, addresses and ints are both 32-bits\nHeader\n\nData\n\n____________________________\n| (int) Block size (Bytes) |\n|_ _ _ _ _ _ _ _ _ _ _ _ _ _ |\n| (int) Previous address\n|\n|____________________________|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|____________________________|\n\n\f•\n\nGiven the contents of the heap\nshown on the left, show the new\ncontents of the heap (in the\nright table) after a call to\nfree(0x400b014) is executed\n\n•\n\nYour answers should be given as\nhex values\n\n•\n\nNote that the address grows\nfrom bottom up\n\n•\n\nAssume that the allocator uses\nimmediate coalescing, that is,\nadjacent free blocks are merged\nimmediately each time a block is\nfreed\n\nBefore\n\nAfter\n\n0x0400b00c\n\n0x0400affc\n\n0x00000002\n\n0x00000002\n\n0x0400affc\n\n0x0400affc\n\n0x00000002\n\n0xFFFFFFFFE\n\n0x0400aff8\n\n0x0400aff8\n\n0xFFFFFFFFE\n\n0xFFFFFFFFA\n\n\fProcesses\n\n\fProcesses\n\n• Consider this code using Linux’s fork:\nint x = 7;\nif ( fork() ) {\nx++;\nprintf(\" %d \", x);\nfork();\nx++;\nprintf(\" %d \", x);\n} else {\nprintf(\" %d \", x);\n}\n\nWhat are all the different\npossible outputs (i.e. order of\nthings printed) for this code?\n(Hint: there are 4 of them.)\n\nTip: try drawing a process graph for this program\n\n\fProcesses\n\n• Consider this code using Linux’s fork:\nint x = 7;\nif ( fork() ) {\nx++;\nprintf(\" %d \", x);\nfork();\nx++;\nprintf(\" %d \", x);\n} else {\nprintf(\" %d \", x);\n}\n\n\fProcesses\n• Consider this code using Linux’s\nfork:\nint x = 7;\nif ( fork() ) {\nx++;\nprintf(\" %d \", x);\nfork();\nx++;\nprintf(\" %d \", x);\n} else {\nprintf(\" %d \", x);\n}\n\nPossible orderings:\n7 8 9 9\n8 7 9 9\n8 9 7 9\n8 9 9 7\n\n\fMore stuff\n\n\fDynamic storage allocation\n• Five helper routines are defined to facilitate the\nimplementation of free(void *p)\n• The functionality of each routine is explained in the\ncomment above the function definition\n• Fill in the body of the helper routines the code\nsection label that implement the corresponding\nfunctionality correctly\n• Important notice: Block sizes are 4 bytes in this\nproblem\n\n\fDynamic storage allocation\n\n• Code 1:\n\/* given a pointer p to an allocated block, i.e., p is a\npointer returned by some previous malloc()\/realloc() call;\nreturns the pointer to the header of the block*\/\nvoid * header(void* p)\n{\nvoid *ptr;\n_______;\nreturn ptr;\n}\nA. ptr = p-2\nB. ptr = (void *)((int *)p-2)\nC. ptr = (void *)((int *)p+2)\n\n\fDynamic storage allocation\n\n• Code 2:\n\/* given a pointer to a valid block header or footer,\nreturns the size of the block *\/\nint size(void *hp)\n{\nint result;\n_______;\nreturn result;\n}\nA. result = (*hp>0)?*hp:-*hp;\nB. result = *(int *)hp;\nC. result = (*(int *)hp > 0)?(*(int *)hp):(-*(int *)hp);\n\n\fDynamic storage allocation\n\n• Code 3:\n\/* given a pointer p to an allocated block, i.e. p is\na pointer returned by some previous malloc()\/realloc() call;\nreturns the pointer to the next of the block*\/\nvoid * next(void *p)\n{\nvoid *ptr;\n_______;\nreturn ptr;\n}\nA. ptr = p+size(header(p))\nB. ptr = p+size(header(p))-8\nC. ptr = (int *)p+size(header(p))-2\n\n\fDynamic storage allocation\n\n• Code 4:\n\/* given a pointer to a valid block header or footer,\nreturns the usage of the current block,\n1 for allocated, 0 for free *\/\nint allocated(void *hp)\n{\nint result;\n______;\nreturn result;\n}\nA. result=(*(int *)hp)>0\nB. result=(*(int *)hp)==0\nC. result=(*(int *)hp)<0\n\n\fDynamic storage allocation\n\n• Code 5:\n\/* given a pointer to a valid block header,\nreturns the pointer to the header of previous block in\nmemory *\/\nvoid * prev(void *hp)\n{\nvoid *ptr;\n______;\nreturn ptr;\n}\nA. ptr = hp – *((int*)hp + 1)\nB. ptr = (void*)*((int*)hp + 1)\nC. ptr = (void*)((int*)hp + 1)\n\n\fCache Puzzle\n• Given the following sequence of access results\n(addresses are given in decimal) on a cold\/empty cache\nof size 16 bytes, what can we deduce about its\nproperties? Assume an LRU replacement policy\n(0, Miss), (8, Miss), (0, Hit), (16, Miss), (8, Miss)\n\nWhat can we say about the block size?\n\n\fCache Puzzle\n• Given the following sequence of access results\n(addresses are given in decimal) on a cold\/empty cache\nof size 16 bytes, what can we deduce about its\nproperties? Assume an LRU replacement policy\n(0, Miss), (8, Miss), (0, Hit), (16, Miss), (8, Miss)\n\nWhat can we say about the block size?\nThe block size must be ≤ 8 because access (2) to address\n8 is a miss after access (1) to address 0 is a hit.\n\n\fCache Puzzle\n(0, Miss), (8, Miss), (0, Hit), (16, Miss), (8, Miss)\n\nAssuming that the block size is 8 bytes, can this\ncache be… (Hint: draw the cache and simulate it)\na. Direct-mapped?\nDoes this cache work for the access\nresults? Yes, Yes, Yes, Yes (evict 0), No (8\nwould still be in cache)\n\n\fCache Puzzle\n(0, Miss), (8, Miss), (0, Hit), (16, Miss), (8, Miss)\n\nAssuming that the block size is 8 bytes, can this\ncache be… (Hint: draw the cache and simulate it)\nb. 2-way set associative?\nDoes this cache work for the access\nresults? Yes, Yes, Yes, Yes (evict 8 b\/c\nit’s the least recently used), Yes (8 is no\nlonger in cache)\n\n\fCache Puzzle\n(0, Miss), (8, Miss), (0, Hit), (16, Miss), (8, Miss)\n\nAssuming that the block size is 8 bytes, can this\ncache be… (Hint: draw the cache and simulate it)\nc. 4-way set associative?\nNo, because the block size is 8, multiplied\nby 4 lines per set, and that’s 32B, which\nis already bigger than the entire cache.\n\n\fCode Analysis Problem\n•Assuming the cache starts cold (all blocks invalid) and sum, i, and j\nare stored in registers, calculate the miss rate:\n𝑚 = 12 bits (addr. range), 𝐶𝑎𝑐ℎ𝑒 = 256 B, 𝐵𝑙𝑜𝑐𝑘 = 32 B, 𝐴𝑠𝑠 = 2\n\n#define SIZE 8\nlong ar[SIZE][SIZE], sum = 0; \/\/ &ar=0x800\nfor (int i = 0; i < SIZE; i++)\nfor (int j = 0; j < SIZE; j++)\nsum += ar[i][j];\n• Code employs a row-wise access pattern\n• Cache block (of 32 bytes) holds 4 long’s\n• There is a miss followed by 3 hits (MR=25%)\n53\n\n\fCode Analysis Problem\n\n• How would the following changes change the\nmiss rate?\n– Increasing the block size?\n– Doubling the cache size\n– Reducing\/increasing the associativity\n\n54\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":196,"segment": "unlabeled", "course": "cs0007", "lec": "lec18", "text":"CS 0007: Introduction to Java\nLecture 18\nNathan Ong\nUniversity of Pittsburgh\nNovember 8, 2016\n\n\fMain Portions of OOP\n• Building a class\n• Determining inter-class relationships\n\n\fClass Components\n• Class-level\/Instance Variables\n• Methods\n• Constructor(s)\n\n\fInter-class Relationships\n• Building a single class is useless. It\nneeds to be used in context.\n• There are two main relationships\nbetween classes\n– Sub\/Super class\n– Ownership\n\n\fModeling a Problem\n“Joe’s Automotive Shop services foreign cars, and\nspecializes in servicing cars made by Mercedes,\nPorsche, and BMW. When a customer brings a car to\nthe shop, the manager gets the customer’s name,\naddress, and telephone number. Then the manager\ndetermines the make, model, and year of the car, and\ngives the customer a service quote. The service\nquote shows the estimated parts charges, estimated\nlabor charges, sales tax, and total estimated\ncharges.”\nSource: Starting Out with Java: From Control Structures\nthrough Objects by Tony Gaddis\n\n\fFind the Nouns\n“Joe’s Automotive Shop services foreign cars,\nand specializes in servicing cars made by\nMercedes, Porsche, and BMW. When a\ncustomer brings a car to the shop, the\nmanager gets the customer’s name, address,\nand telephone number. Then the manager\ndetermines the make, model, and year of the\ncar, and gives the customer a service quote.\nThe service quote shows the estimated parts\ncharges, estimated labor charges, sales tax,\nand total estimated charges.”\n\n\fCoalescing\naddress\nBMW\ncar\nquote\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nPorsche\nJoe’s Automotive Shop sales tax\nmake\nservice\nmanager\nMercedes\nmodel\n\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\n\fNoun Out of Scope\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\n\fSubtypes\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\nname\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated charges\nyear\n\nWhether these subtypes are specific objects\n(instances) or more specific types of cars\n(subclasses) will depend on your needs. In this\nscenario, there is no need to represent them as\nclasses.\n\n\fSimple Properties\naddress\nforeign cars\nBMW\nJoe’s Automotive Shop\ncar\nmake service quote\ncars\nmanager\ncustomer Mercedes\nestimated labor charges\nmodel\nestimated parts charges\nname\n\nPorsche\nsales tax\nshop\ntelephone number\ntotal estimated charges\nyear\n\nSimple properties can be composed of\nprimitives or preexisting object\ndefinitions, and fall under a particular\nobject class.\n\n\fSimple Properties\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\nA car had three simple properties in our\nscenario. What types would they be?\n\n\fSimple Properties\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\nA car had three simple properties in our\nscenario. What types would they be?\n\n\fSimple Properties\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\nA customer had three simple properties\nin our scenario. What types would they\nbe?\n\n\fSimple Properties\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\nA customer had three simple properties\nin our scenario. What types would they\nbe?\n\n\fSimple Properties\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\nA service quote had four simple\nproperties in our scenario. What types\nwould they be?\n\n\fSimple Properties\naddress\nBMW\ncar\ncars\ncustomer\nestimated labor charges\ncharges\nestimated parts charges\n\nforeign cars\nJoe’s Automotive Shop\nmake\nmanager\nMercedes\nmodel\n\nPorsche\nsales tax\nservice quote\nshop\ntelephone number\ntotal estimated\n\nname\n\nyear\n\nA service quote had four simple\nproperties in our scenario. What types\nwould they be?\n\n\fWho Owns Everything?\naddress\nforeign cars\nBMW\nJoe’s Automotive Shop\ncar\nmake service quote\ncars\nmanager\ncustomer\nMercedes\nestimated labor charges\nmodel\nestimated parts charges\nname\n\nPorsche\nsales tax\nshop\ntelephone number\ntotal estimated charges\nyear\n\nOur scenario revolves around the\nautomotive shop, so this will hold the\nmain method and will be the overarching\nclass.\n\n\fCode on the board!\n\n\fThe Object Class\n• Let us examine the API for the highest\nclass.\n• toString() looks interesting. “It is\nrecommended that all subclasses\noverride this method.”\n\n\fWhat Is toString()?\n• It provides a String representation of\nan object\n• It should be descriptive, not just what\ntype it is, but what is contained within it.\n\n\fWho is Responsible?\n• How do we determine which class(es)\nshould have which functions?\n• You need to ask: Who’s state is needed\nor changing?\n\n\fEasy examples\n• Using the automotive shop example, we\nhave four classes, Shop, Customer, Car,\nand Service Quote.\n• Who issues the service quotes?\n• Who deals with customers?\n\n\fMedium example\n• Using the automotive shop example, we\nhave four classes, Shop, Customer, Car,\nand Service Quote.\n• Who does the payment of the service\nquote?\n– Customer?\n– Shop?\n– Both?\n\n\fHard example\n• Using the automotive shop example, we\nhave four classes, Shop, Customer, Car,\nand Service Quote.\n• Who deals with car alterations?\n– Shop?\n– Mechanic?\n– Car?\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":197,"segment": "unlabeled", "course": "cs1550", "lec": "lec25", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Homework 12: due on 4\/25\n• Lab 5: due on 5\/2\n• Project 4 and Quiz 4: due on 5\/2\n• Bonus Opportunities\n• Bonus Homework: due on 5\/2\n• Course Post-Test: due on 5\/2\n• Bonus point for all when OMET response rate >= 80%\n•\n\nCurrently at 25-28%\n\n•\n\nDeadline is Sunday 4\/24\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\n• Disk arm scheduling\n• FCFS, SSTF, SCAN, C-SCAN, LOOK, C-LOOk\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points\n•\n\nJust need to study the material more to be honest.\n\n•\n\nDMA and CPU memory competition\n\n•\n\nhardware view of interrupt\n\n•\n\nThanks for bringing in coffee and snacks for the final! It is much\nappreciated\n\n•\n\nonline vs offline problem\n\n•\n\nWhat sort of questions about this material will be on the exam?\n\n•\n\nIn Sharing file, if the directory A is owner of file \"a_file\" and then this\nfile is shared by directory B, Then we delete directory A, Are all files\neven shared files will be deleted and not more accessible, or we\nhave one copy of \"a_file \" in directory of B?\n\n•\n\nCan you please go over File Linking again?\n\n•\n\nHow do you read in the blocks from the disk file for project 4? I know\nto use fseek and fread but I don't know where to go from there.\n\n•\n\nCan you explain the difference between CSCAN and CLOOK?\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fJournaling File System\n• In a regular file system, changes to files and\ndirectories result in multiple separate writes to disk\n• prone to power failures\n\n• Write changes twice\n• first to an on-disk journal\n• for efficiency, journal can be put on SSD or NVRAM\n• data may or may not be written to the journal\n\n• second to main file system\n\n• Examples\n• Windows NTFS\n• Linux ext3, ext4\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fJournaling File System\n• Interaction of disk arm scheduling?\n• out-of-order writes\n• ext3 and ext4 force disk to flush its cache at certain points\n(barriers)\n\n• Journaling vs. Log-structured file system\n• Journaling is not needed in LSFS\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fFile Linking (left) vs. File Copying (right)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fMax Partition Size\nBlock size\n\nFAT-12\n\nFAT-16\n\n0.5 KB\n\n2 MB\n\n1 KB\n\n4 MB\n\n2 KB\n\n8 MB\n\n128 MB\n\n4 KB\n\n16 MB\n\n256 MB\n\n1 TB\n\n8 KB\n\n512 MB\n\n2 TB\n\n16 KB\n\n1024 MB\n\n2 TB\n\n32 KB\n\n2048 MB\n\n2 TB\n\nCS 1550 – Operating Systems – Sherif Khattab\n\nFAT-32\n\n8\n\n\fMax Partition Size\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fFile-related kernel structures:\nopen file tables and disk quotas\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fHW 10: Q 2-4\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fHW 10: Q 10-13\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fEffective Disk Access Time\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fMinimum and Maximum Seek Time\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fAverage Rotational Delay\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fBuffering device input\nUser\nspace\n\nUser\nspace\n\nUser\nspace\n\nUser\nspace\n\nKernel\nspace\n\nKernel\nspace\n\nKernel 2\nspace\n\nKernel 2\nspace\n\n1\n\nUnbuffered\ninput\n\nBuffering in\nuser space\n\nBuffer in kernel\nCopy to user space\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n1\n\n3\n\nDouble buffer\nin kernel\n16\n\n\fI\/O Buffering\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fProblem of the Day: Protection\n• Protection is about controlling access of programs,\nprocesses, or users to the system resources (e.g.,\nmemory pages, files, devices, CPUs)\n• How to decide who can access what?\n• Specifications must be\n• Correct\n• Efficient\n• Easy to use (or nobody will use them!)\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\f•\n\n•\n\n•\n\nProtection domains\nA process operates within a protection domain\n•\n\nresources accessible by the process\n\n•\n\neach domain lists objects with permitted operations\n\nDomains can share objects & permissions\n•\n\nObjects can have different permissions in different domains\n\n•\n\nThere need be no overlap between object permissions in different\ndomains\n\nHow can this arrangement be specified more formally?\n\nFile1 [R]\nFile2 [RW]\n\nDomain 1\n\nFile3 [R]\nFile4 [RWX] Printer [W]\nFile5 [RW]\n\nDomain 2\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\nFile3 [W]\nScreen1 [W]\nMouse [R]\n\nDomain 3\n19\n\n\fProtection matrix\nFile1\n\nDomain1 Read\n\nFile2\n\nFile3\n\nFile4\n\nFile5\n\nPrinter1\n\nRead\nWrite\n\nDomain2\n\nRead\n\nDomain3\n\nWrite\n\nRead\nWrite\nExecute\n\nRead\nWrite\n\nWrite\n\nWrite\n\n•\n\nEach domain has a row in the matrix\n\n•\n\nEach object (resource) has a column in the matrix\n\n•\n\nEntry for <object, column> has the permissions\n\n•\n\nWho’s allowed to modify the protection matrix?\n•\n\n•\n\nCamera\n\nRead\n\nWhat changes can they make?\n\nHow is this implemented efficiently?\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\fDomains as objects in the protection matrix\nDomain\n\n•\n\n•\n\nFile1\n\nFile2\n\n1 Read\n\nRead\nWrite\n\nFile3\n\nFile4\n\nFile5\n\nPrinter1\n\nCamera\n\nDom1\n\nDom2\n\nDom3\n\nModify\n\n2\n\nRead\n\n3\n\nWrite\n\nRead\nWrite\nExecute\n\nRead\nWrite\n\nWrite\n\nWrite\n\nModify\n\nRead\n\nEnter\n\nSpecify permitted operations on domains in the matrix\n•\n\nDomains may (or may not) be able to modify themselves\n\n•\n\nDomains can modify other domains\n\n•\n\nSome domain transfers (switching) permitted, others not\n\nDoing this allows flexibility in specifying domain permissions\n•\n\nRetains ability to restrict modification of domain policies\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fRepresenting the protection matrix\n• Need to find an efficient representation of the\nprotection matrix (also called the access matrix)\n• Most entries in the matrix are empty!\n• Compress the matrix by:\n• Associating permissions with each object: access control\nlist\n• Associating permissions with each domain: capabilities\n\n• How is this done, and what are the tradeoffs?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fAccess control lists (ACLs)\n• Each object has a list\nattached to it\n• List has\n•\n\nFile2\n\nelm: <R,W>\nznm: <R>\nroot: <R,W,X>\n\nelm: <R,X>\nuber: <R,W>\nroot: <R,W>\nall: <R>\n\nProtection domain (User\nname, Group of users, Other)\n\n•\n\nFile1\n\nAccess rights (Read, Write,\nExecute, Others)\n\n• No entry for domain => no\nrights for that domain\n• Operating system checks\npermissions when access\nis needed\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\f•\n\n•\n\nAccess control lists in the real world\nUnix file system\n•\n\n•\n\nAccess list for each file has exactly three domains on it\n•\n\nUser (owner)\n\n•\n\nGroup\n\n•\n\nOthers\n\nRights include read, write, execute: interpreted differently for directories\nand files\n\nAFS\n•\n\nAccess lists only apply to directories: files inherit rights from the directory\nthey’re in\n\n•\n\nAccess list may have many entries on it with possible rights:\n•\n\nread, write, lock (for files in the directory)\n\n•\n\nlookup, insert, delete (for the directories themselves),\n\n•\n\nadminister (ability to add or remove rights from the ACL)\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n24\n\n\fACL in UNIX\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n25\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":198,"segment": "unlabeled", "course": "cs1550", "lec": "lec00", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fCourse Goal and Objectives\nContinue to demystify a good portion of the magic\nabout how computers work so that you can write more\nefficient programs\nThe specific objectives are:\n1. Modify and compile the Linux kernel to add system\ncalls\n2. Write multi-process\/multi-thread programs free\nfrom race conditions and deadlocks\n3. Simulate page-replacement algorithms for virtual\nmemory management\n4. Implement a user-land file system\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fContact Info\n• Course website: http:\/\/www.cs.pitt.edu\/~skhattab\/cs1550\/\n• Instructor: Sherif Khattab ksm73@pitt.edu\n\nOH: https:\/\/khattab.youcanbook.me\nTuTh: 16:30-19:30 (remote until Jan 26 then in-person\/remote)\nF: 16:00-18:00 (remote only for the entire term)\n• 6307 Sennott Square, Virtual Office: https:\/\/pitt.zoom.us\/my\/khattab\n\n• Teaching Team:\nTBD\n\n• Recitations start this week!\n• Communication\n1.\n\nEnd of week newsletter\n\n2.\n\nPiazza (Please expect a response within 72 hours)\n1.\n\nEmail not recommended!\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fTextbook\n\nOperating System Concepts (9th Edition)\n\nSilberschatz, Galvin, and Gagne\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fGrading\n• 40% on 4 projects\n• System programming using C\n• Posted on Canvas and submitted on Gradescope (autograded)\n\n• 20% on two exams: 12% on higher and 8% on lower\n\n• 18% on weekly homework assignments\n• 14% recitation: 5 labs using MIT’s Xv6 operating\nsystem and 4 quizzes on the projects\n• 8% in-class Tophat questions and discussions\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fCanvas Walkthrough\n• Lectures posted on Tophat by end of each lecture\nday\n• Lecture and recitation recordings under Panopto\nVideo\n• Pre- and Post-course test\n\n• Piazza for discussion\n• Gradescope and autograding policies\n• Homework attempts\n• Academic Integrity\n• NameCoach\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fExpectations\n• Your continuous feedback is important!\n• Anonymous Qualtrics survey\n• Midterm and Final OMET\n\n• Your engagement is valued and encouraged.\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fLecture structure (mostly)\nTime\n\nDescription\n\n~5 min before and\nafter class\n~25 min\n\nInformal chat\n\n~40 min\n\nLecturing\n\n~5 min\n\nTophat questions and\/or activities\n\n~5 minutes\n\nQA and muddiest points\/reflections\n\nAnnouncements, review of\nreflections, and QA on\nprojects\/labs\/homework problems\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fWhy is this class (notoriously) hard?\n• Lots of concepts\n• Attend lectures and recitations (if you absolutely cannot\nattend live, watch the video recordings)\n• Study often!\n• Put effort into the weekly homework assignments\n\n• Projects are relatively hard\n• Refresh your C programming and GDB debugging skills\n(CS 0449!)\n• Start early and show up to office hours!\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":199,"segment": "unlabeled", "course": "cs1550", "lec": "lec18", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Quiz 2: due on 3\/25\n• Homework 9: due on 3\/28\n• Lab 3: due on 4\/1\n• Project 3: due on 4\/11\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\n• Tracing page replacement algorithms\n• FIFO and CLOCK\n• LRU and OPT\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points\n• Do page evictions happen across processes?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fLocal vs. global allocation policies\n◼\n\nWhat is the pool of pages eligible\nto be replaced?\nPages belonging to the process\nneeding a new page\n◼ All pages in the system\n◼\n\n◼\n\nLocal allocation: replace a page\nfrom this process\nMay be more “fair”: penalize\nprocesses that replace many pages\n◼ Can lead to poor performance: some\nprocesses need more pages than\nothers\n◼\n\n◼\n\nGlobal allocation: replace a page\nfrom any process\n\nLast access time\nPage\nA0\nA1\nA2\nA3\nA4\nB0\nB1\nA4\nB2\nC0\nC1\nC2\nC3\nC4\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n12\n8\n5\n10\n9\n3\n16\n12\n8\n5\n4\n\nLocal\nallocation\nA4\n\nGlobal\nallocation\n\n5\n\n\fMuddiest Points\n◼\n\nwhat measures do modern operating systems take\nin order to minimize thrashing?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fPage fault rate vs. allocated frames\n◼\n\nLocal allocation may be more “fair”\n◼\n\n◼\n\nDon’t penalize other processes for high page fault rate\n\nGlobal allocation is better for overall system performance\nTake page frames from processes that don’t need them as much\n◼ Reduce the overall page fault rate (even though rate for a single\nprocess may go up)\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fControl overall page fault rate\nDespite good designs, system may still thrash\n◼ Most (or all) processes have high page fault rate\n◼\n\nSome processes need more memory, …\n◼ but no processes need less memory (and could give\nsome up)\n◼\n\nProblem: no way to reduce page fault rate\n◼ Solution :\nReduce number of processes competing for\nmemory\n◼\n\nSwap one or more to disk, divide up pages they held\n◼ Reconsider degree of multiprogramming\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fBacking up an instruction\n◼\n\nProblem: page fault happens in the middle of instruction\nexecution\nSome changes may have already happened\n◼ Others may be waiting for VM to be fixed\n◼\n\n◼\n\nSolution: undo all of the changes made by the instruction\nRestart instruction from the beginning\n◼ This is easier on some architectures than others\n◼\n\n◼\n\nExample: LW R1, 12(R2)\nPage fault in fetching instruction: nothing to undo\n◼ Page fault in getting value at 12(R2): restart instruction\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fMinimum memory allocation to a process\n◼\n\nExample: ADD (Rd)+,(Rs1)+,(Rs2)+\n◼\n\nPage fault in writing to (Rd): may have to undo an awful lot…\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fMuddiest Points\n• What is disk write?\n• What does the * mean?\n• CLOCK\n•\n\nWhen does the orbit (or bit?) change in the clock simulation\n(FIFO\/slide 14)\n\n•\n\nhow does the clock iterate in the example from class?\n\n• LRU\n•\n\nDetermining page faults in LRU modelling\n\n•\n\nHow to calculate the distance\n\n•\n\nIs there a way to easily determine the disk writes using the\nLRU stack algorithm?\n\n•\n\nWould you please solve some example for LRU and OPT?\nThank you so much\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fMuddiest Points\n• How the tracing changes between different types of\nscheduling\n• FIFO\n• why does FIFO have more page faults with more memory\n(small explanation?)?\n\n• How to implement stack algorithm in code\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fMuddiest Points\n• how to keep track of larger tables that can store more\npages.\n• Problem of the Day!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fSeparating policy and mechanism\n◼\n\nMechanism for page replacement has to be in kernel\nModifying page tables\n◼ Reading and writing page table entries\n◼\n\n◼\n\nPolicy for deciding which pages to replace could be in user\nspace\n◼\n\nMore flexibility\n3. Request page\n\nUser\nspace\nKernel\nspace\n\nUser\nprocess\n\n2. Page needed\n\n4. Page\narrives\n\nExternal\npager\n\n5. Here is page!\n1. Page fault\n\nFault\nhandler\n\n6. Map in page\n\nMMU\nhandler\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fSeparating Policy and Mechanism for Page Replacement\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fProject 3: OPT Simulation\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fOPT Implementation Example\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fProject 3: LRU and miscellaneous hints\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fProject 3: Writeup hints\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fProblem of the Day\n\n• How to keep track of larger page tables that can store\nmore pages\n• How big can a page table be?\n• 64-bit machine\n• 4 KB page size\n\n• How many pages?\n• How many PTE?\n• How big is a PTE?\n• How big is the page table of one process?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":200,"segment": "unlabeled", "course": "cs1567", "lec": "lec07_kobuki_node","text":"Kobuki Node\nThumrongsak Kosiyatrakul\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fKobuki Node\nThe kobuki node communicates with a Kobuki robot via USB\nProvide hardware abstractions\nMotors with synchronization\nBumper switches\nButtons\nLEDs\n\nTo control a robot, simply communicate with the kobuki node\nPublish commands to the kobuki node\nSubscribe for information about the robot\n\nTo start a kobuki node, use the following command:\nroslaunch kobuki_node minimal.launch\n\nTo ensure the stability of the robot’s odometry, the robot should\nbe on the ground\/table before executing the above command\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fKobuki’s Subscriptions and Publications\nBy default the kobuki node subscribes to the following topic:\n\/mobile_base\/commands\/reset_odometry\n\/mobile_base\/commands\/sound\n\/mobile_base\/commands\/led1\n\/mobile_base\/commands\/led2\n\/mobile_base\/commands\/velocity\n\nWe simply publish data to the above topics to control the robot\nThe kobuki node publishes to the following topic:\n\/odom\n\/mobile_base\/events\/bumper\n\/mobile_base\/events\/button\n\nWe simply subscribe to the above topics to receive data from the\nrobot\nThere are more topics related to kobuki node but we will only\nfocus on topics listed above.\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fSounds\nA Kobuki robot has an ability to make sounds\nSimply publish a message of type Sound to the topic named\n\/mobile_base\/commands\/sound\n\nTo recognize a message of type Sound, we need to import it\nfrom kobuki_msgs.msg import Sound\n\nSound message consists of only one component named value\nSimply set the value between 0 and 6 and publish\nValues:\n0 - turn on\n1 - turn off\n2 - recharge start\n3 - press button\n4 - error sound\n5 - start cleaning\n6 - cleaning end\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fSounds\n#!\/usr\/bin\/env python\nimport rospy\nfrom kobuki_msgs.msg import Sound\ndef sendSounds():\nrospy.init_node('sound_sender', anonymous=True)\npub = rospy.Publisher('\/mobile_base\/commands\/sound', Sound,\nqueue_size=10)\nwhile pub.get_num_connections() == 0:\npass\ns = Sound()\nfor x in range (0,7):\ns.value = x\npub.publish(s)\nrospy.sleep(1.5)\nif __name__ == '__main__':\ntry:\nsendSounds()\nexcept rospy.ROSInterruptException:\npass\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fLEDs\nThere are two LEDs on a Kobuki robot named LED1 and LED2\nTo control an LED, simply publish a message type Led to its\nassociate topic:\nLED1: \/mobile_base\/commands\/led1\nLED2: \/mobile_base\/commands\/led2\n\nTo recognize a message of type Led, we need to import it\nfrom kobuki_msgs.msg import Led\n\nLed message consists of only one component named value\nSimply set the value and publish\nValues:\n0 - Black (Off)\n1 - Green\n2 - Orange\n3 - Red\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fLEDs\n#!\/usr\/bin\/env python\nimport rospy\nfrom kobuki_msgs.msg import Led\ndef sendLEDs():\nrospy.init_node('leds_sender', anonymous=True)\npub1 = rospy.Publisher('\/mobile_base\/commands\/led1', Led, queue_size=10)\npub2 = rospy.Publisher('\/mobile_base\/commands\/led2', Led, queue_size=10)\nwhile pub1.get_num_connections() == 0 or pub2.get_num_connections() == 0:\npass\nled = Led()\nx = 3\nwhile x != -1:\nled.value = x\npub1.publish(led)\nx = x - 1\nrospy.sleep(1)\nx = 3\nwhile x != -1:\nled.value = x\npub2.publish(led)\nx = x - 1\nrospy.sleep(1)\nif __name__ == '__main__':\ntry:\nsendLEDs()\nexcept rospy.ROSInterruptException:\npass\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fBumpers\nThere are three bumpers on a Kobuki robot, left, center, and right\nIf a bumper is pressed or release, it will publish a message of\ntype BumperEvent to the topic:\n\/mobile_base\/events\/bumper\n\nNeed to import the message of type BumperEvent as usual:\nfrom kobuki_msgs.msg import BumperEvent\n\nBumperEvent consists of two components:\nbumper with the following possible values:\n0 - Left\n1 - Center\n2 - Right\n\nstate with the following possible values:\n0 - Released\n1 - Pressed\n\nSimply subscribe to the topic to receive the message:\nrospy.Subscriber('\/mobile_base\/events\/bumper', BumperEvent, bumperCallback)\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fBumpers\n#!\/usr\/bin\/env python\nimport rospy\nfrom kobuki_msgs.msg import BumperEvent\ndef bumperCallback(data):\nstr = \"\"\nif data.bumper == 0:\nstr = str + \"Left bumper is \"\nelif data.bumper == 1:\nstr = str + \"Center bumper is \"\nelse:\nstr = str + \"Right bumper is \"\nif data.state == 0:\nstr = str + \"released.\"\nelse:\nstr = str + \"pressed.\"\nrospy.loginfo(str)\ndef bumperExample():\nrospy.init_node('bumper_example', anonymous=True)\nrospy.Subscriber('\/mobile_base\/events\/bumper', BumperEvent, bumperCallback)\nrospy.spin()\nif __name__ == '__main__':\nbumperExample()\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fButtons\nKobuki robot consists of three buttons, B0, B1, and B2\nIf a button is pressed or released, a message of type\nButtonEvent will be published into the topic\n\/mobile_base\/events\/button\n\nYour program must import the message type ButtonEvent\nas shown below:\nfrom kobuki_msgs.msg import ButtonEvent\n\nThe message of type ButtonEvent consists of two\ncomponents:\nbutton with the following possible values:\n0 - B0\n1 - B1\n2 - B2\n\nstate with the following possible values:\n0 - released\n1 - pressed\n\nSimply subscribe to the topic as usual:\nrospy.Subscriber('\/mobile_base\/events\/button', ButtonEvent, bumperCallback)\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fButton\n#!\/usr\/bin\/env python\nimport rospy\nfrom kobuki_msgs.msg import ButtonEvent\ndef buttonCallback(data):\nstr = \"\"\nif data.button == 0:\nstr = str + \"B0 is \"\nelif data.button == 1:\nstr = str + \"B1 is \"\nelse:\nstr = str + \"B2 is \"\nif data.state == 0:\nstr = str + \"released.\"\nelse:\nstr = str + \"pressed.\"\nrospy.loginfo(str)\ndef bumperExample():\nrospy.init_node('button_example', anonymous=True)\nrospy.Subscriber('\/mobile_base\/events\/button', ButtonEvent, buttonCallback)\nrospy.spin()\nif __name__ == '__main__':\nbumperExample()\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fOdometry\nRepresents an estimate of a robot position in free space\nA Kobuki robot has three main information related to position\nand orientation:\nposition.x\n\norientation.z\n\nposition.y\n\nKobuki\n\nInformation from an odometry allows us to move the robot with\nbetter precision\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fOdometry\nType: nav_mags\/Odometry.msg\nThe component that we will focus on in Odometry message is\npose\nThe pose component contains two components and one of them\nis named pose (again)\nThe inner pose component consists of two components:\nposition (in meter) consists of 3 components of type float\nnamed x, y, and z\norientation (in quaternion) consists of 4 components of type\nfloat named x, y, z, and w\n\nFor a Kobuki robot:\npose.pose.position.z is always 0\npose.pose.orientation.x is always 0\npose.pose.orientation.y is always 0\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fOdometry\nNote that the orientation from Kobuki’s odometry is in\nquaternion\nQuaternion is a unit suitable for rotating objects in 3D\nA quaternion value consists of four values, x, y, z, and w\nWe generally familiar with roll, pitch, and yaw\nSince a Kobuki can only turn about the Z-axis, only yaw is\napplicable\nWe can calculate yaw (in radian) using the following formula:\nyaw = arcsin(2 ∗ 𝑥 ∗ 𝑦 + 2 ∗ 𝑧 ∗ 𝑤)\nBecause 𝑥 and 𝑦 orientations of a Kobuki robot are always 0,\nyaw = arcsin(2 ∗ 𝑧 ∗ 𝑤)\nTo convert radian to degree simply multiply by\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n180\n𝜋\n\n\fReset Kobuki’s Odometry\nOften time, we may want to reset the odometry to 0\nThis can be done by publishing an Empty message to\n\/mobile_base\/commands\/reset_odometry\n\nYou must make sure that your program already establish a\nconnection with the robot first\nExample:\n#!\/usr\/bin\/env python\nimport rospy\nfrom std_msgs.msg import Empty\ndef resetter():\npub = rospy.Publisher('\/mobile_base\/commands\/reset_odometry',\nEmpty, queue_size=10)\nrospy.init_node('resetter', anonymous=True)\nwhile pub.get_num_connections() == 0:\npass\npub.publish(Empty())\nif __name__ == '__main__':\nresetter()\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fOdometry\nTopic: \/odom\nPurpose: Information about robot’s where about\nType: nav_msgs\/Odometry.msg\nIf you create a function named odomCallback that will be\ncalled every the program receive message from the topic \/odom\nas follows:\ndef odomCallback(data):\n\ndata.pose.pose.position.x is the position of the robot\nfrom the origin along the x-axis\ndata.pose.pose.position.y is the position of the robot\nfrom the origin along the y-axis\ndata.pose.pose.orientation.z together with\ndata.pose.pose.orientation.w is the direction where\nthe robot is facing (quaternion).\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\fodom\n#!\/usr\/bin\/env python\nimport rospy\nimport math\nfrom nav_msgs.msg import Odometry\nfrom tf.transformations import euler_from_quaternion\ndef odomCallback(data):\n# Convert quaternion to degree\nq = [data.pose.pose.orientation.x,\ndata.pose.pose.orientation.y,\ndata.pose.pose.orientation.z,\ndata.pose.pose.orientation.w]\nroll, pitch, yaw = euler_from_quaternion(q)\n# roll, pitch, and yaw are in radian\ndegree = yaw * 180 \/ math.pi\nx = data.pose.pose.position.x\ny = data.pose.pose.position.y\nmsg = \"(%.6f,%.6f) at %.6f degree.\" % (x, y, degree)\nrospy.loginfo(msg)\ndef odomExample():\nrospy.init_node('odom_example', anonymous=True)\nrospy.Subscriber('\/odom', Odometry, odomCallback)\nrospy.spin()\nif __name__ == '__main__':\nodomExample()\n\nThumrongsak Kosiyatrakul\n\nKobuki Node\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":201,"segment": "unlabeled", "course": "cs1550", "lec": "lec03", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Homework 1 is due next Monday at 11:59 pm\n\n• Lab 1 is due on 2\/4 at 11:59 pm\n• Distributed using GitHub Classroom\n• Explained in this week’s recitations\n\n• TA Office hours available on the syllabus page\n\n• Project 1 will be up on Canvas this Friday\n• Docker images are available on Canvas\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fMuddiest Points\n• the while(1) loop in the spinlocks example (just a placeholder I\nassume?)\n• It would be helpful to have a step-by-step written out\nrepresentation of Context Switching in Xv6 to accompany the\ngraphic\n• Why only certain registers were pushed\/popped in the context\nswitches. Is it just because these are the only registers the\nprocesses could be using?\n• How can hardware be directly accessed by user tasks, like the\nfirst two options in the spin lock slide? My thought was that\nsomething like an atomic swap would need hard wiring, not\nsoftware\n• Are race condition prevention methods implemented by the\nprogrammer or the operating system?\n• What exactly is a critical region again?\n• Are context switching done by the kernel?\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fAgenda\n• Busy Waiting Problem\n\n• Why does it happen?\n• What are its implications?\n• How to solve it?\n• Sempahores\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fToday’s problem: Busy Waiting\n• A process that is trying to acquire a locked spinlock is\nrunning!\n• It continuously checks:\n• can I get the lock? No, lock is held by another process\n• can I get the lock? No, lock is held by another process\n• …\n\n• This continuous check is called spinning or busy waiting\n• But what is wrong with that?\n• Busy waiting wastes CPU cycles\n• on a single-core system it delays the process that is holding the\nlock from releasing it\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fToday’s problem: Busy Waiting\nWhile P1 is in the critical region, P2 is busy waiting\nShared Data\nSpinlock lk;\nint x;\nProcess P1\nlock(lk);\n\nProcess P2\nlock(lk);\n\n\/\/critical region (e.g., x++) \/\/critical region (e.g., x++)\nunlock(lk);\n\nunlock(lk);\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fBut why?\n\nWhy does busy waiting happen with spinlocks?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fAtomic TestAndSet\n• TestAndSet is an\natomic instruction\n\nint TestAndSet(int &x){\n\n• Works for singlecore and multicore Symmetric\nMulti-Processing\n(SMP)\n\nlock memory access to x\nint temp = *x;\n\n*x = 1;\nunlock memory access to x\nreturn temp;\n\n}\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fSpinlock implementation using TestAndSet\n•\n\nSingle shared variable: lock\n\n•\n\nWorks for any number of processes\n\nint lock = 0;\nLock(){\nwhile (TestAndSet(&lock))\n;\n}\nUnlock(){\nlock = 0;\n}\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fAtomic Swap\n• Swap is an atomic\ninstruction\n\nint Swap(int &x, int y){\n\n• Works for singlecore and multicore Symmetric\nMulti-Processing\n(SMP)\n\nlock memory access to x\nint temp = *x;\n\n*x = y;\nunlock memory access to x\nreturn temp;\n\n}\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fSpinlock implementation using Swap\n•\n\nSingle shared variable: lock\n\n•\n\nWorks for any number of processes\n\nint lock = 0;\nLock(){\nwhile (Swap(&lock, 1))\n;\n}\nUnlock(){\nlock = 0;\n}\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fImplication of Busy Waiting\nSubproblem: priority inversion (higher priority process busy\nwaits for lower priority process)\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fImplementation Detail\ncompiler and\/or hardware may reorder instructions\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fXv6 Walkthrough\n• Spinlocks\n• __sync_synchronize() is a memory barrier instruction\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\f•\n\n•\n\nSemaphores\nSolution: use semaphores\n•\n\nSynchronization mechanism that doesn’t require busy waiting\n\nImplementation\n•\n\nSemaphore S accessed by two atomic operations\n•\n\nDown(S): decrement the semaphore if > 0; block otherwise\n\n•\n\nUp(S): increment the semaphore and wakeup one blocked process if any\n\n•\n\nDown() is another name for P()\n\n•\n\nUp() is another name for V()\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fBusy waiting vs. Blocking\n\nBlocking involves 2 context switches\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fCritical sections using semaphores\nShared variables\nSemaphore sem(1);\n\nCode for process Pi\n\nwhile (1) {\n\/\/ non-critical section\ndown(sem);\n\/\/ critical section\nup(sem);\n\/\/ non-critical section\n}\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fTypes of semaphores\n• Two different types of semaphores\n• Counting semaphores\n• Binary semaphores\n\n• Counting semaphore\n• Value can range over an unrestricted range\n\n• Binary semaphore\n• Only two values possible\n• 1 means the semaphore is available\n• 0 means a process has acquired the semaphore\n\n• May be simpler to implement\n\n• Possible to implement one type using the other\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fSemaphore Implementation\n\nBut how do semaphores avoid busy waiting?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\f•\n\n•\n\nImplementing semaphores with blocking\nAssume two operations:\n•\n\nSleep(): suspends current\nprocess\n\n•\n\nWakeup(P): allows process P\nto resume execution\n\nSemaphore data structure\n•\n\nTracks value of semaphore\n\n•\n\nKeeps a list of processes\nwaiting for the semaphore\nstruct Semaphore {\nint value;\nProcessList pl;\n};\n\ndown ()\n{\nvalue -= 1;\nif (value < 0) {\n\/\/ add this process to pl\nSleep ();\n}\n}\nup () {\nProcess P;\nvalue += 1;\nif (value <= 0) {\n\/\/ remove a process P\n\/\/ from pl\nWakeup (P);\n}\n}\n\nHow to protect these shared variables??\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\fSpinlocks in Semaphores\ndown ()\n{\nvalue -= 1;\nif (value < 0) {\n\/\/ add this process to pl\nSleep ();\n}\nstruct Semaphore {\nint value;\nProcessList pl;\n\n};\n\n}\nup () {\nProcess P;\nvalue += 1;\nif (value <= 0) {\n\/\/ remove a process P\n\/\/ from pl\nWakeup (P);\n}\n}\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fSpinlocks are sometimes better than Semaphores\n• Very (very) short waiting time to enter the critical\nsection < the 2 context switches needed for blocking\n• Multi-core\n• so that the spinlock can be unlocked while the process is busy\nwaiting\n\n• Few contending processes for the critical section\n\n• Short critical section code\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":202,"segment": "unlabeled", "course": "cs1550", "lec": "lec22", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Homework 10: due on 4\/11\n• Project 3: due on 4\/11\n• Lab 4: due on 4\/15\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\n• Indexed Allocation\n\n• Directory contents\n• Free block tracking\n• bitmap\n• linked list of free blocks\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (1\/3)\n• Bitmapping for free blocks\n• It seems that directories have a limit to the number of\nfiles \/ subdirectories they contain based on the size of a\ndisk block. Is \/ how is this limit overcome?\n• Everything\n• are the index nodes for the directories stored on the\nheap? is this different than the index nodes from the file\nallocation table?\n• Variable sized file name structures’ pros and cons\n• what exactly is an i-node and where is it stored?\n\n• Might need some recap on inode. So inode contains\ninformation for a specific file and block index is a block\nstoring only the addresses of all the file block?\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (2\/3)\n• a little confused about Fast File System indexing\n• For FFS, all the files will be kept in one directoriy (or\nmany directories?) and then we adopt the direct pointers\nor one-level or multi-level pointers to point to their data\nbased on their size?\n• Typically one directory per index block? What about a\nsubdirectory in a parent directory? Would there be\nanother block index (for the subdirectory) stored in the\nparent directory?\n• Can you explain why smaller page size leads to more\npage fault?\n\n• Linked allocation versus contiguous and when is it\noptimal to use linked\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (3\/3)\n• how do we do midterm corrections\n\n• Top hat question about partition\n• Still a but confused on indexing for the Page Table\nAllocation\n• structure of a disk section\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fProblem of the Day – Part 1\n• How does a file system handle errors?\n• Answer: Defense in Depth\n• multiple layers of error detection\/correction\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fSoftware Stack\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fDevice drivers\n• Device drivers go between\ndevice controllers and rest of\nOS\n\nUser\nspace\n\n• Drivers standardize interface\nto widely varied devices\n\nKernel\nspace\n\n• Device drivers communicate\nwith controllers over bus\n• Controllers communicate with\ndevices themselves\n\nUser\nprogram\n\nRest of the OS\n\nKeyboard\ndriver\n\nDisk\ndriver\n\nKeyboard\ncontroller\n\nDisk\ncontroller\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fLayers of I\/O software\n\nUser-level I\/O software & libraries\nDevice-independent OS software\nDevice drivers\nInterrupt handlers\n\nUser\nOperating\nsystem\n(kernel)\n\nHardware\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fAnatomy of an I\/O request\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fDevice Driver goals\n• Device independence\n•\n•\n\nPrograms can access any I\/O device\nNo need to specify device in advance\n\n• Uniform naming\n•\n•\n\nName of a file or device is a string or an integer\nDoesn’t depend on the machine (underlying hardware)\n\n• Error handling\n•\n•\n\nDone as close to the hardware as possible\nIsolate from higher-level software\n\n• Synchronous vs. asynchronous transfers\n•\n\nBlocked transfers vs. interrupt-driven\n\n• Buffering\n•\n\nData coming off a device cannot be stored in final destination\n\n• Sharable vs. dedicated devices\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fDisk drive structure\n• Data stored on surfaces\n•\n\nUp to two surfaces per platter\n\n•\n\nOne or more platters per disk\n\nhead\n\nsector\n\n• Data in concentric tracks\n•\n•\n\nTracks broken into sectors\n\nplatter\n\n• 256B-1KB per sector\n\ntrack\n\nCylinder: corresponding tracks\non all surfaces\n\ncylinder\n\n• Data read and written by\nheads\n•\n•\n\nActuator moves heads\n\nsurfaces\n\nspindle\n\nHeads move in unison\nCS 1550 – Operating Systems – Sherif Khattab\n\nactuator\n13\n\n\fDisks, cylinders, cylinder groups\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fDisk Sector\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fWhat’s in a disk request?\n• Time required to read or write a disk block\ndetermined by 3 factors\n• Seek time\n\n• Rotational delay\n• Average delay = 1\/2 rotation time\n• Example: rotate in 10ms, average rotation delay = 5ms\n\n• Actual transfer time\n• Transfer time = time to rotate over sector\n• Example: rotate in 10ms, 200 sectors\/track => 10\/200 ms =\n0.05ms transfer time per sector\n\n• Seek time dominates, with rotation time close\n\n• Error checking is done by controllers\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fDisk drive specifics\nIBM 360KB floppy\n\nWD 18GB HD\n\nCylinders\n\n40\n\n10601\n\nTracks per cylinder\n\n2\n\n12\n\nSectors per track\n\n9\n\n281 (average)\n\nSectors per disk\n\n720\n\n35742000\n\nBytes per sector\n\n512\n\n512\n\nCapacity\n\n360 KB\n\n18.3 GB\n\nSeek time (minimum)\n\n6 ms\n\n0.8 ms\n\nSeek time (average)\n\n77 ms\n\n6.9 ms\n\nRotation time\n\n200 ms\n\n8.33 ms\n\nSpinup time\n\n250 ms\n\n20 sec\n\nSector transfer time\n\n22 ms\n\n17 msec\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fStructure of a disk sector\n• Preamble contains information about the sector\n• Sector number & location information\n\n• Data is usually 256, 512, or 1024 bytes\n• ECC (Error Correcting Code) is used to detect &\ncorrect minor errors in the data\nPreamble\n\nData\n\nCS 1550 – Operating Systems – Sherif Khattab\n\nECC\n\n18\n\n\fParity Examples\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\f1-d parity\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\f2-d parity\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fWhen good disks go bad…\nDisks have defects\nIn 3M+ sectors, this isn’t surprising!\n\nECC helps with errors, but sometimes this isn’t enough\n\nDisks keep spare sectors (normally unused) and remap bad\nsectors into these spares\nIf there’s time, the whole track could be reordered…\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fRAIDs, RAIDs, and more RAIDs\nstrip\n\nstrip\n\nStripe\nRAID 0\n(Redundant Array of Inexpensive Disks\n\nRAID 1\n(Mirrored copies)\n\nRAID 4\n(Striped with parity)\n\nRAID 5\n(Parity rotates through disks)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":203,"segment": "unlabeled", "course": "cs1622", "lec": "lec06","text":"Bottom-up Parsing\nCS\/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n● today we're finishing parsing!! yaaay\no so after today you’ll have everything you need for project 2!!!!!!\n● bottom-up parsing is (imo) best used for parsing expressions, so we\nshould start by talking about…\n\n2\n\n\fOperators\n\n3\n\n\fEvaluating (and producing) trees\n● we saw before that given an AST, we could evaluate it:\n*\n\n5\n\nI think you would agree that the order of operations\nhere is to do the addition, then the multiplication.\n\n+\n2\n\n3\n\nbut we're talking about parsing now. what source\ncode would I write to produce this AST?\n\n5 * (2 + 3)\n\n+\n\nwhy did we have to put parentheses there?\nbecause of the order of operations:\nwithout them, 5 * 2 + 3 would parse as:\n\n*\n\n5\n\n3\n\n2\n4\n\n\fPEMDAS, BODMAS, BIDMAS…\n● officially, the order of operations used by mathematicians is:\n1. parentheses\/brackets\n2. exponentiation\/roots\n3. multiplication\/division\n4. addition\/subtraction\n● the order of the letters in the acronym (MD vs. DM) does not say\nthat e.g. multiplication happens before division.\n● when writing math horizontally like 27 \/ 3 \/ 9, there's a problem:\no even among mathematicians, there is no consensus on what\norder horizontal division should be done!\n● in programming languages, all math is written horizontally…\no and you have e.g. %, &&, ||, &, |, ^, !, ~, <, >, <=, >=, ==, !=, …\n● so we need a more rigorous way of dealing with this.\n5\n\n\fOperator Precedence\n● here's a more formalized, algorithmically-checkable method:\no you rank the operators by giving each a number, its precedence\no that precedence decides the order of evaluation\n● another way of thinking of it is: if you wrote the expression without\nparentheses, the precedence tells you where they should go:\nOperator Prec\n\na + b * c\n\na + (b * c)\n\n**\n\n1st\n\na ** b * c\n\n(a ** b) * c\n\n* \/ %\n\n2nd\n\n+ -\n\n3rd\n\nbut this still isn't enough information to\ndisambiguate when operators of the same\nprecedence appear next to each other:\n\n27 \/ 3 \/ 9\n\n(27 \/ 3) \/ 9 or\n27 \/ (3 \/ 9) ?\n6\n\n\fOperator Associativity\n● in a precedence tie, an operator's associativity breaks the tie.\nleft-associative operators\n(e.g. +, -, *, \/, %) evaluate\nthings left-to-right.\n\nand right-associative\noperators (e.g. **, =) evaluate\nthings right-to-left.\n\n27 \/ 3 \/ 9\n\n2 ** 3 ** 4\n\n(27 \/ 3) \/ 9\n\n2 ** (3 ** 4)\n\nin other words, associativity says which way the tree branches.\n\/\n\n\/\n27\n\n**\n\n9\n3\n\n2\n\n**\n\n3\n\n4\n7\n\n\fMore about associativity\n● to be clear: associativity only applies when you have adjacent\noperators of the same precedence.\no a ** b + c parses as (a ** b) + c, because exponentiation's\nprecedence ranks higher than addition's.\n● it's also possible for operators to be non-associative.\no that means it's an error for them to be adjacent.\n● for example, what does x < y < z mean?\no in Java, you get a type error.\no in Python, it's parsed as (x < y) && (y < z).\no in the language we're designing, we could say that comparisons are\nnon-associative, and block this potentially-confusing code as a\nparsing error instead.\n\n8\n\n\fUnary and postfix operators\n● operators that take two operands are called binary. but many\n(especially in C-style languages) are unary and operate on one value.\no -x, ~x, !x, &x, *x, ++x, --x, (int)x\no these are given a precedence higher than any binary operator.\n▪ this matches our intuition: -3 + 5 should give 2, not -8.\no these are all right-associative, but it doesn't come up much.\n▪ -~x would bitwise-complement x first, then negate that.\n● finally there are postfix operators which come after their operand.\no x++, x--, a[i], obj.field, f(x)\no and these have the highest precedence of all!\n▪ -f(x) calls the function first, then negates the return value.\no these are all left-associative, and that's pretty common to see.\n▪ f(x)[i].field calls f first, then indexes the returned array, then\naccesses the field from the array value.\n9\n\n\fSummarizing operators\n● a language's specification will typically have all the operators, their\nprecedences, and their associativities listed in a table:\nOperator\n\nPrec Assoc\n\nf() a[i] o.f\n\n1\n\nL\n\n!x -x *x &x\n++x --x\n\n2\n\nR\n\n* \/ %\n\n3\n\nL\n\n+ -\n\n4\n\nL\n\n< <= > >=\n\n5\n\nL\n\n== !=\n\n6\n\nL\n\n&&\n\n7\n\nL\n\n||\n\n8\n\nL\n\n=\n\n9\n\nR\n\nthis is a pretty typical arrangement for a\nC-style language (excluding bitwise operators, casts,\nreflexive assignments, and some other obscure operators)\n\nbut where are parentheses?\n\nwell, they're not operators. parentheses\nonly control the order of evaluation and\nthe way the AST is built; they don't do\nanything otherwise.\n10\n\n\fBuilding a grammar\nfor expressions\n\n11\n\n\fA grammar from the bottom up\n● rather than starting with the \"overall\" structure of an expression…\no let's start with the most basic parts and gradually combine them.\n● a primary expression is the most basic kind.\nliterals fall into this category.\n\n318\n\nidentifiers are another kind:\nthey name single things.\n\n\"hello\"\n9.5\n\n'x'\n\nx\n\nmain\n\nSystem\n\nlast, we'll include parenthesized expressions, because they\ncontain whole sub-expressions. so, our grammar is:\nPrimaryExp: IdExp | IntLitExp | ParenExp\nIdExp:\n<identifier token>\nIntLitExp: <int literal token>\n(Exp is the top-level expression\nrule that we're building up to)\nParenExp:\n'(' Exp ')'\n\n12\n\n\fTerms: primaries with decorations\n● since unary and postfix operators are so closely associated with their\noperands, we can define a Term rule like this:\nTerm: UnaryOp* PrimaryExp PostfixOp*\n● that is, 0 or more unary operators before a primary \"core,\" followed\nby 0 or more postfix operators.\n● UnaryOp and PostfixOp might be defined like:\nUnaryOp:\n'-' | '!'\nPostfixOp: CallOp | FieldOp | IndexOp\nCallOp:\n'(' ')' | '(' Exp (',' Exp)* ')'\nFieldOp:\n'.' <identifier token>\nIndexOp:\n'[' Exp ']'\n● example productions of this grammar are x, -x, f(), !f(x).z, !!x\n\n13\n\n\fExpressions: the big picture\n● really, every expression looks like this:\nterm op term op term op term op term\nwhere each op is a binary operator.\nso, we'll define these rules:\nExp:\nTerm (BinaryOp Term)*\nBinaryOp: '+'|'-'|'*'|'\/'|'%'|…etc.\n\nbut what about the associativity and precedence??\nwell, we just… uh… won't encode that into the grammar.\n\n14\n\n\fWait. What?\n● it is absolutely possible to encode precedence and associativity into\nthe expression grammar, and then write a fully recursive-descent\nparser for expressions. (I know, because I've done it.)\n● but the result is just… well. here's a simple mathematical language:\nExp: Add\nAdd: Mul (('+'|'-') Mul)*\nMul: Neg (('*'|'\/'|'%') Neg)*\nNeg: Pow | ('-' Neg)\nPow: Pri ('**' Pow)*\nPri: Id | IntLit | '(' Exp ')'\n\nis it obvious that this accurately\ncaptures all expressions?\ncan you identify the\nassociativity and precedence\nof these operators?\n\nwhat if, like C or C++, we had 15 precedence levels and\ndozens of operators? how readable would that be?\n15\n\n\fSo no! We won't do it!\n● because we're defining our language's grammar a bit loosely, it\ndoesn't really matter if our grammar is 100% accurate.\no instead, the ambiguities in the grammar are resolved by the table\nof precedence and associativity.\n● any complex idea can be expressed in multiple different ways.\no unless you have some kind of externally-imposed reason to stick\nwith one way of writing something…\no it makes sense to use different ways according to their strengths.\n● so…\no how do we write an algorithm to parse this stuff??\n\n16\n\n\fBottom-up parsing\n\n17\n\n\fThe core of the algorithm\n● let's assume we only have left-associative binary operators.\n● so, to parse Term (BinaryOp Term)*, we can write something like:\nlet mut LHS = parse_term();\nwhile current token is a binary operator {\nlet op = current token;\nmove to next token;\nlet RHS = parse_term();\nLHS = AstNode::new(LHS, op, RHS);\n}\nreturn LHS;\n● it might seem strange to replace the contents of the LHS variable\ninside the loop, but this is how it builds up the tree.\n\n18\n\n\fOkay, let's try it (animated)\n● given this sequence of tokens, let's see what happens:\n\nx + y - z <eof>\nToken Action\n\nx\n+\ny\nz\n<eof>\n\nLHS = parse term\n\nLHS\n\n+\n\nstop looping!\n\ny\n\n(x + y)\n\nop = -, loop!\nRHS = parse term\nLHS = (LHS op RHS)\n\nRHS\n\nx\n\nop = +, loop!\nRHS = parse term\nLHS = (LHS op RHS)\n\nop\n\n-\n\nz\n((x + y) - z)\n19\n\n\fThat's why it's called bottom-up (animated)\n● this algorithm produces a tree that branches to the left, like this.\n\na + b + c + d + e\nbut if we allow all the operators, what\nwould be a problem input for this?\n\n+\n+\n\n+\n+\na\n\nx + y * z\n\ne\n\n*\n\nd\n+\n\nc\nb\n\nand it builds it…\nbottom up!\n\nx\n\nz\ny\n\nit was a little too\neager to produce\n(x + y), when it\ndidn't know that\nthere was a *\ncoming up.\n20\n\n\fAccounting for precedence\n● we have to tweak our algorithm a bit.\nlet mut LHS = parse_term();\nwhile current token is a binary operator {\nlet op = current token;\nmove to next token;\nat this point, we don't know if RHS is\nlet RHS = parse_term();\nour second operand, or if it's the next\n\noperator's first operand.\n\nso, we'll have to check if the current token is a binary operator\nwhose precedence is higher than op's.\nand if that's the case, we um. uh.\n\nwait, what do we do then?\n\nLHS = AstNode::new(LHS, op, RHS);\n}\n\n21\n\n\fImplicit parentheses\n● if you think about what we're doing in a different way…\n\na +(b * c * d * e)+ f\nwe're sort of inserting parentheses as we go.\nso when we see the first *, we should say \"oh, the left\nparenthesis goes before b…\n\n\"…and the right parenthesis goes before the first operator that has\na lower precedence than multiplication.\"*\nsince we want to put the parsing of + on hold, the most\nnatural way to parse the multiplications is to recurse.\n\n(let's look at the example to see how this is done.)\n22\n\n\fHow that parses (animated)\n● Looking at this input again, let's see how the AST is built for it:\n+\n\na + b * c * d * e + f\n1. a is parsed.\n\nf\n\n+\n\n2. b is parsed.\n3. * is seen, causing us to recurse. + is put \"on hold.\"\n\n*\n\n4. the subtree of multiplications is parsed.\n\n5. that subtree is returned and becomes +'s RHS.\n6. the final + f is parsed.\nRight-branching is done by the\nRecursion;\nLeft-branching is done by the\nLoop.\n\n*\n\n*\na\n\nb\n\ne\n\nd\nc\n23\n\n\fFinishing up by parsing terms\n● finally, these are more straightforward.\nTerm: UnaryOp* PrimaryExp PostfixOp*\n● since the unary operators are Right-associative…\no you got it: we have to Recurse to build the AST for those.\n● after that, primary expressions are straightforward.\no parenthesized expressions will work just fine, because ')' is not an\noperator, and will cause the operator parsing loop to terminate.\n● and finally, postfix expressions come after the primaries.\no the primary has to be passed to the postfix parser, so that it can\nbecome the LHS of those postfix operators.\no postfix operators are Left-associative, so we can use a Loop.\no also, it's not an error to see 0 postfix operators. they’re optional!\n\n24\n\n\fWhat about right-associative binary operators?\n● right-associativity is another kind of right-branching.\no so, we would have to change the recursion condition in the binary\noperator parsing.\n● currently we recurse when we see an operator of higher precedence.\no we will also recurse when we see a right-associative operator of\nthe same or higher precedence.\n● this is so if you have multiple right-associative operators in a row of\nthe same precedence, you still get the right-branching structure.\n● in a ** b ** c…\no we see that the second ** has the same precedence as the first\no so we recurse, giving us an RHS of b ** c\no ultimately giving us a ** (b ** c)\n\n25\n\n\fThings to think about\n\n26\n\n\fBottom-up parsing without recursion\n● by using auxiliary stack data structures, we could remove the need for\nrecursion entirely.\n● did any of you do the expression evaluation assignment in 445?\no where you had an operator stack and an operand stack?\n● if you still have that code, try going back and looking at it.\n● what if the operand stack held AST nodes, instead of Doubles?\no and when you handle names\/operands, you pushed AST nodes?\n● and in the parts where you pop operators and evaluate…\no what if, instead of evaluating, you created\/pushed AST nodes?\n● that's it, that's a parser, you already made most of an expression parser\nand you didn't even know it\n\n27\n\n\fCan we parse everything with bottom-up parsing?\n● Yes!\n● actually, generalized bottom-up parsing is more powerful than topdown parsing!\no there are some grammars that top-down parsers can never parse.\n● however…\no once you move beyond expressions it gets stupidly complex\no the complexity is not really justified for programming languages\n▪ their grammars are typically not that complicated\no so it's mostly out of the scope of this course, IMO\n● for our language, we can parse everything using a hybrid approach:\no use recursive descent for the program's broad structure\no use bottom-up for expressions\n\n28\n\n\fHow does that work though?\n● well, expressions only appear inside other pieces of code.\n● so, a grammar might look like this:\nProgram:\nFunction*\nFunction:\n'fn' Id '(' ')' Block\nBlock:\n'{' Stmt* '}'\nStmt:\nIfStmt | ExpStmt | AssignStmt\nIfStmt:\n'if' Exp Block ('else' Block)?\nExpStmt:\nExp ';'\nAssignStmt: Exp '=' Exp ';'\n● all of these rules would be parsed with recursive descent…\n● and whenever we need to parse an Exp, we use bottom-up parsing.\n\n29\n\n\fSyntactic Sugar\n\n30\n\n\fYummy\n● each language feature adds complexity in every stage of the\ncompiler: lexing, parsing, semantics, optimization, code generation…\n● so, one approach to adding features to our languages is by defining\nthem in terms of other, simpler features that already exist.\nlet lhs = self.parse_term()?;\nthis is syntactic sugar, and it's called that because\nit usually makes your code easier to write.\nlet lhs = match self.parse_term() {\nOk(x) => x,\nErr(e) => return Err(e),\n};\n31\n\n\fAnother example\n● Java's generic for loop is just sugar for using an iterator.\nfor(int i : someCollection) { … }\n\n● becomes something like:\nfor(Iterator<Integer> iter = someCollection.iterator();\niter.hasNext(); )\n{\nint i = iter.next().intValue();\n…\n}\n\n32\n\n\fHow is it implemented?\n● by desugaring: rewriting the AST after parsing.\no we saw previously that trees are easy to create and manipulate.\n● but we have to be a little careful.\no up next is semantic analysis, and if we desugar the code before\nthen, it might give really confusing errors about code that the\nprogrammer didn't write!\n● but that's it for parsing!\n\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":204,"segment": "unlabeled", "course": "cs1541", "lec": "lec3.2_cache_design","text":"Cache Design\nCS 1541\nWonsun Ahn\n\n\fOracle Cache\n● CPU Cycles = CPU Compute Cycles + Memory Stall Cycles\n\n● Oracle cache: a cache that never misses\no In effect, Memory Stall Cycles == 0\no Impossible, since even with infinite capacity, there are still cold misses\no But useful to set bounds on performance\n● Real caches may approach performance of oracle caches but can’t exceed\n\n● What metric can we use to compare and evaluate real cache designs?\no AMAT (Average Memory Access Time)\n\n2\n\n\fAMAT (Average Memory Access Time)\n● AMAT (Average Memory Access Time) is defined as follows:\no AMAT = hit time + (miss rate × miss penalty)\no Hit time: time to get the data from cache when we hit\no Miss rate: what percentage of cache accesses we miss\no Miss penalty: time to get the data from lower memory when we miss\no Shouldn’t it be hit rate × hit time?\n▪ Hit time is incurred regardless of hit or miss\n▪ It is more aptly called access time (the time to search for the data)\n\n● Hit time, miss rate, miss penalty are the 3 components of a cache design\no When evaluating a cache design, we need to consider all 3\no Cache designs trade-off one for the other\n▪ E.g. a large cache trade-offs longer hit time for smaller miss rate\n▪ Whether trade-off is beneficial depends on the resulting AMAT\n3\n\n\fCache Design Parameter 1:\nNumber of Levels\n\n4\n\n\fAMAT for Multi-level Caches\n● For a single-level cache (L1 cache):\no AMAT(L1) = L1 hit time + (L1 miss rate × DRAM access time)\nMiss! Hit!\n\nL1 Cache\n\nHit!\n\nDRAM Memory\n\nL1 hit time\nL1 miss rate × DRAM access time\n\n● For a multi-level cache (L1, L2 caches):\no AMAT(L2) = L1 hit time + (L1 miss rate × L1 miss penalty)\no L1 miss penalty = L2 hit time + (L2 miss rate × DRAM access time)\no AMAT(L2) = L1 hit time + L1 miss rate × L2 hit time\n+ L1 miss rate × L2 miss rate × DRAM access time\nMiss! Miss! Hit!\n\nL1 Cache\n\nL1 hit time\n\nMiss!\n\nL2 Cache\n\nL1 miss rate × L2 hit time\n\nHit!\n\nHit!\n\nDRAM Memory\n\nL1 miss rate × L2 miss rate × DRAM access time\n5\n\n\fAMAT for Multi-level Caches\n● For L2 Cache to be worth it, AMAT(L1) > AMAT(L2) needs to be true.\nL1 Cache\n\nDRAM Memory\n\nL1 hit time\nL1 miss rate × DRAM access time\n\n>?\nL1 Cache\n\nL1 hit time\n\nL2 Cache\n\nL1 miss rate × L2 hit time\n\nDRAM Memory\n\nL1 miss rate × L2 miss rate × DRAM access time\n\n● AMAT(L1) – AMAT(L2)\n= (L1 miss rate – L1 miss rate × L2 miss rate) × DRAM access time\n– L1 miss rate × L2 hit time\n= L1 miss rate × ((1 – L2 miss rate) × DRAM access time – L2 hit time) > 0\n→ (1 – L2 miss rate) × DRAM access time > L2 hit time\n→ Benefit from reduced DRAM accesses > Penalty from L2 accesses\n6\n\n\fAMAT for Multi-level Caches\n● (1 – L2 miss rate) × DRAM access time > L2 hit time\no Let’s assume L2 miss rate = 0.9 and DRAM access time = 100 cycles:\n(1 − 0.9) × 100 > L2 hit time\nL2 hit time < 10\no If L2 hit time can be kept below 10 cycles, worth it to install L2 cache\n● So, should we install the L2 cache, or not? That depends on the program!\no Locality in program determines cache capacity required for 0.9 miss rate\no If we can design a cache with hit time < 10 for that capacity, go for it\n● Again, shows design decisions are heavily impacted by needs of software\n\n7\n\n\fCache Design Parameter 2:\nCache Size\n\n8\n\n\fImpact of Cache Size (a.k.a. Capacity) on AMAT\n● AMAT = hit time + (miss rate × miss penalty)\n\n● Larger caches are good for miss rates\no More capacity means you can keep around cache blocks for longer\no Means you can leverage more of the pre-existing temporal locality\no If entire working set can fit into the cache, no capacity misses!\n● But larger caches are bad for hit times\no Longer wires and larger decoders mean longer access time\n● Exactly why there are multiple levels of caches\no Frequently accessed data where hit time is important stays in L1 cache\no Rarely accessed data which is part of a larger working set stays in L3\n\n9\n\n\fWhat cache size(s) should I choose?\n● How should each cache level be sized?\n\n● That depends on the application\no Working set sizes of the application at various levels. E.g.:\n▪ Small set of data accessed very frequently (typically stack variables)\n▪ Medium set of data accessed often (currently accessed data structure)\n▪ Large set of data accessed rarely (rest of program data)\no Ideally, cache levels and sizes would reflect working set sizes.\n● Simulate multiple cache levels and sizes and choose one with lowest AMAT\no Simulate on the applications that you care about\no In the end, it must be a compromise (giving best average AMAT)\n\n10\n\n\fCache Design Parameter 3:\nCache Block Size\n\n11\n\n\fImpact of Cache Block Size on AMAT\n● AMAT = hit time + (miss rate × miss penalty)\n\n● Cache block (a.k.a. cache line)\no Unit of transfer for cache data (typically 32 or 64 bytes)\no If program accesses any byte in cache block, entire block is brought in\no Each level of a multi-level cache can have a different cache block size\n● Impact of larger cache block size on miss rate\no Maybe smaller miss rate due to better leveraging of spatial locality\no Maybe bigger miss rate due to worse leveraging of temporal locality\n(Bringing in more data at a time may push out other useful data)\n● Impact of larger cache block size on miss penalty\no With a limited bus width, may take multiple transfers for a large block\no E.g. DDR 4 DRAM bus width is 8 bytes, so 8 transfers for 64-byte block\no Could lead to increase in miss penalty\n12\n\n\fCache Block Size and Miss Penalty\n● On a miss, the data must come from lower memory\n● Besides memory access time, there’s transfer time\n● What things impact how long that takes?\no The size of the cache block (words\/block)\no The width of the memory bus (words\/cycle)\no The speed of the memory bus (cycles\/second)\n● So the transfer time will be:\n\nseconds\n𝟏\nwords\n=\n×\ncycles words block\nblock\n×\nsecond cycle block size\nbus speed\n\nCache\n\nMemory\n\nbus width\n13\n\n\fWhat cache block size should I choose?\n● Again, that depends on the application\no How much spatial and temporal locality the application has\n● Simulate multiple cache block sizes and choose one with lowest AMAT\no Simulate on benchmarks that you care about and choose best average\no You may have to simulate different combinations for multi-level caches\n\n14\n\n\fCache Design Parameter 4:\nCache Associativity\n\n15\n\n\fMapping blocks from memory to caches\n● Cache size is much smaller compared to the entire memory space\no Must map all the blocks in memory to limited CPU cache\n● Does this sound familiar? Remember branch prediction?\no Had similar problem of mapping PCs to a limited BHT\no What did we do then?\n▪ We hashed PC to an entry in the BHT\n▪ On a hash conflict, we replaced old entry with more recent one\n\n● We will use a similar idea with caches\no Hash memory addresses to entries in cache\no On a conflict:\n▪ Replace old cache block with more recent one\n▪ Or, chain multiple cache blocks on to same hash entry\n16\n\n\fImpact of Cache Associativity on AMAT\n● Depending on hash function and chaining, a cache is either:\no Direct-mapped (no chaining allowed)\no Set-associative (some chaining allowed)\no Fully-associative (limitless chaining allowed)\n\n● Impact of more associativity on miss rate\no Smaller miss rate due to less misses due to hash conflicts\no Misses due to hash conflicts are called conflict misses\n▪ A third category of misses besides cold and capacity misses\n● Impact of more associativity on hit time\no Longer hit time due to need to search through long chain\n\n17\n\n\fDirect-mapped Caches\n\n18\n\n\fAssumptions\n● Let’s assume for the sake of concise explanations\no 8-bit memory addresses\no 4-byte (one word) cache block sizes\n● Of course these are not typical values. Typical values are:\no 32-bit or 64-bit memory addresses (32-bit or 64-bit CPU)\no 32-byte or 64-byte cache blocks sizes (for spatial locality)\no But too many bits in addresses are going to give you a headache\n● According to our assumption, here’s a breakdown of address bits\nUpper 6 bits: Offset of cache\nblock within main memory\n\nLower 2 bits: Byte offset\nwithin 4-byte cache block\n\no When I refer to addresses, I will sometimes omit the lower 2 bits\n(When we talk about cache block transfer, that part is irrelevant)\n19\n\n\fDirect-mapped Cache Hash Function\n● Each memory address maps to one cache block\n● No chaining allowed so no need to search\n● Implementing this is relatively simple\nHash function:\nFor this 8-entry cache, to\nfind cache block index,\ntake the lowest 3 cache\nblock offset bits in address.\n\nBut if our program\naccesses 001000, then\n000000, how do we tell\nthem apart?\nTags!\n\nCache\n000\n001\n010\n011\n\n100\n101\n110\n111\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n20\n\n\fTags help differentiate between conflicting blocks\nTag: part of address excluding cache block index\n● On allocation of 001000: tag = 001\nTag\n000\n001\n010\n011\n\n100\n101\n110\n111\n\n001\n\nCache\n\nData\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n21\n\n\fTags help differentiate between conflicting blocks\nTag: part of address excluding cache block index\n● On allocation of 001000: tag = 001\n● On allocation of 000000: tag = 000\nTag\n000\n001\n010\n011\n\n100\n101\n110\n111\n\n000\n\nCache\n\nData\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n22\n\n\fValid bit indicates that block contains valid data\nValid bit: indicates that the block is valid\n● Set to 0 initially when cache block is empty\n● Set to 1 when a cache block is allocated\nV\n\nTag\n\n000 1\n\n000\n\nCache\n\nData\n\n001 0\n010 0\n011 0\n\n100 0\n101 0\n110 0\n111 0\n\n● Cache hit: V == 1 &&\nCacheBlock.Tag == MemoryBlock.Tag\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n23\n\n\fQuiz: Address Bits Breakdown\n● Now with the following parameters:\no 8-bit memory addresses\no 4-byte cache block sizes\no 8-block cache\n● How would we breakdown the memory address bits?\nTag\n\nBlock index\n\nOffset within\ncache block\n\no First, the correct cache block is accessed using the block index\no Then, the tag is compared to the cache block tag\no If matched, offset is used to access specific byte within block\n\n24\n\n\fExample: A Direct-mapped Cache\n● When the program first starts, we set all the valid bits to 0.\no Signals all cache lines are empty\nV\nTag\nData\n● Now let's try a sequence of reads...\n000\n0\n1\n000 something\n010\ndo these hit or miss? How do the\n001\n0\ncache contents change?\n000000 miss\n100101 miss\n100100 miss\n100101 hit\n010000 miss\n000000 miss\n\nCold misses\n\nCold miss\nCapacity miss?\n\n010\n\n0\n\n011\n\n0\n\n100\n\n0\n1\n\n100\n\nsomething\n\n101\n\n1\n0\n\n100\n\nsomething\n\n110\n\n0\n\n111\n\n0\n\n25\n\n\fConflict Misses\n● What should we call 2nd miss on 000000?\no Awkward to call it a capacity miss\n(It’s not like capacity was lacking)\n000\no Let’s call it a conflict miss\n\nV\n\nTag\n\nData\n\n0\n1\n\n000\n010\n\nsomething\n\n001\n\n0\n\n010\n\n0\n\n011\n\n0\n\n100\n\n0\n1\n\n100\n\nsomething\n\n101\n\n1\n0\n\n100\n\nsomething\n\n110\n\n0\n\n111\n\n0\n\n000000 miss\n100101 miss\n100100 miss\n100101 hit\n010000 miss\n000000 miss\n\nCold misses\n\nCold miss\nCapacitymiss!\nConflict\nmiss?\n\n26\n\n\fTypes of Cache Misses (Revised)\n● Besides cold misses and capacity misses, there are conflict misses\n\n● Cold miss (a.k.a. compulsory miss)\no Miss suffered when data is accessed for the first time by program\n● Capacity miss\no Miss on a repeat access suffered due to a lack of capacity\no When the program's working set is larger than can fit in the cache\n● Conflict miss\no Miss on a repeat access suffered due to a lack of associativity\no Associativity: degree of freedom in associating cache block with an index\no Direct mapped caches have no associativity\n▪ Since cache blocks are directly mapped to a particular block index\n27\n\n\fAssociative caches\n\n28\n\n\fFlexible block placement\n● Direct-mapped caches can have lots of conflicts\no Multiple memory locations \"fight\" for the same cache line\n● Suppose we had a 4-block direct-mapped cache\nV Tag Data\no As before, 4-byte per cache block\n00\n0 0000\n1\n0001\n0011\n0010\no Memory addresses are 8 bits.\n01\n0\n● The following locations are accessed in a loop:\n10\n0\no 0, 16, 32, 48, 0, 16, 32, 48...\n11\n0\no or 000000, 000100, 001000, 001100, …\n● What would happen?\no They will all land on the same block index, and all conflict miss!\no Those other 3 blocks are not even getting used!\no What if we used the space to chain conflicting blocks?\n\n29\n\n\fFull associativity\n● Let's make our 4-block cache 4-way set-associative.\nV\n\nTag\n\n1\n0 000000\n\nD\n\nV\n\nTag\n\n*0\n\n1 001100\n0\n\nD\n\nV\n\nTag\n\n*48\n\n1\n0 000100\n\nD\n\nV\n\nTag\n\n*16\n\n0\n1 001000\n\nD\n*32\n\n● What's the difference?\no Now a hashed location can be associated with any of the 4 blocks\no Analogous to having a hash conflict chain 4-entries long\no The 4 cache blocks are said to be part of a cache set\no When set size == cache size, it is said to be fully associative\n● Let's do that sequence of reads again: 0, 16, 32, 48, 0, 16, 32, 48...\n● Notice tag is now bigger, since there are no block index bits\no Or set index bits in this context (just one set, so none needed)\n● Now cache holds the entire working set: no more misses!\n30\n\n\fExample: A 2-way Set-Associative Cache\n● 16-block 2-way set-associative cache\n● Let’s try the same stream of accesses as direct-mapped cache\n● Yay! 2nd access to 000000 is no longer a conflict miss!\n\n000000 miss\n100101 miss\n100100 miss\n100101 hit\n010000 miss\n000000 hit\n\nSet\n\nV\n\nTag\n\nData\n\nV\n\nTag\n\nData\n\n000\n\n0\n1\n\n000\n\nsomething\n\n0\n1\n\n010\n\nsomething\n\n001\n\n0\n\n0\n\n010\n\n0\n\n0\n\n011\n\n0\n\n0\n\n100\n\n0\n1\n\n100\n\nsomething\n\n0\n\n101\n\n1\n0\n\n100\n\nsomething\n\n0\n\n110\n\n0\n\n0\n\n111\n\n0\n\n0\n31\n\n\fAddress Bits Breakdown\n● A fully associative cache (doesn’t matter how many blocks):\nTag\n\n● With 16-block 4-way set-associative cache:\nTag\n\nSet index\n\nOffset within\ncache block\n\nOffset\n\no 16 \/ 4 = 4 sets in cache. So, 2 bits required for set index.\n● With 64-block 8-way set-associative cache:\nTag\n\nSet index\n\nOffset\n\no 64 \/ 8 = 8 sets in cache. So, 3 bits required for set index.\n32\n\n\fWant More Examples?\n● Try out the Cache Visualizer on the course github:\no https:\/\/github.com\/wonsunahn\/CS1541_Spring2022\/tree\/main\/re\nsources\/cache_demo\no Courtesy of Jarrett Billingsley\n\n● Visualizes cache organization for various parameters\no Cache block size\no Number of blocks in cache (capacity)\no Cache associativity\n\n33\n\n\fAssociativity is Costly\n● Associativity requires complex circuitry and may increase hit time\n● Full associativity is only used for very small caches\no And where a cache miss is extremely costly\n● Usually caches are 2-, 4-, or maybe 8- way set-associative\nBottom bit selects set (row)\n\nV\n\nTag\n\nD\n\nV\n\n1\n\n10010\n\n-2\n\n0\n\nV\n\nTag\n\nD\n\n1\n\n01010\n\n64\n\n000110?\n\n00011\n\nAddress\n\nRemaining tag bits\nShare comparators across all rows\n\nTag\n\nD\n\nV\n\nTag\n\nD\n\n1\n\n11111\n\n9999\n\n=\n\n=\nOR\n34\n\n\fAccess\/cycle time as a function of associativity\n\nThoziyoor, Shyamkumar & Muralimanohar,\nNaveen & Ahn, Jung Ho & Jouppi, Norman.\n(2008). CACTI 5.1.\n\n35\n\n\fCache Design Parameter 5:\nCache Replacement Policy\n\n36\n\n\fCache Replacement\n● If we have a cache miss and no empty blocks, what then?\nV\n\nTag\n\n1\n0 000000\n\nD\n\nV\n\nTag\n\n*0\n\n1 001100\n0\n\nD\n\nV\n\nTag\n\n*48\n\n000001\n1\n0 000100\n\nD\n\nV\n\nTag\n\n*4\n*16\n\n0\n1 001000\n\nD\n*32\n\n● Let's read memory address 4 (00000100).\no Uh oh. That's a miss. Where do we put it?\n● With associative caches, you must have a replacement scheme.\no Which block to evict (kick out) when you're out of empty slots?\n● The simplest replacement scheme is random.\no Just pick one. Doesn't matter which.\n● What would make more sense?\no How about taking temporal locality into account?\n37\n\n\fLRU (Least-Recently-Used) Replacement\n● When you need to evict a block, kick out the oldest one.\nV\n\nTag\n\n000001\n1\n0 000000\n\nD\n\nV\n\n*4\n*0\n\n1 001100\n0\n\n4 reads old\n\nTag\n\nD\n\nV\n\n*48\n\n1\n0 000100\n\n1 read old\n\nTag\n\nD\n\nV\n\n*16\n\n0\n1 001000\n\n3 reads old\n\nTag\n\nD\n*32\n\n2 reads old\n\n● Our read history looked like 0, 16, 32, 48. How old are the blocks?\n● Now we want to read address 4. Which block should we replace?\n● But now we must maintain the age of the blocks\no Easy to say. How do we keep track of this in hardware?\n● Have a saturating counter for each cache block indicating age\no When accessing a set, increment counter for each block in set\no On a cache hit, reset counter to 0 (most recently used)\n\n38\n\n\fImpact of LRU on AMAT\n● AMAT = hit time + (miss rate × miss penalty)\n\n● Impact of LRU on miss rate\no Smaller miss rate due to better leveraging of temporal locality\n(Recently used cache lines more likely to be used again)\n● Saturating counter for LRU uses bits and adds to amount of metadata\no Cache tag, the valid bit, the saturating counter are all metadata\no Every bit you spend on metadata is a bit you don’t spend on real data\no Spending many bits on counter may reduce capacity for real data\no This may lead to a larger miss rate, if LRU is not very effective\n\n39\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":205,"segment": "unlabeled", "course": "cs1550", "lec": "lec19", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Lab 3: due on 4\/1\n• Homework 9: due on 4\/4\n• Project 3: due on 4\/11\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• The problem of large page tables\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (1\/2)\n• How to determine number of bits (2^n) from day 4gb or\n4kb\n\n• Calculating the PTE frame number size.\n• calculating the number of frames\n• userspace page replacement\n• For the final top hat question, how did we get 24 in the\nanswer 24*(2^20)? I understand that we did log(2^20) to\nget 20, but where did the extra 4 come from?\n• I don't understand the last two examples about\ncalculating the PTE s and frames, what information\n(values) do we need to look at in terms of different\nquestions?\n• Could you list the formulas for page table entries?\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (2\/2)\n• In the OPT simulation, why do we only re order the\npages above the one we want? Wouldn't it be better\nto re order everything?\n• OPT trace was slightly confusing of what switches\nwhere\n• How to find the number of writes for LRU and OPT?\n• How to calculate the distance in the tables we made\n• Are local or global selection policies more common in\nmodern OS’s?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fProblem of the Day\n\n• How to keep track of larger page tables that can store\nmore pages\n• How big can a page table be?\n• 64-bit machine\n• 4 KB page size\n\n• How many pages?\n• How many PTE?\n• How big is a PTE?\n• How big is the page table of one process?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fAddress Translation Structures\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fMemory & paging structures\nPhysical\nmemory\n\nPage frame number\n\nPage 0\nPage 1\nPage 2\nPage 3\nPage 4\nLogical memory (P0)\n\n6\n3\n4\n9\n2\nPage table (P0)\n\nPage 0\nPage 1\n\n8\n0\n\nLogical memory (P1)\n\nPage table (P1)\n\n0\n\nPage 1 (P1)\n\n1\n2\n3\n\nFree\npages\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\nPage 4 (P0)\nPage 1 (P0)\nPage 2 (P0)\n\n5\n6\n\nPage 0 (P0)\n\n7\n8\n9\n\nPage 0 (P1)\nPage 3 (P0)\n\n8\n\n\fMapping logical => physical address\n◼\n\nSplit address from CPU into two\npieces\nPage number (p)\n◼ Page offset (d)\n◼\n\n◼\n\nPage number\n\nExample:\n• 4 KB (=4096 byte) pages\n• 32 bit logical addresses\n\nIndex into page table\n2d = 4096\n◼ Page table contains base address of\npage in physical memory\n◼\n\n◼\n\nPage offset\n◼\n\n◼\n\nd = 12\n\nAdded to base address to get actual\nphysical memory address\n\n32-12 = 20 bits\n\n12 bits\n\np\n\nd\n\nPage size = 2d bytes\n\n32 bit logical address\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fAddress translation architecture\npage number\n\nCPU\n\np\n\nPage frame number\n\nPage frame number\n\npage offset\n\nd\n0\n1\np-1\np\np+1\n\nf\n\nd\n\n..\n.\n\n..\n.\nf\n\n..\n.\n\n0\n1\n\nf-1\nf\nf+1\nf+2\n\nphysical memory\npage table\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fMemory Sizes\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fAddress Translation Relations\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fPage number vs. Offset\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fSolution 1: Two-level page tables\n◼\n\nProblem: page tables can be too\nlarge\n◼\n\n◼\n\n◼\n\n◼\n\n220\n657\n\nSolution: use multi-level page\ntables\n◼\n\n◼\n\n232 bytes in 4KB pages need 1\nmillion PTEs\n\n“Page size” in first page table is\nlarge (megabytes)\nPTE marked invalid in first page\ntable needs no 2nd level page\ntable\n\n..\n.\n\n401\n\n..\n.\n\n1st level page table has pointers to\n1st level\n2nd level page tables\npage table\n2nd level page table has actual\nphysical page numbers in it\n2nd level\npage tables\nCS 1550 – Operating Systems – Sherif Khattab\n\n125\n613\n\n..\n.\n\n961\n884\n960\n\n..\n.\n\n955\n\n..\n.\n..\n.\n..\n.\n..\n.\n..\n.\n..\n.\n..\n.\n..\n.\n..\n.\nmain\nmemory\n14\n\n\fTwo-level Page Table\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fMore on two-level page tables\n◼\n\nTradeoffs between 1st and 2nd level page table\nsizes\nTotal number of bits indexing 1st and 2nd level table is\nconstant for a given page size and logical address\nlength\n◼ Tradeoff between number of bits indexing 1st and\nnumber indexing 2nd level tables\n◼\n\nMore bits in 1st level: fine granularity at 2nd level\n◼ Fewer bits in 1st level: maybe less wasted space?\n◼\n\nAll addresses in table are physical addresses\n◼ Protection bits kept in 2nd level table\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fTwo-level paging: example\n◼\n\nSystem characteristics\n◼\n◼\n\n◼\n\nPage number divided into:\n◼\n◼\n\n◼\n\n8 KB pages\n32-bit logical address divided into 13 bit page offset, 19 bit page number\n10 bit page number\n9 bit page offset\n\nLogical address looks like this:\n◼\n◼\n\np1 is an index into the 1st level page table\np2 is an index into the 2nd level page table pointed to by p1\n\npage number\np1 = 10 bits\n\np2 = 9 bits\n\npage offset\noffset = 13 bits\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\f2-level address translation example\npage number\np1 = 10 bits\n\nPage\ntable\nbase\n\npage offset\n\np2 = 9 bits\n\noffset = 13 bits\n\nframe\nnumber\n0\n1\n\nphysical address\n\n19\n\n0\n1\np1\n\n..\n.\n\n0\n1\n\n..\n.\n\np2\n\n1st level page table\n\n13\n\n..\n.\n..\n.\n\n..\n.\n..\n.\nmain memory\n\n2nd level page table\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fAddress Translation: 2-level Page Table\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fTLB\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\fTranslation Lookaside Buffer (TLB)\n◼\n\nSearch the TLB for the desired logical\npage number\n◼\n◼\n\n◼\n\n◼\n\nSearch entries in parallel\nUse standard cache techniques\n\nIf desired logical page number is found,\nget frame number from TLB\nIf desired logical page number isn’t found\n◼\n\n◼\n\nGet frame number from page table in\nmemory\nReplace an entry in the TLB with the\nlogical & physical page numbers from this\nreference\n\nLogical\npage #\n\nPhysical\nframe #\n\n8\nunused\n2\n3\n12\n29\n22\n7\n\n3\n1\n0\n12\n6\n11\n4\n\nExample TLB\nCS 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fHandling TLB misses\nIf PTE isn’t found in TLB, OS needs to do the lookup in the\npage table\n◼ Lookup can be done in hardware or software\n◼ Hardware TLB replacement\n◼\n\nCPU hardware does page table lookup\n◼ Can be faster than software\n◼ Less flexible than software, and more complex hardware\n◼\n\n◼\n\nSoftware TLB replacement\nOS gets TLB exception\n◼ Exception handler does page table lookup & places the result into the\nTLB\n◼ Program continues after return from exception\n◼ Larger TLB (lower miss rate) can make this feasible\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fHow long do memory accesses take?\n◼\n\nAssume the following times:\nTLB lookup time = a (often zero - overlapped in CPU)\n◼ Memory access time = m\n◼\n\n◼\n\nHit ratio (h) is percentage of time that a logical page number\nis found in the TLB\nLarger TLB usually means higher h\n◼ TLB structure can affect h as well\n◼\n\n◼\n\nEffective access time (an average) is calculated as:\nEAT = (m + a)h + (m + m + a)(1-h)\n◼ EAT =a + (2-h)m\n◼\n\n◼\n\nInterpretation\nReference always requires TLB lookup, 1 memory access\n◼ TLB misses also require an additional memory reference\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\fEffective Access Time\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n24\n\n\fEAT Calculation\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n25\n\n\fSolution 2: Inverted page table\nReduce page table size further: keep one entry for\neach frame in memory\n◼ PTE contains\n◼\n\nVirtual address pointing to this frame\n◼ Information about the process that owns this page\n◼\n\n◼\n\nSearch page table by\nHashing the virtual page number and process ID\n◼ Starting at the entry corresponding to the hash result\n◼ Search until either the entry is found or a limit is\nreached\n◼\n\nPage frame number is index of PTE\n◼ Improve performance by using more advanced\nhashing algorithms\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n26\n\n\fInverted page table architecture\n\nprocess ID\n\npid\n\npage number\n\npage offset\n\np = 19 bits\n\noffset = 13 bits\n\np\nsearch\n0\n1\nk\n\nphysical address\n19\n13\npid0\npid1\npidk\n\np0\n\n..\n.\n..\n.\n\n..\n.\n\n..\n.\n\np1\npk\n\nPage frame\nnumber\n0\n1\n\nk\n\nmain memory\n\ninverted page table\nCS 1550 – Operating Systems – Sherif Khattab\n\n27\n\n\fInverted Page Table Example\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n28\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":206,"segment": "unlabeled", "course": "cs1550", "lec": "lec16", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Project 2: due on 3\/18\n• Homework 8: due on 3\/21\n• Quiz 2: due on 3\/25\n• Lab 3: due on 4\/1\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\n• Memory allocation and protection (Take II)\n• Virtual memory\n• Fixed-size pages, on-demand, appear as if having more\nmemory that physically in the system\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fProblem of the Day\n• Page fault forces a choice\n• No room for new page (steady state)\n• A page must be removed to make room for an incoming\npage.\n• Which page to select?\n• Victim page\n\n• Evicted\/purged\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fPage replacement algorithms\n◼\n\nHow is a page removed from physical memory?\nIf the page is unmodified, simply overwrite it: a copy\nalready exists on disk\n◼ If the page has been modified, it must be written back\nto disk: prefer unmodified pages?\n◼\n\n◼\n\nBetter not to choose an often used page\n◼\n\nIt’ll probably need to be brought back in soon\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fOptimal page replacement algorithm\n◼\n\nWhat’s the best we can possibly do?\nAssume perfect knowledge of the future\n◼ Not realizable in practice (usually)\n◼ Useful for comparison: if another algorithm is within\n5% of optimal, not much more can be done…\n◼\n\n◼\n\nAlgorithm: replace the page that will be used\nfurthest in the future\nOnly works if we know the whole sequence!\n◼ Can be approximated by running the program twice\n◼\n\nOnce to generate the reference trace\n◼ Once (or more) to apply the optimal algorithm\n◼\n\n◼\n\nNice, but not achievable in real systems!\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fOPT Examples\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fNot-recently-used (NRU) algorithm\n◼\n\nEach page has reference bit and dirty bit\n◼\n\n◼\n\nBits are set when page is referenced and\/or modified\n\nPages are classified into four classes\n0: not referenced, not dirty\n◼ 1: not referenced, dirty\n◼ 2: referenced, not dirty\n◼ 3: referenced, dirty\n◼\n\n◼\n\nClear reference bit for all pages periodically\n\nCan’t clear dirty bit: needed to indicate which pages need to be\nflushed to disk\n◼ Class 1 contains dirty pages where reference bit has been\ncleared\n◼\n\n◼\n\nAlgorithm: remove a page from the lowest non-empty\nclass\n◼\n\nSelect a page at random from that class\n\nEasy to understand and implement\n◼ Performance adequate (though not optimal)\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fNRU Operation\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fFirst-In, First-Out (FIFO) algorithm\n◼\n\nMaintain a linked list of all pages\n◼\n\nMaintain the order in which they entered memory\n\nPage at front of list replaced\n◼ Advantage: (really) easy to implement\n◼ Disadvantage: page in memory the longest may\nbe often used\n◼\n\nThis algorithm forces pages out regardless of usage\n◼ Usage may be helpful in determining which pages to\nkeep\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fPage Replacement Algorithms Components\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\f•\n\n•\n\nSecond chance page replacement\nModify FIFO to avoid throwing out heavily used pages\n•\n\nIf reference bit is 0, throw the page out\n\n•\n\nIf reference bit is 1\n•\n\nReset the reference bit to 0\n\n•\n\nMove page to the tail of the list\n\n•\n\nContinue search for a free page\n\nStill easy to implement, and better than plain FIFO\nreferenced unreferenced\nA\nt=0\n\nB\nt=4\n\nC\nt=8\n\nD\nt=15\n\nE\nt=21\n\nF\nt=22\n\nG\nt=29\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\nH\nt=30\n\nA\nt=32\n\n\fClock algorithm\n•\n\nSame functionality as second\nchance\n\n•\n\nSimpler implementation\n\n•\n\n•\n\n“Clock” hand points to next\npage to replace\n\n•\n\nIf R=0, replace page\n\n•\n\nIf R=1, set R=0 and advance\nthe clock hand\n\nContinue until page with R=0\nis found\n•\n\nH\nt=30\n\nA\nt=32\nt=0\n\nB\nt=32\nt=4\n\nG\nt=29\nF\nt=22\n\nThis may involve going all the\nway around the clock…\n\nC\nt=32\nt=8\n\nE\nt=21\n\nD\nJ\nt=15\nt=32\n\nreferenced unreferenced\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fLeast Recently Used (LRU)\n• Assume pages used recently will be used again soon\n• Throw out page that has been unused for longest time\n\n• Must keep a linked list of pages\n• Most recently used at front, least at rear\n• Update this list every memory reference!\n• This can be somewhat slow: hardware has to update a linked list\non every reference!\n\n• Alternatively, keep counter in each page table entry\n• Global counter increments with each CPU cycle\n• Copy global counter to PTE counter on a reference to the\npage\n• For replacement, evict page with lowest counter value\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fSimulating LRU in software\n• Few computers have the necessary hardware to\nimplement full LRU\n• Linked-list method impractical in hardware\n• Counter-based method could be done, but it’s slow to find\nthe desired page\n\n• Approximate LRU with Not Frequently Used (NFU)\nalgorithm\n• At each clock interrupt, scan through page table\n• If R=1 for a page, add one to its counter value\n• On replacement, pick the page with the lowest counter\nvalue\n\n• Problem: no notion of age—pages with high counter\nvalues will tend to keep them!\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\f•\n\nAging replacement algorithm\nReduce counter values over time\n•\n\nDivide by two every clock cycle (use right shift)\n\n•\n\nMore weight given to more recent references!\n\n•\n\nSelect page to be evicted by finding the lowest counter value\n\n•\n\nAlgorithm is:\n•\n\nEvery clock tick, shift all counters right by 1 bit\n\n•\n\nOn reference, set leftmost bit of a counter (can be done by copying the reference\nbit to the counter at the clock tick)\nReferenced\nthis tick\nPage 0\nPage 1\nPage 2\nPage 3\nPage 4\nPage 5\n\nTick 0\n\nTick 1\n\nTick 2\n\nTick 3\n\nTick 4\n\n10000000\n\n11000000\n\n11100000\n\n01110000\n\n10111000\n\n00000000\n10000000\n00000000\n\n10000000\n01000000\n00000000\n\n01000000\n00100000\n00000000\n\n00100000\n10010000\n10000000\n\n00010000\n01001000\n01000000\n\n10000000\n10000000\n\n01000000\n11000000\n\n10100000\n01100000\n\n11010000\n10110000\n\n01101000\n11011000\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fWorking set\n• Demand paging: bring a page into memory when it’s\nrequested by the process\n\n• How many pages are needed?\n•\n\nCould be all of them, but not likely\n\n•\n\nInstead, processes reference a small set of pages at any given\ntime—locality of reference\n\n•\n\nSet of pages can be different for different processes or even\ndifferent times in the running of a single process\n\n• Set of pages used by a process in a given interval of time is\ncalled the working set\n•\n\nIf entire working set is in memory, no page faults!\n\n•\n\nIf insufficient space for working set, thrashing may occur\n\n•\n\nGoal: keep most of working set in memory to minimize the number\nof page faults suffered by a process\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fHow big is the working set?\n\nw(k,t)\n\nk\n\n•\n\nWorking set is the set of pages used by the k most recent\nmemory references\n\n•\n\nw(k,t) is the size of the working set at time t\n\n•\n\nWorking set may change over time\n•\n\nSize of working set can change over time as well…\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fKeeping track of the Working Set\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fWorking set page replacement algorithm\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":207,"segment": "unlabeled", "course": "cs1550", "lec": "lec24", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Homework 11: due on 4\/18\n• Project 3: Late due date on 4\/13\n• Late penalty waived\n\n•\n•\n•\n•\n•\n•\n\nLab 4: due on 4\/15\nQuiz 3: due on 4\/15\nHomework 12: due on 4\/25\nLab 5: due on 5\/2\nProject 4 and Quiz 4: due on 5\/2\nBonus Opportunities\n• Bonus Homework: due on 5\/2\n• Course Post-Test: due on 5\/2\n• Bonus point for all when OMET response rate >= 80%\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• How do device drivers program I\/O devices?\n\n• Answer: three methods\n• polling\n• Interrupts\n• DMA\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\f•\n\nMuddiest Points (1\/2)\nRAIDs\n•\n\nDifference between RAID 4 and 5\n\n•\n\nSo is the log-structured file system the same as journaling, or is that\ndifferent? If it is the same, does that mean all journaling systems\nalso track file version histories?\n\n•\n\nInterrupt driven I\/O\n\n•\n\npolling\n\n•\n\nOther situations like that bug\/feature example? Seems interesting.\n\n•\n\nIn file block cache data structures slide, how do you track hash table\ncollisions to make the chains?\n\n•\n\nvoltage about 0\/1 on solid state disk\n\n•\n\nCan you explain again why SSDs degrade over time?\n\n•\n\nSingle bus versus dual-bus\n\n•\n\nWhat tradeoff exists having larger disk caches?\n\n•\n\nmemory mapped and separate io spaces\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (2\/2)\n\n• can you please release final preparation material a bit\nearlier? i’m nervous for the final and want to start\nstudying ahead of time\n• not about the lecture specifically but in general: are\nwe allowed to work with each other on midterm exam\ncorrections or should they be done alone?\n\n• Are we really going to having 8am final?\n• Everything was clear\n• Nothing good. Need to review myself\n\n• All good this lecture, though things went a bit fast\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fPolling\n\ncopy_from_user (buffer, p, count); \/\/ copy into kernel buffer\nfor (j = 0; j < count; j++) {\n\/\/ loop for each char\nwhile (*printer_status_reg != READY)\n;\n\/\/ wait for printer to be ready\n*printer_data_reg = p[j]; \/\/ output a single character\n}\nreturn_to_user();\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fHardware’s view of interrupts\n\nBus\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fInterrupt-driven I\/O\ncopy_from_user (buffer, p, count);\nj = 0;\nenable_interrupts();\nwhile (*printer_status_reg != READY)\n;\n*printer_data_reg = p[0];\nscheduler(); \/\/ and block user\nif (count == 0) {\nunblock_user();\n} else {\nj++;\n*printer_data_reg = p[j];\ncount--;\n}\nacknowledge_interrupt();\nreturn_from_interrupt();\n\nCode run by system call\n\nCode run at interrupt time\n(Interrupt handler)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fWhat happens on an interrupt\n• Set up stack for interrupt service procedure\n• Ack interrupt controller, reenable interrupts\n\n• Copy registers from where saved\n• Run service procedure\n• (optional) Pick a new process to run next\n• Set up MMU context for process to run next\n• Load new process' registers\n• Start running the new process\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fDirect Memory Access (DMA) operation\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fDMA\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fI\/O using DMA\ncopy_from_user (buffer, p, count);\nset_up_DMA_controller();\nscheduler(); \/\/ and block user\n\nCode run by system call\n\nacknowledge_interrupt();\nunblock_user();\nreturn_from_interrupt();\n\nCode run at interrupt time\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fDisk drive structure\n• Data stored on surfaces\n•\n\nUp to two surfaces per platter\n\n•\n\nOne or more platters per disk\n\nhead\n\nsector\n\n• Data in concentric tracks\n•\n•\n\nTracks broken into sectors\n\nplatter\n\n• 256B-1KB per sector\n\ntrack\n\nCylinder: corresponding tracks\non all surfaces\n\ncylinder\n\n• Data read and written by\nheads\n•\n•\n\nActuator moves heads\n\nsurfaces\n\nspindle\n\nHeads move in unison\nCS 1550 – Operating Systems – Sherif Khattab\n\nactuator\n13\n\n\f•\n\n•\n\nDisk scheduling algorithms\nSchedule disk requests to minimize disk seek time\n•\n\nSeek time increases as distance increases (though\nnot linearly)\n\n•\n\nMinimize seek distance -> minimize seek time\n\nDisk seek algorithm examples assume a request\nqueue & head position (disk has 200 cylinders)\n•\n\nQueue = 100, 175, 51, 133, 8, 140, 73, 77\n\n•\n\nHead position = 63\n\nOutside edge\n8\n\nInside edge\n51\n\n73\n77\n\nread\/write head position\n\n100\n\n133\n140\n\n175\n\ndisk requests\n(cylinder in which block resides)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fDisk Arm Scheduling\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fFirst-Come-First Served (FCFS)\n• Requests serviced in the order in which they arrived\n•\n•\n\nEasy to implement!\nMay involve lots of unnecessary seek distance\n\n• Seek order = 100, 175, 51, 133, 8, 140, 73, 77\n• Seek distance = (100-63) + (175-100) + (175-51) + (133-51) +\n(133-8) + (140-8) + (140-73) + (77-73) = 646 cylinders\nread\/write head start\n\n100\n175\n\n51\n133\n8\n140\n73\n77\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fFCFS Example 2\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\f•\n\nShortest Seek Time First (SSTF)\nService the request with the shortest seek time from the current\nhead position\n•\n•\n\nForm of SJF scheduling\nMay starve some requests\n\n•\n\nSeek order = 73, 77, 100, 133, 140, 175, 51, 8\n\n•\n\nSeek distance = 10 + 4 + 23 + 33 + 7 + 35 + 124 + 43 = 279\ncylinders\n\nread\/write head start\n\n73\n77\n\n51\n8\n\n100\n\n133\n\n140\nCS 1550 – Operating Systems – Sherif Khattab\n\n175\n18\n\n\fSSTF Example 2\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\f•\n\nSCAN (elevator algorithm)\nDisk arm starts at one end of the disk and moves towards the other\nend, servicing requests as it goes\n•\n•\n\nReverses direction when it gets to end of the disk\nAlso known as elevator algorithm\n\n•\n\nSeek order = 51, 8, 0 , 73, 77, 100, 133, 140, 175\n\n•\n\nSeek distance = 12 + 43 + 8 + 73 + 4 + 23 + 33 + 7 + 35 = 238 cyls\n\nread\/write head start\n51\n8\n73\n77\n\n100\n133\n140\nCS 1550 – Operating Systems – Sherif Khattab\n\n175\n20\n\n\fSCAN Example 2\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fC-SCAN\n• Identical to SCAN, except head returns to cylinder 0 when it\nreaches the end of the disk\n•\n•\n\nTreats cylinder list as a circular list that wraps around the disk\nWaiting time is more uniform for cylinders near the edge of the disk\n\n• Seek order = 73, 77, 100, 133, 140, 175, 199, 0, 8, 51\n• Distance = 10 + 4 + 23 + 33 + 7 + 35 + 24 + 199 + 8 + 43 = 386\ncyls\nread\/write head start\n73\n77\n100\n\n133\n140\n8\n\n175\n\n51\nCS 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fC-LOOK\n• Identical to C-SCAN, except head only travels as far as\nthe last request in each direction\n•\n\nSaves seek time from last sector to end of disk\n\n• Seek order = 73, 77, 100, 133, 140, 175, 8, 51\n• Distance = 10 + 4 + 23 + 33 + 7 + 35 + 167 + 43 = 322\ncylinders\nread\/write head start\n73\n77\n100\n\n133\n140\n8\n\n175\n\n51\nCS 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\fC-LOOK Example 2\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n24\n\n\fTwo-level directory system\n• Solves naming problem: each user has her own\ndirectory\n• Multiple users can use the same file name\n• By default, users access files in their own directories\n• Extension: allow users to access files in others’\nRoot\ndirectories\ndirectory\n\nA\n\nA\nfoo\n\nB\n\nA\nbar\n\nB\nfoo\n\nC\n\nB\nbaz\n\nC\nbar\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\nC\nfoo\n\nC\nblah\n\n25\n\n\fHierarchical directory system\nRoot\ndirectory\nA\n\nB\n\nA\nPapers\n\nA\nfoo\n\nA\nPhotos\n\nB\nfoo\n\nB\nPapers\n\nA\nos.tex\n\nA\nsunset\n\nA\nFamily\n\nB\nfoo.tex\n\nB\nfoo.ps\n\nA\nsunset\n\nA\nkids\n\nA\nMom\n\nC\n\nC\nbar\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\nC\nfoo\n\nC\nblah\n\n26\n\n\fUnix directory tree\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n27\n\n\fProject 4: Directory Traversal\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n28\n\n\fProject 4 Hint\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n29\n\n\fSharing files\nRoot\ndirectory\nA\n\nB\n\nA\nPapers\n\nA\nfoo\n\nA\nPhotos\n\nA\nos.tex\n\nA\nsunset\n\nA\nFamily\n\nA\nsunset\n\nA\nkids\n\nB\nfoo\n\nB\nPhotos\n\nC\n\nC\nbar\n\nC\nfoo\n\nC\nblah\n\nB\nlake\n\n?\n???\nCS 1550 – Operating Systems – Sherif Khattab\n\n30\n\n\fSolution: use links\n• A creates a file, and inserts into her directory\n\n• B shares the file by creating a link to it\n• A unlinks the file\n• B still links to the file\n• Owner is still A (unless B explicitly changes it)\nA\n\na.tex\n\nOwner: A\nCount: 1\n\nA\n\nB\n\nB\n\nb.tex\n\nb.tex\n\na.tex\n\nOwner: A\nCount: 2\nCS 1550 – Operating Systems – Sherif Khattab\n\nOwner: A\nCount: 1\n31\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":208,"segment": "unlabeled", "course": "cs1550", "lec": "lec12", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Quiz 1: due 2\/25\n• Homework 6: due 2\/28\n• Lab 2: due on 2\/28\n• Project 2: due on 3\/18\n\n• Midterm exam on Thursday 3\/3\n• In-person, on paper, closed book\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\n• How to implement condition variables”\n\n• Reflections on using semaphores and condition\nvariables\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (Condition Variable Implementation)\n•\n\nfor the user side implementation of condition variables, is the next\nqueue only holding processes that have signaled and yielded to the\nawakened process and therefore have to sleep?\n\n•\n\nHoare and Mesa semantics.\n\n•\n\nare there any performance drawbacks when implementing condition\nvariables at the application level?\n\n•\n\nI think the code for the implementation of the CV was a little fast, so\nI'm not confident I got it fully.\n\n•\n\nHow you can connect the cv implementation in and outside kernel\n\n•\n\nI don't understand what all the locks do for the condition variable\nimplementation.\n\n•\n\nWould you please explain the implementation of Signal() in Userlevel implement of Condition variable? Thank you.\n\n•\n\nWhy Hoare semantics are necessary for a user level\nimplementation. The implementation of the user level condition\nvariable was also generally confusing without fully understanding\nwhy it’s being done that way. I’ll need to review this on my own\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (Deadlocks)\n• How often do deadlocks occur even if the algorithm tells\nus it is possible to not deadlock\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (Misc.)\n• n\/a BUT PLEASE EXTEND PROJ 1. THOTH IS\nALWAYSSS DOWNNN!!!!\n• where the sleepy barber would actually be used\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fFinal Remarks on Process Synchronization\n• Many other synchronization mechanisms\n• Message passing\n• Barriers\n• Futex\n• Re-entrant locks\n\n• Atomic*\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fProblem of the Day: CPU Scheduling\n\nHow does the short-term scheduler select the next\nprocess to run?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fCPU Scheduling\n• Scheduling the processor among all ready\nprocesses\n• User-oriented criteria\n• Response Time: Elapsed time between the submission\nof a request and the receipt of a response\n\n• Turnaround Time: Elapsed time between the submission\nof a process to its completion\n\n• System-oriented criteria\n• Processor utilization\n\n• Throughput: number of process completed per unit time\n• Fairness\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fShort-Term Scheduler Dispatcher\n• The dispatcher is the module that gives control of the\nCPU to the process selected by the short-term\nscheduler\n• The functions of the dispatcher include:\n• Switching context\n• Switching to user mode\n• Jumping to the location in the user program to restart\nexecution\n\n• The dispatch latency must be minimal\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fThe CPU-I\/O Cycle\n• Processes require alternate use of processor and I\/O\nin a repetitive fashion\n• Each cycle consist of a CPU burst followed by an I\/O\nburst\n• A process terminates on a CPU burst\n\n• CPU-bound processes have longer CPU bursts than\nI\/O-bound processes\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fResponse time vs. Turnaround time\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fScheduling Algorithms\n• First-Come, First-Served Scheduling\n• Shortest-Job-First Scheduling\n• Also referred to as Shortest Process Next\n\n• Priority Scheduling\n\n• Round-Robin Scheduling\n• Multilevel Queue Scheduling\n• Multilevel Feedback Queue Scheduling\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fCharacterization of Scheduling Policies\n•\n\nThe selection function determines which ready process is\nselected next for execution\n\n•\n\nThe decision mode specifies the instants in time the selection\nfunction is exercised\n•\n\nNonpreemptive\n• Once a process is in the running state, it will continue until it\nterminates or blocks for an I\/O\n\n•\n\nPreemptive\n• Currently running process may be interrupted and moved to the\nReady state by the OS\n• Prevents one process from monopolizing the processor\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fProcess Mix Example\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nService time = total processor time needed in one (CPU-I\/O) cycle\nJobs with long service time are CPU-bound jobs and are referred\nto as “long jobs”\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fFirst Come First Served (FCFS)\n• Selection function: the process that has been waiting\nthe longest in the ready queue (hence, FCFS)\n• Decision mode: non-preemptive\n• a process runs until it blocks for an I\/O\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fAverage Response Time\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fFCFS drawbacks\n•\n\nFavours CPU-bound processes\n•\n\nCPU-bound processes monopolize the processor\n\n•\n\nI\/O-bound processes have to wait until completion of CPUbound process\n• I\/O-bound processes may have to wait even after their I\/Os are\ncompleted (poor device utilization)\n• Convoy effect\n\n•\n\nBetter I\/O device utilization could be achieved if I\/O bound\nprocesses had higher priority\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fConvoy Effect\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\f•\n\nShortest Job First (Shortest Process Next)\nSelection function: the process with the shortest expected CPU\nburst time\n\n•\n\nI\/O-bound processes will be selected first\n\n•\n\nDecision mode: non-preemptive\n\n•\n\nThe required processing time, i.e., the CPU burst time, must be\nestimated for each process\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\fSJF \/ SPN Critique\n•\n\nPossibility of starvation for longer processes\n\n•\n\nLack of preemption is not suitable in a time sharing\nenvironment\n\n• SJF\/SPN implicitly incorporates priorities\n•\n\nShortest jobs are given preference\n\n•\n\nCPU bound processes have lower priority, but a process\ndoing no I\/O could still monopolize the CPU if it is the first to\nenter the system\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fPriorities\n• Implemented by having multiple ready queues to\nrepresent each level of priority\n• Scheduler selects the process of a higher priority\nover one of lower priority\n• Lower-priority may suffer starvation\n\n• To alleviate starvation allow dynamic priorities\n• The priority of a process changes based on its age or\nexecution history\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n24\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n25\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n26\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n27\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n28\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n29\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n30\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n31\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n32\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n33\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n34\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n35\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n36\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n37\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n38\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n39\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n40\n\n\fRound-Robin\nSelection function: same as\nFCFS\n\nProcess\n\nArrival Time\n\nService Time\n\n1\n\n0\n\n3\n\n2\n\n2\n\n6\n\n3\n\n4\n\n4\n\n4\n\n6\n\n5\n\n5\n\n8\n\n2\n\nDecision mode: pre-emptive\na process is allowed to run until\nthe time slice period (quantum,\ntypically from 10 to 100 ms) has\nexpired\na clock interrupt occurs and the\nrunning process is put on the\nready queue\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n41\n\n\fRR Time Quantum\n• Quantum must be substantially larger than the time\nrequired to handle the clock interrupt and dispatching\n• Quantum should be larger then the typical interaction\n• but not much larger, to avoid penalizing I\/O bound\nprocesses\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n42\n\n\fRR Time Quantum\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n43\n\n\fQuantum Length\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n44\n\n\f•\n\nRound Robin: critique\nStill favors CPU-bound processes\n•\n\nAn I\/O bound process uses the CPU for a time less than the\ntime quantum before it is blocked waiting for an I\/O\n\n•\n\nA CPU-bound process runs for all its time slice and is put\nback into the ready queue\n•\n\nMay unfairly get in front of blocked processes\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n45\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":209,"segment": "unlabeled", "course": "cs1550", "lec": "lec04", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines\n• Homework 1 is due today at 11:59 pm\n• Homework 2 is due next Monday at 11:59 pm\n• Lab 1 is due on 2\/4 at 11:59 pm\n• Project 1 is due on 2\/18 at 11:59 pm\n\n• TA Office hours available on the syllabus page\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\n• Busy waiting problem and how to solve it using\nsemaphores\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points\n•\n\na little confused on the relationship between semaphores and\nspinlocks\n\n•\n\nwhat’s the advantage between a binary semaphore and a spinlock if\na binary semaphore can only be for one process?\n\n•\n\nWhat happens to each process with each up() and down() for\nmultiple processes with varying priorities? Another walkthrough\nbasically\n\n•\n\ncould you further explain how the semaphores with spinlocks still are\nable to keep their benefits?\n•\n•\n\nWhat is the point of having semaphores if you are going to tie them with\nspinlocks? Does that not defeat the benefits of using a semaphore?\nBy introducing a spinlock within our semaphore implementation, doesn't\nthat bring up the issue of busy waiting? Or is this not an issue because\nthe critical section is a relatively small block of code?\n\n•\n\nwhat is the different between swap and test&set?\n\n•\n\nDo Semaphores have the same issue with reordering instructions\nthat spinlocks have?\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\f•\n\nMuddiest Points (2\/3)\nwhere the lock() and unlock() go in the pseudo code of the spinlock in\nsemaphores example\n•\n\nHow to stop the up\/down from breaking (the if else thing you brought up at the\nend)\n\n•\n\nfor the process list I am assuming we would use some sort of priority queue\nto sort the processes for the ProcessList?\n\n•\n\nHow a spinlock is implemented in a semaphore\n\n•\n\nI think the functional difference between blocking and waiting. As in, what is\nactually happening while a process is infinite looping and awaiting a lock?\nSince it's not \"blocked\", does that mean that it prevents other processes from\nrunning, or can it be context switched out like any normal process?\n\n•\n\nwhich implementation is most commonly used for semaphores\n\n•\n\ndifference between counting and binary semaphore (why counting would be\nused)\n\n•\n\ncan the semaphore be > 0, and if so what does that mean?\n\n•\n\nHow are spinlocks implemented in hardware? Are they the only locking\nmechanism implemented in hardware?\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (3\/3)\n•\n\nHow process states are switched\/clarification on\nready\/waiting\/running\n\n•\n\nWhat does the comment above the line of assembly code mean?\n•\n\nI'm curious about memory barriers. could you talk more about how to use\n_sync_synchronize() and how it works?\n\n•\n\nWhere exactly the code blocks\n\n•\n\nfrom the tophat question: why can down move a process into ready\n\n•\n\nWhen\/why is reordering instructions in a critical section beneficial?\n\n•\n\nToday, We heard \"Block\" a lot but it has the same meaning\n\n•\n\nImplementation of mutex vs standard semaphore\n\n•\n\nWhen would you know to use a spinlock instead of a semaphore?\n\n•\n\nWhy two context switches in semaphore\n\n•\n\nWhat is\/makes an operation atomic?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fSemaphore Usage Problem: Compromising Mutual Exclusion\n\n• Any process can up() the semaphore\n\n• Solution: A Mutex can be up()’d only by the same\nprocess that down()’d it\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\f•\n\n•\n\nSemaphore Usage Problem: Deadlock and Starvation\nDeadlock: two or more processes are\nwaiting indefinitely for an event that can\nonly be caused by a waiting process\n•\n\nP0 gets A, needs B\n\n•\n\nP1 gets B, needs A\n\n•\n\nEach process waiting for the other to signal\n\nShared variables\nSemaphore A(1),B(1);\n\nStarvation: indefinite blocking\n•\n\nProcess is never removed from the\nsemaphore queue in which its suspended\n\n•\n\nMay be caused by ordering in queues\n(priority)\n\nProcess P0\nA.down();\nB.down();\n.\n.\n.\nB.up();\nA.up();\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\nProcess P1\nB.down();\nA.down();\n.\n.\n.\nA.up();\nB.up();\n\n8\n\n\fSemaphore Usage Problem: Priority Inversion\n• Priority inversion is still possible using semaphores\n• Slightly less likely\n• Needs at least three processes\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\ffork() tracing\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\ffork()’s of fork()’s\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":210,"segment": "unlabeled", "course": "cs1622", "lec": "lec17","text":"Local Optimization\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● buhhhhhhh??????\n\n2\n\n\fOptimization\n\n3\n\n\fWhat is it?\n● optimization is the name for any technique which:\no rewrites part of a program…\no in order to improve resource utilization…\no without changing its behavior.\n● \"resource utilization\" can mean many things, such as:\no time (how quickly the code executes)\no code size (how many bytes the code takes up)\no memory size (how many bytes the data\/variables take up)\no power (how many watts the code uses when executed)\no registers (how many\/what kinds of registers are used)\n● some of these goals are even contradictory!\no so compilers let you control which optimizations are performed.\n\n4\n\n\fWe do it all the time\n● many of the optimizations we'll discuss are just automated versions\nof things human programmers do.\nfunc1(some.long + code[i]);\nfunc2(some.long + code[i]);\n\nlet v = some.long + code[i];\nfunc1(v);\nfunc2(v);\n\nthis technique is called common subexpression elimination (CSE).\nlet x = 1024 * 4;\n\nlet x = 4096;\n\nthis is constant folding.\nlet x = pow(y, 2);\n\nlet x = y * y;\n\nthis is strength reduction: using a simpler, faster operation\nthat gives the same answer as a more complex one.\n5\n\n\fOptimization scope\n● we'll be doing optimizations on our IR, rather than the AST.\nfunc\n\nlocal optimizations\nwork on code within\na single BB.\n\nglobal optimizations\nwork on one function's\nwhole CFG. (I know, \"global\" sounds\n\nmain\n\ninterprocedural\noptimizations work\non all functions in\nthe program.\n\nlike it should be the whole program but it's not)\n\nas you might imagine, local is the simplest; global is more\ncomplex; and interprocedural is the most complex.\n\n6\n\n\fDead code elimination (DCE)\n● a common technique in Java is to use static final variables to\nenable or disable pieces of code. in those cases, the condition is a\nconstant, and one or more control paths become “dead code.”\nstatic final boolean DEBUG = false;\npublic int getWhatever() {\nif(DEBUG)\nSystem.err.println(\"here\");\n\npublic int getWhatever() {\nreturn this.whatever;\n}\n\nreturn this.whatever;\n}\n\nthis is a global optimization, as it operates on the CFG of the\nmethod. some BBs were removed from this method.\n7\n\n\fFunction (or method) inlining\n● a very common interprocedural optimization is function inlining:\neliminating a call by copying the code of the callee into the caller.\npublic void printWhatever() {\nint w = this.getWhatever();\nSystem.out.println(w);\n}\npublic int getWhatever() {\nreturn this.whatever;\n}\n\npublic void printWhatever() {\nint w = this.whatever;\nSystem.out.println(w);\n}\n\nthink about all the work this is saving\n– no more passing arguments, doing\nthe function prologue and epilogue,\nmessing with the stack…\n\nbut knowing when it’s okay to do this, and knowing whether doing\nthis will save time or waste time is really, really complicated.\n8\n\n\fCareful now…\n● remember the \"without changing its behavior\" part of the definition?\nfunc1(problem(1));\nfunc2(problem(1));\n\nlet v = problem(1);\nfunc1(v);\nfunc2(v);\n\nwhat if problem were defined like this:\nfn problem(x: int): int {\nprint_s(\"problem: \");\nprintln_i(x);\nreturn x;\n}\n\nnow the original and \"optimized\"\nversions do different things!\n\nwhen performing optimizations, we must respect the\nsemantics and evaluation rules of the source language,\nand we cannot ignore side effects.\n9\n\n\fTo optimize, or not to optimize?\n● optimization might sound like a no-brainer. why not do it? well…\nthey can slow down\ncompilation, a lot.\n\nthey can be difficult to\nimplement correctly.\n\nFinished [release] target(s) in 1859.3s\n\n$ .\/hello\nHello, world!\n$ .\/hello_optimized\nSegmentation fault\n\nthey may only give a\nsmall improvement.\n\nthey can make it harder to\ndebug a running program.\n\n$ .\/bench\n18.93s\n$ .\/bench_optimized\n18.11s\n\n$ gdb bench_optimized\n(gdb) b main\nFunction \"main\" not defined.\n(gdb) _\n\nor some combination of all of these!\n10\n\n\fPeephole Optimizations\n\n11\n\n\fStarting small\n● the simplest kinds of optimizations are peephole optimizations:\nthey work on the level of one or two instructions at a time.\n● despite their simplicity, they are the foundation of all the more\ncomplicated kinds of optimizations.\na = 5 == 5\n\na = true\n\ndoing peephole\noptimizations…\n\na = true\nif a then bb1\nelse bb2\n\nif true then bb1\nelse bb2\n\n…can open up\nopportunities for larger\nlocal optimizations…\n\nif true then bb1\nelse bb2\n\ngoto bb1\n<delete bb2>\n\n…which are the\nbasis for global\noptimizations.\n12\n\n\fOperations vs. Moves (assignments)\n● each instruction in our IR looks (basically) like one of these two:\nx = y + z\na = b\n● the first is an operation: it requires computation to complete.\n● the second is a move: copying a value from place to place.\no but, a lot of moves are unnecessary and could be eliminated.\n● furthermore, if we can somehow convert operations into moves…\no we can eliminate entire steps of the program.\n● all the local optimizations basically follow these principles:\no make operations simpler than what was written;\no turn operations into moves if possible; and then\no eliminate the moves.\n● and then repeat!\n13\n\n\fConstant Folding\n● this one is easy: if you see arithmetic being done on constants,\nreplace the arithmetic with the result of the operation.\n\na = 640 * 480\n\na = 307200\n\nb = not true\n\nb = false\n\nc = \"hi\" + \"bye\"\n\nc = \"hibye\"\n\nthis turns each operation into a move. nice! but what about:\n\n$t1 = 640 * 480\nx\n= $t1 * 32\n\n$t1 = 307200\nx\n= $t1 * 32\n\nclearly there's more work to be done, but we'll come back to this.\n14\n\n\fDanger ahead\n● some mathematical operations can cause… problems.\n\na = 1000 \/ 0\n\nuh oh. don't try to simplify this\nor you'll crash the compiler!\n\na = 1000000000 * 5\n\nthat doesn't fit in a 32-bit int…\n\nconfusingly, these may or may not be errors in the code, so we\nprobably shouldn't give an error here; just give up instead.\ndue to interactions with control flow analysis,\nthese may not even get executed at runtime.\n\nlast, unless we're explicitly looking for errors, we really should\njust let the code through. we can't catch every mistake. it’s\nnever wrong to leave code the way the programmer wrote it.\n\n15\n\n\fAlgebraic Simplification\n● this uses algebraic laws to simplify or even eliminate operations.\n\na = b * 1\n\na = b\n\na = b + 0\n\na = b\n\na = b * 0\n\na = 0\n\na = a + 0\n\n(nothing!)\n\n$t1 = a == a\n\n$t1 = true\n\n$t1 = a < a\n\n$t1 = false\n16\n\n\fStrength Reduction\n● strength reduction might not reduce complexity, but it can improve\nperformance by using cheaper operations to do the same task.\n\na = b * 2\n\na = b + b\n\na = pow(b, 2)\n\na = b * b\n\na = b * 8\n\na = b << 3\n\na = b * 9\n\na = b << 3\na = a + b\n\na = b % 8\n\na = b & 7\n\n(many compilers \"know\" about\nstandard library math functions\nand can optimize them out.)\n\ntwo instructions may\nstill be faster than a\nsingle multiplication!\n\n17\n\n\fOptimization: not just for IR\n● peephole optimizations can be done on the target code too.\n● here are some Silly Sequences my code generator is producing:\naddi sp, sp, -4\naddi sp, sp, -4\n\nli s2, 5\nadd s1, s1, s2\n\nshows up in nested\ncalls like f(g(x))\n\nadding a constant to\na variable, x + 5\n\njal func\nmove s0, v0\nsw\ns0, -16(fp)\n\njal func\nmove s0, v0\nmove v0, s0\n\nassigning a return\nvalue to a variable\n\nreturning the value\nthat a call returned\n18\n\n\fMake it better, do it faster\n● a final pass after codegen can go through and replace these silly\nsequences with simpler equivalent sequences.\naddi sp, sp, -4\naddi sp, sp, -4\n\naddi sp, sp, -8\n\nli s2, 5\nadd s1, s1, s2\n\naddi s1, s1, 5\n\njal func\nmove s0, v0\nsw\ns0, -16(fp)\n\njal\nsw\n\nfunc\nv0, -16(fp)\n\njal func\nmove s0, v0\nmove v0, s0\n\njal\n\nfunc\n19\n\n\fSingle Static\nAssignment Form (SSA)\n\n20\n\n\fHitting a wall\n● let's apply all the optimizations we've seen to this bit of code.\nx = 10 * 16 constant folding…\nprintln_i(x)\nstrength reduction…\nx = x * 4\nprintln_i(x)\nx = x * 1\nalgebraic simplification…\nprintln_i(x)\n$t0 = x\nkind of leaves something\nreturn\nto be desired, no?\n\nx = 160\nprintln_i(x)\nx = x << 2\nprintln_i(x)\n\nprintln_i(x)\n$t0 = x\nreturn\n\nthink about it: do we really even need this x variable?\nwell, to make things easier on ourselves, first we're going to\nconvert the code into a single assignment form.\n21\n\n\fI AM LEAVING OUT A LOT OF DETAILS HERE\n● single static assignment (SSA) form rewrites the code so that each\nlocation is only assigned once, and is never reassigned.\n● this does not \"optimize\" the code at all, but it does make several\nother optimizations much easier to perform.\nx = 10 * 16\nprintln_i(x)\nx = x * 4\nprintln_i(x)\nx = x * 1\nprintln_i(x)\n$t0 = x\nreturn\n\nSSA!\n\nx1 = 10 * 16\nprintln_i(x1)\nx2 = x 1 * 4\nprintln_i(x2)\nx3 = x 2 * 1\nprintln_i(x3)\n$t0 = x3\nreturn\n\neach time x is assigned,\nwe replace it with a new,\nunique variable.\n\nreferences to x are\nreplaced with the \"most\nrecent version\" of x.\n\n22\n\n\fNot a requirement\n● SSA is a super useful form for optimization buuuuuuuuuut…\no it has to be done on the whole CFG, which is complicated.\no consider a for loop. how do you represent the counter variable in\nthis form at all?? (heheheheh hahah hohohoho ɸ)\n● then, once you're done optimizing, you have to convert back out of\nSSA to do the final codegen.\no so it's a bit of a mixed bag, depending on what kinds of\noptimizations you want to do.\n● it's not required for all optimizations, it just simplifies a lot of them.\no the other local optimizations we’ll talk about can be implemented\nwith or without SSA, but the algorithms for detecting and applying\nthem are more complicated without it.\n\n23\n\n\fMore advanced\nlocal optimizations\n\n24\n\n\fCopy Propagation\n● if the code is in SSA form, we \"unlock\" this optimization:\n● if we see a move x = y, then we can replace all uses of x with y.\n\nx = a\nb = x + c\n\nx = a\nb = a + c\n\nx = 10\na = x + 5\n\nx = 10\na = 10 + 5\n\ntwo things to notice here:\n\nin the second example, we've produced something\nthat can be further optimized to a = 15!\nin both examples, x is no longer used anywhere.\nso what do we do with it?\n25\n\n\fDead store elimination\n● if a variable is assigned a value, and that value is never read, then\nthe assignment is a dead store and can be removed.\no …as long as the assignment has no side effects!\n● so continuing with the improved code from last slide:\n\nbut:\n\nx = a\nb = a + c\n\nb = a + c\n\nx = 10\na = 10 + 5\n\na = 10 + 5\n\nx = f()\n$t0 = 5\nreturn\n\nwe can't remove the assignment to x,\nbecause f() may have side effects.\n26\n\n\fCommon subexpression elimination (CSE)\n● if the code is in SSA form…\no and two variable assignments have the same rhs…\no and the rhs has no side effects…\n● then the second variable assignment can be changed to a move\nfrom the first variable.\n\nx = a + c\nprintln_i(x)\ny = a + c\nprintln_i(y)\n\nx = a + c\nprintln_i(x)\ny = x\nprintln_i(y)\n\nbecause we're using SSA, the variables on the RHS are\nguaranteed not to change value between the assignments.\n(if we weren't using SSA, we'd have to check that.)\n\n27\n\n\fTeamwork makes the dream work\n● each local optimization only does a little work.\n● but every time you run one, it can make it possible for another one\nto do a little more work…\na copy propagation makes a\nconstant folding possible;\nwhich makes another copy\npropagation possible, followed\nby a dead store elimination;\n\naround and around until\nthe code is as \"simple\" as\nwe can make it.\n\n28\n\n\fHow the compiler does it\n● it just does a simple round-robin scheme:\ndo {\nconst_fold(bb);\nstrength_reduce(bb);\nalgebraic_simpl(bb);\ncopy_propagate(bb);\ndead_store_elim(bb);\n} while(the bb changed);\n\neach optimization is so simple and\nquick that you just… keep doing them\nuntil they don't do anything anymore.\nit is literally more work to \"check\nif the optimization is possible\"\nthan it is to just try to do it\n\nif this makes you feel uneasy (couldn't it get stuck in an\ninfinite loop??), I'm with you! but if it works, it works.\n(the compiler might stop after some n iterations anyway).\n\n29\n\n\fAn example\n● let's take that code from before and optimize it real good.\nx1 = 10 * 16\nprintln_i(x1)\nx2 = x1 * 4\nprintln_i(x2)\nx3 = x2 * 1\nprintln_i(x3)\n$t0 = x3\nreturn\n\nx1 = 160\nprintln_i(x1)\nx2 = x1 << 2\nprintln_i(x2)\nx3 = x2\nprintln_i(x3)\n$t0 = x3\nreturn\n\nx1 = 160\nprintln_i(160)\nx2 = 160 << 2\nprintln_i(x2)\nx3 = x2\nprintln_i(x2)\n$t0 = x2\nreturn\n\nprintln_i(160)\nx2 = 640\nprintln_i(x2)\nprintln_i(x2)\n$t0 = x2\nreturn\n\nprintln_i(160)\nx2 = 640\nprintln_i(640)\nprintln_i(640)\n$t0 = 640\nreturn\n\nprintln_i(160)\nprintln_i(640)\nprintln_i(640)\n$t0 = 640\nreturn\n\nprintln_i(160)\nx2 = 160 << 2\nprintln_i(x2)\nprintln_i(x2)\n$t0 = x2\nreturn\n\nwhew!\n\n30\n\n\fA bigger example of CSE\n● even larger common subexpressions can be eliminated.\nfunc_1(a.x + b.y - c);\nfunc_2(a.x + b.y - c);\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t3 = a.x + b.y\n$t4 = $t3 - c\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t3 = $t1\n$t4 = $t3 - c\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t3 = $t1\n$t4 = $t1 - c\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t4 = $t2\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t4 = $t2\nfunc_2($t2)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\nfunc_2($t2)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t4 = $t1 - c\nfunc_2($t4)\n\n31\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":211,"segment": "unlabeled", "course": "cs1541", "lec": "lec3.3_cache_design2","text":"Cache Design 2\nCS 1541\nWonsun Ahn\n\n\fCache Design Parameter 6:\nWrite-Through vs. Write-Back\n\n2\n\n\fWrites and Cache Consistency\n● Assume &x is 1110102, and x == 24 initially\n\nlw\nt0, &x\naddi t0, t0, 1\nsw\nt0, &x\n\n# x++\n\n● How will the lw change the cache?\n● How will the sw change the cache?\no Uh oh, now the cache is inconsistent.\n(Memory still has the old value 24.)\n\nV\n000\n\n0\n\n001\n\n0\n\n010\n\n1\n0\n\n011\n\n0\n\n100\n\n0\n\n101\n\n0\n\n110\n\n0\n\n111\n\n0\n\nTag\n\nData\n\n111\n\n25\n24\n\n● How can we solve this? Two policies:\no Write-through: Propagate write all the way through memory\no Write-back: Write back cache block when it is evicted from cache\n3\n\n\fWrite-Through Policy\n\n4\n\n\fPolicy 1: Write-through\n● Write-through:\no On hit, write to cache block and propagate write to lower memory\no On miss, keep on propagating the write to lower memory\n● What happens if we write 25 to address 1110102?\n● What happens if we write 94 to address 0000102?\n→ Caches are kept consistent at all points in time!\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV Tag Data\n\nV Tag Data\n\n...\n\n...\n\n000 0\n\n000 0\n\n000010\n\n94\n93\n\n001 0\n\n001 0\n\n...\n\n...\n\n010 1 111\n\n24\n25\n\n010 1 000\n\n93\n94\n\n111010\n\n25\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n5\n\n\fWrite-through: Reads\n● What happens if we read from address 0000102?\no We can just discard the conflicting cache block 1110102\no It’s just an extra copy of the same data\n● Note how we allocate blocks only on read misses\no Write misses don’t allocate blocks because it doesn’t help anyway\n--- writes are propagated to lower memory even on write hits\no This policy is called no write allocate\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV Tag Data\n\nV Tag Data\n\n...\n\n...\n\n000 0\n\n000 0\n\n000010\n\n94\n93\n\n001 0\n\n001 0\n\n...\n\n...\n\n010 1 000\n111\n\n24\n94\n25\n\n010 1 000\n\n93\n94\n\n111010\n\n25\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n6\n\n\fWrite-through: Drawbacks\n● Drawback: Long write delays regardless of hit or miss\no Must always propagate writes all the way to DRAM\n● Solution: Write buffer maintaining pending writes\no CPU gets on with work after moving pending write to write buffer\no But does the write buffer solve all problems?\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV Tag Data\n\nV Tag Data\n\n...\n\n...\n\nWrite Buffer\n\n000 0\n\n000 0\n\n000010\n\n94\n93\n\nV Tag Data\n\n001 0\n\n001 0\n\n...\n\n...\n\n010 1 111\n\n24\n25\n\n010 1 000\n\n93\n94\n\n111010\n\n25\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n7\n\n\fWrite-through: Drawbacks\n● The write buffer does not solve all problems.\n\n1. Write buffer must be very big to store all pending writes\no May take more than 100 cycles for write to propagate to memory\no Write buffer is always checked before L1$ → adds to hit time\n2. Write buffer does not solve bandwidth problems\no If memory bandwidth < rate of writes in program,\nwrite buffer will fill up quickly, no matter how big it is\n● Impractical to write-through all the way to memory\no Typically only L1 caches are write-through, if any\n● We need another strategy that is not so bandwidth-intensive\n8\n\n\fWrite-Back Policy\n\n9\n\n\fPolicy 2: Write-back\n● Dirty block: a block that is temporarily inconsistent with memory\no On a hit, write to cache block, marking it dirty. No propagation.\no Write back dirty block to lower memory only when it is evicted\n→ Saves bandwidth since write hits no longer access memory\n● A dirty bit is added to the cache block metadata (marked “D”)\no Block 0000012 is clean → can be discarded on eviction\no Block 1110102 is dirty → needs to be written back on eviction\nMemory\n\nCache\nV D Tag Data\n000 0 0\n001 0 0 000\n\n93\n\n010 1 1 111\n\n25\n\n...\n\n...\n\n...\n\nAddress\n\nData\n\n...\n\n...\n\n000001\n\n93\n\n...\n\n...\n\n111010\n\n24\n\n...\n\n...\n10\n\n\fWrite-back: Write allocate\n● What happens on a write miss?\no If no write allocate like write-through, will miss again on next write\no And on the next write, and on the next write, …\no No bandwidth savings from hitting in cache\n\n● Unlike write-through, write-back has a write allocate policy\no On write miss, block is allocated in cache to stop further misses\no On allocation, the block is read in from lower memory\n● Q: Why the wasted effort?\no Aren’t we going to overwrite the block anyway with new data?\no Why read in data that is going be overwritten?\n\n11\n\n\fWrite-back: Write allocate\n● Because a block is multiple bytes, and you are updating just a few\no Suppose a cache block is 8 bytes (2 words)\no Suppose you are writing to only the first word\nV D\n1\n\n1\n\nTag\n\nData\nfirst word (written)\n\nsecond word (not written)\n\no After allocate, the entire cache block is marked valid\n▪ That means second word as well as first word must be valid\n▪ That means second word must be fetched from lower memory\n▪ Otherwise if later second word is read, it will contain junk data\n▪ Unavoidable, unless you have a valid bit for each byte\n– That means spending 1 bit for every 8 bits of data\n– That’s just too much metadata overhead\n12\n\n\fPolicy 2: Write-back\n● What happens if we write 25 to address 1110102?\n\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n010 1 0 111\n\n24\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n13\n\n\fPolicy 2: Write-back\n● What happens if we write 25 to address 1110102?\no L1 Cache hit! Update cache block and mark it dirty.\no That’s it! How quick is that compared to write-through?\n\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n010 1 0\n1 111\n\n24\n25\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n14\n\n\fPolicy 2: Write-back\n● What happens if we write 94 to address 0000102?\no L1 Cache miss! First thing we will do is add store to Write Buffer.\n(So that the CPU can continue executing past the store)\n\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n1\n\n94\n\n010 1 1 111\n\n25\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n15\n\n\fPolicy 2: Write-back\n● What happens if we write 94 to address 0000102? (cont’d)\no Next the L2 Cache is searched and it’s a hit!\no To bring in block to L1 Cache, we first need to evict block 25.\no It’s a dirty block, so we can’t just discard it. Need to write it back!\no Since block 25 misses in L2, it will take the long trip to Memory\no Is there a way to put it aside and get to it later?\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n1\n\n94\n\n010 1 1 111\n\n25\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n16\n\n\fPolicy 2: Write-back\n● What happens if we write 94 to address 0000102? (cont’d)\no Yes! Add Write Buffers to caches, just like we did for the pipeline!\no Move block to L1 Write Buffer so L1 Cache can continue working\no Pending block will get written back to Memory eventually\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n010 1\n0 1\n0 111\n\n001 0 0\n25\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n1\n\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n0 0\n\n...\n\n...\n\n94\n\n17\n\n\fPolicy 2: Write-back\n● What happens if we write 94 to address 0000102? (cont’d)\no Now we can finally read in block 93 to the L1 Cache\n\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 0 0\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n1\n\n0 1\n0 111\n1\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n...\n\n...\n\n94\n\n25\n\n18\n\n\fPolicy 2: Write-back\n● What happens if we write 94 to address 0000102? (cont’d)\no Now we can finally read in block 93 to the L1 Cache\no And write 94 into the cache block, also marking it dirty\no Store is finished, so now remove it from pipeline Write Buffer!\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n1\n\n0 1\n0 111\n1\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n...\n\n...\n\n94\n\n25\n\n19\n\n\fPolicy 2: Write-back\n● What happens if we write 94 to address 0000102? (cont’d)\no Eventually, the pending block in L1 Write Buffer will write back\no But this didn’t affect the original store latency\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 1\n0 111\n1\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n...\n\n...\n\n25\n\n20\n\n\fWrite-back: Reads\n● What happens if we read 25 from address 1110102?\no Misses in L1 and L2 caches and must go all the way to Memory\n\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n0 0\n\n...\n\n...\n21\n\n\fWrite-back: Reads\n● What happens if we read 25 from address 1110102?\no Misses in L1 and L2 caches and must go all the way to Memory\no Fills the L2 Cache with 25 on the way back after evicting block 93\n(Note that block 93 can simply be discarded since it’s clean)\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 111\n000\n\nMemory\n25\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n0 0\n\n...\n\n...\n22\n\n\fWrite-back: Reads\n● What happens if we read 25 from address 1110102? (cont’d)\no Now it needs to evict block 94 in L1 Cache before filling with 25\no But block 94 needs to be written back since it’s dirty!\no So move to Write Buffer temporarily to make space.\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 111\n000\n\nMemory\n25\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n0 0\n\n...\n\n...\n23\n\n\fWrite-back: Reads\n● What happens if we read 25 from address 1110102? (cont’d)\no Now L1 Cache can be filled with block 25\n\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n010 1 0\n1 111\n\n010 0 0\n\nMemory\n25\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 1\n0 000\n1\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n...\n\n...\n\n94\n\n24\n\n\fWrite-back: Reads\n● What happens if we read 25 from address 1110102? (cont’d)\no Now L1 Cache can be filled with block 25\no Block 94 will eventually be written back to Memory\no Write buffers in this context are also called victim caches\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n010 1\n0 0 111\n\n001 0 0\n25\n\n010 1 1 111\n\nMemory\n25\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 1\n0 000\n1\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n...\n\n...\n\n94\n\n25\n\n\fImpact of Write Policy on AMAT\n● AMAT = hit time + (miss rate × miss penalty)\n\n● Write-through caches can have a larger write hit time\no With write-back, a read hit and write hit take the same amount of time\no With write-through, a write hit takes the same time as a write miss\n● Write-back caches can have a larger miss penalty\no Due to write allocate policy on write misses\no Due to write-backs of dirty blocks when making space for new block\n● Both issues can be mitigated using write buffers to varying degrees\n● All in all, write-back caches usually outperform write-through caches\no Because write hits are much more frequent compared to misses\n● But write-through sometimes used in L1 cache due to simplicity\no Plenty of L1 → L2 (intra-chip) bandwidth to handle write propagation\no For L3, L3 → DRAM bandwidth cannot support write propagation\n26\n\n\fCache Design Parameter 7:\nBlocking vs. Non-blocking\n\n27\n\n\fBlocking Cache FSM for Write Back Caches\n● FSM must be in Idle state for\ncache to receive new requests\n● While “Memory not Ready”,\nblocks subsequent requests\n→ Called Blocking Cache\n● Write buffer allows cache to\ndefer write-back until later\no Allows quickly return to Idle\n\n● But how about “Memory not\nReady” on Allocate?\n28\n\n\fNon-blocking caches service requests concurrently\nCache\nMiss\n\n● Blocking caches:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\n\nCache Cache Stall on\nMiss Hit\nUse\n\n● Hit under miss:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\n\nCache Cache Stall on\nMiss Miss\nUse\n\n● Miss under miss:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\nMemory Stall\n\n29\n\n\fNon-blocking caches service requests concurrently\nCache Cache Stall on\nMiss Hit\nUse\n\n● Hit under miss:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\n\nCache Cache Stall on\nMiss Miss\nUse\n\n● Miss under miss:\n\nCPU Compute\n\nCPU Compute\n\nMemory Stall\nMemory Stall\n\n● Non-blocking cache allows both to happen\no Allows Memory Level Parallelism (MLP)\no As important to performance as Instruction Level Parallelism (ILP)\n● Miss Status Handling Register (MSHR) table tracks pending requests\n30\n\n\fImpact of non-blocking caches\n● Non-blocking caches do not impact our three cache metrics\no Hit time, miss rate, and miss penalty remain mostly the same\n● Impact is that miss penalty can be overlapped with:\no Computation of instructions not dependent on the miss\no Miss penalties of other memory requests\n● Out-of-order processors are always coupled with a non-blocking cache\no Otherwise, the ability to do out-of-order execution is severely stymied\n\n31\n\n\fCache Design Parameter 8:\nUnified vs. Split\n\n32\n\n\fProblem with Split Caches\n● If cache is split into two (i-cache and d-cache)\no Space cannot be flexibly allocated between data and code\nCode\nI-Cache\n\nData\n\nD-Cache\n\nIf our working\nset looks like\nthis – say, in a\nsmall loop\nthat's accessing\na large array –\nthen we run\nout of data\nspace.\n\nCode\n\nIf our working\nset looks like\nthis – say, in a\nlarge function\nthat's only\nusing stack\nvariables – then\nwe run out of\ncode space.\n\nData\n\n33\n\n\fImpact of Unifying Cache\n● The answer to the problem is to simply unify the cache into one\n\n● AMAT = hit time + (miss rate × miss penalty)\n● Impact of unifying cache on miss rate:\no Smaller miss rate due to more flexible use of space\n● Impact of unifying cache on hit time:\no Potentially longer hit time due to structural hazard\no With split caches, i-cache and d-cache can be accessed simultaneously\no With unified cache, access request must wait until port is available\n● L1 cache is almost always split\no Frequent accesses directly from pipeline trigger structural hazard often\n● Lower level caches are almost always unified\no Accesses are infrequent (filtered by L1), so structural hazards are rare\n34\n\n\fCache Design Parameter 9:\nPrivate vs. Shared\n\n35\n\n\fPrivate vs. Shared Cache\n● On a multi-core system, there are two ways to organize the cache\n\n● Private caches: each core (processor) uses its own cache\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\n● Shared cache: all the cores share one big cache\n\nShared L1$\n\n36\n\n\fShared Cache can Use Space More Flexibly\n● Suppose only 1st core is active and other cores are idle\no How much cache space is available to 1st core? (Shown in red)\n● Private caches: 1st core can only use its own private cache\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\n● Shared cache: 1st core can use entire shared cache!\n\nShared L1$\n\n37\n\n\fBanking: Solution to Structural Hazards\n● Now what if all the cores are active at the same time?\no Won’t that cause structural hazards due to simultaneous access?\n\nShared L1$\n\no Could add more ports, but adding banks is more cost effective\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L1$\n\n▪ Each bank has its own read \/ write port\n▪ As long as two cores do not access same bank, no hazard!\n38\n\n\fBanking: Solution to Structural Hazards\n● Cache blocks are interleaved between banks\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L1$\n\no Blocks 0, 4, 8 … → Bank 0\no Blocks 1, 5, 9 … → Bank 1\no Blocks 2, 6, 10 … → Bank 2\no Blocks 3, 7, 11 … → Bank 3\no That way, blocks are evenly distributed across banks\n▪ Causes cache accesses to also be distributed → less hazards\n\n39\n\n\fShared Cache have Longer Access Times\n● Again, suppose only 1st core is active and other cores are idle\no The working set data is shown in red\n● Private caches: entire working set data in nearby private cache\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\n● Shared cache: data sometimes distributed to remote banks\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L1$\n\n40\n\n\fShared Cache have Longer Access Times\n● Remember this picture?\n\n2\/20\/2017\n\nCS\/COE 1541 term 2174\n\n41\n\n\fImpact of Shared Cache\n● AMAT = hit time + (miss rate × miss penalty)\n\n● Impact of shared cache on miss rate:\no Smaller miss rate due to more flexible use of space\n● Impact of shared cache on hit time:\no Longer hit time due to sometimes having to access remote banks\n● L1 caches are almost always private\no Hit time is important for L1. Cannot afford access to remote banks.\n● L3 (last level) caches are almost always shared\no Reducing miss rate is top priority to avoid DRAM access.\n\n42\n\n\fCache Organization of Broadwell CPU\n● This is the cache organization of Broadwell used in our Linux server\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n● Intel rebrands the shared cache as the “Smart Cache”\n\n43\n\n\fCache Design Parameter 10:\nPrefetching\n\n44\n\n\fPrefetching\n● Prefetching: fetching data that is expected to be needed soon\no Allows you to hide the latency of fetching that data\no E.g. Web browsers prefetch resources from not-yet-clicked links\n→ when user later clicks on link, response is almost instantaneous\no Caches also prefetch data that is expected to be used soon\n▪ Can be used to avoid even cold misses\n● Two ways prefetching can happen:\no Compiler-driven: compiler emits prefetch instructions\n▪ Can manually insert one in C program: __builtin_prefetch(addr)\n▪ Or rely on compiler to insert them using heuristics\no Hardware-driven: CPU prefetcher emits prefetches dynamically\n▪ Relies on prefetcher to detect a pattern in memory accesses\n45\n\n\fHardware Prefetching\n● What do you notice about both these snippets of code?\n● They both access memory sequentially. for(i = 0 .. 100000)\no The first one data, the next instructions.\nA[i]++;\n00 lw\n● These kinds of access patterns are very common.\n00 04 08 0C 10 14 18 1C\n\nSequential\n\n00 04 08 0C 10 14 18 1C\n\nReverse sequential\n\n00 04 08 0C 10 14 18 1C\n\nStrided sequential\n(think \"accessing one field\nfrom each item in an array\nof structs\")\n\n04 lw\n08 lw\n0C addi\n10 sub\n14 mul\n18 sw\n1C sw\n20 sw\n46\n\n\fHardware Prefetching Stride Detection\n● What kinds of things would you need?\n● A table of the last n memory accesses would be a good start.\nn-7\n\nn-6\n\nn-5\n\nn-4\n\nn-3\n\nn-2\n\nn-1\n\nn\n\n40C0\n\n40C4\n\n40C8\n\n40CC\n\n40D0\n\n40D4\n\n40D8\n\n40DC\n\n● Some subtractors\nto calculate the stride\n● Some comparators to\nsee if strides are the same\n● Some detection logic\n\n-\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\nStride Detector\n\n47\n\n\fWhere Hardware Prefetching Doesn’t Work\n● Sequential accesses are where prefetcher works best\no E.g. Iterating over elements of an array\n● Some accesses don’t have a pattern or is too complex to detect\no At below is how a typical linked-list traversal looks like\n\n00 04 08 0C 10 14 18 1C 20 24 28 2C 30 34 38 3C\n(colors are different cache blocks)\n\no Other pointer-chasing data structures (graphs, trees) look similar\no Can only rely on naturally occurring locality to avoid misses\no Or, have compiler insert prefetch instructions in middle of traversal\n48\n\n\fMystery Solved\n\n● How come Array performed well for even an array 1.28 GB large?\no No spatial locality since each node takes up two 64-byte cache blocks\no No temporal locality since working set of 1.28 GB exceeds any cache\n\n● The answer is: Array had the benefit of a strided prefetcher!\no Access pattern of Linked List was too complex for prefetcher to detect\n49\n\n\fImpact of Prefetching\n● Prefetcher runs in parallel with the rest of the cache hardware\no Does not slow down any on-demand reads or writes\n● What if prefetcher is wrong? It can be wrong in two ways:\no It fetched a block that was never going to be used\no It fetched a useful block but fetched it too soon or too late\n▪ Too soon: the block gets evicted before it can be used\n▪ Too late: the prefetch doesn’t happen in time for the access\n● A bad prefetch results in cache pollution\no Unused data is fetched, potentially pushing out other useful data\n● On the other hand, good prefetches can reduce misses drastically!\n\n50\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":212,"segment": "unlabeled", "course": "cs1550", "lec": "lec05", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Lectures and recitations are back in-person\n\n• Upcoming deadlines\n• Homework 2 is due next Monday at 11:59 pm\n• Lab 1 is due on 2\/4 at 11:59 pm\n• Project 1 is due on 2\/18 at 11:59 pm\n• Explained in this week’s recitations\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious Lecture …\nThree-usage problems of Semaphores\n• compromising mutual exclusion\n• Solution: Mutex\n\n• deadlock\n• Solution: Not yet discussed\n\n• priority inversion\n• Solution: priority inheritance\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (basic concepts)\n• Deadlock\n\n• mutual exclusion\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (semaphore)\n• What the semaphore value means.\n• Can we see a process that utilizes a\nsemaphore\/spinlock and trace it?\n\n• Where we move after down is called\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (Mutex)\n• With mutex, what happens when a process tries to\nup() when it hasn't down()'d, does it skip over the\ninstruction or does it do something else?\n• how does a mutex semaphore verify that the same\nprocess that called down on it in the first place is the\none that called up on it?\n\n• Can a mutex have a negative value or is it only 0 or\n1?\n• Advantages\/disadvantages of Mutex\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fMuddiest Points (priority inheritance)\n• What is priority inheritance?\n• I'm confused on what the solution for priority\ninversion is or did we not mention it yet?\n\n• Does Priority Inheritance solve the Deadlock and\nStarvation problems along with the Priority Inversion\nproblem?\n• Why running P1 in the priority inversion question not\ndown the semaphore?\n• Does priority inversion (of semaphores) cause the\nlower process to run instead of the higher priority\nprocess indefinitely or does this resolve itself? If so\nhow?\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fSemaphore Usage Problem: Priority Inversion\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fMuddiest Points (forking 1)\n• if fork creates copies of the process that called it, how\ncan you tell which copy has priority to run first?\n• child processes & where they pick up in the code\n• Not exactly sure how semaphores are inherited between\nprocesses\n• In the last example, when p1 calls fork and creates the\nchild p3, will p1’s PCB be copied to p3, which means the\ncontent of p0’s pcb will be in p3 too?\n• What is a PID and how is it related to the return value of\nfork()?\n• P3 is forked and created before P2 so why is it named\nP2? Is it simply renamed after P1 gets forked again?\n• Is the order of possible outcomes from a forking program\nthe permutation of any of the leaf nodes from a forking\ntree?\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\ffork()’s of fork()’s\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fMuddiest Points (forking 2)\n• does the fork return zero for all children?\n• Somewhat confused about what order the various forks\nwill actually form. I see why and from where they form,\nbut not how it decides which one to run\n• how are all processes a fork of another process? i know\nthe root of the tree is another process in the OS, but i\ndon't get how for example a program you create and\nexecute is a fork of something else.\n• the forking process with child processes. The\nrelationship between the number of child processes\ncreated and the number of times fork() is called by the\nparent process\n• what int fork returns for the child and parent process\n• practical uses of forking?\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fToday’s Problem\/Question\n\nHow are processes created and terminated?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fProcess Lifecycle (AKA Process States)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fProcess Creation\n• Via fork() syscall\n\n• Parent process: the process that calls fork()\n• Child process: the process that gets created\n• Memory of parent process copied to child process\n• Too much copying\n\n• Even not necessary sometimes\n• e.g., fork() followed by exec() to run a different program\n\n• Optimization trick:\n• copy-on-write\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\ffork() example\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fProcess Termination\n• Via exit(), abort(), or kill() syscalls\n\n• The parent process may wait for termination of a\nchild process by using the wait()system call.The\ncall returns status information and the pid of the\nterminated process\npid = wait(&status);\n• When a process terminates\n\n• If no parent waiting (did not invoke wait()) process is a\nzombie\n• If parent terminated without invoking wait , process is an\norphan\n• adopted by the init process\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fOrphan vs. Zombie Processes\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fBenefits of Orphan Processes\n• Allow a long-running job to continue running even\nafter session (e.g., ssh connection) ends.\n• The nohup command does that\n\n• Create daemon processes\n• Long-running background processes adopted by the init\nprocess.\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fThread Synchronization\nSynchronization issues apply to threads as well\n• Threads can share data easily (same address space)\n• Other two issues apply to threads\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fProcess vs. Thread\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":213,"segment": "unlabeled", "course": "cs1550", "lec": "lec14", "text":"Introduction to Operating Systems\nCS\/COE 1550\nFall 2021\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Homework 7: due on 3\/14\n• Homework 8: due on 3\/21\n• Project 2: due on 3\/18\n\n• Midterm exam on Thursday 3\/3\n• In-person, on paper, closed book\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• CPU scheduling\n• Multi-level Feedback Queues\n• Service time estimation for SJF\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (multi-level feedback queue)\n• in multilevel scheduling, do processes in the lower\npriority ready queues need to wait for every higher\npriority ready queue to be empty?\n• How many queues would there realistically be using\nMultilevel feedback scheduling?\n• I need some time to build an intuition for why the\nMultilevel Feedback Scheduling is good.\n• The most muddy was how the MFS has absolute priority.\n• In multilevel feedback scheduling, is the number of\npriority queues pre-set or is each queue generated\ndynamically?\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (exponential averaging)\n• The graphs for CPU Burst Exponential estimation\n\n• Could you please explain why changing the value of\nalpha weights more recent \/ older observations?\n• determining or choosing the alpha value\n• what does exponential average reflect\n\n• Does S mean the all previous estimate? What is the\ndifference between the most recent and S[n]\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (misc.)\n• CPU Burst vs Quantum vs Time Slice?\n\n• The whole lecture\n• active time in the scheduler\n• Everything is clear\n• Nothing felt good.\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fIn an ideal world…\n◼\n\nThe ideal world has memory that is\nVery large\n◼ Very fast\n◼ Non-volatile (doesn’t go away when power is turned\noff)\n◼\n\n◼\n\nThe real world has memory that is:\nVery large\n◼ Very fast\n◼ Affordable!\nPick any two…\n◼\n\n◼\n\nMemory management goal: make the real world\nlook as much like the ideal world as possible\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fMemory hierarchy\n◼\n\nWhat is the memory hierarchy?\nDifferent levels of memory\n◼ Some are small & fast\n◼ Others are large & slow\n◼\n\n◼\n\nWhat levels are usually included?\n◼\n\nCache: small amount of fast, expensive memory\nL1 (level 1) cache: usually on the CPU chip\n◼ L2 & L3 cache: off-chip, made of SRAM\n◼\n\nMain memory: medium-speed, medium price memory (DRAM)\n◼ Disk: many gigabytes of slow, cheap, non-volatile storage\n◼\n\n◼\n\nMemory manager handles the memory hierarchy\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fProblem of the Day\n\nHow can we share computer’s memory between\nmultiple processes?\nHow can we protect each process’s memory\npartition from other processes?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fHow many programs is enough?\nSeveral memory partitions (fixed or variable size)\n◼ Lots of processes wanting to use the CPU\n◼ Tradeoff\n◼\n\nMore processes utilize the CPU better\n◼ Fewer processes use less memory (cheaper!)\n◼\n\n◼\n\nHow many processes do we need to keep the\nCPU fully utilized?\nThis will help determine how much memory we need\n◼ Is this still relevant with memory costing $10\/GB?\n◼\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fWhy do we need more processes?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fModeling multiprogramming\n◼\n\nMore I\/O wait means\nless processor\nutilization\nAt 20% I\/O wait, 3–4\nprocesses fully utilize\nCPU\n◼ At 80% I\/O wait, even 10\nprocesses aren’t enough\n\n1\n0.9\n\n◼\n\n0.7\n\nCPU Utilization\n\nThis means that the OS\nshould have more\nprocesses if they’re I\/O\nbound\n◼ More processes =>\nmemory management &\nprotection more\nimportant!\n◼\n\n0.8\n\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\nDegree of Multiprogramming\n80% I\/O Wait\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n50% I\/O Wait\n\n20% I\/O Wait\n\n12\n\n\fBasic memory management\n◼\n\nComponents include\nOperating system (perhaps with device drivers)\n◼ Single process\n◼\n\n◼\n\nGoal: lay these out in memory\nMemory protection may not be an issue (only one program)\n◼ Flexibility may still be useful (allow OS changes, etc.)\n◼\n\n◼\n\nNo swapping or paging\n0xFFFF\n\n0xFFFF\nUser program\n(RAM)\n\n0\n\nOperating system\n(RAM)\n\nOperating system\n(ROM)\n\nUser program\n(RAM)\n\nDevice drivers\n(ROM)\nUser program\n(RAM)\nOperating system\n(RAM)\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n0\n13\n\n\fMemory Management for Embedded Systems\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fFixed partitions: multiple programs\n◼\n\nFixed memory partitions\nDivide memory into fixed spaces\n◼ Assign a process to a space when it’s free\n◼\n\n◼\n\nMechanisms\nSeparate input queues for each partition\n◼ Single input queue: better ability to optimize CPU usage\n◼\n\n900K\nPartition 4\nPartition 3\nPartition 2\n\nPartition 4\n700K\n600K\n500K\n\nPartition 1\nOS\n\n900K\n\nPartition 3\nPartition 2\n\n700K\n600K\n500K\n\nPartition 1\n\n100K\n0\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\nOS\n\n100K\n0\n15\n\n\fProblem of the Day\n\nHow can we share computer’s memory between\nmultiple processes?\nHow can we protect each process’s memory\npartition from other processes?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fBase and limit registers\n◼\n\nSpecial CPU registers: base\n& limit\n\n0xFFFF\n\nAccess to the registers limited\nto kernel (privileged) mode\n◼ Registers contain\n\n0x2000\n\n◼\n\nLimit\nProcess\npartition\n\nBase: start of the process’s\nmemory partition\n◼ Limit: length of the process’s\nmemory partition\n◼\n\n◼\n\nBase\n0x9000\n\nAddress generation\nPhysical address: location in\nactual memory\n◼ Logical address: location from\nthe process’s point of view\n◼ Physical address = base +\nlogical address\n◼ Logical address larger than\nlimit => error\n◼\n\nOS\n0\n\nLogical address: 0x1204\nPhysical address:\n0x1204+0x9000 = 0xa204\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fVirtual and physical addresses\n◼\n\nAddresses local to the process\n◼ Hardware translates virtual\naddress to physical address\n◼\n\nCPU chip\nCPU\n\nProgram uses virtual\naddresses\n\nMMU\n◼\n\nVirtual addresses\nfrom CPU to MMU\n\nTranslation done by the\nMemory Management Unit\nUsually on the same chip as the\nCPU\n◼ Only physical addresses leave the\nCPU\/MMU chip\n◼\n\nMemory\n◼\n\nPhysical addresses\non bus, in memory\n\nPhysical memory indexed by\nphysical addresses\n\nDisk\ncontroller\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fAddress Translation\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fVirtual memory\nBasic idea: allow the OS to hand out more\nmemory than exists on the system\n◼ Keep recently used stuff in physical memory\n◼ Move less recently used stuff to disk\n◼ Keep all of this hidden from processes\n◼\n\nProcesses still see an address space from 0 – max\naddress\n◼ Movement of information to and from disk handled by\nthe OS without process help\n◼\n\n◼\n\nVirtual memory (VM) especially helpful in\nmultiprogrammed system\n◼\n\nCPU schedules process B while process A waits for its\nmemory to be retrieved from disk\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\fPaging and page tables\n◼\n\nVirtual addresses mapped to\nphysical addresses\n◼\n◼\n\n◼\n\n◼\n\nTable translates virtual page\nnumber to physical page number\n◼\n\n◼\n\n◼\n\nUnit of mapping is called a page\nAll addresses in the same virtual\npage are in the same physical\npage\nPage table entry (PTE) contains\ntranslation for a single page\n\nNot all virtual memory has a\nphysical page\nNot every physical page need be\nused\n\nExample:\n◼\n◼\n\n64 KB virtual memory\n32 KB physical memory\n\n60–64K\n56–60K\n52–56K\n48–52K\n44–48K\n40–44K\n36–40K\n32–36K\n28–32K\n24–28K\n20–24K\n16–20K\n12–16K\n8–12K\n4–8K\n0–4K\n\n-\n\n6\n5\n1\n\n-\n\n3\n\n0\n4\n7\n\nVirtual\naddress\nspace\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n28–32K\n24–28K\n20–24K\n16–20K\n12–16K\n8–12K\n4–8K\n0–4K\n\nPhysical\nmemory\n\n21\n\n\fWhat’s in a page table entry?\n◼\n\nEach entry in the page table contains\n◼\n\nValid bit: set if this logical page number has a corresponding physical frame in\nmemory\n◼\n\n◼\n◼\n◼\n\n◼\n\nIf not valid, remainder of PTE is irrelevant\n\nPage frame number: page in physical memory\nReferenced bit: set if data on the page has been accessed\nDirty (modified) bit :set if data on the page has been modified\nProtection information\n\nProtection\n\nDirty bit\n\nD\n\nR\n\nV\n\nReferenced bit\n\nPage frame number\n\nValid bit\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fImplementing page tables in hardware\nPage table resides in main (physical) memory\n◼ CPU uses special registers for paging\n◼\n\nPage table base register (PTBR) points to the page table\n◼ Page table length register (PTLR) contains length of page table:\nrestricts maximum legal logical address\n◼\n\n◼\n\nTranslating an address requires two memory accesses\nFirst access reads page table entry (PTE)\n◼ Second access reads the data \/ instruction from memory\n◼\n\n◼\n\nReduce number of memory accesses\nCan’t avoid second access (we need the value from memory)\n◼ Eliminate first access by keeping a hardware cache (called a\ntranslation lookaside buffer or TLB) of recently used page table\nentries\n◼\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":214,"segment": "unlabeled", "course": "cs1622", "lec": "lec08","text":"Scoping and Naming\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● how’s project 2 coming along (:\n\n2\n\n\fSymbol Tables and\nScope Trees\n\n3\n\n\fScoping it out\n● the first phase of semantic analysis is name-checking, which:\no matches names to the things they refer to; and\no checks for scoping violations.\n▪ remember that the scope of a name is where it can be seen.\n● so how do we do this stuff?\no well, like many other things we've talked about, it's about\nformalizing our intuition: taking rules that we kind of know\nalready, and turning them into an algorithm.\n● we also need a data structure or two…\no we need to remember where each name is declared\no and we need some mapping from names to referents\n● so let’s start with…\n\n4\n\n\fSymbol tables\n● a symbol table maps from names to the \"things\" they refer to.\no symbol means anything that can have a name: variables,\nfunctions, classes, modules, packages, interfaces, macros, etc. etc.\n● as you declare things, they are added to the symbol table.\nName\nReferent\nclass A {\n\"A\"\n<class A>\nstatic int x = 5;\nstatic void main() {\n\"x\"\n<int x>\nfoo(x);\n\"main\"\n<void main()>\n}\n\"foo\"\n<void foo()>\nstatic void foo(int y) {\nS.o.println(y);\n\"y\"\n<int y>\n}\n}\nbut you can't see y from main… so should it be in this table?\n5\n\n\fInstead of ONE symbol table…\n● we nest the symbol tables in a tree. there's one for each scope.\nclass A {\nstatic int x = 5;\nstatic void main() {\nfoo(x);\n}\n\nstatic void foo(int y) {\nS.o.println(y);\n}\n}\n\nthis outermost (global) scope\nis where e.g. System lives. A is\ninserted there too.\nthe class A gets a scope to\nhold its members, like its\nvariables and methods. this\nscope is a child of the\nglobal scope.\n\nthen each method gets its\nown scope, both of which\nare children of A's scope.\n6\n\n\fThe scope tree data structure\n● it's a tree, where each node is a scope, which has a symbol table.\n● the symbol tables map from names to the nodes in the AST which\ndefine those names.*\n\"The AST\"\n<global scope>\n\n\"A\"\n\"System\"\n\nA's scope\n\nmain's scope\n\nclass A {\nstatic int x = 5;\nstatic void main() {\nfoo(x);\n\n\"x\"\n\"main\"\n\n}\n\n\"foo\"\n\nstatic void foo(int y) {\nS.o.println(y);\n\n<empty>\n\n}\n}\n\nfoo's scope\n\n\"y\"\n7\n\n\fParent scopes\n● each child scope also needs to know which scope is its parent.\no (this is used in the name resolution algorithm.)\n● that means we've got bidirectional links between the scopes.\n<global scope>\n\nA's scope\n\nmain's scope\n\nfoo's scope\n\nthis is going to have implications\nfor the way we implement this\ndata structure in Rust…\nbut then each scope has a symbol\ntable which somehow points into the\nAST? how do we deal with that?\nmaybe we could integrate\nthis into the AST??\nWELL, NO……..\n\n8\n\n\fSyntax, not semantics\n● since the scopes more or less follow the syntax ({} ≅ scope)…\no we might be tempted to jam the scope tree into the AST somehow.\no this is a really bad idea.\n● the AST is an abstract syntax tree, but we're moving beyond syntax.\no the relationships established by name resolution can crisscross the\nwhole program and form any shape of graph, including cycles.\no and as we get further into semantic analysis – and then into\noptimization and code generation – the syntax will become less\nand less relevant.\n● so, the scope tree data structure should exist in parallel to the AST.\no we'll represent the links between the two data structures in an\nindirect way, as we'll see…\n\n9\n\n\fName Resolution\ntime check ≤ 20\n\n10\n\n\fDeclaration vs. use\n● when we declare classes, variables, functions etc. that makes the\nname available for use by other pieces of code.\no a declaration essentially inserts a name into a symbol table.\n\nint x = 0;\nx = x + 1;\nprintln(x);\nx = f();\nreturn x;\n\nthis is the declaration of x. it puts x\ninto this scope’s symbol table.\nall of these other places where we\nrefer to x by name are called uses.\n\nfor each of these uses, we have to perform name\nresolution: determining which symbol the use refers to.\n\nthe scope tree\/symbol tables and the scoping rules are\nwhat we'll use to implement name resolution!\n11\n\n\fClimbing the tree (animated)\n● the name resolution algorithm is actually pretty straightforward.\n<global scope>\n\n\"A\"\n\"System\"\n\nA's scope\n\n\"x\"\n\"main\"\n\"foo\"\n\nfoo's scope\n\n3. go to the parent scope of\nthat one. is System there?\nyes! name resolved.\n\n2. go to the foo's parent\nscope. is System there?\n\n\"y\"\n\nstatic void foo(int y) {\nSystem.out.println(y);\n}\n\nstill no?\n\n1. is System declared in foo's scope?\n\nthis use of System appears in foo's scope. let's start there.\n\nno?\n\n12\n\n\fWhat if it fails?\n● what if we get up to the global scope and there's no match?\no well, that's an error. that's when your compiler says \"unresolved\nsymbol\" or \"undefined name\" or whatever.\n● but it could \"fail\" in another way:\nint x = System + 5;\no assuming I didn't declare a variable named System…\no this is nonsense. how do you add a class and an integer?\n● well, don't worry about it. not at this stage, anyway.\no name resolution only cares about matching names to the things\nthat declared them.\no this example is a type error and will be caught further on during a\nsubsequent phase of semantic analysis.\n\n13\n\n\fKeeping track of resolved names\n● we could perform name resolution every time we need to know the\nreferent during subsequent compiler phases.\no but real-world experience (by other people) has shown that these\nresolutions happen a lot and that can waste a lot of time.\n● since the referent never changes, it makes sense to remember that.\n\nint x = 0;\nx = x + 1;\nprintln(x);\nx = f();\nreturn x;\n\nwe can do this by keeping a map which maps\nfrom the AST nodes which use a name to the\nnode which declares it. this is yet another data\nstructure that I call the use map.\n\nit’s a bit like drawing arrows all over the code.\nagain, we'll see how to implement this soon…\n14\n\n\fAccessing names inside things\n● this piece of code presents another interesting scenario:\nSystem.out.println(y);\nis this also a name resolution??\nwell, yes! this is saying \"access out from the System scope.\"\nbut there's a complication: we may not be able to\nperform the resolution until after we do typechecking:\nobj.x = 10;\n\nhere, we have to know what type obj is to\nknow if x is a valid name inside of that type.\n\nthen we will have to check that the type of x is actually valid\nfor this assignment! wow! very circular!!\nfortunately this is a relatively limited situation that can be resolved later…\n15\n\n\fForward\/mutual references\n● a forward reference is when the use of a name occurs before its\ndeclaration in the source code.\nclass A { B b; } this is a forward reference, because B is\nclass B {}\ndeclared later.\nif we did all the name stuff in one pass, we would get an\nerror here because B doesn’t exist in the symbol table yet!\nthis means we have to do name stuff in two passes:\non the first pass, we build the scope tree and insert\ndeclarations into each scope’s symbol table;\n\nthen on the second pass, we can do the name\nresolution using the already-built symbol tables.\n16\n\n\f…but that doesn’t work for locals\n● consider this code:\n\nstatic void func() {\nyou know that this is wrong. but why?\nS.o.println(x);\nint x = 10;\n}\nthe code in a function executes in order, including local\nvariable declarations. so on the first line, x has no value yet.\nthere are two ways to solve this:\n1. introduce a new scope that begins after each local\ndeclaration and ends at the enclosing close-brace; or\n2. do the name resolution at the same time as scopebuilding, but only for the code within functions.\n17\n\n\fAnnotating the AST\ntime check ≤ 55\n\n18\n\n\fThrowing arrows all over the place\n● we've now got two things that want to draw arrows all over the AST:\no symbol tables want to point at declarations\no name resolution wants to make AST nodes point at other ones\n● in a language like Java, \"arrows\" are object reference variables, and\nwe might be tempted to start throwing those everywhere.\no “add a field to Identifier AST nodes so they can point at the\ndeclarations they refer to!”\no “add fields to { Block Statement } AST nodes to hold scope info!”\n● OOP makes it really easy to start doing this…\no but it's very difficult to stop once you've started, because it\ncreates tight coupling between the two \"ends\" of the arrow.\n● since we'll only be adding more info from now on (typechecking!), it\nmakes sense to reconsider how we \"add information\" to the AST.\n19\n\n\fNametags, not arrows\n● first, let's make a small change to our AST.\n● every AST node is given a unique identifier when it is created.\no this can be as simple as an int that's incremented for each node.\nf(y + 2)\n\nthis lets us create relationships\nwithout adding any arrows.\n\nCallExp:5\ncallee\n\nargs[]\n\nIdentExp:1\n\nAddExp:4\n\nname\n\nlhs\n\n\"f\"\n\nlet's say the name f resolves to\na function declared by node 49.\nthen in our name resolution\nmap, we add an entry with a\nkey of 1 and a value of 49.\n\nrhs\n\nIdentExp:2\n\nIntExp:3\n\nname\n\nvalue\n\n\"y\"\n\n2\n\n20\n\n\fWh.. but… why? Why not just point to the node?\n● decoupling is good practice. it gives you a cleaner design.\n● we can add arbitrarily many pieces of data to an AST node, without\nhaving to change the AST node types at all!\no we just add more maps that use the node IDs as keys.\no this lets us add new compilation passes and modularize the\ncompiler much more easily.\n● it improves compiler compile times by not requiring a recompile of\nthe entire compiler every time you change the AST type.\no and real compilers can be massive, so this is a big timesaver!\n● it can improve performance by exploiting physical locality.\no the cache in your CPU likes arrays and small structs.\no the AST node struct stays small, meaning it'll likely fit into cache.\no associated data can even be stored in a contiguous array.\n21\n\n\fAnother reason\n● we are also nudged towards this design by Rust.\nin Java, each object reference is\nan arrow, and there are no\nrestrictions on them.\nclass Node { Node[] others; }\n1\n\n2\n\n3\n\nin Rust, there are multiple kinds\nof pointers, but Box cannot be\nused to make cycles.\nstruct Node { others: Vec<Box<Node>> }\n1\n\n5\n\n4\n\n2\n\n3\n\n5\n\n4\n\nthis \"no cycles allowed\" thing is a central part of Rust's\nownership, borrowing, and memory safety semantics.\n\n22\n\n\fWhat??!? You can't have cycles in Rust?\n● well, you can, but it's discouraged by the language and libraries.\no Rc<T> is another kind of pointer that, with some care, can let you\nrepresent certain kinds of cyclical data structures.\n● it's not the Rust designer's fault that they made it harder to represent\ncyclical data structures.\no it's that cyclical data structures are just weird, and a lot of other\nlanguages don't really make that fact obvious.\no they either hide it, or don’t give you the tools to manage it!\n● this restriction does give Rust some great advantages in memory\nsafety and multithreaded code correctness!\no which is kind of Rust's whole deal\n● but it does mean you have to stretch your mind a little.\no and interestingly, many of the solutions around this restriction end\nup being faster and easier to reason about.\n23\n\n\fImplementation Details\ntime check ≤ 85\n\n24\n\n\fWell Actually\n● in practice, it’s a good idea to represent symbols as objects.\nclass A {\nstatic int x = 5;\nstatic void main() {\nfoo(x);\n}\nstatic void foo(int y) {\nS.o.println(y);\n}\n}\n\nID\n\nAST\n\nScope Name\n\nKind\n\n…\n\n1\n\n3\n\n0\n\nA\n\nclass\n\n…\n\n2\n\n7\n\n1\n\nx\n\nstatic var\n\n…\n\n3\n\n12\n\n1\n\nmain\n\nstatic func\n\n…\n\n4\n\n22\n\n1\n\nfoo\n\nstatic func\n\n...\n\n5\n\n24\n\n3\n\ny\n\nlocal var\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\nevery name declared in the program gets its own\nsymbol object with a unique symbol ID.\nextracting this info from the AST is time-consuming, so we only\ndo it once. these are what the symbol tables actually refer to.\n\n25\n\n\fBidirectionally linked scopes\n● an easy way to solve this is to keep all scopes in a vector, and have\nthem refer to each other by their index into the vector.\n<global scope>\nparent:\nNone\nchildren: [1]\nsymbols:\n\"A\"\n\n1\n\n\"System\"\n\n33\n\nindex:\n\n0\n\nA's scope\nparent:\nSome(0)\nchildren: [2, 3]\nsymbols:\n\"x\"\n\n2\n\n\"main\"\n\n3\n\n\"foo\"\n\n4\n\n1\n\nthis neatly dodges\nthe \"no cycles\" rule,\nand in practice isn’t\nmuch harder to use\nthan direct pointers\n(Box<T>).\n…\n\n2\n\n…\n\n3\n26\n\n\fThe declaration and use maps\n● since we now represent symbols as their own “thing,” the declaration\nmap maps from AST nodes that declare a symbol, to that symbol.\n● the use map maps from AST nodes to the symbol they use.\n\nint x = 0;\nx = x + 1;\nprintln(x);\nx = f();\nreturn x;\nthe actual maps\nmap from Node IDs\nto Symbol IDs.\n\nID\n\nAST\n\n…\n\n…\n\n…\n\n17\n\n54\n\n…\n\n…\n\nDecls\n\nScope Name\n\nKind\n\n…\n\n…\n\n…\n\n…\n\n3\n\nx\n\nlocal var\n\n…\n\n…\n\n…\n\n…\n\n…\n\nUses\n\nAST\n\nSym\n\nAST\n\nSym\n\n74\n\n17\n\n76\n\n17\n\n…\n\n…\n\n78\n\n17\n\n…\n\n…\n27\n\n\fBundling it all up\n● the symbols, scope tree, declaration map, and use map are all\npackaged into one big object, the name context.\n● this can be used in later phases of the compiler!\nAST\n\nName\nchecking\n\nname context\nAST\n\nType\nchecking\n\ntype\ncontext\n…?\n\neach phase of semantic analysis generates more information\nabout the program which later phases use.\nfor now, the AST will remain the central representation of\nthe program, but that may change later in compilation…\n28\n\n\fBuilding the scope tree and symbol tables\n● the actual algorithm for building the scope tree is surprisingly simple.\n● the idea is like this:\no we recursively visit each node in the AST.\no if we take a function declaration node as an example…\n▪ it will create a new scope as a child of the current scope…\n▪ then visit its children (arguments and code) within that scope.\no something similar happens for { Block Statements }, classes etc.\no and when we see a declaration AST node, we insert a symbol into\nthe current scope.\n▪ this also inserts an entry into the declaration map.\n● and that’s about it!\no there’s the “forward reference” thing we talked about, and the stuff\nabout local variables, but those are just tweaks to this algorithm.\n29\n\n\fName resolution and building the use map\n● finally, name resolution is another recursive AST visit.\n● if this is done simultaneously with building the scope tree…\no it’s very simple: whenever you see an Identifier AST node, you\nperform name resolution starting at the current scope.\no if the name exists, add an entry to the use map.\no if it doesn’t, it’s an error.\n● if you built the scope tree in one pass and then do name resolution\nin a second pass after it…\no there’s some bookkeeping to make sure you start the name\nresolution in the correct scope, but it’s otherwise the same.\n● and at the end, you now have all your data structures built and\nname-checking is complete!\no name-checking is the easiest semantic analysis pass, and things\nonly get harder from here (:\n30\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":215,"segment": "unlabeled", "course": "cs1541", "lec": "lec2.6_superscalar_processors","text":"SuperScalar Processors\nCS 1541\nWonsun Ahn\n\n\fIn-order vs. Out-of-order superscalars\n● Superscalar: a wide-issue processor that does dynamic scheduling\no Extracts instruction level parallelism (ILP) within the processor\n● In-order superscalar: does not reorder instructions\no Only detects hazards between instructions to insert bubbles\no Only extracts ILP that arises from given ordering of instructions\no The processor simulated in Project 1\n● Out-of-order superscalar: does reorder instructions\no Reorders instructions to remove hazards and increase utilization\no Typically results in higher performance compared to in-order\no But dynamic reordering consumes lots of power\n● Out-of-order sounds more exciting so let’s talk about that\n2\n\n\fName of the game is still ILP\n● The processor internally constructs the data dependency graph\n● The processor tries to take advantage of ILP as much as possible\no By executing the red nodes in parallel with the blue nodes\n\n3\n2\n\n1\n\n2\n\n1\n\n1\n2\n\n1\n1\n\n2\n\ntotal 7 cycles\n\nillustration courtesy of Dr. Melhem\n3\n\n\fInstruction Queue\n\nRegister\nFile\n\nInstruction Queue\n\nI-Mem\n\nIns. Decoder\n\n● In order to expose ILP, superscalars need a big instruction window\no Just like the compiler did for VLIWs\no HW structure for storing instructions is called instruction queue\no Now EX stage has a big pool of instructions to choose from\n\nALU\n\n+\n\nD-Mem\n\n4\n\n\fInstruction Queue\n\nInstruction Queue\n\n● In order to expose ILP, superscalars need a big instruction window\no Just like the compiler did for VLIWs\no HW structure for storing instructions is called instruction queue\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n\n● At ID, instructions are decoded\no And dispatched to the i-queue\n● At EX, ready instructions are chosen\nfrom the instruction queue\no Ready as in operands are available\no And issued to an EX unit\n● Typically queue is always full\no Insts start queueing up when insts\nfail to issue due to hazards\n5\n\n\fScheduling the Instruction Queue\n\n[Dispatched]\nli t0, 1\nli t1, 2\nadd t2, t0, t1\n\nInstruction Queue\n\n● Now we have pool of instructions. When do they become ready?\no Ready operands and instructions are in green\no Not ready operands and instructions are in red\nLoad\/\nStore\n\n[Issued]\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\n\nFloat\nALU\n6\n\n\fScheduling the Instruction Queue\n\n[Dispatched]\nli t0, 1\nli t1, 2\nadd t2, t0, t1\n\nInstruction Queue\n\n● Initially both li t0, 1 and li t1, 2 are ready\no The li instruction does not have any register operands\no Instruction scheduler has a choice of what to issue\nLoad\/\nStore\n\n[Issued]\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\n\nFloat\nALU\n7\n\n\fScheduling the Instruction Queue\n\n[Dispatched]\nli t0, 1\nli t1, 2\nadd t2, t0, t1\n\nInstruction Queue\n\n● Let’s say the scheduler issues li t1, 2 first\n● Then the t1 operand becomes ready after it completes\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\n[Issued]\nli t1, 2\n\nInt ALU 2\n\nFloat\nALU\n8\n\n\fScheduling the Instruction Queue\n\n[Dispatched]\nli t0, 1\nadd t2, t0, t1\n\nInstruction Queue\n\n● Now the only ready instruction li t0, 1 issues\n● Then the t0 operand becomes ready after it completes\n● Now add t2, t0, t1 is finally ready to issue\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\n[Issued]\nli t1, 2\nli t0, 1\n\nInt ALU 2\n\nFloat\nALU\n9\n\n\fScheduling the Instruction Queue\n\n[Dispatched]\n\nInstruction Queue\n\n● And we are done!\n\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\n\n[Issued]\nli t1, 2\nli t0, 1\nadd t2, t0, t1\n\nFloat\nALU\n10\n\n\fScheduling the Instruction Queue\n\n[Dispatched]\nli t0, 1\nli t1, 2\nadd t2, t0, t1\n\nInstruction Queue\n\n● Note how we reordered li t0, 1 and li t1, 2\no There are no dependencies between the two, so no issues\no Also, RAW dependency with add t2, t0, t1 was enforced\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\n\n[Issued]\nli t1, 2\nli t0, 1\nadd t2, t0, t1\n\nFloat\nALU\n11\n\n\fWhat if we had a WAW dependency?\n\n[Dispatched]\nli t0, 1 WAW!\nli t0, 2\nadd t2, t0, t0\n\nInstruction Queue\n\n● Reordering li t0, 1 and li t0, 2 still allowed (both are ready)\no Now t2 = 4 in original code, but t2 = 2 during execution!\no How do we disallow this from happening?\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\n\n[Issued]\nli t0, 2\nli t0, 1\nadd t2, t0, t0\n\nFloat\nALU\n12\n\n\fWAW and WAR dependencies are tricky\n● RAW (true) dependencies are automatically enforced\no Instructions cannot issue until all operands are ready (written)\n● WAW and WAR dependencies are not enforced\no There is no data passing between the two instructions\no The two instructions can become ready in any order\n● We could somehow enforce WAW and WAR dependencies\no But there is a better solution: register renaming!\no Remember? That’s what the compiler did to remove WAW\/WAR.\n\n13\n\n\fRegister Renamer and the RAT\n\n[Decoded]\nli t0, 1 WAW!\nli t0, 2\nadd t2, t0, t0\n\nAll 32 MIPS registers\n\nRegister\nRenamer\nRAT\n\nt0\n\np1\n\nt1\n\n-\n\nt2\n\np2\n\n…\n\n…\n\n[Dispatched]\nli p0, 1 No\nli p1, 2 WAW!\nadd p2, p1, p1\n\nInstruction Queue\n\n● As soon as decode, Register Renamer renames all registers\no Done with the help of the Register Alias Table (RAT)\no RAT is current mapping between architectural and physical registers\n▪ Architectural registers: Registers in ISA used in programs (t0, t1, t2, …)\n▪ Physical registers: Renamed registers used in processor (p0, p1, p2, …)\n\n14\n\n\fRegister Renamer and the RAT\n● So how does the RAT work?\n● Initially, no assignments have been done, so mapping is empty.\n\nRegister\nRenamer\nRAT\n\nAll 32 MIPS registers\n\nt0\n\n-\n\nt1\n\n-\n\nt2\n\n-\n\n…\n\n…\n\n[Dispatched]\n\nInstruction Queue\n\n[Decoded]\n\n15\n\n\fRegister Renamer and the RAT\n\n[Decoded]\nli t0, 1\n\nRegister\nRenamer\nRAT\n\nAll 32 MIPS registers\n\nt0\n\np0\n\nt1\n\n-\n\nt2\n\n-\n\n…\n\n…\n\n[Dispatched]\nli p0, 1\n\nInstruction Queue\n\n1. li t0, 1 is decoded, destination t0 is renamed to p0\n\n16\n\n\fRegister Renamer and the RAT\n\n[Decoded]\nli t0, 1\nli t0, 2\n\nRegister\nRenamer\nRAT\n\nAll 32 MIPS registers\n\nt0\n\np1\n\nt1\n\n-\n\nt2\n\n-\n\n…\n\n…\n\n[Dispatched]\nli p0, 1\nli p1, 2\n\nInstruction Queue\n\n1. li t0, 1 is decoded, destination t0 is renamed to p0\n2. li t0, 2 is decoded, destination t0 is renamed to p1\no Remember the single assignment rule?\no A new value always gets a new register\n\n17\n\n\fRegister Renamer and the RAT\n\n[Decoded]\nli t0, 1\nli t0, 2\nadd t2, t0, t0\n\nAll 32 MIPS registers\n\nRegister\nRenamer\nRAT\n\nt0\n\np1\n\nt1\n\n-\n\nt2\n\np2\n\n…\n\n…\n\n[Dispatched]\nli p0, 1\nli p1, 2\nadd p2, p1, p1\n\nInstruction Queue\n\n1. li t0, 1 is decoded, destination t0 is renamed to p0\n2. li t0, 2 is decoded, destination t0 is renamed to p1\n3. add t2, t0, t0 is decoded:\no Two t0 input registers use current mapping p1\no Destination register t2 is renamed to p2\n\n18\n\n\fRegister Rename Removes all WAW\/WAR Deps\n\n[Decoded]\nli t0, 1 WAW!\nli t0, 2\nadd t2, t0, t0\n\nRegister\nRenamer\nt0\n\nRAT\nrenamed p\n\n…\n\n…\n\nt9\n\nrenamed p\n\nInstruction Queue\n\n● By the time instructions are dispatched to i-queue\no All architectural registers have been renamed to physical registers\no All WAR and WAR dependencies have been removed\n[Dispatched]\nli p0, 1 No\nli p1, 2 WAW!\nadd p2, p1, p1\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\n[Issued]\nli p1, 2\nli p0, 1\nadd p2, p1, p1\n\nInt ALU 2\n\nFloat\nALU\n19\n\n\fAll Computation Done using Physical Registers\n\nPhysical\nRegister File\n\np0\np1\n\nRegister\nRenamer\nt0\n\nRAT\nrenamed p\n\n…\n\n…\n\nt9\n\nrenamed p\n\np2\n…\n\np(n-1)\np(n)\n\nInstruction Queue\n\nInstruction Decoder\n\n● Now ID stage (dispatch) reads registers from physical register file\n● All data forwarding also done based on physical registers\nLoad\/\nStore\n\nInstruction\nScheduler\n\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n20\n\n\fILP limits performance improvements of wide-issue\n● We already discussed limits on pipelining.\n● Time to discuss limits on IPC improvements through wide-issue!\n● There is a fundamental limit to achievable IPC\no Amount of ILP (Instruction Level Parallelism) in code\no Remember the data dependence graph?\no ILP is constrained only by true RAW dependencies\no How about control dependencies?\n▪ Not a fundamental limit → can elide using branch prediction\n● ILP is a property of the program, not the processor\no This limit applies to both VLIW and superscalar processors\n\n21\n\n\fILP present in different programs\n● After renaming, theoretical limit of IPC is 35 ~ 4003!\n\nMatthew Postiff et al. “The Limits\nof Instruction Level Parallelism in\nSPEC95 Applications”. ACM\nSIGARCH Computer Architecture\nNews, 1999\n22\n\n\fPractical limits on performance of Superscalar\n● Achieving the theoretical limit would be awesome\no In reality, superscalars are typically no more than 10-wide\n● Practical limits on superscalar processors\no Number of execution units (e.g. ALU, Load\/Store) not really a limit\no Practical limits on the IPC you can achieve\n▪ Instruction queue size (impacts scheduling window)\n▪ Physical register file size (also impacts scheduling window)\no Upsizing above structures negatively impacts cycle time\n▪ Time to search and schedule instruction queue\n▪ Time to access register file (increased size and number of ports)\no Upsizing above structures negatively impacts energy efficiency\n\n23\n\n\fExceptions\n\n24\n\n\fExceptions Review\n● Exception: an event which causes the CPU to stop the normal flow of\nexecution and go somewhere else (the exception handler)\n● There are mainly two causes of exceptions:\no Software exceptions (or traps): Triggered by a program instruction\n▪ Trap instruction: typically used to call OS routines (system calls)\n▪ Page fault: instruction accessed a page not mapped to memory\n▪ Divide-by-0: instruction performed a divide-by-0 arithmetic\n▪ Arithmetic overflow: instruction overflowed MAX_INT of register\no Hardware exceptions (or interrupts): Triggered by hardware event\n▪ User has typed on the keyboard\n▪ A network packet has arrived\n▪ A file block read has completed\n\n● In all cases, the OS exception handler is invoked\n\n25\n\n\fHandling exceptions\n● What happens when an exception is triggered:\n1. Processor stops execution of user program.\n2. Processor stores information about exception (cause, PC).\n3. Processor jumps to the OS exception handler.\n4. Handler creates backup of program register values in memory.\n5. Handler inspects exception info and handles it accordingly.\n▪ While overwriting some of the registers that were backed up.\n6. Handler restores program register values from memory.\n7. Processor resumes execution of user program.\n● Processor must provide precise register values at point of exception\no Otherwise, when processor resumes, program will malfunction\no Guaranteeing this is called a precise exception\n26\n\n\fRules for Precise Exceptions\n1. All instructions before the exception must have executed\n\n2. No instructions after the exception must execute\n● Architectural state: the state visible to the ISA (i.e. software)\no State in architectural registers (For MIPS: t0, t1, t2, …)\no State in data memory\n\n● Architectural state at point of exception must reflect above rules\n\n27\n\n\fPrecise Exceptions in In-order Processors is Easy\n\nI-Mem\n\nIns. Decoder\n\n● Exceptions are typically detected at the EX stage\no Stage where all arithmetic happens as well as address calculations\n● On exception, flush EX and all previous stages (ID and IF)\no Since in-order, guarantees instructions following EX do not writeback\no Only state leading up to instruction at EX will be written to reg \/ mem\n\nRegister\nFile\n\nALU\n\nD-Mem\n\nALU\n28\n\n\fPrecise Exceptions in Out-of-order Processors is Hard\n\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\n\nInstruction Queue\n\n● Suppose div t2,t1,t0 and li t0, 3 issue out-of-order as below\no div p2,p1,p0 triggers a divide-by-zero exception (p0 = 0)\no But at point of exception, t0 appears to be 3 due to li p3, 3!\n▪ At that point, t0 is mapped to p3 in the RAT (not p0)\n[Dispatched]\nli p0, 0\nli p1, 1\ndiv p2,p1,p0\nli p3, 3\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\n\n[Issued]\nli p0, 0\nli p1, 1\nli p3, 3\ndiv p2,p1,p0\nDivide-by-zero\nException!\n\nFloat\nALU\n29\n\n\fPrecise Exceptions in Out-of-order Processors is Hard\n● This is the challenge with out-of-order processors\no Instructions execute and complete out-of-order\no For precise exceptions, instructions must appear to complete in-order\n● Solution: update architectural state in-order\no When instructions execute, have them only update “internal” state\n▪ Physical registers\n▪ Store queue (MEM queues up stores instead of performing them)\no Internal state is transferred to visible state during in-order commit\n▪ Physical registers are copied to architectural registers\n▪ Store queue entries are written to memory\n\n30\n\n\fIn-order Commit\n\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstructions dest done?\nli p0, 0\nt0\nY\nli p1, 1\nt1\nY\ndiv p2,p1,p0 t2\nN\nli p3, 3\nt0\nY\n\nArchitectural\nRegisters\n\ncommit in order\n\nInstruction Decoder\n\n● Decoded instructions are stored to i-queue in-order\n● Instructions execute out-of-order (updating done? field)\n● Done instructions commit in-order to Retirement Register File (RRF)\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n31\n\n\fIn-order Commit Example: Cycle 1\n\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstructions dest done?\nli p0, 0\nt0\nY\nli p1, 1\nt1\nY\ndiv p2,p1,p0 t2\nN\nli p3, 3\nt0\nY\n\nArchitectural\nRegisters\n\ncommit in order\n\nInstruction Decoder\n\n● At this point, all li instructions have completed but not the div\n● li p0, 0 and li p1, 1 can commit on the next cycle\no But not li p3, 3 since we have in-order commit!\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n32\n\n\fIn-order Commit Example : Cycle 2\n\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstructions dest done?\ndiv p2,p1,p0 t2\nY\nli p3, 3\nt0\nY\n…\n…\n\nArchitectural\nRegisters\n\ncommit in order\n\nInstruction Decoder\n\n● li p0, 0 and li p1, 1 have committed updating t0 and t1\n● div p2,p1,p0 has completed execution and is finally ready to commit\no On completion, div has set an “exception bit” in i-queue (not shown here)\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n33\n\n\fIn-order Commit Example : Cycle 3\n\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstructions dest done?\ndiv p2,p1,p0 t2\nY\nli p3, 3\nt0\nY\n…\n…\n\nArchitectural\nRegisters\n\nInstruction\nScheduler\nException!\n\nFlush!\n\nInstruction Decoder\n\n● An exception is raised for div p2,p1,p0 when it tries to commit\no Instructions following div are flushed, without modifying RRF\n● Retirement Register File contains a precise architectural state\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n34\n\n\fIn-order Commit also solves branch misprediction\n\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstructions dest done?\nli p0, 0\nt0\nY\nli p1, 1\nt1\nY\nbeq p0,p1,… t2\nN\nli p3, 3\nt0\nY\n\nArchitectural\nRegisters\n\nInstruction\nScheduler\n\nFlush!\n\nInstruction Decoder\n\n● What if processor finds out it mispredicted a branch?\no Just flush instructions below it after the branch executes!\no Also restore an RAT snapshot that was taken at point of branch.\n\nMispredict!\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n35\n\n\fIn-order Commit also solves physical register recycling\n\nInstruction Decoder\n\n● When can the processor recycle physical registers?\no The prev column records previous physical register mapped to dest.\no When li p3, 3 commits, p0 previously mapped to t0 can be recycled\n[Decoded]\nli t0, 0\nli t1, 1\ndiv t2,t1,t0\nli t0, 3\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstructions dest prev\nli p0, 0\nt0\nli p1, 1\nt1\nbeq p0,p1,… t2\nli p3, 3\nt0\np0\n\nArchitectural\nRegisters\n\nInstruction\nScheduler\nRecycle p0\non commit\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n36\n\n\fLoad \/ Store Queue\n\n37\n\n\fHow about data dependencies through memory?\n● RAW, WAR, and WAW dependencies happen through memory as well\no Clearly, the below code has no data dependencies through registers\nsw\n$t0, 4($s0)\n\/\/ stores to 0xdeadbeef\nlw\n$t1, 8($s1)\n\/\/ loads from 0xdeadbeef\no But there is a RAW dependency through the location 0xdeadbeef\n● Question: how does processor enforce RAW dependencies?\n● Question: how does processor deal with WAR and WAW dependencies?\n\n● Answer: through memory renaming using a load \/ store queue\no Just like registers, a new queue entry created for every store instruction\n→ All WAR and WAW memory dependencies are removed\no Loads fetch data from most recent queue entry with same address\n→ All RAW memory dependencies are enforced\n38\n\n\fEvery store gets a new store queue entry\n\nInstruction Decoder\n\n● Loads \/ stores are inserted into load \/ store queue as well instruction queue\no Age denotes age of memory operation (incremented at every mem op)\no Address and value of mem op is unknown until mem op is complete\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0($s0) ???\n???\nsw $t1, 0($s1) 1\n0($s1) ???\n???\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0($s2) ???\nN\n…\n…\n…\n\nRenamed\nMemory\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n39\n\n\fScenario 1: WAW reordering of two stores\n\nInstruction Decoder\n\n● Let’s say sw $t1, 0($s1) becomes ready first in the i-queue and executes\no 0($s1) is resolved to 0xdeadbeef and $t1 is resolved to 1\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0($s0) ???\n???\nsw $t1, 0($s1) 1\n0xdeadbeef\n1\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0($s2) ???\nN\n…\n…\n…\n\nRenamed\nMemory\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n40\n\n\fScenario 1: WAW reordering of two stores\n\nInstruction Decoder\n\n● Next, sw $t0, 0($s0) becomes ready in the i-queue and executes\no 0($s0) is also resolved to 0xdeadbeef and $t0 is resolved to 0\no So, we have effectively reordered execution of a WAW dependency\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0xdeadbeef\n0\nsw $t1, 0($s1) 1\n0xdeadbeef\n1\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0($s2) ???\nN\n…\n…\n…\n\nRenamed\nMemory\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n41\n\n\fScenario 1: WAW reordering of two stores\n\nInstruction Decoder\n\n● Finally, lw $t2, 0($s2) becomes ready in the i-queue and executes\no 0($s2) also resolves to 0xdeadbeef meaning a RAW dependence\no Load Unit searches Store Queue for most recent store matching address\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0xdeadbeef\n0\nsw $t1, 0($s1) 1\n0xdeadbeef\n1\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0xdeadbeef\nY\n…\n…\n…\n\n$t2 = 1\n\naddr == 0xdeadbeef\n&&\nage < 3?\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n42\n\n\fScenario 2: Flush due to RAW violation\n\nInstruction Decoder\n\n● In this scenario, lw $t2, 0($s2) becomes ready first and executes\no Load Unit searches store queue but does not find matching entry\no So, it simply fetches value for $t2 from memory\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0($s0) ???\n???\nsw $t1, 0($s1) 1\n0($s1) ???\n???\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0xdeadbeef\nY\n…\n…\n…\n\naddr == 0xdeadbeef\n&&\nage < 3?\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n43\n\n\fScenario 2: Flush due to RAW violation\n\nInstruction Decoder\n\n● Next, sw $t0, 0($s0) becomes ready in the i-queue and executes\no Store Unit searches Load Queue to see if there were RAW violations\no And, yes, there is a Load that performed earlier than it should have!\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0xdeadbeef\n0\nsw $t1, 0($s1) 1\n0($s1) ???\n???\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\nPhysical\nRegister\nFile\n\nInstruction\nQueue\n\nLoad\/\nStore\n\nInstruction\nScheduler\n\nLoad Queue\nage\naddr\ndone?\naddr == 0xdeadbeef\n3\n0xdeadbeef\nY\n&& age > 1\n…\n…\n…\n&& done?\n\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n44\n\n\fScenario 2: Flush due to RAW violation\n\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0xdeadbeef\n0\nsw $t1, 0($s1) 1\n0($s1) ???\n???\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0xdeadbeef\nY\n…\n…\n…\n\nLoad\/\nStore\n\nInstruction\nScheduler\n\nFlush!\n\nInstruction Decoder\n\n● Flush lw $t2, 0($s2) and all instructions that follow it in i-queue\no All following execution has been polluted by incorrect value of $t2\n\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n45\n\n\fPrecise exceptions through in-order commit\n\nInstruction Decoder\n\n● Values in Store Queue are committed in-order\no When store instruction reaches head of i-queue, value stored to memory\no Guarantees precise exceptions\nStore Queue\n[Decoded]\naddr\nvalue\nsw $t0, 0($s0) age\n0xdeadbeef\n0\nsw $t1, 0($s1) 1\n0xdeadbeef\n1\nlw $t2, 0($s2) 2\n…\n…\n…\n\nRegister\nRenamer\n\nInstruction\nQueue\n\nPhysical\nRegister\nFile\n\nLoad Queue\nage\naddr\ndone?\n3\n0xdeadbeef\nY\n…\n…\n…\n\nCommitted\nIn-order\n\nInstruction\nScheduler\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n46\n\n\fReal Life Superscalars\n\n47\n\n\fThe ARM Cortex-A8 architecture\n● The ARM Cortex-A8 is an in-order superscalar processor\no Notice the use of the architectural register file\n\nFIGURE 4.75 The A8 pipeline. The first three stages fetch instructions into a 12-entry instruction\nfetch buffer. The Address Generation Unit (AGU) uses a Branch Target Buffer (BTB), Global\nHistory Buffer (GHB), and a Return Stack (RS) to predict branches to try to keep the fetch queue\nfull. Instruction decode is five stages and instruction execution is six stages.\n\n48\n\n\fThe AMD Opteron X4 Microarchitecture\n● The AMD Opteron is an out-of-order superscalar processor\no Commit unit oversees retiring instructions from operation queue\nInstruction Fetch\nand Decode\n\nRegister read\nand Dispatch\n\nExecute\n\nMem\n\nWrite Back\n\n49\n\n\fThe Intel Core i7 architecture\n● The Intel Core i7 is another out-of-order superscalar processor\n\nFIGURE 4.77 The Core i7 pipeline with\nmemory components. The total pipeline\ndepth is 14 stages, with branch\nmispredictions costing 17 clock cycles.\nThis design can buffer 48 loads and 32\nstores. It is a 4-wide processor but has 6\nexecution units of different types to reduce\nstructural hazards.\n\n50\n\n\fIntel Core i7 Performance\n● Ideal CPI = 0.25 since this is a 4-wide processor\n\n51\n\n\fIntel Core i7 Impact of Branch Misprediction\n● Due to deep pipeline, tiny misprediction can have outsized impact\n\n52\n\n\fRecap:\nVLIWs vs SuperScalars\n\n53\n\n\fAbility to deal with hazards\n● Hazards prevent the full exploitation of ILP (Instruction Level Parallelism)\n● Which processor type is better at dealing with various hazards?\nVLIW\n\nOut-of-order SuperScalar\n\nStructural Hazards\nData Hazards (Registers)\nData Hazards (Memory)\nControl Hazards\nInstruction Window\n\n54\n\n\fAbility to operate energy efficiently\n● We learned that performance and power are two sides of the same coin.\n● Which processor type has less power-hungry control structures?\nVLIW\n\nOut-of-order SuperScalar\n\nBig Register File\nRegister Alias Table\nInstruction Queue\nData Forwarding Wires\n\n55\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":216,"segment": "unlabeled", "course": "cs1622", "lec": "lec18","text":"Global Optimization\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● exam 1 grades were posted on Monday, if you missed that\no if you have questions about it, please ask me in private\n● don’t forget project 4 is due on Saturday (or late Sunday)!\n\n2\n\n\fFrom Local to Global\n\n3\n\n\fLocal optimization only gets you so far\n● last time we saw that optimizations like copy propagation and dead\nstore elimination could greatly simplify our code.\nlet x = 5 * 5;\nreturn x;\n\nx = 5 * 5\n$t0 = x\nreturn\n\nx = 25\n$t0 = x\nreturn\n\n$t0 = 25\nreturn\n\nbut local optimizations only work on a single basic block,\nwhich means they have two significant weaknesses:\n1. real functions can have many BBs, and they\nconnect to and affect each other in nontrivial ways.\n2. many of those BBs are short, only a few instructions long,\nmeaning there just isn't much opportunity for optimization.\n4\n\n\fStumbling block\nbasic\n\n● if we make that code just a little more complex… (assume a is an argument)\nlet x = 5 * 5;\nif a { f(); }\nreturn x;\n\nbb0: x = 5 * 5\nif a bb1 else bb2\nbb1: f()\ngoto bb2\n\nbb2: $t0 = x\nreturn\n\nwe can still constfold up here…\n\nbut now the\nassignment to $t0\nis in a different BB!\n\nif we want our optimizations to be worthwhile at all, we're\ngoing to have to make them operate on the whole CFG.\nintuitively, the above should be possible to optimize, no?\n5\n\n\fTrying to intuit our way through it\n● we're going to use the same rules for copy prop\/dead stores as\nbefore, but we'll look at all the BBs instead of just one at a time.\nbb0: x = 25\nif a bb1 else bb2\nbb1: f()\ngoto bb2\nbb2: $t0 = x\nreturn\n\nx is only assigned\nonce, so copy its RHS\neverywhere it's used.\n\nbb0: x = 25\nif a bb1 else bb2\nbb1: f()\ngoto bb2\nbb2: $t0 = 25\nreturn\n\nnow x is never\nused, so delete it.\n\nbb0: if a bb1 else bb2\nbb1: f()\ngoto bb2\nbb2: $t0 = 25\nreturn\n\nthat wasn't so\nbad, was it?\n\nwell…\n6\n\n\fIntuit THIS\n● if x is reassigned, does that always mean we can't propagate?\nbb0: x = 25\nif a bb1 else bb2\nbb1: x = 10\ngoto bb2\nbb2: $t0 = x\nreturn\n\nhere, x has different\nvalues on each path\nto bb2, so we can't\npropagate.\n\nbb0: x = 25\nif a bb1 else bb2\nbb1: x = 10\ngoto bb3\n\nbb2: x = 10\ngoto bb3\n\nbb3: $t0 = x\nreturn\n\nbut here, although x is assigned in\nmultiple places, it has the same value\non both paths to bb3, so… we can\npropagate it to $t0 = 10???\n7\n\n\fFalse confidence\n● it only gets worse.\n\nintuitively, we can simplify this whole function\n\nlet x = 0;\nto return 5; but how do you prove that?\nfor i in 0, 10 {\nx = 5;\nx is assigned twice, or maybe 11 times?\nbb0: x = 0\n}\n(do assignments in loops count as \"once\"?)\ni = 0\ngoto bb1\nreturn x;\nbb1: $t1 = i < 10\nif $t1 bb2 else bb3\nbb2: x = 5\ni = i + 1\ngoto bb1\n\nbut the first assignment\nx = 0 is never used…\n\nand the assignment(s) in\nthe loop are redundant…\n(does that mean the whole\nloop can be removed?)\n\nbb3: $t0 = x\nreturn\n8\n\n\fWe need some RIGOR\n● optimizations are basically proofs.\no if you can prove that a variable is never read, you can remove it.\no if you can prove that a variable only ever holds a constant value,\nyou can replace all uses of it with that constant.\n● what we need is some kind of framework to build these proofs from.\no many optimizations have the same kind of \"algorithmic shape.\"\no most of them have repeated steps that stem from a common\nunderlying reason.\n● so let's talk about data flow analysis.\n\n9\n\n\fData Flow Analysis\n\n10\n\n\fComing for a visit (animated)\n● remember this graph-visiting algorithm?\nthe visited set records which nodes\nfn visit_node(n, visited) {\nif visited[n] { return; }\nhave already been visited, and is\nvisited[n] = true;\nnecessary to prevent infinite loops.\nfor s in n.successors() {\nvisit_node(s, visited);\n}\n\n}\nvisit_node\n\n1\n\nso you can imagine all the nodes\nstarting in an \"unvisited\" set, and\ngradually being moved to visited.\nUnvisited\n\n2\n\n1\n\n2\n\n3\n\n3\n\n4\n\n4\n\nVisited\n\n11\n\n\fWhy does it terminate?\n● it might seem silly to ask, but termination is crucial to being able to\nspecify optimizations that don't get our compiler stuck in a loop.\n1. every node is in one of a finite number\nof sets. (here, it's one of two sets.)\n4. on every step,\n3. once a node\nwe move at least\nreaches the \"last\"\nUnvisited\nVisited\none node from\nset, we don't look\none set to another.\nat it anymore.\n2. nodes can move from one set to\nanother, but only in one direction.\n\ntherefore, the big-O upper bound on the number\nof steps in the algorithm is the number of nodes\nmultiplied by the number of sets .\nminus one\n\n12\n\n\fA simple control flow optimization\n● in this function, can we ever run the else code (g())?\nif true {\nf();\n} else {\ng();\n}\n\nno. what does that\nlook like in the CFG?\n\nthis can come up in real code:\n\nthese nodes\/edges\nare useless; we can\nremove them.\n\nif\n\nf()\n\ng()\nf()\n…\n\nconst FEATURE_ENABLED = true;\n...\nif FEATURE_ENABLED {\nor, the condition may have\nf();\nbecome a constant due to\n} else {\nother optimizations.\ng();\n}\n\n…\n\n13\n\n\fTweaking the visitor algorithm\n● we want to detect if a BB is unreachable, meaning it can never run.\n● we'll modify the visiting algorithm in a simple way to do this.\nfn visit_node(n, visited) {\nif visited[n] { return; }\nvisited[n] = true;\nlet t = n.terminator;\nif t's condition is constant true {\nvisit_node(t.true_side, visited);\n} else if it's constant false {\nvisit_node(t.false_side, visited);\n} else {\nfor s in n.successors() {\nvisit_node(s, visited);\n}\n}\n}\n\nI'm paraphrasing the \"real\ncode\" but the idea is simple:\nif the condition is constant,\nonly recursively visit the BB\nthat corresponds to it.\nat the end of running this,\nany nodes not in the visited\nset are unreachable and can\nbe removed from the CFG.\n14\n\n\fRunning it on this CFG (animated)\n● we'll mark any visited node with a green circle.\nif true {\nf();\n} else {\ng();\n}\n\nif\nf()\n\ng()\n\n…\n\ndone. now we can see that the\nelse node (g()) was not visited,\nand is therefore not reachable.\n(how we remove it from the CFG is a separate\nissue, but it's not super complicated.)\n\ndo you remember another algorithm that worked like\nthis? you start at a root and mark reachable things,\nthen sweep away anything that isn't reachable…\n;o\n\n15\n\n\fForward analysis\n● the way this (and most optimizations) works is by transferring some\nkind of \"knowledge\" from BB to BB by following edges.\no here, that knowledge is the reachability of a node.\no when we enter visit_node, we know that n is reachable…\no …and that reachability is transferred to its successors.\n● a forward analysis spreads this knowledge forward with the edges:\nfrom the predecessors to the successors.\no a backward analysis does the opposite – from successors to\npredecessors – but we won't see one of those until next time.\n● so: let's use this new knowledge to solve the problems we\nencountered before!\n\n16\n\n\fGlobal Constant Copy\nPropagation (GCCP)\n\n17\n\n\fDefs and Uses\n● a def of a variable is when you assign it (it appears on ='s LHS).\no defs of x: x = 0, x = y\n● a use of a variable is when you get its value.\no uses of x: z = x, if x, f(x)\n● we'll use def-use to mean a pair of def and use, where the def sets\nthe value that the use gets.\nx = 3\ny = 5\nz = a + b\nf(x)\nhere's a def-use.\n\nx = 3\n\nx = 5\nf(x)\n\nthere can be multiple\ndefs for one use, too.\n18\n\n\fWhat is Global Constant Copy Propagation?\n● remember that copy propagation said that if we have an instruction\nof the form ”x = y”, and x is never reassigned, then we can replace\nall uses of x with y.\n● constant copy propagation is the same thing, but only in the cases\nwhere the RHS of ”x = y” is a constant.\no and global constant copy propagation is that, but applied to the\nentire CFG instead of just one BB!\n● for the purposes of this example, we will not be using SSA, so\nvariables can be reassigned.\no essentially SSA makes it so every use has exactly one def, which\nsimplifies some things…\no but the details of SSA are too much for this course, so we’ll stick\nwith non-SSA for these global optimizations.\n19\n\n\fGlobal Constant Copy Propagation\n● if we are looking at a use of x…\n● and, every def for that use of x set it to some constant C…\n● then, we can replace that use of x with that constant C.\nbb0: x = 25\nif a bb1 else bb2\n\nbb0: x = 25\nif a bb1 else bb2\n\nbb1: f()\ngoto bb2\n\nbb1: x = 10\ngoto bb2\n\nbb2: $t0 = x\nreturn\n\nbb2: $t0 = x\nreturn\n\n1 def-use, sets x\nto 25; we can\nreplace x with 25.\n\n2 def-uses; set x\nto different\nvalues; no good.\n\nbb0: x = 25\nif a bb1 else bb2\nbb1: x = 10\ngoto bb3\n\nbb2: x = 10\ngoto bb3\n\nbb3: $t0 = x\nreturn\n\n2 def-uses; both set x\nto same value; we can\nreplace x with 10.\n20\n\n\fFrom the bottom up (animated)\n● that's cool and all, but how do we start implementing this?\n● let's start by looking at the instructions within a basic block. we'll\nfocus on the variable x right now.\nbefore this code\nruns, we don't\nknow what's in x.\nbut each line\nchanges what\nwe know.\n\nKnowledge\nx = ???\nx = 4\nx = 4\ny = 5\nx = 4\n\nx = f()\nx = ???\n\nx = 9\nx = 9\n\nthe change in knowledge\nfrom one step to the next\nis formally known as the\ntransfer function.\nso let's be a bit more\nrigorous about this.\n21\n\n\fOur states and transfer function\n● the \"knowledge\" is properly called the state, and we have two states:\no x = ANY, meaning that x could be one of several values; and\no x = CONST(c), meaning x holds a constant c. (e.g. CONST(7))\n● each instruction has an in-state and an out-state…\no and the out-state of one instruction is the in-state of the next.\n● the transfer function takes an instruction and its in-state, and\nproduces its out-state.\n● the transfer function here is simple:\no if the instruction is of the form x = c for some constant c,\n▪ then the out-state is CONST(c).\no else, if the instruction is of the form x = … for any other RHS,\n▪ then the out-state is ANY.\no else, the out-state is the in-state, unmodified.\n22\n\n\fLather, rinse, repeat\n● we can use this transfer function to compute the out-state for a\nbasic block as a whole, as well.\nx = ANY\nx = CONST(4)\nx = CONST(4)\nx = ANY\nx = CONST(9)\nx = CONST(9)\n\nthere is some in-state for this BB.\nx = 4\ny = 5\nx = f()\nx = 9\ny = g()\ngoto bb7\n\nx = CONST(9)\n\n$t0 = x\nreturn\n\nwe repeatedly apply the transfer\nfunction to the instructions inside…\n\nand that gives us the BB's out-state.\nand as you might imagine, that can\nbecome the in-state for the next BB!\n…but wait, BBs can have\nmultiple predecessors.\n23\n\n\fThe join function\n● a BB's predecessors may all be feeding different states into it.\n● the join function combines those states to produce a BB's in-state.\n● let's look at some examples. the labels are the state for x.\nx = 5\nCONST(5)\n\nx = f()\nANY\n\nso, red's in-state\nmust be x = ANY.\n\nx = 5\nCONST(5)\n\nx = 3\nCONST(3)\n\nstill, red's in-state\nmust be x = ANY.\n\nx = 3\nCONST(3)\n\nx = 3\nCONST(3)\n\nsuccess! red's in-state\nis x = CONST(3).\n\nso: if all the predecessors say CONST(c) for the same constant c,\nthen the in-state is CONST(c); otherwise, it's ANY.\n24\n\n\fLet's try it out!\n● here are two functions from before. let's annotate the edges with\nthe state for x.\nx = 25\nif a bb1 else bb2\n\nx = 25\nif a bb1 else bb2\n\nCONST(25)\nx = 10\n\nCONST(25)\n\nCONST(25)\nx = 10\n\nCONST(25)\n\nx = 10\n\nCONST(10)\n\n$t0 = x\nCONST(10) return\n\nCONST(10)\n$t0 = x\nreturn\n\nso, red's in-state\nmust be x = ANY.\n\nso, red's in-state is\nx = CONST(10)!\n\nproblem solved!\n............right?\n\n25\n\n\fThe catch\n● let's try it on this CFG! again, we're annotating the state for x.\nx = 10\ni = 0\n\nwait, there's nothing on this\nedge. then how do we compute\nthe in-state for the orange BB?\n\nand if we try to follow it\nbackwards, we end up back\nat the same orange BB!\n\nCONST(10)\n$t1 = i < 10\nif $t1 bb2 else bb3\n\nprint(\"ha\")\ni = i + 1\n\nIT KEEPS HAPPENING! I TOLD YOU\nABOUT CYCLIC GRAPHS BRO!!\n\n$t0 = x\nreturn\n\n26\n\n\fFixing it\n\n27\n\n\fOh yeah, we forgot a state\n● we need one more state to indicate that we haven't visited that\ninstruction\/BB, UNK for \"unknown.\"\n● it will be the initial value: every instruction\/BB's in-state and outstate will be set to UNK before the algorithm begins.\n● our transfer function isn't going to have to change, fortunately.\no it implicitly handles UNK in the \"else\" case: unknown in, unknown\nout!\n● but there's the other function…\n\n28\n\n\fThe new, improved join function\n● here's where things get a bit weird, but it will all work out.\nif any predecessor says x = ANY,\nthen the output is x = ANY.\n\np1: x = CONST(10)\np2: x = CONST(4)\np3: x = ANY\np4: x = UNK\n\nx = ANY\n\nif all predecessors say x = UNK,\nthen the output is x = UNK.\n\np1: x = UNK\np2: x = UNK\np3: x = UNK\np4: x = UNK\n\nx = UNK\n\nif the predecessors are a mix of\nx = UNK and x = CONST, then\nignore the UNKs, and the output\nis CONST or ANY like before.\n\np1: x = UNK\np2: x = CONST(3)\np3: x = UNK\np4: x = CONST(3)\n\nx = CONST(3)\n\n29\n\n\fLet's watch it go (animated, important, you have to watch this)\n● if these rules seem strange, wait till you see how it behaves.\n● notice that x starts off as UNK everywhere.\nguess what: we are visiting this\norange BB a second time!\n\nUNK x = 10\ni = 0\n\nUNK\nCONST(10)\njoin(CONST(10),\njoin(CONST(10),\nCONST(10))\nUNK)\n= CONST(10)\n$t1 = i < 10\nif $t1 bb2 else bb3\n\nCONST(10)\nUNK\n\nbut no more changes occur.\nwe have reached equilibrium:\nthe algorithm is done.\n\nCONST(10)\nUNK\n\nCONST(10)\nUNK\n\nprint(\"ha\")\ni = i + 1\n$t0 = x\nreturn\n30\n\n\fSometimes one visit is not enough\n● because each visit to a BB might move it from one set (UNK) to\nanother (say, CONST(10))… and because we have three sets…\no then we may have to visit a BB more than one time!\n● we're not risking an infinite loop though. why?\n\nCONST\n\nUNK\n\nwe have a finite number of states (3),\nand the transitions between them\nare unidirectional.\nANY\n\nthat's all we need to prove to\nguarantee termination. nice.\n\n31\n\n\fShortcomings (animated)\n● now let's see what happens when we reassign x in the loop.\nUNK x = 10\ni = 0\n\nUNK\nCONST(10)\njoin(CONST(10),\njoin(CONST(10),\nCONST(20))\nUNK)\n= CONST(10)\n= ANY\n$t1 = i < 10\n\nwhat happened? well, it\nCONST(20)\nUNK\ndetermined that at the start of the\norange BB, x could be 10 or 20. so\nit assumes it's ANY from then on.\n\nthe algorithm doesn't know that this\nloop always runs. it doesn't know\nanything about loops at all!\n\nif $t1 bb2 else bb3\n\nCONST(10)\nUNK\nANY\n\nCONST(10)\nUNK\nANY\n\nx = 20\ni = i + 1\n$t0 = x\nreturn\n\n32\n\n\fIt never hurts to not optimize\n● the example on the previous slide is an example of the algorithm\nbeing a little cautious.\no yeah, we can see that x = 20 at the return, but that wouldn't be\ntrue if the loop never ran!\no consider a slight modification where the loop upper bound is an\nargument – in that case, the loop may run 0 times.\n● what you don't want is for your algorithm to optimize in a situation\nwhere it shouldn't.\no cause that's a broken proof, and you'll get a broken program.\n● sometimes an optimization pass won't find anything to do.\no that's okay. there's no judgment. it can't know until it tries.\n● and if it doesn't optimize anything, you'll still have a correct program.\n\n33\n\n\fSumming it up\n● to recap how dataflow analysis works:\n\njoin(p1, p2, p3)\n\nin-state\n\nthe transfer function\nis repeatedly applied\nto its instructions,\nwhich computes the\nBB's out-state.\n\ninst1\ninst2\ninst3\ninst4\ninst5\ngoto bb2\n\nout-state\n\nstate flows from a BB's\npredecessors into its join\nfunction, which computes\nthe in-state for the BB.\n\nthis is repeated for every\nBB until the in- and outstates reach equilibrium:\nthey stop changing.\n\nas long as there are a finite number of states, and they\nchange monotonically, this algorithm will terminate.\n\n34\n\n\fLiveness\ntime check ≤ 87\n\n35\n\n\fLiveness\n● a local variable is live if its value will be used in the future.\no this is not its lifetime; liveness can be – and often is – shorter!\n● it lasts from a def until the last use of that def.\n\narg is only used once on the\nfirst line, so it's dead after that.\n\narguments are \"def'ed\" at\nthe start of the function.\narg\nret\n\nfn lifey(arg: int): int {\nlet ret = arg + 10;\n\nret becomes live when we\ndeclare it, and lives until its last\nuse in the return statement.\n\nprintln_i(ret);\nreturn ret;\n\n}\n\nthese lines are the\nlocals' liveness ranges.\n\n36\n\n\fOne local, many ranges\n● sometimes, a local can be live and dead multiple times!\nx\n\nx is live twice in this code…\nwith a sort of \"dead zone\" in between.\nthis seems weird, but it's telling us\nsomething useful: x is behaving like\ntwo different variables in this code.\n\nlet x = 10;\nprintln_i(x);\nprintln_s(\"?\");\n\nx = 20;\nprintln_i(x);\n\n(remember SSA? this feels like a step towards it…)\n\nliveness is the basis for a lot of other optimizations (and error\nchecking!), and we'll see other examples along the way.\n37\n\n\fTwo perspectives\n\ni\nobj\n\nnow we can answer:\nat any point in this\nfunction, which\nvariables are live?\n\nfound\nn\nval\nl\n\n● we already saw that we can view liveness as a set of ranges of\ninstructions (and basic blocks) during which a variable is live.\n● but another view becomes useful when you have multiple variables.\n\nthis is very relevant to\nregister allocation, since\nwe're trying to map these\nvariables onto the limited\nCPU registers.\n\nfn has(l: List, val: int): bool {\nlet n = l.length();\nlet found = false;\nfor i in 0, n {\nlet obj = l.get(i);\nif obj.value() == val {\nfound = true;\n}\n}\nreturn found;\n}\n38\n\n\fTrying (and failing) to determine liveness\n● below, the O and X say whether x is alive O or dead X.\no the state is tracked between instructions, hence the misalignment.\nlet's assume by default that it's dead.\nthis def seems to make x live.\nhere's a use of x. but is x used again after this?\nlet's assume it'll be used again…\n\nx = 5\n\nprintln_i(x)\ny = 10\n\nprintln_i(y)\n\nanother use. let's keep assuming.\n\nz = x == y\nprintln_b(z)\n\nuh oh. it's the end. clearly, x's last use\nwas in x == y. so this is wrong…\n\n39\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":217,"segment": "unlabeled", "course": "cs1541", "lec": "lec2.4_branch_prediction","text":"Branch Prediction and\nPredication\nCS 1541\nWonsun Ahn\n\n\fBranch Prediction\n\n2\n\n\fSolution 3: Branch Prediction\n● Comparator at ID stage is not completely satisfactory\no Still creates one bubble on a taken branch\no Also extra bubbles due to data hazards at the ID stage\n● What if …\no We were able to predict the branch outcome?\no But without comparing registers?\n\n● What would that get us?\n1. We could make the prediction at the IF stage\n▪ We can start fetching on the correct path at very next cycle!\n2. No extra data hazard bubbles\n▪ We are not even reading register values, remember?\n3\n\n\fWhat if branch is mispredicted?\n● HDU can flush pipeline of wrong path instructions, just like before\no Misprediction becomes a performance, not a correctness issue\nTime\n\nblt s0,10,top\n\nla a0,done_msg\n\njal printf\ns0 < 10...\nOOPS!\nmove a0,s0\n\n0\n\n1\n\n2\n\n3\n\n4\n\nIF\n\nID\n\nEX\n\nMEM\n\nWB\n\nIF\n\nID\n\nIF\n\nID\n\n5\n\n6\n\n7\n\nEX\n\nMEM\n\nWB\n\nIF\n\n4\n\n\fTaken \/ Not Taken Branch Prediction\n● We have been doing a form of branch prediction all along!\no We assumed that all branches will be not taken\n● Two simple policies:\no Predict not taken: continue fetching PC + 4, flush if taken\nPro: Can start fetching the next instruction immediately\nCon: ~67% of branches are taken (due to loops) → many flushes\no Predict taken: fetch branch target, flush if not taken\nPro: ~67% of branches are taken (due to loops) → less flushes\nCon: ID stage must decode branch target before fetch → bubble\n\n● Both are non-ideal: there are better ways to predict!\n\n5\n\n\fTypes of Branch Prediction\n● Static Branch Prediction\no Predicting branch behavior based on code analysis\no Compiler gives hints about branch direction through ISA\no Not used nowadays due to inaccuracy of compiler predictions\n\n● Dynamic Branch Prediction\no Predicting branch behavior based on (dynamic) branch history\no Typically using hardware that tracks history information\no Premise: history repeats itself\n▪ Branches not taken in the past → likely not taken in the future\n(e.g. branches to error handling code)\n▪ Branches taken in the past → likely taken in the future\n(e.g. branch back to the next iteration of the loop)\n6\n\n\fThe Branch History Table (BHT)\n● BHT stores Taken (T) or Not Taken (NT) history info for each branch\no If branch was taken most recently, T is recorded\no If branch was not taken most recently, NT is recorded\n● BHT is indexed using PC (Program Counter)\no Each branch has a unique PC, so a unique entry per branch\n● BHT, being hardware, is limited in capacity\no Cannot have a huge table with all PCs possible in a program\no Besides, not every PC address contains a branch\no Best to use hash table to map branch PCs to (limited) entries\n\n7\n\n\fThe Branch History Table (BHT)\nHash\nPC:\n0x007FA004\n\n0\n\n==?\n\nentry = Hash(PC)\nif(entry.PC == PC\n&& entry.pred == T)\nNextPC = inst.target\nelse\nNextPC = PC+4\n\n#\n\nBranch PC\n\nPred.\n\n0 0x007FA004\n\nT\n\n1 0x007FC60C\n\nNT\n\n2 0x007FA058\n\nT\n\n3\n\nNT\n\n...\n\n4 0x007FC380\n\nT\n\n5\n\n...\n\nT\n\n6\n\n...\n\nNT\n\n7\n\n...\n\nNT\n\nT?\n\nTo filter out conflicts in hash function\n8\n\n\fLimitations of Branch History Table (BHT)\n● Ideally, we would like know what next to fetch at the IF stage\no So that correct instruction is immediately fetched in next cycle\n● BHT can give us branch direction IF stage\no All the information needed is the PC (which is available at IF)\n● But also need the branch target to know what to fetch\no Must wait until the ID stage for branch target to be decoded\no If NT in BHT: no need to wait (branch target is irrelevant)\nBut if T in BHT: need to wait until ID stage\n\n● That introduces a bubble for taken branches\n\n9\n\n\fThe Branch Target Buffer (BTB)\n● BTB stores branch target for each branch\n\n● BTB is also indexed using PC of branch using a hash table\n● BTB allows branch target to be known at the IF stage\no No need to wait until ID stage for branch target to be decoded\n\n10\n\n\fThe Branch Target Buffer (BTB)\nHash\nPC:\n0x007FA004\n\n0\n\n==?\n\nentry = Hash(PC)\nif(entry.PC == PC)\nNextPC = entry.target\nelse\nNextPC = PC+4\n\n#\n\nBranch PC\n\nBranch Target\n\n0 0x007FA004\n\n0x007FA03C\n\n1 0x007FC60C\n\n0x007FC704\n\n2 0x007FA058\n\n0x007FA040\n\n3\n\n...\n\n4 0x007FC380\n\n...\n0x007FC398\n\n5\n\n...\n\n...\n\n6\n\n...\n\n...\n\n7\n\n...\n\n...\n\n11\n\n\fBHT + BTB Combined Branch Predictor\nHash\nPC:\n0x007FA004\n\n0\n\n==?\n\nentry = Hash(PC)\nif(entry.PC == PC\n&& entry.pred == T)\nNextPC = entry.target\nelse\nNextPC = PC+4\n\n#\n\nBranch PC\n\nPred.\n\nBranch Target\n\n0 0x007FA004\n\nNT\n\n0x007FA03C\n\n1 0x007FC60C\n\nNT\n\n0x007FC704\n\n2 0x007FA058\n\nT\n\n0x007FA040\n\n3\n\nNT\n\n...\n\n4 0x007FC380\n\nT\n\n0x007FC398\n\n5\n\n...\n\nT\n\n...\n\n6\n\n...\n\nNT\n\n...\n\n7\n\n...\n\nNT\n\n...\n\n...\n\n12\n\n\fBranch Prediction Decision Tree\nAssuming that branch condition and target are resolved in ID stage\nSend PC to\nInstruction memory\nand BTB\n\nIF\n\nno\nID\n\nno\n\nSet PC =\nPC+4\n\nIs\ninstruction\na branch?\n\nNormal\ninstruction\nexecution\n\nEntry\nfound in\nBTB?\n\nyes\nBranch\ntaken?\n\nno\nRecord the\nentry in BTB\n\nyes\n\nno\nyes\n\nBranch\npredictor\n\nSet PC =\npredicted target\nor PC+4\n\nPrediction\nwas\ncorrect?\n\n1. PC = correct target PC\n2. Kill instruction in IF\n3. Add\/correct the entry in BTB\n\ncondition\ntarget\nIF.Flush\n\nM\nu\nx\n\nPC + 4\n\n4\n\nyes\nNormal\ninstruction\nexecution\n\nP\nC\n\n+\n\nRegisters\n\nInstruction\nmemory\n\nIF\/ID\n\nImmediate\nconstant\n\nID\/EX\n\n13\n\n\fLimitations of 1-bit BHT Predictor\n● Is 1-bit (T \/ NT) enough history to make a good decision?\n\n● Take a look at this example:\nfor (j=0; j<100; j++) {\nfor (i=0; i< 5; i++) {\nA[i] = B[i] * C[i];\nD[i] = E[i] \/ F[i];\n}\n}\n\nPredicted\n\n-\n\nT\n\nT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nActual\n\nT\n\nT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nT\n\nthis branch is predicted wrong\ntwice every inner loop\ninvocation (every 5 branches)\n\n● It would have been better to stay with T than flip back and forth!\n● Idea behind the 2-bit predictor: make predictions more stable\no So that predictions don’t flip immediately\n\n14\n\n\f2-bit BHT Predictor\n● State transition diagram of 2-bit predictor:\n\nchange in prediction\n\n● Can be implemented using a 2-bit saturating counter\no Strongly not taken: 00\no Weakly not taken: 01\no Weakly taken: 10\no Strongly taken: 11\n15\n\n\f2-bit BHT Predictor\n● How well does the 2-bit predictor do with our previous example?\n\n● Our previous example:\nfor (j=0; j<100; j++) {\nfor (i=0; i< 5; i++) {\nA[i] = B[i] * C[i];\nD[i] = E[i] \/ F[i];\n}\n}\n\nPredicted\n\n-\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nActual\n\nT\n\nT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nT\n\nthis branch is predicted wrong\nonly once every inner loop\ninvocation (every 5 branches)\n\n● Does it help beyond 2 bits? (e.g. 3-bit predictor, or 4-bit predictor)\no Empirically, no. 2 bits already cover loop which is most common.\no 2 bits + large BHT gets you ~93% accuracy\n● We need other tricks to improve accuracy!\n16\n\n\fLimitations of 2-bit BHT Predictor\n● Here is an example where 1-bit BHT predictor fails miserably\nfor (j=0; j<100; j++) {\nif (j % 2) {\n}\n}\n\nYou get the prediction wrong every single time!\nPredicted\n\n-\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nActual\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\n● And a 2-bit predictor doesn’t do very well either\nfor (j=0; j<100; j++) {\nif (j % 2) {\n}\n}\n\nYou get the prediction wrong every other time!\nPredicted\n\n-\n\nNT NT NT NT NT NT NT NT NT NT NT NT\n\nActual\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\no Would a 3-bit predictor do any better?\n● Idea: Base prediction on a pattern found in history of branches!\no Rather than relying on a single prediction for a branch\no If History: T → predict NT, if History: NT → predict T\n17\n\n\fCorrelating Predictors leverage patterns\n● Correlating Predictor: Uses patterns in past branches for prediction\no Often branch behavior more complex than just taken or not taken\no Often correlates to a pattern of past branches\n● Pattern may exist in two ways:\no Pattern in local branch history (history of only current branch)\no Pattern in global branch history (history of all branches)\n\n● Maintaining longer history allows detection of longer patterns\no Local branch history for each branch maintained at all times\no One global branch history maintained at all times\n\n18\n\n\fLocal Branch History Correlating Predictor\n● With a local branch history of 1, can predict perfectly!\nfor (j=0; j<100; j++) {\nif (j % 2) {\n}\n}\n\nPredict with branch PC + 1 local branch history\nPredicted\n\n-\n\n-\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nActual\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\nT\n\nNT\n\n● Local branch history changes as such:\no NT → T → NT → T → NT → T → NT → T → NT → T → …\n● Prediction based on branch PC and local branch history:\no PC: if (j % 2) + History: NT\n→ Prediction: T\no PC: if (j % 2) + History: T\n→ Prediction: NT\n\n19\n\n\fLocal Branch History Correlating Predictor\n● You need a local branch history of 2 for this one.\nfor (j=0; j<100; j++) {\nif (j % 3) {\n}\n}\n\nPredict with branch PC + 2 local branch history\nPredicted\n\n-\n\n-\n\n-\n\n-\n\n-\n\nT\n\nNT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nNT\n\nActual\n\nNT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nNT\n\nT\n\nT\n\nNT\n\n● Local branch history changes as such:\no NT, T → T, T → T, NT → NT, T → T, T → T, NT → NT, T → …\n● Prediction based on branch PC and local branch history:\no PC: if (j % 3) + History: NT, T\n→ Prediction: T\no PC: if (j % 3) + History: T, T\n→ Prediction: NT\no PC: if (j % 3) + History: T, NT\n→ Prediction: T\no PC: if (j % 3) + History: NT, NT\n→ No prediction\n20\n\n\fGlobal Branch History Correlating Predictor\n● Knowing the result of other branches in your history also helps\nIf (j == 0) {\n}\n…\nIf (j != 0) {\n}\n\ncurrent\n\nKnowing result of a previous different branch in\nyour history helps in predicting current branch!\n\n● This is called global branch history (involves all branches).\n\n● Can be helpful when local branch history can’t capture pattern.\n\n21\n\n\fUnified Correlating Predictor\n● Correlates prediction with branch history as well as branch PC\no Local branch history + Global branch history\no An entry with matching history gives more precise prediction!\n● Now, instead of indexing into BHT by branch PC only\no Use hash(PC, Local branch history, Global branch history)\n● History is stored in register called Branch History Shift Register (BHR)\no T\/NT bit is shifted on to BHR whenever branch is encountered\n1. One Global BHR (there is just one global history)\n2. Multiple Local BHRs (local histories for each branch PC)\n\n22\n\n\fCorrelating Predictors\nHash\n\n0\n\nPC\n# Tags Local BHRs\n0 PC T, NT, NT, T, …\n1 PC T, T, NT, T, …\n2 PC\nT, T, T, T, …\n3 ...\n...\n\nGlobal BHR\nT, NT, NT, T, T, NT, …\n\n#\n\nTags\n\nPred.\n\nBranch Target\n\n0\n\nPC+History\n\n01\n\n0x007FA03C\n\n1\n\nPC+History\n\n00\n\n0x007FC704\n\n2\n\nPC+History\n\n11\n\n0x007FA040\n\n3\n\n...\n\n01\n\n...\n\n4\n\nPC+History\n\n10\n\n0x007FC398\n\n5\n\n...\n\n00\n\n...\n\n6\n\n...\n\n10\n\n...\n\n7\n\n...\n\n11\n\n...\n\n● Can reach up to 97% accuracy!\n23\n\n\fHow about jr $ra?\n● jr $ra: Jump return to address stored in $ra\no When a function is called, the caller stores return address to $ra\n(jal funcAddr stores PC of next instruction to $ra)\no When a function returns, jr $ra jumps to return address in $ra\n\n● Why is this a problem?\no Unlike other branches, branch target is not an immediate value!\n(Jumping to a variable target is called an indirect branch)\no Target can change for same jr depending on who caller is\no Makes life difficult for BTB which relies on target being constant\n● Target of jr is predicted using the Return Stack Buffer\no Not the Branch Target Buffer (BTB)\n24\n\n\fThe Return Stack Buffer\n● Since functions return to where they were called every time,\nit makes sense to cache the return addresses (in a stack)\nWhen we encounter\n4AB33C jal someFunc\nthe jal, push the\n4AB340 beq v0, $0, blah return address.\n...\nWhen we encounter\nthe jr $ra, pop the\nsomeFunc:\nreturn address. Easy!\n...\njr $ra\n\n● On misprediction or stack overflow, empty stack\no Not a problem since this is for prediction anyway\n\n40CC00\n46280C\n4AB108\n000000\n4AB340\n000000\n000000\n000000\n000000\n\n25\n\n\fPerformance Impact with Branch Prediction\n● Now, CPI = CPInch + a * p * K\n\no CPInch : CPI with no control hazard\no a : fraction of branch instructions in the instruction mix\no p : probability a branch is mispredicted\no K : penalty per pipeline flush\n\n● With deep pipelines, mispredictions can have outsize impact\nExample: If 20% of instructions are branches and the misprediction rate is\n5%, and pipeline flush penalty 20 cycles, then:\nCPI = CPInch + 0.2 * 0.05 * 20 = CPInch + 0.2 cycles per instruction\n\n● If, CPInch is 0.5, then that is 40% added to execution time!\n● Problem is a small percentage of hard to predict branches\no How do we deal with these?\n26\n\n\fPredication\n\n27\n\n\fBranch Mispredictions have Outsize Impact\n● Assume a deep pipeline and if(s1 >= 0) is hard to predict\n\nif(s1 >= 0)\ns2 = 0;\nfor(s0 = 0 .. 10)\ns3 = s3 + s0;\n\nblt\nli\n\ns1, 0, top\ns2, 0\n\ntop:\nadd s3, s3, s0\naddi s0, s0, 1\nblt s0, 10, top\n\nMispredict\n\nFlush!\n\n● On a misprediction, every following instruction is flushed\no Not only the control dependent instructions (li s2, 0)\no But also multiple iterations of the “bystander” loop that were fetched\n28\n\n\fSolution 4: Predication\n● Predicate: a Boolean value used for conditional execution\no Instructions that use predicates are said to be predicated\no A predicated instruction will modify state only if predicate is true\no ISA is modified to add predicated versions for all instructions\n\n● Example of code generation using predication:\npge p1, s1, 0\n# Store boolean s1 >= 0 to predicate p1\nli.p s2, 0, p1\n# Assign 0 to s2 if p1 is true\nsw.p s3, 0(s4), p1 # Store s3 to address 0(s4) if p1 is true\n\n● Now there is no branch. It is just straight-line code!\no Control dependencies have been converted to data dependencies\n\n29\n\n\fCode with predication\n● Now there are no branches!\n\nif(s1 >= 0)\ns2 = 0;\nfor(s0 = 0 .. 10)\ns3 = s3 + s0;\n\npge p1, 0, s1\nli.p s2, 0, p1\ntop:\nadd s3, s3, s0\naddi s0, s0, 1\nblt s0, 10, top\n\n● Drawback: even if branch not taken, li.p fetched (acts like a bubble)\no But often worth it for hard to predict branches!\no For easy to predict branches, often not worth it.\n30\n\n\fWhat does predication mean for the pipeline?\n● Again, predicates are registers just like any other register\n● Predicate dependencies work just like other data dependencies\nTime\n\n0\n\n1\n\n2\n\n3\n\n4\n\npge p1, 0, s1\n\nIF\n\nID\n\nEX\n\nMEM\n\nWB\n\nIF\n\nID\n\nEX\n\nMEM\n\nli.p s2, 0, p1\n\n5\n\n6\n\n7\n\nWB\n\n● With data forwarding, no stalls required!\no Predicate forwarded to li.p EX stage\no Later predicate enables\/disables regwrite control in li.p WB stage\n31\n\n\fWhat does predication mean for the compiler?\n● Compiler can schedule instruction more freely!\n\nif(s1 >= 0)\ns2 = 0;\nfor(s0 = 0 .. 10)\ns3 = s3 + s0;\n\npge\n\np1, 0, s1\n\ntop:\nadd s3, s3, s0\naddi s0, s0, 1\nblt s0, 10, top\nli.p s2, 0, p1\n\n● Low-power compiler-scheduled processors often support predicates\n32\n\n\fPredication in the Real World\n● Predication is only beneficial for hard to predict branches\n\n● So how does the compiler figure out the hard to predict branches?\no Through code analysis\no Through software profiling (model a branch predictor)\n● Supported in various ISAs\no ARM allows most instructions to be predicated\no Intel x86 has conditional move instructions (cmov)\no SIMD architectures use predication in the form of a logical mask\n▪ Only data items that are not masked are updated\n▪ Intel AVX vector instructions\n▪ GPU instructions (e.g. CUDA)\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":218,"segment": "unlabeled", "course": "cs1550", "lec": "lec11", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Project 1: due on 2\/18\n• Homework 5: due 2\/21\n• Lab 2: due on 2\/28\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fPrevious lecture …\n• Deadlock detection and avoidance using the\nBanker’s algorithm\n• Sleepy Barbers problem\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fMuddiest Points\n• Checked on Tophat\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSleepy Barbers Solution: Take 2\nstruct mysems {\nSemaphore RV1a(0), RV1b(0), RV2a(0), RV2b(0);\n};\nSharedBuffer buff; \/\/producers-consumers problem\nWorker Process\n\nCustomer Process\n\nstruct mysems sems = buff.consume();\n\nstruct mysems sems = new struct mysems\n\nup(sems.RV1a);\n\nbuff.produce(sems);\n\ndown(sems.RV1b);\n\ndown(sems.RV1a);\n\n\/\/do work\n\nup(sems.RV1b);\n\ndown(sems.RV2a);\n\n\/\/get work\n\nup(sems.RV2b);\n\nup(sems.RV2a);\n\n\/\/check-in for next customer\n\ndown(sems.RV2b);\n\/\/leave\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fQuestions of the Day\n• How to implement condition variables?\n\n• Reflect more on all the solutions\/problems that we\nhave studied\n\n\fUser-level implementation of Condition Variables\nA Lock with two waiting queues\n\nstruct Lock {\nSemaphore mutex(1);\nSemaphore next(0);\nint nextCount = 0;\nRelease(){\n\n}\n\nif(nextCount > 0){\n\nAcquire(){\n\nnext.up();\n\nmutex.down();\n\nnextCount--;\n\n}\n\n} else mutex.up();\n}\n\n\fCondition Variable\nstruct ConditionVariable {\nSemaphore condSem(0);\nint semCount = 0;\nLock *lk;\n}\nWait(){\nif(lk->nextCount > 0)\n\nSignal(){\n\nlk->next.up();\n\nif(semCount > 0){\n\nlk->nextCount--;\n\ncondSem.up()\nlk->nextCount++\n\nelse {\n\nlk->next.down();\n\nlk->mutex.up();\n\nlk->nextCount—\n\n}\nsemCount++;\ncondSem.down();\nsemCount--;\n}\n\n}\n}\n\n\fLock and Condition Variable Implementation\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fImplementing locks with semaphores\n•\n\nUse mutex to ensure exclusion within the lock bounds\n\n•\n\nUse next to give lock to processes with a higher priority (why?)\n\n•\n\nnextCount indicates whether there are any higher priority waiters\n\nclass Lock {\nSemaphore mutex(1);\nSemaphore next(0);\nint nextCount = 0;\n};\nLock::Acquire()\n{\nmutex.down();\n}\nLock::Release()\n{\nif (nextCount > 0)\nnext.up();\nelse\nmutex.up();\n}\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fImplementing condition variables\n•\n\nAre these Hoare or Mesa semantics?\n\n•\n\nCan there be multiple condition variables for a single Lock?\n\nclass Condition {\nLock *lock;\nSemaphore condSem(0);\nint semCount = 0;\n};\nCondition::Wait ()\n{\nsemCount += 1;\nif (lock->nextCount > 0)\nlock->next.up();\nelse\nlock->mutex.up();\ncondSem.down ();\nsemCount -= 1;\n}\n\nCondition::Signal ()\n{\nif (semCount > 0) {\nlock->nextCount += 1;\ncondSem.up ();\nlock->next.down ();\nlock->nextCount -= 1;\n}\n}\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fProcess Synchronization inside Monitors\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fCondition Variable-based Solutions\n• Code Walkthrough at:\n\nhttps:\/\/cs1550-2214.github.io\/cs1550-codehandouts\/ProcessSynchronization\/Slides\/\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fReflections on semaphore usage\n• Semaphores can be used as\n• Resource counters\n• Waiting spaces\n• For mutual exclusion\n\n\fReflections on Condition Variables\n• Define a class and put all shared variables inside the\nclass\n• Include a mutex and a condition variable in the class\n• For each public method of the class\n• Start by locking the mutex lock\n\n• If need to wait, use a while loop and wait on the condition\nvariable\n• Before broadcasting on the condition variable, make\nsure to change the waiting condition\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":219,"segment": "unlabeled", "course": "cs1567", "lec": "lec05_joy_node","text":"Joy Node\nThumrongsak Kosiyatrakul\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fJoy Node\njoy_node node communicates with a joystick connected via a\nUSB port\nProvide hardware abstractions\nTo receive data from a joystick, simply subscribe for messages\nfrom the topic named joy\nTo start a joy node, use the following series of commands on a\nconsole screen:\nrosparam set joy_node\/dev \"\/dev\/input\/js1\"\nrosrun joy joy_node\n\nMake sure roscore has been executed in another console\nscreen\nThe command rostopic list should show the following:\n\/diagnostics\n\/joy\n\/rosout\n\/rosout_agg\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fXbox 360 Controller\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fTopic joy\n\nThe joy node publishes joystick data to a topic named joy\nThe type of message is Joy\nThe message of type Joy is a data structure consisting of the\nfollowing components:\n1\n2\n3\n\nData structure named header,\nArray of floating-point numbers named axes, and\nArray of integers named buttons\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fbuttons\nArray of integers named buttons are organized as follows:\nIndex Button name on the controller\n0\nA\n1\nB\n2\nX\n3\nY\n4\nLB\n5\nRB\n6\nback\n7\nstart\n8\npower\n9\nButton stick left\n10\nButton stick right\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\faxes\n\nArray of floating-point numbers named axes are organized as\nfollows:\nIndex Button name on the controller\n0\nLeft\/Right Axis stick left\n1\nUp\/Down Axis stick left\n2\nLT\n3\nLeft\/Right Axis stick right\n4\nUp\/Down Axis stick right\n5\nRT\n6\ncross key left\/right\n7\ncross key up\/down\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fTopic joy\n\nNote that if the state of joystick does not change, no messages\nwill be published to the topic joy\nIf you run the command\nrostopic echo -c joy\n\nyou may not see anything\nIf you press the button A, you may see something as shown below:\nheader:\nseq: 265\nstamp:\nsecs: 1464206519\nnsecs: 129026422\nframe_id: `'\naxes: [-0.0, -0.0, 1.0, -0.0, -0.0, 1.0, 0.0, 0.0]\nbuttons: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fTopic joy\n\nTo import the data structure joy to your Python script, you need\nto insert the following statement in the beginning:\n#!\/usr\/bin\/env python\nimport rospy\nfrom sensor_msgs.msg import Joy\n\nTo subscribe to the topic joy of type Joy, use the following\nstatement in your Python script:\nrospy.Subscriber(\"joy\", Joy, callback)\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fExample\n\n#!\/usr\/bin\/env python\nimport rospy\nfrom sensor_msgs.msg import Joy\ndef callback(data):\nprint data.buttons[0]\nprint data.axes[0]\ndef testJoy():\nrospy.init_node(\"joystick\", anonymous=True)\nrospy.Subscriber(\"joy\", Joy, callback)\nrospy.spin()\nif __name__ == '__main__':\ntestJoy()\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fControlling a Kobuki\nA robot can be controlled by sending a message of type Twist\nTo import the data structure Twist, use the following command:\nfrom geometry_msgs.msg import Twist\n\nTo create a variable named command of type Twist, use the\nfollowing command:\ncommand = Twist()\n\nWe will only use two components:\ncommand.linear.x\nValue can be between -1.0 to 1.0\nPositive value makes the robot moves forward\nNegative value makes the robot moves backward\n\ncommand.angular.z\nValue can be between -1.0 to 1.0\nPositive value makes the robot turns to its left\nNegative value makes the robot turns to its right\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fControlling a Kobuki\nTo control the robot, the message of type Twist must be publish\nto the following topic:\n\/mobile_base\/commands\/velocity\n\nExamples:\nIf you want to make the robot moves forward in a straight line at\nhalf speed:\ncommand.linear.x = 0.5\ncommand.angular.z = 0.0\n\nFor example, if you want to make the robot makes a stationary\nturn to its left at half speed:\ncommand.linear.x = 0.0\ncommand.angular.z = 0.5\n\nFor example, if you want to make the robot moves forward and\nturn at the same time:\ncommand.linear.x = 0.5\ncommand.angular.z = 0.5\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\fUse Simulator (for now)\nROS allows you to see how robot moves in a virtual world\nFirst, run the following command in a console screen:\nroslaunch kobuki_softnode full.launch\nOpen a new console screen and run the following command:\nrosrun rviz rviz\n\nIn rviz\n1\n2\n\nOn the left pane, set Fixed Frame to \/odom\nAt the bottom left corner, click Add and choose RobotModel\n\nYou should see an image of a Kobuki robot on your screen\nAt this point, if you publish a command of type Twist, robot\nwill move according to the command\n\nThumrongsak Kosiyatrakul\n\nJoy Node\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":220,"segment": "unlabeled", "course": "cs1550", "lec": "lec20", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Lab 3: due on 4\/1\n• Homework 9: due on 4\/4\n• Project 3: due on 4\/11\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• Large Page Table problem\n• multi-level page tables\n• inverted page tables\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points\n• EAT\n• Inverted page tables\n• Would you please solve a problem about Inertial\nPage Table? Thank you so much\n• why again will there be multiple memory accesses\nusing a TLB?\n• Two level page tables and how they are better than\nsingle page tables (take up less space)\n• Where are the second and third level page tables\nstored? Are they paged in and out of memory like a\nnormal page?\n• when will we get our midterm grades\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fHow big should a page be?\n◼\n\nSmaller pages have advantages\nLess internal fragmentation\n◼ Better fit for various data structures, code sections\n◼ Less unused physical memory (some pages have 20\nuseful bytes and the rest isn’t needed currently)\n◼\n\n◼\n\nLarger pages are better because\n◼\n\nLess overhead to keep track of them\nSmaller page tables\n◼ TLB can point to more memory (same number of pages, but\nmore memory per page)\n◼ Faster paging algorithms (fewer table entries to look\nthrough)\n◼\n\n◼\n\nMore efficient to transfer larger pages to and from disk\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fSharing Pages\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fSharing pages\n◼\n\nProcesses can share pages\nEntries in page tables point to the same physical page\nframe\n◼ Easier to do with code: no problems with modification\n◼\n\n◼\n\nVirtual addresses in different processes can be…\nThe same: easier to exchange pointers, keep data\nstructures consistent\n◼ Different: may be easier to actually implement\n◼\n\nNot a problem if there are only a few shared regions\n◼ Can be very difficult if many processes share regions with\neach other\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fPage Sharing\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fImplementation issues\nFour times when OS involved with paging\n◼ Process creation\nDetermine program size\n◼ Create page table\n◼\n\n◼\n\nDuring process execution\nReset the MMU for new process\n◼ Flush the TLB (or reload it from saved state)\n◼\n\n◼\n\nPage fault time\nDetermine virtual address causing fault\n◼ Swap target page out, needed page in\n◼\n\n◼\n\nProcess termination time\nRelease page table\n◼ Return pages to the free pool\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fHow is a page fault handled?\nHardware causes a page\nfault\n◼ General registers saved (as\non every exception)\n◼ OS determines which virtual\npage needed\n◼\n\nActual fault address in a\nspecial register\n◼ Address of faulting instruction\nin register\n\n◼\n\n◼\n\n◼\n\n◼\n\n◼\n\nPage fault was in fetching\ninstruction, or\n◼ Page fault was in fetching\noperands for instruction\n◼ OS must figure out which…\n◼\n\nOS checks validity of address\n\n◼\n\n◼\n◼\n\n◼\n◼\n\n◼\n\nProcess killed if address was\nillegal\n\nOS finds a place to put new\npage frame\nIf frame selected for\nreplacement is dirty, write it\nout to disk\nOS requests the new page\nfrom disk\nPage tables updated\nFaulting instruction backed up\nso it can be restarted\nFaulting process scheduled\nRegisters restored\nProgram continues\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fPage locking\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fLocking pages in memory\nVirtual memory and I\/O occasionally interact\n◼ P1 issues call for read from device into\nbuffer\n◼\n\nWhile it’s waiting for I\/O, P2 runs\n◼ P2 has a page fault\n◼ P1’s I\/O buffer might be chosen to be paged out\n◼\n\n◼\n\n◼\n\nThis can create a problem because an I\/O device is\ngoing to write to the buffer on P1’s behalf\n\nSolution: allow some pages to be locked into\nmemory\nLocked pages are immune from being replaced\n◼ Pages only stay locked for (relatively) short\nperiods\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fMap of MMU, TLB, Page Table, etc.\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fWhen are dirty pages written to disk?\n◼\n\nOn demand (when they’re replaced)\nFewest writes to disk\n◼ Slower: replacement takes twice as long (must wait for\ndisk write and disk read)\n◼\n\n◼\n\nPeriodically (in the background)\n◼\n\n◼\n\nBackground process scans through page tables, writes\nout dirty pages that are pretty old\n\nBackground process also keeps a list of pages\nready for replacement\nPage faults handled faster: no need to find space on\ndemand\n◼ Cleaner may use the same structures discussed\nearlier (clock, etc.)\n◼\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fProblem of the Day\n\nHow to allocate disk blocks to files and directories?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fHow complex is the OS’s job?\nLet’s look at one of the resources managed by the OS:\nI\/O devices\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fCarving up the disk\nEntire disk\nPartition table\nMaster\nboot record\n\nPartition 1\n\nPartition 2\n\nPartition 3\n\nBoot\nblock\n\nFree space\nmanagement\n\nIndex\nnodes\n\nFiles & directories\n\nSuper\nblock\n\nCS 1550 – Operating Systems – Sherif Khattab\n\nPartition 4\n\n17\n\n\fContiguous allocation for file blocks\n• Contiguous allocation requires all blocks of a file to\nbe consecutive on disk\n• Problem: deleting files leaves “holes”\n• Similar to memory allocation issues\n• Compacting the disk can be a very slow procedure…\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\nA\n\nFree\n\nC\n\nFree\n\nE\n\nF\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fContiguous allocation\n• Data in each file is stored in\nconsecutive blocks on disk\n• Simple & efficient indexing\n•\n•\n\nStarting location (block #) on disk\n(start)\nLength of the file in blocks (length)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n• Random access well-supported\n• Difficult to grow files\n•\n\nMust pre-allocate all needed space\n\n•\n\nWasteful of storage if file isn’t using\nall of the space\n\n• Logical to physical mapping is easy\nblocknum = (pos \/ 1024) + start;\noffset_in_block = pos % 1024;\n\nStart=5\nLength=2902\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fLinked allocation\n• File is a linked list of disk\nblocks\n\n0\n\n•\n\nBlocks may be scattered\naround the disk drive\n\n4\n\n•\n\nBlock contains both pointer\nto next block and data\n\n4\n\n•\n\nFiles may be as long as\nneeded\n\n• New blocks are allocated\nas needed\n•\n•\n\nLinked into list of blocks in\nfile\nRemoved from list (bitmap)\nof free blocks\n\n1\n\n3\n6\n\n5\n\nx\n\n8\n\n2\n\n6\n\n7\n\nx\n\n9\n\n10\n\n11\n\n0\n\nStart=9\nEnd=4\nLength=2902\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\nStart=3\nEnd=6\nLength=1500\n20\n\n\fData Structures for Linked Allocation\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n21\n\n\fFinding blocks with linked allocation\n• Directory structure is simple\n•\n•\n\nStarting address looked up from directory\nDirectory only keeps track of first block (not others)\n\n• No wasted blocks - all blocks can be used\n• Random access is difficult: must always start at first\nblock!\n• Logical to physical mapping is done by\nblock = start;\noffset_in_block = pos % 1020;\nfor (j = 0; j < pos \/ 1020; j++) {\nblock = block->next;\n}\n•\n•\n\nAssumes that next pointer is stored at end of block\nMay require a long time for seek to random location in file\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n22\n\n\fOffset Calculation\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n23\n\n\fFile Allocation Table (FAT)\n• Links on disk are slow\n• Keep linked list in memory\n\n• Advantage: faster\n• Disadvantages\n• Have to copy it to disk at some point\n• Have to keep in-memory and on-disk\ncopy consistent\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n4\n-1\n-1\n-2\n-2\n-1\n3\n-1\n-1\n0\n-1\n-1\n-1\n-1\n-1\n-1\n\nB\n\nA\n\n24\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":221,"segment": "unlabeled", "course": "cs1550", "lec": "lec08", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines\n• Homework 4 is due next Monday 2\/14\n• Project 1 due on 2\/18\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• It is easy to make mistakes when using semaphores\n\n• Solution: Mutex and Condition Variables\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fProblem of the Day\nReaders & Writers\n•\n\nMany processes that may read and\/or write\n\n•\n\nOnly one writer allowed at any time\n\n•\n\nMany readers allowed, but not while a process is writing\n\n•\n\nReal-world Applications\n•\n\nDatabase queries\n\n•\n\nWe have this problem in Project 1\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fSemaphore-based Solution\nShared variables\nint nreaders;\nSemaphore mutex(1), writing(1);\n\nReader process\n\nWriter process\n\n…\nmutex.down();\nnreaders += 1;\nif (nreaders == 1) \/\/ wait if\nwriting.down(); \/\/ 1st reader\nmutex.up();\n\/\/ Read some stuff\nmutex.down();\nnreaders -= 1;\nif (nreaders == 0)\n\/\/ signal if\nwriting.up();\n\/\/ last reader\nmutex.up();\n…\n\n…\nwriting.down();\n\/\/ Write some stuff\nwriting.up();\n…\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fSolution Tracing\n• enterRead\n\nReader process\n…\nmutex.down();\nnreaders += 1;\nif (nreaders == 1) \/\/ wait if\nwriting.down(); \/\/ 1st reader\nmutex.up();\n\/\/ Read some stuff\nmutex.down();\nnreaders -= 1;\nif (nreaders == 0)\n\/\/ signal if\nwriting.up(); \/\/ last reader\nmutex.up();\n…\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fSolution Tracing\n• read\n\nReader process\n…\nmutex.down();\nnreaders += 1;\nif (nreaders == 1) \/\/ wait if\nwriting.down(); \/\/ 1st reader\nmutex.up();\n\/\/ Read some stuff\nmutex.down();\nnreaders -= 1;\nif (nreaders == 0)\n\/\/ signal if\nwriting.up(); \/\/ last reader\nmutex.up();\n…\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fSolution Tracing\n• doneRead\n\nReader process\n…\nmutex.down();\nnreaders += 1;\nif (nreaders == 1) \/\/ wait if\nwriting.down(); \/\/ 1st reader\nmutex.up();\n\/\/ Read some stuff\nmutex.down();\nnreaders -= 1;\nif (nreaders == 0)\n\/\/ signal if\nwriting.up(); \/\/ last reader\nmutex.up();\n…\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fWriter Events\n• enterWrite\n\nWriter process\n…\nwriting.down();\n\/\/ Write some stuff\nwriting.up();\n…\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fWriter Events\n• write\n\nWriter process\n…\nwriting.down();\n\/\/ Write some stuff\nwriting.up();\n…\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fWriter Events\n• doneWrite\n\nWriter process\n…\nwriting.down();\n\/\/ Write some stuff\nwriting.up();\n…\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fSequence 1\n• W0 enterWrite\nReader process\n\n• W0 write\n\n…\nmutex.down();\nnreaders += 1;\nif (nreaders == 1) \/\/ wait if\nwriting.down(); \/\/ 1st reader\nmutex.up();\n\/\/ Read some stuff\nmutex.down();\nnreaders -= 1;\nif (nreaders == 0)\n\/\/ signal if\nwriting.up();\n\/\/ last reader\nmutex.up();\n…\n\n• R0 enterRead\n• R1 enterRead\n• R2 enterRead\n• W0 doneWrite\n• R2 read\n• W1 enterWrite\n\n• R2 doneRead\n• W1 write\n\nWriter process\n…\nwriting.down();\n\/\/ Write some stuff\nwriting.up();\n…\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\f•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n\nSequence 2\nR0 enterRead\nR0 read\nR1 enterRead\nR1 read\nW0 enterWrite\nR2 enterRead\nR2 read\nR2 doneRead\nR1 doneRead\nR0 doneRead\nW0 write\nW0 doneWrite\n\nReader process\n…\nmutex.down();\nnreaders += 1;\nif (nreaders == 1) \/\/ wait if\nwriting.down(); \/\/ 1st reader\nmutex.up();\n\/\/ Read some stuff\nmutex.down();\nnreaders -= 1;\nif (nreaders == 0)\n\/\/ signal if\nwriting.up();\n\/\/ last reader\nmutex.up();\n…\n\nWriter process\n…\nwriting.down();\n\/\/ Write some stuff\nwriting.up();\n…\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fSolution using Mutex and Condition Variables\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":222,"segment": "unlabeled", "course": "cs1622", "lec": "lec02","text":"Rust, Strings, and Text\nCS\/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n● I'm assuming, for this lecture, that you really did follow along with\nchapters 1-3 of the Rust book\n● also, if you didn’t see the announcement yesterday…\no go look :^)\n● the examples from today are in the examples repo\no see the materials page!\n\n2\n\n\fVectors\n\n3\n\n\fVectors!\n● Rust's Vec<T> can work like a Java T[] or ArrayList<T>.\nlet v = vec![1, 2, 3, 4, 5];\nthis creates a new, immutable (unchangeable) Vec.\n\nlet mut v = vec![1, 2, 3, 4, 5];\nthis one can be modified, appended, etc.\n\nfor the most part, they work like Java arrays\/ArrayLists:\nlet x = v[3];\n\n\/\/ indexing\n\nlet l = v.len(); \/\/ length\n\nv.push(6);\n\n\/\/ appends a 6\n4\n\n\fIteration\n● Rust has a system similar to Java's Iterable\/Iterator interfaces.\no .iter() is like Java's .iterator() method.\n● the for-in loop iterates over iterators.\nfor i in 0 .. 10 {}\nfor i in 1 ..= 10 {}\n\n\/\/ i = 0, 1, 2, …, 8, 9\n\/\/ i = 1, 2, …, 9, 10\n\nlet v = vec![1, 2, 3];\nfor val in v.iter() {} \/\/ iterate over values in v\nfor val in &v {}\n\/\/ same as above, but shorter\nfor (i, val) in v.iter().enumerate() {}\n\nthe last one shows calling a method on an iterator object to\nget a new iterator – this one gives the index and value.\n5\n\n\fan Extremely Common Pattern: Mapping\n● you have an array A.\n● you want to perform some transformation on each item in A.\n● you want to put the results of those transformations into an array B.\nint[] B = new int[A.length()];\nfor(int i = 0; i < A.length(); i++) {\nB[i] = A[i] * 2;\n}\nthis operation is called mapping, and it's so, so common.\nin Rust, it looks like this:\n\nlet B = A.iter().map(|x| x * 2).collect();\nthis is a function literal, or\nanonymous function, or \"lambda.\"\n6\n\n\fAlgebraic Data Types\nand Structs\ntime check: ≤ 35 min\n\n7\n\n\fNot the same as ADTs (abstract data types)\n● primitive types contain a single value, e.g. i32, f32, bool, char…\n● algebraic types let us combine types into larger ones.\nProduct Types bundle\nvalues \"side-by-side\".\n\nSum Types hold one of\nseveral choices or variants.\n\nclass Point {\nint x;\nint y;\n}\n\nboolean b;\nb = true;\nb = false;\n\/\/ that's it.\n\nJava classes are product\ntypes. each Point can\nhold any combination\nof two integers.\n\nJava's boolean can only\nbe one of two possibilities,\nbut that's built into the\nlanguage for you.\n8\n\n\fRust's product types: Structs\n● structs are the main kind. (there are also \"tuples.\")\nstruct Point {\nx: i32,\ny: i32,\n}\n\nthey're like Java classes, but no\ninheritance, no interfaces, no\nmethods, just fields.\n\nlet p = Point { x: 10, y: 20 };\nyou construct a struct by listing the values for each field.\nprintln!(\"{}, {}\", p.x, p.y);\nand you access fields with . just like in Java, C, etc.\nother than privacy (put pub on a field to make it public), that's it.\n9\n\n\f\"Constructors\" and \"methods\"\n● Rust does not have classes and is not an OOP language.\n● but it does let us add methods to any type, with impl.\nimpl Point {\nfn new(x: i32, y: i32) -> Point {\nPoint { x, y }\n}\nfn flip_x(&self) -> Point {\nPoint { x: -self.x, y: self.y }\n}\n}\n\na Rust convention is\nto have a constructor\nfunction named new.\n\nmethods are made\nby writing &self as\nthe first argument.\n\nnow we can access these as\nPoint::new() and p.flip_x().\n10\n\n\fA side note - :: vs .\n● in Java, you use . to access all \"things inside other things\".\no e.g. java.util.Arrays, Integer.parseInt()\n● in Rust, the . operator is only used on objects.\no when you see . it always means \"get a field from a struct\" or \"call a\nmethod on some value.\"\n● for anything else, you use ::\no e.g. std::cmp::Ordering, Point::new()\no I like to think of :: as being like the (back)slashes in file paths\n▪ C:\\Windows\\system32\n▪ \/bin\/sh\n\n11\n\n\fFlavors of 'self'\n● inside an impl there are a few kinds of functions:\nimpl MyStruct {\nfn new() -> MyStruct\nwith no self argument, it's an associated function.\nconstructors are the most common example.\n\nfn method(&self)\nwith a &self, it's a method, but cannot change any fields\nin the object it's called on.\n\nfn mutator(&mut self)\nwith a &mut self, it's a method that can change the\nobject, but can only be called on mut variables.\n12\n\n\fSum Types\ntime check: ≤ 65 min\n\n13\n\n\fA motivating example\n● the classic Animal class hierarchy might look like this in Java:\nabstract class Animal {\nan Animal variable\nabstract void speak();\nwould be able to hold an\n}\ninstance of any of these.\nclass Cat extends Animal {\nvoid speak() { println(\"meow!\"); }\ncommon fields and\n}\nmethods go in Animal;\nclass Dog extends Animal {\nunique things go in the\nvoid speak() { println(\"woof!\"); }\nsubclasses.\n}\nclass Camel extends Animal {\nvoid speak() { println(\"ghhghhg!\"); }\n}\n14\n\n\fThe equivalent (?) Rust\n● when you want to have a choice of types, you use an enum.\nenum Animal {\nCat, Dog, Camel\n}\n\nwhat's happening here is not really\nlike the Java code, though.\n\nit's much more like a boolean: a fixed set\nimpl Animal {\nof possible values, and an Animal\nfn speak(&self) {\nuse Animal::*;\nvariable can only take on those values.\nmatch self {\nCat\n=> println!(\"meow!\"),\nbut unlike Java, we get\nDog\n=> println!(\"woof!\"),\nto define our own types\nCamel => println!(\"ghhghhg!\"),\nlike this in Rust.\n}\n}\n}\n15\n\n\fCommonalities and differences\n● in OOP, we can reuse code\/fields by putting them in the base class…\no and we can specialize code\/fields in the subclasses.\nabstract class Animal {\nfloat weight;\nAnimal(float w) { weight = w; }\nvoid printWeight() { println(weight); }\n}\nclass Cat extends Animal {\nhere, weight is common to\nString pattern;\nall Animals, but pattern is\nCat(float w, String p) {\nunique to Cats.\nsuper(w); pattern = p;\n}\nvoid printPattern() { println(pattern); }\n}\n16\n\n\fWhen a struct and an enum love each other very\nmuch\n\n● in Rust, this can be represented with this pattern that has no name:\nstruct Animal {\nweight: f32,\nkind:\nAnimalKind,\n}\n\ncommon fields appear in this\nAnimal struct. but it also has a\nkind field which is an enum.\n\nenum AnimalKind {\nCat\n{ pattern: String },\nDog\n{ loudness: i32 },\nCamel { num_humps: i32 },\n}\n\nunique fields appear in the\nenum variants.\n\nthis pattern is going to show up over and over!\nit definitely looks different than the Java code, but\nthe same concepts are coming into play.\n17\n\n\fLooking inside enums\n● there are two ways to see what's inside the enum:\n\/\/ \"if k is an AnimalKind::Cat,\n\/\/ extract its data and assign it to 'pattern'.\"\nif let AnimalKind::Cat { pattern } = &self.kind {\nprintln!(\"cat with pattern: '{}'\", pattern);\n}\nmatch &self.kind { .. means \"throw away\" or \"I don't care\"\nAnimalKind::Cat{..} => println!(\"meow!\"),\nAnimalKind::Dog{..} => println!(\"woof!\"),\n_\n=> println!(\"???\"),\n}\n18\n\n\fNullPointerException? Never heard of it\n● there is no such thing as null in Rust! 🎉 🎉 🎉 🎉\n● well, there's something like it, but it's opt-in, not forced on you.\n● it's an enum called Option.\nlet nullable_ints: Vec<Option<i32>> = vec![\nSome(1), Some(45), None, Some(7) ];\nfor i in nullable_ints {\nif let Some(v) = i { println!(\"{}\", v); }\n}\n\nOption is so widely used that it's built into the compiler.\nyou don't have to write Option::None, for instance.\n\n19\n\n\fStrings and Unicode\ntime check: ≤ 90 min\n\n20\n\n\fSoooo easy\n● lexing is the process of splitting the source text into tokens.\n● that means we'll have to deal with strings as the input to our lexer.\nif you're a native English speaker, you're probably\nused to thinking of strings as working like this:\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nh\n\ne\n\nl\n\nl\n\no\n\n! \\n :\n\n) \\n\n\n104\n\n101\n\n108\n\n108\n\n111\n\n33\n\n41\n\n10\n\n7\n\n58\n\n8\n\n9\n\n10\n\na string is an array of characters, where each\ncharacter is encoded as a small integer. easy, right?\n21\n\n\fThose dang human languages\n● the reality is: it's way more complicated than that.\n● see StringWeirdness.java for some… string weirdness.\nString s = \"tést\";\nString t = \"tést\";\nSystem.out.println(s.length()); \/\/ 4\nSystem.out.println(s.charAt(1)); \/\/ é\nSystem.out.println(t.length()); \/\/ 5??\nSystem.out.println(t.charAt(1)); \/\/ e??\n\nwelcome to the wonderful world of Unicode!!!\n\n22\n\n\fUnicode\n● Unicode is The String Encoding that the whole world* uses now.\n● it attempts to map every written character in every human language\nto a number, its codepoint.\n● but it's more complicated than that, because human writing systems.\n\né = [U+00E9]\né = [U+0065, U+0301]\ne + ´\n\nthis is a combining mark: it\nmodifies the previous glyph.\n23\n\n\fUTF-8? UTF-16? UTF-32? BE? LE?\n● each codepoint is conceptually a 21-bit number.\n● Unicode text is encoded into one of a few transformation formats.\n● the encodings of [U+0074, U+00E9, U+0073, U+0074]:\nUTF-32: each codepoint\nis a 32-bit int. simple,\nbut wastes space.\n\nLE 74 00 00 00 e9 00 00 00 73 00 00 00 74 00 00 00\nBE 00 00 00 74 00 00 00 e9 00 00 00 73 00 00 00 74\n\nUTF-16: each is a 16-bit int…\nunless it's in a certain range, in\nwhich case it's two 16-bit ints.\nUTF-8: ASCII characters are 1 byte;\nall others are 2, 3, or 4 bytes long.\n\nLE\n\n74 00 e9 00 73 00 74 00\n\nBE\n\n00 74 00 e9 00 73 00 74\n\n74 c3 a9 73 74\n\nUTF-8 has become the dominant encoding.\n24\n\n\fSuch sights to show you\n● Rust uses UTF-8 strings (and so do many other languages).\n● Unicode and UTF-8 mean you have to abandon many of your\nassumptions about how strings work.\nwhat is this string's length?\n\ntあ̊st\n\n74 e3 81 82 cc 8a 73 74\nin bytes? 0\n1 2 3 4 5 6 7 len=8\nin codepoints? 0\n1\n2\n3 4 len=5\nin \"grapheme 0\nlen=4\n1\n2\n3\nclusters?\"\nalso, because each codepoint is a different number of bytes,\nindexing by codepoints is not necessarily O(1)!\n\n25\n\n\fOh god\n● what about getting a substring?\n● what about reversing strings?\n● what about toUpper\/toLower?\n● if we loop over a string, do we see bytes, codepoints, or clusters?\n● should the \"e with acute\" precomposed character compare equal to\nthe \"e followed by combining acute\" character pair?\n● what about collating (\"alphabetizing\")?\n● what if there are multiple combining marks on a character? does\nthe order of those marks affect its display or meaning?\n\nit's not Unicode's fault, it's ours.\n26\n\n\fWhat about Java?\n● Java adopted Unicode early! …maybe a little too early.\n● in Java, String is UTF-16, and char is 16 bits (2 bytes).\no at the time Java was created, that's what Unicode was.\n● but Unicode was extended to 21 bits after Java adopted it. so…\no .charAt() is O(1), nice!\no …but you might only get half a character??\n● see, codepoints above U+FFFF are encoded as two Java chars\no e.g. many emoji are in this range\n● so, \"proper\" codepoint indexing is O(n) in Java too\no there is a .codePointAt() method to help, at least\no and a .codePoints() iterator\n\n27\n\n\fLiving in a post-ASCII world\n● do not assume:\no …that your program will only be used by English speakers.\no …that your program will only be fed ASCII text.\no …that there is one definition of \"string length.\"\no …that string indexing\/substring is a constant-time operation.\n● do:\no trust the people who do this stuff for a living.\no read about how your programming language handles text.\no write your text processing to be language-agnostic.\no prefer iterating over strings instead of indexing them, if possible.\no look into canonicalization if you're dealing with thorny issues.\no accept that some Rust string stuff looks weird\/complicated for a\nvery good reason: text is weird\/complicated.\n28\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":223,"segment": "unlabeled", "course": "cs1541", "lec": "lec2.5_vliw_processors","text":"VLIW Processors\nCS 1541\nWonsun Ahn\n\n\fLimits on Deep Pipelining\n● Ideally, CycleTimePipelined = CycleTimeSingleCycle \/ Number of Stages\no In theory, can indefinitely improve performance with more stages\n● Limitation 1: Cycle time does not improve indefinitely\no With shorter stages, delay due to latches become significant\no With many stages, hard to keep stage lengths perfectly balanced\no Manufacturing variability exacerbates the stage length unbalance\n● Limitation 2: CPI tends to increase with deep pipelines\no Penalty due to branch misprediction increases\no Stalls due to data hazards cause more bubbles\n● Limitation 3: Power consumption increases with deep pipelines\no Wires for data forwarding increase quadratically with depth\n● Is there another way to improve performance?\n2\n\n\fWhat if we improve CPI?\n● Remember the three components of performance?\ninstructions\nX\nprogram\n\ncycles\ninstruction\n\nX\n\nseconds\ncycle\n\n● Pipelining focused on seconds \/ cycle, or cycle time\n● Can we improve cycles \/ instruction, or CPI?\no But the best we can get is CPI = 1, right?\no How can an instruction be executed in less than a cycle?\n\n3\n\n\fWide Issue Processors\n\n4\n\n\fFrom CPI to IPC\n● How about if we fetch two instructions each cycle?\no Maybe, fetch one ALU instruction and one load\/store instruction\n\n● Then, IPC (Instructions per Cycle) = 2\no And by extension, CPI = 1 \/ IPC = 0.5 !\n● Wide-issue processors can execute multiple instructions per cycle\n5\n\n\fPipeline design for previous example\n● One pipeline for ALU\/Branches and one for loads and stores\n\nI-Mem\n\nIns. Decoder\n\nALU\nPipeline\nRegister\nFile\n\nMemory\nPipeline\n\nALU\n\n+\n\nD-Mem\n\n● This introduces new structural hazards that we didn’t have before!\n6\n\n\fStructural Hazard in Storage Locations (Solved)\n\nI-Mem\n\nIns. Decoder\n\n● Two instructions must be fetched from instruction memory\n→ Add extra read ports to the instruction memory\n● Two ALUs must read from the register file at the same time\n→ Add extra read ports to the register file\n● Two instructions must write to register file at WB stage (not shown)\n→ Add extra write ports to the register file\n\nRegister\nFile\n\nALU\n\n+\n\nD-Mem\n7\n\n\fStructural Hazard in Functional Units (Still Remaining)\n\nI-Mem\n\nIns. Decoder\n\n● Structural hazard on EX units\no Top ALU can handle all arithmetic ( +, -, *, \/)\no Bottom ALU can only handle +, needed for address calculation\n● Structural hazard on MEM unit\no ALU pipeline does not have a MEM unit to access memory\n\nRegister\nFile\n\nALU\n\n+\n\nD-Mem\n8\n\n\fStructural Hazard in Functional Units: Example\n● Code on the left will result in a timeline on the right\no If it were not for the bubbles, we could have finished in 4 cycles!\n\nlw\n$t0, 0($s1)\nlw\n$t1, -4($s1)\naddi $t2, $t2, -8\nadd $t3, $t0, $s1\nadd $t4, $s1, $s1\nsw\n$t5, 8($t3)\nsw\n$t6, 4($s1)\n\nCC\n\nALU Pipeline\n\n1\n\nlw t0\n\n2\n\naddi t2\n\n3\n\nadd t3\n\n4\n\nadd t4\n\n5\n\nMem Pipeline\n\nlw t1\nsw t5\nsw t6\n\n9\n\n\fStructural Hazard Solution: Reordering\n● Of course we can come up with a better schedule\no While still adhering to the data dependencies\n\nlw\n$t0, 0($s1)\nlw\n$t1, -4($s1)\naddi $t2, $t2, -8\nadd $t3, $t0, $s1\nadd $t4, $s1, $s1\nsw\n$t5, 8($t3)\nsw\n$t6, 4($s1)\n\nCC\n\nALU Pipeline\n\n1\n\nlw t0\n\n2\n\naddi t2\n\n3\n\nadd t3\n\n4\n\nadd t4\n\n5\n\nMem Pipeline\n\nlw t1\nsw t5\nsw t6\n\n10\n\n\fWhy not just duplicate all resources?\n● Why not have two full ALUs, have MEM units at both pipelines?\no That way, we can avoid those structural hazards in the first place\no But that leads to low utilization\n§ ALU\/Branch type instructions will not use the MEM unit\n§ Load\/Store instructions will not need the full ALU\n● Most processors have specialized pipelines for different instructions\no Integer ALU pipeline, FP ALU pipeline, Load\/Store pipeline, …\no With scheduling, can achieve high utilization and performance\n● Who does the scheduling? Well, we talked about this already:\no Static scheduling → Compiler\no Dynamic scheduling → Processor\n11\n\n\fVLIW vs. Superscalar\n● There are two types of wide-issue processors\n● If the compiler does static scheduling, the processor is called:\no VLIW (Very Long Instruction Word) processor\no This is what we will learn this chapter\n● If the processor does dynamic scheduling, the processor is called:\no Superscalar processor\no This is what we will learn next chapter\n\n12\n\n\fVLIW Processors\n\n13\n\n\fVLIW Processor Overview\n● What does Very Long Instruction Word mean anyway?\no It means one instruction is very long!\no Why? Because it contains multiple operations in one instruction\n● A (64 bits long) VLIW instruction for our example architecture:\nALU\/Branch Operation (32 bits)\n\nLoad\/Store Operation (32 bits)\n\n● An example instruction could be:\naddi $t2, $t0, -8\n\nlw $t1, -4($s1)\n\n● Or another example could be:\nnop\n\nlw $t1, -4($s1)\n14\n\n\fA VLIW instruction is one instruction\n\nIns. Decoder\n\nI-Mem\n\nLoad\/Store Op\n\nALU Op\n\n● For all purposes, a VLIW instruction acts like one instruction\no It moves as a unit through the pipeline\n\nRegister\nFile\n\nALU\n\n+\n\nD-Mem\n\n15\n\n\fVLIW instruction encoding for example\nnop\nlw\n$t0, 0($s1)\naddi $t2, $t2, -8\nlw\n$t1, -4($s1)\nadd $t3, $t0, $s1\nnop\nadd $t4, $s1, $s1\nsw\n$t5, 8($t3)\nnop\nsw\n$t6, 4($s1)\n\nInst\n\nALU Op\n\nLoad\/Store Op\n\n1\n\nnop\n\nlw t0\n\n2\n\naddi t2\n\nlw t1\n\n3\n\nadd t3\n\nnop\n\n4\n\nadd t4\n\nsw t5\n\n5\n\nnop\n\nsw t6\n\n•\n•\n\nEach square is an instruction.\n(There are 5 instructions.)\nNops are inserted by the compiler.\n16\n\n\fVLIW instruction encoding (after reordering)\nadd $t4, $s1, $s1\nlw\n$t0, 0($s1)\naddi $t2, $t2, -8\nlw\n$t1, -4($s1)\nadd $t3, $t0, $s1\nsw\n$t6, 4($s1)\nnop\nsw\n$t5, 8($t3)\n\nInst\n\nALU Op\n\nLoad\/Store Op\n\n1\n\nadd t4\n\nlw t0\n\n2\n\naddi t2\n\nlw t1\n\n3\n\nadd t3\n\nsw t6\n\n4\n\nnop\n\nsw t5\n\n•\n\nSame program with 4 instructions!\n\n17\n\n\fVLIW Architectures are (Very) Power Efficient\n● All scheduling is done by the compiler offline\n● No need for the Hazard Detection Unit\no Nops are inserted by the compiler when necessary\n● No need for a dynamic scheduler\no Which can be even more power hungry than the HDU\n● Even no need for the Forwarding Unit\no If compiler is good enough and fill all bubbles with instructions\no Or, may have cheap compiler-controlled forwarding in ISA\n\n18\n\n\fChallenges of VLIW\n● All the challenges of static scheduling apply here X 2\n● Review: what were the limitations?\no Compiler must make assumptions about the pipeline\n→ ISA now becomes much more than instruction set + registers\n→ ISA restricts modification of pipeline in future generations\no Compiler must do scheduling without runtime information\n→ Length of MEM stage is hard to predict (due to Memory Wall)\n→ Data dependencies are hard (must do pointer analysis)\n● These limitations are exacerbated with VLIW\n19\n\n\fNot Portable due to Assumptions About Pipeline\n● VLIW ties ISA to a particular processor design\no One that is 2-wide and has an ALU op and a Load\/Store op\no What if future processors are wider or contain different ops?\n● Code must be recompiled repeatedly for future processors\no Not suitable for releasing general purpose software\no Reason VLIW is most often used for embedded software\n(Because embedded software is not expected to be portable)\n● Is there any way to get around this problem?\n\n20\n\n\fMaking VLIW Software Portable\n● There are mainly two ways VLIW software can become portable\n1. Allow CPU to exploit parallelism according to capability\no Analogy: multithreaded software does not specify number of cores\n§ SW: Makes parallelism explicit by coding using threads\n§ CPU: Exploits parallelism to the extent it has number of cores\no Portable VLIW: ISA does not specify number of ops in instruction\n§ SW: Makes parallelism explicit by using bundles\n– Bundle: a group of ops that can execute together\n– Wider processors fetch several bundles to form one instruction\n– A “stop bit” tells processor to stop fetching the next bundle\n§ Intel Itanium EPIC(Explicitly Parallel Instruction Computing)\n– A general-purpose ISA that uses bundles\n21\n\n\fMaking VLIW Software Portable\n● There are mainly two ways VLIW software can become portable\n2. Binary translation\no Have firmware translate binary to new VLIW ISA on the fly\no Very similar to how Apple Rosetta converts x86 to ARM ISA\no Doesn’t this go against the power efficiency of VLIWs?\n§ Yes, but if SW runs for long time, one-time translation is nothing\n§ Translation can be cached in file system for next run\no Transmeta Crusoe converted x86 to an ultra low-power VLIW\n\n22\n\n\fScheduling without Runtime Information\n● Up to the compiler to create schedule with minimal nops\no Use reordering to fill nops with useful operations\n● All the challenges of static scheduling remain\no Length of MEM stage is hard to predict (due to Memory Wall)\no Data dependencies are hard to figure out (due to pointer analysis)\n● But these challenges become especially acute for VLIW\no For 4-wide VLIW, need to find 4 operations to fill “one” bubble!\no Operations in one instruction must be data independent\n§ Data forwarding will not work within one instruction\n(Obviously because they are executing on the same cycle)\n\n23\n\n\fPredicates Help in Compiler Scheduling\n● Use of predicates can be a big help in finding useful operations\no Reordering cannot happen across control dependencies\nlw $t1, 0($s0)\naddi $t1, $t1, 1\nbne $s1, $s2, else\nthen:\nli $t0, 1\n\nCan only reorder within this block\n\nelse:\nli $t0, 0\n\no Predicates can convert if-then-else code into one big block\npne $p1, $s1, $s2\nlw $t1, 0($s0)\nli.p $t0, 1, !$p1\nli.p $t0, 0, $p1\naddi $t1, $t1, 1\n\nCan reorder within a larger window!\n\n24\n\n\fBut Predicates Cannot Remove Loopback Branches\n● Loops are particularly challenging to the compiler. Why?\no Scheduling is limited to within the loop\no For tight loops, not much compiler can do with a handful of insts\nloop:\nlw $s0, 0($t1)\naddi $s0, $s0, 10\naddi $t1, $t1, 1\nbne $s0, $s1, loop\n\nCan only reorder within this block\n\n…\n\nThis block is off limits!\n\n25\n\n\fCompiler Scheduling of a Loop\n● Suppose we had this example loop (in MIPS):\n\nLoop:\nlw\n$t0, 0($s1)\nadd $t0, $t0, $s2\nsw\n$t0, 0($s1)\naddi $s1, $s1, -4\nbne $s1, $zero, Loop\n\n\/\/ $t0 = array[$s1]\n\/\/ $t0 = $t0 + $s2\n\/\/ array[$s1] = $t0\n\/\/ $s1++\n\/\/ loopback if $s1 != 0\n\n● Loop iterates over an array adding $s2 to each element\n\n26\n\n\fCompiler Scheduling of a Loop\n● Let’s first reschedule to hide the use-after-load hazard\n\nLoop:\nlw\n$t0, 0($s1)\nadd $t0, $t0, $s2\nsw\n$t0, 0($s1)\naddi $s1, $s1, -4\nbne $s1, $zero, Loop\n\nLoop:\nlw\n$t0, 0($s1)\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nbne $s1, $zero, Loop\n\n● Now dependence on $t0 is further away\n● Note we broke a WAR (Write-After-Read) dependence on $s1\no Now $s1 in sw $t0, 0($s1) is result of addi $s1, $s1, -4\n\n● We must compensate by changing the sw offset by +4:\no sw $t0, 0($s1) → sw $t0, 4($s1)\n\n27\n\n\fCompiler Scheduling of a Loop\n● Below is the VLIW representation of the rescheduled MIPS code:\nLoop:\nALU\/Branch Op\nLoad\/Store Op\nlw\n$t0, 0($s1)\nLoop: addi $s1, $s1, -4 lw $t0, 0($s1)\naddi $s1, $s1, -4\nnop\nnop\nadd $t0, $t0, $s2\nadd $t0, $t0, $s2\nnop\nsw\n$t0, 4($s1)\nbne $s1, $0, Loop sw $t0, 4($s1)\nbne $s1, $zero, Loop\n\n● We can’t fill any further nops due to data hazards\n● In terms of MIPS instructions, IPC = 5 \/ 4 = 1.25\no Ideally, IPC can reach 2 so we are not doing very well here\n● Is there a way compiler can expand the “window” for scheduling?\no Idea: use multiple iterations of the loop for scheduling!\n\n28\n\n\fLoop unrolling\n\n29\n\n\fWhat is Loop Unrolling?\n● Loop unrolling : a compiler technique to enlarge loop body\no By duplicating loop body for an X number of iterations\n\nfor(i = 0; i < 100; i++)\nUnrolled loop (2X)\na[i] = b[i] + c[i];\nfor(i = 0; i < 100; i += 2){\nOriginal loop\na[i] = b[i] + c[i];\na[i+1] = b[i+1] + c[i+1];\n}\n● What does this buy us?\no More instructions inside loop to reorder and hide bubbles\no And less instructions to execute as a whole\n§ Less frequent loop branches\n§ Two i++ are merged into one i+= 2\n30\n\n\fLet’s try unrolling our example code\nLoop:\nlw\n$t0, 0($s1)\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nbne $s1, $zero, Loop\n\nLoop:\nlw\n$t0, 0($s1)\nUnroll 2X\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nlw\n$t1, -4($s1)\naddi $s1, $s1, -4\nadd $t1, $t1, $s2\nsw\n$t1, 4($s1)\nbne\n\n$s1, $zero, Loop\n\n● Instructions are duplicated but using $t1 instead of $t0\n● This is intentional to minimize false dependencies during reordering\n31\n\n\fNow time to reorder the code!\nLoop:\nlw\nlw\n\n$t0, 0($s1)\n$t1, -4($s1)\n\naddi $s1, $s1, -4\naddi $s1, $s1, -4\nadd\nadd\n\n$t0, $t0, $s2\n$t1, $t1, $s2\n\nsw\nsw\n\n$t0, 8($s1)\n$t1, 4($s1)\n\nbne\n\n$s1, $zero, Loop\n\nReorder!\n\nLoop:\nlw\n$t0, 0($s1)\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nlw\n$t1, -4($s1)\naddi $s1, $s1, -4\nadd $t1, $t1, $s2\nsw\n$t1, 4($s1)\nbne\n\n$s1, $zero, Loop\n\n● Interleaving iterations spaces out dependencies (2X = unroll factor)\n32\n\n\fMerge induction variable increment\nLoop:\nlw\nlw\n\n$t0, 0($s1)\n$t1, -4($s1)\n\naddi $s1, $s1, -4\naddi $s1, $s1, -4\nadd\nadd\nsw\nsw\nbne\n\n$t0, $t0, $s2\n$t1, $t1, $s2\n$t0, 8($s1)\n$t1, 4($s1)\n\nMerge\n\nLoop:\nlw\nlw\n\n$t0, 0($s1)\n$t1, -4($s1)\n\naddi $s1, $s1, -8\nadd\nadd\n\n$t0, $t0, $s2\n$t1, $t1, $s2\n\nsw\nsw\n\n$t0, 8($s1)\n$t1, 4($s1)\n\nbne\n\n$s1, $zero, Loop\n\n$s1, $zero, Loop\n\n● Two addi $s1, $s1, -4 are merged into addi $s1, $s1, -8\n33\n\n\fScheduling unrolled loop onto VLIW\nLoop:\nlw\n$t0, 0($s1)\nALU\/Branch Op\nLoad\/Store Op\nlw\n$t1, -4($s1)\nLoop:\nnop\nlw $t0, 0($s1)\naddi $s1, $s1, -8\nadd $t0, $t0, $s2\naddi $s1, $s1, -8 lw $t1, -4($s1)\nadd $t1, $t1, $s2\nadd $t0, $t0, $s2 nop\nsw\n$t0, 8($s1)\nadd $t1, $t1, $s2 sw $t0, 8($s1)\nsw\n$t1, 4($s1)\nbne $s1, $0, Loop sw $t1, 4($s1)\nbne $s1, $zero, Loop\n\n● Now we spend 5 cycles for 2 iterations of the loop\no So, 5 \/ 2 = 2.5 cycles per iteration\no Much better than the previous 4 cycles for 1 iteration!\n\n34\n\n\fLet’s try unrolling our example code 4X\n● 4X Unrolled loop converted to VLIW:\nALU\/Branch Op\nLoop: addi $s1, $s1, -16\nnop\nadd $t0, $t0, $s2\nadd $t1, $t1, $s2\nadd $t2, $t1, $s2\nadd $t3, $t1, $s2\nnop\nbne $s1, $0, Loop\n\nLoad\/Store Op\nlw $t0, 0($s1)\nlw $t1, 12($s1)\nlw $t2, 8($s1)\nlw $t3, 4($s1)\nsw $t0, 16($s1)\nsw $t1, 12($s1)\nsw $t2, 8($s1)\nsw $t3, 4($s1)\n\nInst\n1\n2\n3\n4\n5\n6\n7\n8\n\n● Now we spend 8 cycles for 4 iterations of the loop\no So, 8 \/ 4 = 2 cycles per iteration\no Even better 2.5 cycles per iteration for 2X unrolling\n35\n\n\fWhat happens when you unroll 8X?\n● 8X Unrolled loop converted to VLIW:\nALU\/Branch Op\nLoop: addi $s1, $s1, -32\nnop\n…\nadd $t1, $t1, $s2\nadd $t2, $t1, $s2\nadd $t3, $t1, $s2\n…\nbne $s1, $0, Loop\n\nLoad\/Store Op\nlw $t0, 0($s1)\nlw $t1, 28($s1)\n…\nlw $t7, 4($s1)\nsw $t0, 32($s1)\nsw $t1, 28($s1)\n…\nsw $t7, 4($s1)\n\nInst\n1\n2\n…\n8\n9\n10\n…\n16\n\n● Now we spend 16 cycles for 8 iterations of the loop\no So, 16 \/ 8 = 2 cycles per iteration (no improvement over 4X)\no 2 is minimum because you need one lw and one sw per iteration\n36\n\n\fWhen should the compiler stop unrolling?\n● Obviously when there is no longer a benefit as we saw just now\n● But there are other constraints that can prevent unrolling\n1. Limitation in number of registers\no More unrolling uses more registers $t0, $t1, $t2, …\no For this reason, VLIW ISAs have many more registers than MIPS\n§ Intel Itanium has 256 registers!\n2. Limitation in code space\no More unrolling means more code bloat\no Embedded processors don’t have lots of code memory\no Matters even for general purpose processors because of caching\n(Code that overflows i-cache can lead to lots of cache misses)\n37\n\n\fList Scheduling\n\n38\n\n\fHow does the compiler schedule instructions?\n● Compiler will first expand the instruction window that it looks at\no Instruction window: block of code without branches\no Compiler uses predication and loop unrolling\n● Once compiler has a sizable window, it will construct the schedule\n● A popular scheduling algorithm is list scheduling\no Idea: list instructions in some order of priority and schedule\no Instructions on the critical path should be prioritized\n● List scheduling can be used with any statically scheduled processor\no Simple single-issue statically scheduled processor (not just VLIW)\no GPUs are also statically scheduled using list scheduling\n39\n\n\fCritical Path in Code\n● At below is a data dependence graph for a code with 7 instructions\no Nodes are instructions\no Arrows are data dependencies annotated with required delay\n● Q: How long is the critical path in this code?\n3\n2\n\n1\n\n1\n2\n\n1\n1\n\n1\n\n2\n\n2\n\nThat means, at minimum,\nthis code will take 7\ncycles, period. Regardless\nof how wide your\nprocessor is or how well\nyou do your scheduling.\n\ntotal 7 cycles\n\n40\n\n\fInstruction Level Parallelism (ILP)\n● The 7 cycles is achievable only through instruction level parallelism\no That is, parallel execution of instructions\no The nodes marked in red can execute in parallel with blue nodes\n● This tell us that this code is where a VLIW processor can shine\n3\n2\n\n1\n\n1\n2\n\n1\n1\n\n1\n\n2\n\n2\n\ntotal 7 cycles\n\n41\n\n\fMaximizing Instruction Level Parallelism (ILP)\n● The more ILP code has, the more VLIW will shine\n● So before list scheduling, compiler maximizes ILP in code\no What constrains ILP? Data dependencies!\no Some data dependencies can be removed by the compiler\n● There are 3 types of data dependencies actually:\no RAW (Read-After-Write): cannot be removed\no WAR (Write-After-Read): can be removed\no WAW (Write-After-Write): can also be removed\n● How about Read-After-Read? Not a data dependency.\n\n42\n\n\fRead-After-Write (RAW) Dependency\n● RAW dependencies are also called true dependencies\no In the sense that other dependencies are not “real” dependencies\n● Suppose we reorder this snippet of code:\nRAW!\n\nlw t0, 0(s0)\naddi t1, t0, 4\n\nReorder\n\naddi t1, t0, 4\nlw t0, 0(s0)\n\n● The code is incorrect because now t1 has a wrong value\no No amount of compiler tinkering will allow this reordering\no Value must be loaded into t0 before being used to compute t1\n\n43\n\n\fWrite-After-Read (WAR) Dependency\n● WAR dependencies are also called anti-dependencies\no In the sense that they are the opposite of true dependencies\n● Suppose we reorder this snippet of code:\nWAR!\n\nlw t0, 0(s0)\naddi t1, t0, 4\nlw t0, 0(s1)\n\nReorder\n\nlw t0, 0(s0)\nlw t0, 0(s1)\naddi t1, t0, 4\n\n● The code is again incorrect because t1 has the wrong value\no addi should not read t0 produced by lw t0, 0(s1)\n● Q: Is there a way for addi to not use that value?\no t0 and t0 contain different values. Why use the same register?\no Just rename register t0 to some other register!\n44\n\n\fRemoving WAR with SSA\n● Static Single Assignment: Renaming registers with different values\no A register is assigned a value only a single time (never reused)\n● Reordering after converting to SSA form:\nNO WAR!\n\nlw t0, 0(s0)\naddi t1, t0, 4\nlw t2, 0(s1)\n\nReorder\n\nlw t0, 0(s0)\nlw t2, 0(s1)\naddi t1, t0, 4\n\n● Note how destination registers always use a new register\no Yes, if you do this, you will need lots of registers\no But, no more WAR dependencies!\n\n45\n\n\fWrite-After-Write (WAW) Dependency\n● WAW dependencies are also called false dependencies\no In the sense that they are not real dependencies\n● Suppose we reorder this snippet of code:\nWAW!\n\nlw t0, 0(s0)\nlw t0, 0(s1)\naddi t1, t0, 4\n\nReorder\n\nlw t0, 0(s1)\nlw t0, 0(s0)\naddi t1, t0, 4\n\n● The code is again incorrect because t1 has the wrong value\no addi should not read t0 produced by lw t0, 0(s0)\n● Q: Is there a way for addi to not use that value?\no Again, rename register t0 to some other register!\n\n46\n\n\fRemoving WAW with SSA\n● Again, Static Single Assignment (SSA) to the rescue!\no SSA removes both WAR and WAW dependencies\n● Reordering after converting to SSA form:\nNO WAW!\n\nlw t0, 0(s0)\nlw t1, 0(s1)\naddi t2, t1, 4\n\nReorder\n\nlw t1, 0(s1)\nlw t0, 0(s0)\naddi t2, t1, 4\n\n● SSA form is now the norm in all mature compilers\no Clang \/ LLVM (“Apple” Compiler)\no GCC (GNU C Compiler)\no Java Hotspot \/ OpenJDK Compiler\no Chrome JavaScript Compiler\n47\n\n\fList scheduling guarantees < 2X of optimal cycles\n● Back to our original example.\n● The critical path length is 7 cycles but that is not always achievable\no If processor is not wide enough for the available parallelism\no If compiler does a bad job at scheduling instructions\n→ List scheduling guarantees compiler is within 2X of optimal\n3\n2\n\n1\n\n1\n2\n\n1\n1\n\n1\n\n2\n\nQ: Can the graph be cyclic?\n2\n\ntotal 7 cycles\n\n48\n\n\fList Scheduling is a Greedy Algorithm\n● Idea: Greedily prioritize instructions on the critical path\n● Steps:\n1. Create a data dependence graph\n2. Assign a priority to each node (instruction)\n§ Priority = critical path length starting from that node\n3. Schedule nodes one by one starting from ready instructions\n§ Ready = all dependencies have been fulfilled\n(Initially, only roots of dependency chains are ready)\n§ When there are multiple nodes that are ready\n→ Choose the node with the highest priority\n● 2 – 2\/(n+1) X of optimal schedule, where n = processor width\n\no Graham, Ronald L.. “Bounds on Multiprocessing Timing Anomalies.” SIAM\nJournal of Applied Mathematics 17 (1969): 416-429.\n49\n\n\fList Scheduling Example\n● Assume all edges have a delay of 1\no Red dashed lines indicate priority levels\nn insertions +\nn removals.\nIf using priority heap\n= O(nlogn)\n\n6\nReady instructions List\n\n5\n4\n\nOperation 1\n\n3\n\nOperation 2\n\n2\n1\n50\n\n\fList Scheduling Example\n● This will result in the following schedule:\nOperation 1\n1\n2\n3\n4\n6\n8\n10\n12\n13\n\nOperation 2\n6\n\n5\n7\n9\n11\n\n● 9 cycles. We couldn’t achieve 7 cycles!\no But could’ve if we had a wider processor\n\n5\n4\n3\n2\n1\n51\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":224,"segment": "unlabeled", "course": "cs1550", "lec": "lec07", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines\n• Homework 3 is due next Monday 2\/7\n• Lab 1 is due this Friday 2\/4\n• Project 1 due on 2\/18\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fPrevious lecture …\n• Bounded buffer problem\n• semaphore-based solution\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fMuddiest Points\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fProblem of the Day\n\nIt is easy to make mistakes when using semaphores\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fProduces Consumers Problem\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSolving Producers Consumers using Semaphores\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fIs this sequence feasible?\nn == 3\nfor (i=0; i<3; i++){\nPi arrives\nPi enters\nPi leaves\n}\nP3 arrives\nC0 arrives\nC0 enters\nC0 leaves\nP3 enters\nP3 leaves\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSome thoughts\n• If we have one producer and one consumer\n• do we need count?\n• do we need the mutex?\n\n• For multiple producers and consumers\n• what benefit do we get if we have one mutex for\nproducers and one for consumers?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fLet’s make a “small” change\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fLet’s make a “small” change\nSemaphore empty(n), full(0);\nMutex sem(1);\nConsumer\n\nProducer\n\ndown(sem)\n\ndown(full)\n\ndown(empty)\n\ndown(sem)\n\nbuffer[in] = new item\n\nItem = buffer[out]\n\nin = (in + 1) % n\n\nout = (out + 1) % n\n\ncount++\n\ncount--\n\nup(empty)\n\nup(sem)\n\nup(sem)\n\nup(full)\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fIs this sequence feasible?\nn == 3\nfor (i=0; i<3; i++){\nPi arrives\nPi enters\nPi leaves\n}\nP3 arrives\nC0 arrives\nC0 enters\nC0 leaves\nP3 enters\nP3 leaves\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSolution\n• Condition Variable\n•\n\nYet another construct (Add to Spinlock and Semaphore)\n\n•\n\nHas 3 operations\n• These 3 operations have to be called while holding a mutex lock\n• wait()\n•\n•\n•\n•\n\nunlock mutex\nblock process\nwhen awake, relock mutex\nwhen successful, return\n\n• signal()\n•\n\nwakeup one waiting process in the condition variable’s queue if any\n\n• broadcast()\n•\n\n•\n\nwakeup all waiting processes in the condition variable’s queue if any\n\nNot foreign to us at all\n• Every object variable in Java is a Condition Variable\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSolving Bounded Buffer Using Condition Variables\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":225,"segment": "unlabeled", "course": "cs1550", "lec": "lec27", "text":"Introduction to Operating Systems\nCS\/COE 1550\nFall 2021\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":226,"segment": "unlabeled", "course": "cs1622", "lec": "lec19","text":"Liveness and\nRegister Allocation\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● uhhhhhh exam Wednesday! woo!\n● keep in mind this stuff from today, while interesting, is not likely to\nbe covered in-depth on the exam for obvious practical reasons\n● also I should acknowledge Stephen Chong of Harvard’s CS\ndepartment as I would not have understood this stuff myself nearly\nas well without the slides from his compilers course\n\n2\n\n\fThe liveness algorithm\n\n3\n\n\fBackwards Analyses\n● it sounds silly, but some analyses make more sense to do backwards.\nout-state\nbefore-state\ninst1\ninst2\ninst3\ninst4\ninst5\nif x bb2 else bb3\n\nin-state\nafter-state\njoin(s1, s2)\n\na backwards analysis works exactly the same\nway, but in the opposite direction: state flows\nfrom the successors into the join function.\n\nthe terms \"in-state\" and \"out-state\" take on\ndifferent meanings, so it can get confusing…\nso I'm going to use before and after\ninstead of in and out.\n\nin forward, state goes before → after;\nin backward, state goes after → before.\n4\n\n\fStates and transfer function\n● let's just focus on one variable x for now.\n● x can be in one of two states: DEAD or LIVE.\no DEAD is also the initial value, so the whole CFG is filled with it.\no DEAD is also pulling triple duty as the \"unvisited\/unknown\" value.\n● the transfer function works like this:\nx = LIVE\nprintln_i(x)\n\nx = DEAD\nx = 5\n\nif an instruction\nuses x, then x is\nLIVE before it.\n\nelse, if an instruction\ndefs x, then x is\nDEAD before it.\n\nx = DEAD\ny = 5\nx = DEAD\n\nx = LIVE\ny = 5\nx = LIVE\n\nelse, before = after\n(just copy the state).\n\n(in x = x + 1, the first rule takes\nprecedence, so x is LIVE before it.)\n5\n\n\fThe join function\n● the join function is pretty simple.\nif x is LIVE at the beginning\nof any successor, then x is\nLIVE at the end of this block.\nLIVE\nDEAD\ns1\n\nelse, x is DEAD at the\nend of this block.\n\nDEAD\nLIVE\ns2\n\nDEAD\ns3\n\nDEAD\ns1\n\nDEAD\ns2\n\nDEAD\ns3\n\n(yeah, in our IR, BBs can have at most 2 successors,\nbut this rule works for any number of successors.)\n6\n\n\fTrying it out (animated)\n● let's try it on this simple, one-BB function.\nwe initialize it by setting x\nto DEAD everywhere.\n\nthen we run the transfer\nfunction on each instruction.\n\nx\nDEAD\nDEAD\nLIVE\nDEAD\nLIVE\nDEAD\nDEAD\n\nx = arg + 10\n\ndefs x…\n\nprintln_i(x) uses x…\n$t0 = x\n\nuses x…\n\nreturn doesn't touch x…\n\nand there we go!\nnotice how on the last\nstep (the first instruction),\nx was marked dead before\nit, so we didn't have to\nchange that state.\nand? why's that matter?\n7\n\n\fIt matters for termination\n● we have a finite number of states (2)…\n● and the states only change monotonically:\no every location starts as DEAD.\no a location's DEAD can become LIVE, but not the other way around.\no \"changing\" DEAD to DEAD like in that last step is not breaking the\nrule, because you're not changing anything!\n● so we've satisfied the conditions for termination!\no let's try it on some more complex functions to convince ourselves.\n\n8\n\n\fOne with a diamond (animated)\n● a diamond shape will force us to visit some nodes twice.\njust a def here,\nno change.\n\nDEAD\nx = 10\n\nDEAD\nLIVE\nif arg bb1 else bb2\n\njoin DEAD with\nDEAD, get DEAD.\n\njoin DEAD with\nLIVE, get LIVE!\n\nDEAD\nLIVE\nLIVE\nDEAD\n\nDEAD\n\nprint_s(\"x = \")\n\nprintln_s(\"argh!\")\n\nLIVE\nDEAD\n\nDEAD\n\nprintln_i(x)\n\nooh, a use!\n\nDEAD\n\nlet's go left..\n\nwe start at the\nreturn node.\n\nDEAD\n\nwait, we have\nthis other path.\n\nreturn\n\nDEAD\n\n9\n\n\fOne with a loop (animated)\n● this should be interesting…\nprintln_s(\"start!\")\n\nDEAD x = 0\nLIVE i = 0\n\nLIVE\nDEAD\nDEAD $t1 = i < 10\nLIVE\nif $t1 bb2 else bb3\n\nfollow the\nloop back…\n\nLIVE\nprintln_i(x)\ni = i + 1\n\noh that's\ninteresting!\n\nthe liveness changes in\nthe middle of this BB.\n\njoin DEAD\nwith DEAD,\nthings\nare different\nthe\ngettime\nDEAD.\nsecond\naround!\n\nDEAD\n$t0 = 5\nreturn\n\nx isn't mentioned, so it's\ndead before this BB.\n10\n\n\fWait, is that right…?\n● these might look wrong at first, but the liveness algorithm is telling\nyou something important about x.\nfn f(a: int) {\nlet x = 10;\nlet x = whatever();\n\nx = 20;\nprintln_i(a);\nprintln_i(x);\n}\n\nx is always dead??\nyeah, the code never used x.\nmaybe the compiler would\nreport a warning\/error here.\n\nx is only live for one instruction??\nthe code never uses the first\nvalue assigned to x. maybe a\nwarning\/error again!\n11\n\n\fSome implementation thoughts\n● there are only two states.\no so we could store the state of one variable with one bit. efficient!\n● therefore, to track n variables' states, we'd need n bits per location.\no e.g. if we have 3 variables x, y, z…\n▪ then 000 would mean all 3 are dead\n▪ and 010 would mean y is live and x\/z are dead.\n● this places liveness into a special class of analysis problems called bit\nvector problems, and you can guess why they're called that.\no bitwise operations are fast, and bits are tiny!\no think about the join function: if any of the successors are live (1)…\n▪ that means just ORing together all the successors.\n● okay. enough about liveness. how do we allocate registers?\n\n12\n\n\fRegister Allocation\ntime check ≤ 27\n\n13\n\n\fState of the art circa 1980\n● currently, our compiler places all locals (including args) in memory.\no so, accessing them requires loads and stores.\nx = y + z;\n\nlw s0, -12(fp)\nlw s1, -16(fp)\nadd s0, s0, s1\nsw s0, -20(fp)\n\nthis is definitely correct (our main goal),\nbut it has serious performance problems.\nin, say, 1980, the memory in your\ncomputer was faster than the CPU.\nso loads, stores, and adds all took the\nsame amount of time.\n\nthat is absolutely not the case anymore.\n\nin the absolute worst case, a load or store can\ntake around 100 times longer than an add.\n14\n\n\fNot all doom and gloom\n● due to Reasons You Learn In 1541™, that's just the worst case\nperformance. some memory accesses are as fast as an add!\n● but memory accesses still cause issues for the CPU's pipeline.\n● and it's still, like, more instructions. cause what if our variables lived in\nregisters instead?\nx = y + z;\n\nadd s0, s1, s2\n\nthis is one of the reasons RISC ISAs like\nMIPS have so many registers to begin with:\nso you can use them for stuff like this.\n\nmore values in registers\nmeans fewer in memory\nmeans fewer loads\/stores\nmeans higher performance.\n15\n\n\fReal ABIs use registers\n● as you learned in 447, the real MIPS ABI has four main kinds:\no a registers, for passing arguments (which we aren't using)\no v registers, for returning values\no t registers, for temporaries which do not cross jals\no s registers, for temporaries which do cross jals\n● our compiler isn't even using a or t registers! how wasteful.\n● register allocation algorithms allow us to set up constraints to say\n\"hey, this value must go in a0\" or similar, so that we can generate\ncode that follows these real ABIs.\n\n16\n\n\fUnder pressure\n● the core idea of register allocation is simple:\no a function has v variables, and the CPU has r registers.\no if v ≤ r, it's easy: assign each variable to a register. done!\no if v > r, put (v - r) variables in memory, and the rest in registers.\n● but this is too simplistic, even for simple functions.\no our IR creates a lot of temporary variables.\no this increases register pressure: the number of values we \"want\"\nto keep in registers at any given time.\n● some optimization passes also increase register pressure!\no e.g. function inlining can really blow up the number of locals.\n● so, we need something that tells us which variables are \"in use\" at\nany given time, and assign registers based on that.\no…\no …oh right, that's liveness :)\n17\n\n\fThe Register\nInterference Graph\ntime check ≤ 40\n\n18\n\n\fOh boy, more graphs! Whee!\n● the Register Interference Graph (RIG) is the data structure used for\nregister allocation. it is an undirected graph with:\no one node for each local in the function; and\no an edge between any two locals which are live at the same time.\n● here is a small function and its RIG.\nfn riggy(a: int): int {\nlet x = a + 10;\nprintln_i(x);\nlet y = \"---\";\na\nprintln_s(y);\nreturn a;\n}\nx\n\nthese two edges indicate that a\nand x are live at the same time,\nand so are a and y, but x and y\nare not live at the same time.\ny\n\nclearly, \"live at the same\ntime\" is derived directly from\nthe liveness we computed!\n19\n\n\fBuilding the RIG (animated)\n● after you've computed liveness, it's really straightforward.\n● you start with a graph with no edges.\nat every point in the function, if two locals are\nlive at that point, add an edge between them.\nn\n\nobj\n\ni\n\nobj\n\ni\n\nval\n\nfound\nn\nval\nl\n\nl\n\nfound\n\nfn has(l: List, val: int): bool {\nlet n = l.length();\nlet found = false;\nfor i in 0, n {\nlet obj = l.get(i);\nif obj.value() == val {\nfound = true;\n}\n}\nreturn found;\n}\n20\n\n\fOk but what's it mean and why is it a graph\n● it's the register interference graph: an edge between two nodes\nmeans that those two locals cannot go in the same register!\no but the absence of an edge means they can go in the same register.\n● as for \"why a graph,\" well, lots of things reduce to graph algorithms.\nl\nval\n\nn\n\nobj\n\ni\nfound\n\nthis is graph coloring: we assign each\nnode a color such that no two nodes of\nthe same color share an edge (touch).\nthe \"color\" is just an arbitrary label or set:\nfor our purposes, \"color\" will mean the\nregister used to store the variable.\n\nhere, n and obj are the same color,\nso they can use the same register!\n21\n\n\fGraph Coloring\ntime check ≤ 56\n\n22\n\n\fColorability and chromatic number\n● because our \"colors\" represent the registers on the target CPU…\no we have a fixed number of colors to use.\n● we say a graph is k-colorable if it can be colored with k colors.\na\n\nx\n\na\n\ny\n\nx\n\na\n\ny\n\nwe could color this\ngraph with 3 colors…\n\nx\n\ny\n\nor just 2. but 2 is\nthe minimum.\n\nthe chromatic number of a graph is the\nminimum number of colors needed to color it.\nif we have r registers, we'd really like the\nchromatic number of the RIG to be ≤ r!\n\n23\n\n\fYou didn't think it would be THAT easy, did you\n● unfortunately, determining k-colorability is NP-complete.\no (and determining the chromatic number is NP-hard!)\n\n● so, what do we do?\no sometimes, an imperfect solution is good enough.\n● we're going to use a heuristic to figure out a coloring order that is\nlikely to lead to a successful coloring.\no and if that fails, we can tweak the graph a bit and try it again.\n● this leads to an iterative solution that is near-linear for most\ncommon cases.\no yeah we might not find the perfect register allocation, but\nperfection is the enemy of good.\n\n24\n\n\fThe coloring heuristic (animated)\n● let's say r = 5 (where r = number of CPU registers).\n● if the RIG has a node with < r neighbors, and you remove that node…\no and the resulting graph is r-colorable…\no then the original graph is r-colorable.\nStack\nwe'll repeatedly remove\nl\nnodes with < 5 neighbors\n(edges), and put them in\nval\nn\nthis stack as we do so.\n\nobj\n\ni\nfound\n25\n\n\fPop'n'color (animated)\n● now we pop the nodes off the stack, and as we do so, color them\naccording to what their neighbors are.\nStack\ni\n\nn\n\nval\n\nl has red and green neighbors,\nso blue it is.\nl\nn has no colored neighbors,\nso let's make it red too.\nn\n\nfound\n\nval needs to be a fourth color…\n\nl\n\nand obj needs to be a fifth.\n\nval\n\nobj\n\nobj\n\ni\nfound\n\ni has no colored neighbors,\nso let's make it red.\n\nfound has two red neighbors,\nso let's make it green.\n\n26\n\n\fThis works??\n● it's a little mind-blowing, but it does work, because that heuristic\nactually goes both ways: it's an \"if and only if.\"\no because we were able to 5-color each sub-graph of the RIG as we\nbuilt it back up by popping…\no then the whole RIG was 5-colorable.\n● BUT: it doesn't always work. if the RIG is not actually r-colorable, we\nwill run into one of two things:\no no nodes with < r neighbors on the removal phase; or:\no impossible-to-color nodes in the popping phase!\n\n27\n\n\fFailure to color\n● let's see what happens on the same RIG if r = 4 instead.\no we have to remove nodes with < 4 neighbors now.\nStack\nl\n\nfound\nobj\n\nval\n\naaaand we're stuck.\nwe can't color l!\nn\n\nuh oh. everyone has ≥ 4\nneighbors. let's just remove\none and move on anyway.\n\nval\ni\nl\nn\n\nobj\n\ni\nfound\n28\n\n\fSpilling\ntime check ≤ 85\n\n29\n\n\fSo what do we do??\n● a failure to color the RIG means that there are more values being\nused at one time than will fit into the CPU's registers.\no it's a setback, but it's not catastrophic, right? we can use memory.\n● so in this case we turn to spilling: picking one or more locals to live\non the stack instead of in registers.\n● ideally, we’d like to pick some local that isn’t used very often to be\nplaced into memory instead of a register… but which one?\no if we pick poorly, we could slow down our program a lot by\ncausing a bunch of excess loads and stores…\n\n30\n\n\fStop interfering with me!\n\nobj\n\ni\n\nfound\nn\nval\nl\n\n● picking which local to spill is, again, something we can’t do perfectly.\n● so we can again use heuristics to pick a likely candidate. maybe some\nvariable that interferes with a lot of others but isn’t used much…\nlocals with long live ranges\n\nfn has(l: List, val: int): bool {\ntend to interfere a lot.\nlet n = l.length();\nlet found = false;\nwe also have to consider\nfor i in 0, n {\nthings like loops – how many\nlet obj = l.get(i);\naccesses to the local will\nif obj.value() == val {\nfound = true;\nhappen because of the loop?\n}\ncan we estimate that?\n}\nreturn found;\nfound seems like a good\n}\n\nspilling candidate here.\n\n31\n\n\fSo how does it work?\n● if we get stuck when removing nodes from the RIG…\no we pick a local to maybe spill. but we don’t spill it yet.\no we push that node on the stack, along with a note that says,\n“maybe spill this?”\n● then, when we pop-and-color…\no if we get to a maybe-spill entry on the stack...\no AND that node cannot be colored…\no then we know that we have to spill it, and we rewrite the code!\n● why do we wait to rewrite until the pop-and-color phase?\no because we’re just using heuristics here. it’s entirely possible that\nthe graph is colorable when we remove a node with ≥ r neighbors!\nso, we have to be conservative.\n\n32\n\n\fHow the code is rewritten\n● in our IR, we might add new instructions to represent loads\/stores.\n● then, each use and def of the spilled variable is replaced with loads\nand stores to the stack.\nfound = false\n\nfound_1 = false\nStore(fp-12, found_1)\n\nfound = true\n\nfound_2 = true\nStore(fp-12, found_2)\n\n$t0 = found\n\nfound_3 = Load(fp-12)\n$t0 = found_3\n\nwe also rename the spilled local at each use location with a\nunique name. each of these new locals has a very short live range!\n33\n\n\fThen what?\n● then… we start all over again! compute liveness, build the RIG…\n● this is the iterative register allocation algorithm in a nutshell:\nRewrite\nspill\nLiveness\nand RIG\n\nRemove\nnodes\n\nPop and\ncolor\n\nDone!\n\nPick spill\ncandidate\n\n34\n\n\fAnd it keeps going…\n● this is just the basic shape of the algorithm.\n● there are several more features, such as:\no coalescing pairs of nodes that are likely to end up in the same\nregister anyway\n▪ e.g. in a = b where their liveness doesn’t overlap\no pre-coloring some nodes to indicate they must be in certain\nregisters. this is used for:\n▪ argument and return value registers (MIPS “a” and “v” registers)\n▪ saved temporary registers (MIPS “s” registers)\no and more…\n\n35\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":227,"segment": "unlabeled", "course": "cs1541", "lec": "lec3.5_multi_processors","text":"Multiprocessors\nand Caching\nCS 1541\nWonsun Ahn\n\n\fTwo ways to use multiple processors\n● Distributed (Memory) System\no Processors do not share memory (and by extension data)\no Processors exchange data through network messages\no Programming standards:\n▪ Message Passing Interface (MPI) – C\/C++ API for exchanging messages\n▪ Ajax (Asynchronous JavaScript and XML) – API for web apps\no Data exchange protocols: TCP\/IP, UDP\/IP, JSON, XML…\n● Shared Memory System (a.k.a. Multiprocessor System)\no Processors share memory (and by extension data)\no Programming standards:\n▪ Pthreads (POSIX threads), Java threads – APIs for threading\n▪ OpenMP – Compiler #pragma directives for parallelization\no Cache coherence protocol: protocol for exchanging data among caches\n→ Just like Ethernet, caches are part of a larger network of caches\n2\n\n\fShared Data Review\n● What bad thing can happen when you have shared data?\n\n● Dataraces!\no You should have learned it in CS 449.\no But if you didn’t, don’t worry I’ll go over it.\n\n3\n\n\fReview: Datarace Example\nint shared = 0;\nvoid *add(void *unused) {\nfor(int i=0; i < 1000000; i++) { shared++; }\nreturn NULL;\n}\nbash-4.2$ .\/datarace\nint main() {\npthread_t t;\nshared=1085894\n\/\/ Child thread starts running add\nbash-4.2$ .\/datarace\npthread_create(&t, NULL, add, NULL);\nshared=1101173\n\/\/ Main thread starts running add\nbash-4.2$ .\/datarace\nadd(NULL);\n\/\/ Wait until child thread completes\nshared=1065494\npthread_join(t, NULL);\nprintf(\"shared=%d\\n\", shared);\nreturn 0;\n}\nQ) What do you expect from running this? Maybe shared=2000000 ?\nA) Nondeterministic result! Due to datarace on shared.\n4\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nshared\n1\n\nshared++\nThread 1\n\n• You may think shared becomes 3.\n(shared++ on each thread)\n• But that’s not the only possibility!\n• I’ll show you shared becoming 2.\n\nshared++\nThread 2\n\n5\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n0\n\n1\n\n0\n\nR1 = shared\nR1 = R1 + 1\nshared = R1\nThread 1\n\nR1 = shared\nR1 = R1 + 1\nshared = R1\nThread 2\n\n6\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n1\n\n1\n\n0\n\n① R1 = shared\nR1 = R1 + 1\nshared = R1\nThread 1\n\nR1 = shared\nR1 = R1 + 1\nshared = R1\nThread 2\n\n7\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n1\n\n1\n\n1\n\nR1 = shared②\nR1 = R1 + 1\nshared = R1\n\n① R1 = shared\nR1 = R1 + 1\nshared = R1\nThread 1\n\nThread 2\n\n8\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n1\n\n1\n\nR1 = shared②\nR1 = R1 + 1\nshared = R1\n\n① R1 = shared\n③ R1 = R1 + 1\nshared = R1\nThread 1\n\nThread 2\n\n9\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n1\n\n2\n\nR1 = shared②\nR1 = R1 + 1④\nshared = R1\n\n① R1 = shared\n③ R1 = R1 + 1\nshared = R1\nThread 1\n\nThread 2\n\n10\n\n\fReview: Datarace Example\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n2\n\n2\n\nR1 = shared②\nR1 = R1 + 1④\nshared = R1\n\n① R1 = shared\n③ R1 = R1 + 1\n⑤ shared = R1\nThread 1\n\nThread 2\n\n11\n\n\fReview: Datarace Example\n• Why did this occur in the first place?\n• Because data was replicated to CPU registers and each worked on its own copy!\n\n● When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n2\n\n2\n\n① R1 = shared\n③ R1 = R1 + 1\n⑤ shared = R1\nThread 1\n\nR1 = shared②\nR1 = R1 + 1④\nshared = R1⑥\n\n• End result is 2 instead of 3!\n• Only on simultaneous access\n(with this type of interleaving)\n\nThread 2\n\n12\n\n\fReview: Datarace Example\npthread_mutex_t lock;\nint shared = 0;\nvoid *add(void *unused) {\nfor(int i=0; i < 1000000; i++) {\npthread_mutex_lock(&lock);\nshared++;\npthread_mutex_unlock(&lock);\n}\nreturn NULL;\n}\nint main() {\n…\n}\n\nbash-4.2$ .\/datarace\nshared=2000000\nbash-4.2$ .\/datarace\n\nshared=2000000\nbash-4.2$ .\/datarace\nshared=2000000\n\n• Data race is fixed! Now shared is always 2000000.\n• Problem solved? No! CPU registers is not the only place replication happens!\n\n13\n\n\fCaching also does replication!\n● What happens if caches sit in between processors and memory?\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n14\n\n\fCaching also does replication!\n● Let’s say CPU 0 first fetches shared for incrementing\n\nshared\n\n0\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n15\n\n\fCaching also does replication!\n● Then CPU 0 increments shared 100 times to 100\n\nshared\n\n100\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n16\n\n\fCaching also does replication!\n● Then CPU 2 gets hold of the mutex and fetches shared from L3\n\nshared\n\n100\nL1$\n\nL1$\n\nshared\n\nL1$\n0\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nshared\n\nL2$\n0\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n17\n\n\fCaching also does replication!\n● Then CPU 2 increments shared 10 times to 10\n\nshared\n\n100\nL1$\n\nL1$\n\nshared\n\nL1$\n10\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nshared\n\nL2$\n0\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n18\n\n\fCaching also does replication!\n● Clearly this is wrong. L1 caches of CPU 0 and CPU 2 are incoherent.\n\nshared\n\n100\nL1$\n\nL1$\n\nshared\n\nL1$\n10\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nshared\n\nL2$\n0\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n19\n\n\fCache Incoherence: Problem with Private Caches\n● This problem does not occur with a shared cache.\no All processors share and work on a single copy of data.\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\no The problem exists only with private caches.\n● The problem exists for private caches.\no Private copy is at times inconsistent with lower memory.\no Incoherence occurs when private copies differ from each other.\n→ Means processors return different values for same location!\n\n20\n\n\fCache Coherence\n\n21\n\n\fCache Coherence\n● Cache coherence (loosely defined):\no All processors of system should see the same view of memory\no Copies of values cached by processors should adhere to this rule\n● Each ISA has a different definition of what that “view” means\no Memory consistency model: definition of what that “view” is\n● All models agree on one thing:\no That a change in value should reflect on all copies (eventually)\n\n22\n\n\fHow Memory Consistent Model affects correctness\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nQ) What do you expect the value of data will be when it gets printed?\nA) Most people will say 42 because that is the logical ordering.\nBut is it? Not always. There are situations where data is still 0!\n\n23\n\n\fScenario 1: Stores arrive out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n0\n\nflag\n\ndata\n\nfalse\n\n0\n\nflag\nfalse\n\nLet’s assume initially both data and flag are cached in each CPU’s L1 caches.\n\n24\n\n\fScenario 1: Stores arrive out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n42\n\nflag\n\ndata\n\ntrue\n\n0\n\nflag\nfalse\n\nCPU 0 updates both data and flag to 42 and true.\n\n25\n\n\fScenario 1: Stores arrive out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n42\n\nflag\n\ndata\n\ntrue\n\n0\n\nflag\nfalse\n\nNow the cached values in CPU 1 are stale and need to be invalidated.\nInvalidation: act of marking a cache block with stale data invalid.\n26\n\n\fScenario 1: Stores arrive out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\ndata\n42\n\nL1$\n\nCPU 1\n\nInvalidate for flag\nflag\ntrue\n\nInvalidate for data\n\ndata\n0\n\nL1$\n\nflag\nfalse\n\nThe invalidate messages travel through a network and may arrive out-of-order.\nLet’s say invalidate for flag arrives first to CPU 1 and marks flag invalid.\n27\n\n\fScenario 1: Stores arrive out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\ndata\n42\n\nL1$\n\nCPU 1\n\nFetch for flag\nflag\ntrue\n\nInvalidate for data\n\ndata\n0\n\nL1$\n\nflag\ntrue\n\nCPU 1 fetches updated flag from CPU 0 when comparing flag == false.\nInvalidate for data is still traveling through the network.\n28\n\n\fScenario 1: Stores arrive out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\ndata\n42\n\nL1$\n\nCPU 1\n\nFetch for flag\nflag\n\nInvalidate for data\n\ntrue\n\ndata\n0\n\nL1$\n\nflag\ntrue\n\nSince flag is true, CPU 1 breaks out of while loop and prints data.\ndata=0 gets printed!\n29\n\n\fScenario 2: Loads perform out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n0\n\nflag\n\ndata\n\nfalse\n\n0\n\nLet’s assume now flag is not cached in CPU 1.\nCPU 1 suffers a cache miss on flag when it compares flag == false.\n30\n\n\fScenario 2: Loads perform out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nInstruction Queue\nlw r1, flag (miss)\n\ndata\n0\n\nL1$\n\nflag\n\ndata\n\nfalse\n\n0\n\nL1$\n\nbeq r1, $zero, _loop\nlw r2, data (hit)\ncall println on r2\n\nInstead of stalling, CPU 1 predicts the branch not taken and issues lw r2, data.\nNow, r2 == 0. (Unless pipeline flushes due to branch misprediction.)\n31\n\n\fScenario 2: Loads perform out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nInstruction Queue\nlw r1, flag (miss)\n\ndata\n42\n\nL1$\n\nflag\ntrue\n\nFetch for flag\n\ndata\n0\n\nL1$\n\nbeq r1, $zero, _loop\n\nflag\n\nlw r2, data (hit)\n\ntrue\n\ncall println on r2\n\nNow let’s say CPU 0 updates data and flag before the fetch for flag arrives.\nNow, lw r1, flag completes, allowing beq r1, $zero, _loop to issue (with r1 == true)\n32\n\n\fScenario 2: Loads perform out-of-order\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\nCPU 0\n\nCPU 1\n\nInstruction Queue\nlw r1, flag (miss)\n\ndata\n42\n\nL1$\n\nflag\n\nFetch for flag\n\ntrue\n\ndata\n0\n\nL1$\n\nbeq r1, $zero, _loop\n\nflag\n\nlw r2, data (hit)\n\ntrue\n\ncall println on r2\n\nSince r1 == true, that validates the not-taken prediction for the branch.\nSince r1 == 0, the println outputs data=0!\n33\n\n\fMemory Consistency Models are often very lax\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\n• A memory consistency model where above ordering is guaranteed is called,\nSequential Consistency (SC): Instructions appear to execute sequentially.\n• Real models allow many other orderings to allow optimizations:\n• Write buffers that allow multiple stores to be pending and perform out-of-order\n• Instruction queues that allow loads and other instructions to perform out-of-order\n• Compiler optimizations to reschedule stores and loads out-of-order\n\n• Intel, ARM, Java Virtual Machine all have relaxed memory consistency models\n• Moral: never do custom synchronization unless you know what you are doing!\n34\n\n\fMemory Consistency Models are often very lax\n● Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { \/* wait *\/ }\n\nflag = true;\n\nSystem.out.println(“data=“ + data);\n\n• Regardless of memory consistency model, they all agree on one thing:\nthat values of data and flag must be made coherent eventually.\n• They only disagree on when that eventually is.\n\n• This property is called cache coherence.\n\n35\n\n\fImplementing Cache Coherence\n● How to guarantee changes in value are propagated to all caches?\n\n● Cache coherence protocol: A protocol, or set of rules, that all\ncaches must follow to ensure coherence between caches\no MSI (Modified-Shared-Invalid)\no MESI (Modified-Exclusive-Shared-Invalid)\no … often named after the states in cache controller FSM\n● Three states of MSI protocol (maintained for each block):\no Modified: Dirty. Only this cache has copy.\no Shared: Clean. Other caches may have copy.\no Invalid: Block contains no data.\n\n36\n\n\fMSI Snoopy Cache Coherence Protocol\nProcessor\n\nSnoop\ntag\n\nCache tag\nand data\n\nProcessor\n\nSnoop\ntag\n\nCache tag\nand data\n\nProcessor\n\nSnoop\ntag\n\nCache tag\nand data\n\nSingle bus\n\nMemory\n\nI\/O\n\n●Each processor monitors (snoops) the activity on the bus\noIn much the same way as how nodes snoop the Ethernet\n●Cache state changes in response to both:\noRead \/ writes from the local processor\noRead misses \/ write misses from remote processors it snoops\n37\n\n\fMSI: Example\n• All bus activity is show in blue. Cache changes block state in response.\n• Bus activity is generated only for cache misses, or for invalidates\n• Other caches must maintain coherence by monitoring that bus activity\n\nEvent\n\nIn P1’s cache\nL = invalid\n\nP1 writes 10 to A\n(write miss)\nP1 reads A\n(read hit)\nP2 reads A\n(read miss)\nP2 writes 20 to A\n(write hit)\nP2 writes 40 to A\n(write hit)\nP1 write 50 to A\n(write miss)\n\nIn P2’s cache\nL = invalid\nRead Exclusive A (from write in P1)\n\nL  A = 10 (modified)\n\nL = invalid\n\nL  A = 10 (modified)\n\nL = invalid\n\nRead A (from read in P2)\n\nL  A = 10 (shared)\n\nL  A = 10 (shared)\n\nInvalidate A (from write in P2)\n\nL = invalid\n\nL  A = 20 (modified)\n\nL = invalid\n\nL  A = 40 (modified)\nRead Exclusive A (from write in P1)\n\nL  A = 50 (modified)\n\nL = invalid\n38\n\n\fCache Controller FSM for MSI Protocol\n● Processor activity in red, Bus activity in blue\nRead\nWrite\n\nRead\nBusRead\n\nBusRead\n\nModified\n\nShared\n\nWrite\nBusReadX\nBusReadX BusInvalidate\nWrite\n\n•\n•\n\nRead\n•\n\nInvalid\n\nBusRead: Read request is snooped\nBusReadX: Read exclusive request is snooped\n• Sent by a processor on a write miss\n• Since line will be modified, need to invalidate\nBusInvalidate: Invalidate request is snooped\n• Sent on a write hit on shared cache line\n• To invalidate all other shared lines in system\n\nBusRead\nBusReadX\nBusInvalidate\n39\n\n\fTLB Coherence\n\n40\n\n\fHow about TLBs?\n● We said TLBs are also a type of cache that caches PTEs.\no So what happens if a processor changes a PTE?\no How does that change get propagated to other processor TLBs?\n● Unfortunately, there is no hardware coherence for TLBs. \n● That means software (the OS) must handle the coherence\no Which is of course much much slower\n\n41\n\n\fTLB shootdown\n● In order to update a PTE (page table entry)\no Initiator OS must first flush its own TLB\no Send IPIs (Inter-processor interrupts) to other processors\n▪ To flush the TLBs for all other processors too\no Source of significant performance overhead\n\n* Courtesy of Nadav Amit et al. at VMWare\n42\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":228,"segment": "unlabeled", "course": "cs1622", "lec": "lec11","text":"Backend and Runtime\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● today is kind of an intro to the second half of the course\n● how was the exaaaaaaaam?\n● also I’m realizing that there are actually way more minutes of lecture\nin the summer term than in spring\/fall so I shouldn’t feel pressured\nto make every lecture take all 105 minutes\n\n2\n\n\fWelcome to the Backend!\n\n3\n\n\fThrough the looking glass\n● a compiler translates from a source language to a target language.\no well, now we're done with the source language.\n● once the AST has been constructed and semantically analyzed…\no we have a \"correct\" program!\n● from here on out, we're assuming that we're working with an AST\nthat represents a correct program.\no later we'll come back and look at intermediate representation\n(IR), which lets us abstract away even the AST itself.\n\n4\n\n\fWhat is the backend responsible for?\n● a compiler translates from the source language to the target.\n● so far, all we've done is validate the source code.\n● that means the backend does the actual translation!\nit maps each part of the source language into some target language\ninstructions that do the same thing as what the programmer wrote.\nf();\n\njal\n\nf\n\nx = 5;\n\nli\nsw\n\nt0, 5\nt0, x\n\nf(x + 3);\n\nlw\nadd\njal\n\na0, x\na0, a0, 3\nf\n5\n\n\fCorrectness\n● the goal of an HLL is to provide a solid base of abstractions on which\nyou can build software better than you could do in assembly.\n● above all, a compiler's number one priority is to produce a correct\ntranslation of the source code.\no if a compiler mistranslates code, you've lost all the supposed\nguarantees that the HLL provides.\n● bugs in compilers can be incredibly serious.\no a compiler that fails to catch certain mistakes, or which produces\nincorrect code, can produce executables which leave millions of\ncomputers vulnerable to attacks, in the worst cases.\n● so, what does \"correct\" code look like?\n\n6\n\n\fIt doesn't have to look pretty\n● think back to 447. I want to do x++ for some global variable x.\n# x++\nlw\nt0, x\naddi t0, t0, 1\nsw\nt0, x\n\nadd_ints:\nadd v0, a0, a1\njr\nra\nmain:\n# x++\nlw\na0, x\nli\na1, 1\njal add_ints\nsw\nv0, x\n\nthese all have the same effect: x's value is\nincremented by 1. so, they are all correct.\nwhat varies between them is the code quality.\n\n# push 1\nli\nt0, 1\naddi sp, sp, -4\nsw\nt0, 0(sp)\n# push x\nlw\nt0, x\naddi sp, sp, -4\nsw\nt0, 0(sp)\n# pop 2, add, push sum\nlw\nt0, 0(sp)\nlw\nt1, 4(sp)\naddi sp, sp, 8\nadd t0, t0, t1\naddi sp, sp, -4\nsw\nt0, 0(sp)\n# pop x\nlw\nt0, 0(sp)\naddi sp, sp, 4\nsw\nt0, x\n\n7\n\n\fOutput code quality\n● the code our compiler outputs can be measured in a few ways:\no speed: how fast the output code runs\n▪ in the same span of time, a fast program can do more useful\nwork than a slow program can.\no size: how many bytes the output code takes up\n▪ if e.g. your target is a microcontroller with 4 KiB of ROM…\n▪ your output code has to be as small as possible to fit.\n● despite the last slide, smaller code is not necessarily faster code…\no so compilers often give you the choice of which to optimize for.\no but optimizing code to be faster or smaller is an optional task, and\nis not required for correct output.\n\n8\n\n\fGoals and non-goals\n● in the old days, the code quality of most compilers was… lacking.\no much more like the third example on that slide than like the first.\n● higher code quality requires a more sophisticated backend.\no these algorithms were impractical on the slow computers those\nearly compilers ran on, or literally hadn't been invented yet.\n● but more complex algorithms means a higher chance of messing\nthem up and making a compiler that produces incorrect code!\n● so our goal in this class will be to make a correct compiler.\no the code it produces may be big, slow, and ugly, but it's okay for\nteaching purposes.\n● towards the end we'll start to look at improving code quality…\no but you won't be implementing that. it's some heavy stuff.\n\n9\n\n\fRuntime\n\n10\n\n\fFrom compile-time to runtime\n● the compiler is just the first part of enforcing the HLL's abstractions.\n● depending on how well the source language's semantics match the\ntarget's, we can have a little or a lot to do at runtime.\n\nC ≅\ndynamic dispatch\n\nJava\n\nreflection\n\nclass loading\n\nCPU\n\ngarbage collection\nexceptions\n\n11\n\n\fDoes that mean we need a VM?\n● well, not necessarily…\n● there can be many source language features which the target\nlanguage does not support, which can be even small things.\no think about it: does MIPS have an if-else instruction?\n▪ no. so those have to be built out of simpler instructions.\n● if the target language is a CPU, chances are all it can do is:\no move numbers around\no do arithmetic and logical operations on numbers\no choose which steps to go to\no uhhhhhhhhhhhhhhhhhhh that's it!\n● so there are like, three parts to this?\no the code generation\no the ABI (application binary interface)\no and the runtime library\n12\n\n\fCode generation (codegen)\n● we saw this: codegen is where each operation in the source\nlanguage is mapped to the target language.\n\nx = y\n\nlw\nsw\n\nf(x)\n\nlw a0, x\njal f\n\nwhile(x != 10)\n\nt0, y\nt0, x\n\n_top:\nlw t0, x\nbeq t0, 10, _end\n13\n\n\fThe ABI\n● most programs run on computers with operating systems.\no even those that don't, have to interact with the hardware and\nthemselves (e.g. one function calling another).\n● the Application Binary Interface (ABI) defines several things:\no the target language – which in our case is a CPU ISA\no the calling convention(s), which dictate how function calls work\no the way values are represented in memory\no how system calls work, for interacting with the OS\no where things are located in memory (stack, heap, globals, etc.)\no how the code is packaged into an executable file\no and much more!\n\n14\n\n\fThe runtime library\n● also called \"the runtime,\" confusingly\n● it comes with your language; either statically or dynamically linked\no for real why is 449 not a prereq for this course\n\nStandard\nLibrary (stdlib)\n\nRuntime\n\nthe standard library (stdlib) has a set of\nuseful, but non-critical functionality.\n\nthe runtime library is essential to make\nlanguage features work at run-time.\nin Rust, the runtime is called \"core\",\nand the standard library is \"std\".\n\nin a lot of languages, the line is… blurrier.\n15\n\n\fPutting it all together\n● to sum up:\n\nCompiler\n\n(the runtime library might piggy-back on it.)\n\nAST\nBackend\n\nthe backend generates\nmachine code and puts it\nin an executable which\nconforms to the ABI.\n\nRuntime\nLibrary\nExecutable\nProgram\n\nwhen executed, the runtime\nlibrary handles the sourcelanguage features which\ndon't exist on the target.\n\nuhh… so how is this code\ngenerated, anyway?\n16\n\n\fCodegen\n\n17\n\n\fHow do you eat an elephant?\n● codegen seems like a giant problem with no easy place to start.\n● but any problem can be broken down into smaller ones.\na lot of it is filling in templates,\nlike mad libs for code.\nwhile cond {\ncode\n}\n\n_top:\nb__ __, __, _end\ncode\n_end:\n\nother parts are allocation, deciding\nwhat lives where, and when.\n\nVariables\n\nRegisters\n\nx\ni\nret\nnum_cats\n\ns0\ns1\nt0\nt1\n\njust keep in mind that our goal is to\ngenerate correct code, not great code.\n18\n\n\fGetting a flavor of it: codegen for a function\n● as you (hopefully) learned before, every function gets a stack frame.\no this is where it stores saved registers and local variables.\nthe template for a function's\ncode is something like…\nname:\nset up stack\nfunction body\nclean up stack\nreturn\n\nhow big does the stack\nframe have to be?\nfn main() {\nlet x = 0;\nfor i in 0, 10 {\nx += i;\n}\nlet y = g(x);\nprintln(y);\n}\n\nthe symbol table can tell us how\nmany local variables we have.\n\n19\n\n\fCodegen for expressions\n● expressions calculate values, and in a CPU values go in registers.\n● an expression in isolation doesn't really tell you what to do though.\nf(16)\n\nli a0, 16\njal f\n\nreturn 16;\n\nli v0, 16\nj _return\n\nx = 16;\n\nli t0, 16\nsw t0, 4(sp)\nif it's a local, or…\n\nli t0, 16\nsw t0, x\nif it's a global!\n\nthe same expression 16 is\ntranslated into different code\ndepending on how it's used.\nif this seems complicated,\nyeah, it is\nbut we'll come back to this\nand solve it by being lazy!\n20\n\n\fCodegen for statements\n● lots of statements do control flow, meaning the output code is\ngonna have labels and jumps and branches (ew).\no fortunately, the templates for these are pretty straightforward and\nset in stone – there's only one real \"right\" way to do an if-else.\n● sequential statements (like { blocks }) are no problem.\no you just translate each statement one after another, and\nconcatenate the code together. (yes, seriously!)\n● even nested statements are simple thanks to the AST.\no with the power of recursion, it all works out. trust recursion.\n● but I think that's enough of an introduction to codegen.\n● let's assume we have it working. does that mean we now have a\nworking program? are we done with compiler??\no well…\n21\n\n\fLinking and Executables\n\n22\n\n\fAbunchafunctions\n● a simple program might be one main() function and nothing else.\n● if we have more functions, we can concatenate their code.\nfn main() {\nf(10);\n}\nfn g(x) {\nprintln(x);\n}\nfn f(x) {\ng(x + 5);\n}\n\n00: li\na0, 10\n04: jal f\n08: li\nv0, 10\n0C: syscall\n10: li\nv0, 1\n14: syscall\n18: jr\nra\n1C: sub sp, sp, 4\n20: sw\nra, 0(sp)\n24: add a0, a0, 5\n28: jal g\n2C: lw\nra, 0(sp)\n30: add sp, sp, 4\n34: jr\nra\n\nwhat addresses should\nthe jals jump to?\nbut when do we know the\naddresses of the functions?\n\nwhat if I define the functions\nin a different order?\nwhat if I call functions\nfrom the stdlib?\n23\n\n\fTrying to do too much at once\n● really, we're moving past what the compiler should be doing…\na program's call graph can be complex,\nwith cycles, multiple dependencies, etc.\n\nmain\n\nf\ng\n\nprint_list\n\nwe have to serialize this graph when\nconverting it to an executable form.\nthis process is called linking.\n\nrather than the compiler producing a whole program, we have it\nproduce incomplete fragments, and let the linker finish the job.\n\n24\n\n\fSymbolic linking\n● the dependencies between fragments are indicated symbolically:\n● instead of referring to them by address, we do it by name.\nmain\nf blah\n\nprintln\n\nprint_list\n\nf\n\ncode g\nmore stuff\n\neach fragment has \"blanks\" to\nindicate what it references.\n\ng\nblah blah\n\nprint_list\nif { }\nelse\nprint_list\n\n;\n\nthe linker serializes the fragments\nand \"fills in the blanks.\"\n\nit can also include fragments from\nother parts of the program or from\nlibraries (like the stdlib).\n\n25\n\n\fRelocations\n● for a number of reasons, the addresses that your code and data end\nup at may not be known until right before it's executed!\n● an executable file can have relocations: \"blanks\" where absolute\naddresses are needed, which are filled in at load-time.\nIn the executable\n00: li a0, 10\n04: jal 0\n08: li v0, 10\n0C: syscall\n10: li v0, 1\n14: syscall\n18: jr ra\n\nReloc { addr: 0x0004,\nkind: Jump26,\ntarget: \"g\", }\nSymbol { addr: 0x0010,\nkind: Func,\nname: \"g\", }\n\nAfter loading\n8000: li a0, 10\n8004: jal 0x8010\n8008: li v0, 10\n800C: syscall\n8010: li v0, 1\n8014: syscall\n8018: jr ra\n\nif this code is loaded at address 0x8000…\n26\n\n\fPosition-independent code\n● compilers often have a flag to output position-independent code,\nwhich uses different instructions to never use absolute addresses.\no this avoids relocations entirely, making things faster to load, and it\navoids duplicating shared libraries in RAM.\n● to do this, the target ISA must be able to calculate addresses of code\nand data based on the PC (\"PC-relative addressing\"):\nlike a branch-and-link instruction:\n\nbal func\n\nand load\/store instructions which\nuse the pc as the base register:\n\nlw t0, 0x38C(pc)\n\n27\n\n\fDebugging info\n● converting to the target language is a lossy operation.\no a bunch of info about the source program is lost!\n● to make debugging possible, the compiler can also output info like:\no what source file and line each instruction corresponds to\no the names and locations of functions, globals, and locals\no the types of storage locations\no the structure of custom types (structs\/classes)\no the arrangement of stack frames for each function\n● this way, when you run your program in a debugger, you can:\no step through it line-by-line\no inspect the contents of variables, arrays, etc.\no have the debugger display that stuff in a human-readable way\n● this info is usually HUGE, so it's optional and typically removed in\n\"release\" versions.\n28\n\n\fExecutable formats\n● we can't dump a bunch of instructions out and expect them to run.\n● an executable format is like a seed, or an…………. egg\no it has the code and all the metadata needed to support it.\n● the OS ABIs define these formats, but they basically all look like this:\nheader\n\na header that identifies the type of the file\n\nsymbols\n\na symbol table (names, kinds, addresses)\n\nrelocs\n\nrelocation records\n\n.text\n\nand then multiple sections for things like code\n(.text), globals (.data), constants (.rodata), etc.\n\n.data\n.debug\n\nand optional debug info sections\n29\n\n\fObject files, executables, and libraries\n● typically this format is used to represent three kinds of files:\n\nhello.exe\n\nobject files: incomplete\npieces which might\ncorrespond to a single\nsource file or module.\nlinking them together\ncan produce…\n\nan executable, which\nhas an entry point\n(the first function to be\nrun when loaded), or…\n\na library (either static\nor dynamically linked),\nwhich… doesn't have an\nentry point.\nhello.dll\n\ninstead, it exports\nfunctions to be used\nby other files.\n\n30\n\n\fSo what will we do??\n● well, this is a compilers course, not a linkers course…\n● rather than output machine code, our compiler will output assembly\nlanguage code. you know, just text.\no see, MARS – the MIPS emulator – has a linker built into it.\no so we'll let it \"finish the job\" and do the linking for us.\n● this might sound like a cop-out, but lots of compilers work this way!\no not just toy compilers. gcc does this, if you ask it to.\n● many compilers output object files full of machine code so they can\nskip the assembly step and just go straight to linking…\no but that's a compilation speed optimization, not a requirement.\n\n31\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":229,"segment": "unlabeled", "course": "cs1622", "lec": "lec10","text":"Advanced Typing\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● the exam is on Monday, don’t forget!\no we’ll do a little review before we start.\no it will be taken on Canvas, but in person, so bring a computer.\no also do not use Safari, mac users! issues with Canvas exams!\n● now let’s do some ADVANCED TYPING\n\n2\n\n\fPolymorphism\n\n3\n\n\fPoly = Many, Morph = Shape\n● abstraction is about ignoring concrete details.\n● polymorphism lets us write abstract code once, that can work on\nmany types of values, without having to change the code.\n● it covers a wide range of techniques and language features, like…\no subclasses in Java, C++, Python\no interfaces in Java, Go and traits in Rust\no generics in Java, Rust\no templates in C++\no function name overloading in Java, C++\no operator overloading in C++, Rust, Python\no unions and void pointers in C\no duck typing in most dynamically-typed languages\no anything with \"late binding\" in the name\n4\n\n\fDon't repeat yourself.\n● repeating code is a cardinal sin. encode your thoughts once.\n● polymorphism lets us avoid repeating code.\n\nArrayList<Integer> i = new ArrayList<>();\nArrayList<String> s = new ArrayList<>();\nthe task of keeping an array-based list of things doesn't\nchange depending on what type of things are in the array.\nwe don’t have to make a new ArrayList class for\nevery type of value we want to store. we wrote it\nonce, and now it will work for any type of value.\n\nwe can even make functions which operate on any type of ArrayList:\n\n<T> void foo(ArrayList<T> list)\n5\n\n\fWhat type is it?\n● the basic idea is that polymorphism lets us make values which can\nhave more than one possible type.\nObject\nBase\nA\n\nB\n\nwith this class hierarchy, I can make a Base\nvariable which can hold 3 different types.\n\nvoid print(int i);\nvoid print(String s);\nwhat type is this print method?\n\nI guess it's a set of types…?\n\n6\n\n\f\"Ad-Hoc\" Polymorphism\n\n7\n\n\fYou know about this: overloading!\n● this is something you can do in Java (but not Rust):\nvoid print(int i);\nvoid print(String s);\n● now when I call print(x)…\no which version, or overload, is used is decided statically based on\nthe type of the argument.\n● each overload can do different things, which can be useful!\no …but if they're all doing basically the same thing, it feels silly.\no have a look at java.util.Arrays for an example of what I mean.\n● it's called ad-hoc because you just add overloads as you need them,\nand the compiler figures out what you mean.\n● so how would this be handled by a compiler?\n\n8\n\n\fImplementing overloading in a compiler\n● essentially, we're just extending the symbol table.\nas the user adds more overloads,\nthe set can be extended.\n\nName\n\nReferent\n\n\"A\"\n\n<class A>\n\n\"print\"\n\n<overload set>\n\nSignature\n\nReferent\n\nnow a name can refer to either\na single symbol or a set of\nsymbols with the same name.\n\nvoid(int)\n\n<print 1>\n\nvoid(char)\n\n<print 2>\n\nvoid(String)\n\n<print 3>\n\nnow, when print is called, it searches for the\nright signature in the overload set, and gives an\nerror if none match or more than one match.\n9\n\n\fMore than one can match??\n● overloading is complicated by implicit type conversions.\n● for example, Java will implicitly convert ints to floats.\n\nstatic void print(int i, float f)...\nstatic void print(float f, int i)...\nprint(10, 10);\n10 can be implicitly converted to float, so this use of print could\nmean either overload. the compiler says this is ambiguous.\n\nyou don't even wanna know how ugly this gets in languages\nlike C++ where you can define your own implicit conversions.\n\n10\n\n\fName mangling\n● for practical reasons (linkers, debuggers, etc), these overloaded\nfunctions must be given unique names in the output object files.\n● name mangling encodes the function's signature – its argument\nand return types – into a textual form that these other tools can use.\n\nvoid print(int i, float f)\n=> \"_Z5printif\"\n(g++)\n=> \"?print@@YAXHM@Z\" (msvc++)\nvoid print(float f, int i)\n=> \"_Z5printfi\"\n(g++)\n=> \"?print@@YAXMH@Z\" (msvc++)\nnow you know what these horrid things are, if they\never show up in error messages…\n11\n\n\fSubtype Polymorphism\n\n12\n\n\fYou know about this one too!\n● in subtype polymorphism, types can be placed in a hierarchy, where\nsubtypes are more specialized than their base types.\n● the important feature is: anywhere a base type is expected, a\nsubtype can be used in its place.\n\nvoid print(Object o) {\nSystem.out.println(o.toString());\n}\nany object type can be passed to this function, because all\nobject types are, directly or transitively, subtypes of Object.\nanother way of thinking of it is that the set of operations of each\nobject type is a superset of the set of operations of Object.\n13\n\n\fInterfaces\n● Java also has interfaces. every type can extend one class but can\nimplement any number of interfaces.\nObject\n\nBase\n\nComparable\n\nCloneable\n\nnow Base is a subtype of\nthree other types.\n\nwe write these relations as Base <: Object\n(or Object :> Base), Base <: Comparable,\nand Base <: Cloneable.\n\nbut interfaces exist \"outside\" the class hierarchy, letting us group\ntypes by their capabilities, not just their inheritance.\n\n14\n\n\fImplementing subtyping in a compiler\n● the compiler has to keep track of the subtype relations between\nclasses and interfaces.\no it also has to ensure that there are no cycles in the subtypes, like:\nclass A extends B {}\nclass B extends A {} \/\/ how??\n● then when doing typechecking, it will allow any type or its subtype to\nbe put into a variable, argument, array slot etc.\no Object o = new A(); is valid because A is a subtype of Object\no A a = new Object(); is not, because the opposite is not true.\n● it really is that straightforward!\no …. ha ha ha sure yeah right\n\n15\n\n\fBut it's got… issues.\n● subtyping comes at a cost: complexity.\n● it interacts with every other feature of a language, often in non-trivial\nways, and even making some type systems undecidable.\nvoid print(Object o)...\nvoid print(A a)...\n\ndoes this make sense? is\nthis what you'd expect?\n\nthis happens because Java's\nA a = new A();\nad-hoc polymorphism is\n\/\/ calls print(A)\nstatically resolved, but its\nprint(a);\nsubtype polymorphism is\nObject o = a;\ndynamically resolved.\n\/\/ calls print(Object)\nprint(o);\nother languages have other ideas about\nthis – look up multiple dispatch.\n16\n\n\fImplementing subtyping at runtime\n● we won't get into the details for a few weeks, but…\n● each class gets a data layout and a virtual method table (vtable).\n● subclasses' layouts and vtables are prefixed by their superclasses'.\nclass Object\n\nclass A { int x; void foo() {} }\n\nlayout\n\nvtable\n\nlayout\n\nvtable\n\nvtable\n\n<Object>\n\nvtable\n\n<A>\n\nmonitor\n\nclone()\n\nmonitor\n\nclone()\n\nequals()\n\nx\n\nequals()\n\ntoString()\n\ntoString()\nfoo()\n\nany A reference therefore points to a piece of memory\nwhich looks identical to an Object at the beginning.\n17\n\n\fSubtyping in Rust\n● Rust has no OOP, but it does support a form of subtyping and\ndynamic dispatch using traits, which are similar to Java interfaces.\ntrait Base {\nnow any type which implements\nfn base(&self);\nDerived must also implement Base.\n}\n\/\/ Derived is called a\nyou can also have variables of\n\/\/ supertrait of Base\ntype Box<dyn Base>, which are\ntrait Derived: Base {\nlike Java's interface variables.\nfn derived(&self) {\n\/\/ we can use\nthey can hold any type which\n\/\/ methods of Base\nimplements Base, and their methods\nself.base();\nwill be dispatched at runtime.\n}\n}\n\n18\n\n\fParametric Polymorphism\n\n19\n\n\fStatic and Dynamic\n● ad-hoc polymorphism is statically dispatched.\n● subtype polymorphism is (mostly) dynamically dispatched.\n● but dynamic dispatch is actually kind of costly on modern CPUs…\n\no.toString();\n\nlw a0, 4(sp) # a0 = o\nlw t0, 0(a0) # t0 = o.vtbl\nlw t0, 16(t0) # t0 = o.vtbl[4]\njalr t0\n# indirect call\n\nthree dependent loads from three totally different places\nin memory make the pipeline and cache very unhappy.\nand then you have an indirect jump, which can confuse\nthe branch predictor and stall the instruction pipeline!\n20\n\n\fParametric polymorphism to the rescue\n● parametric polymorphism is a more explicit kind of polymorphism,\nwhere the types are a sort of \"fill in the blank.\"\n● the types themselves become an argument.\n\nfn id<T>(t: T) -> T {\nreturn t;\n}\n\nthis is a silly function which just returns its\nargument. \"id\" means \"identity.\" it's math stuff.\n\nthis is not one function. this is an entire family of\nfunctions, which take a T and return a T, for all types T.\nthis is universal quantification.\nwe're saying ∀T, you can get an id function of type T→T\n(a function that maps from T to T).\nwhat \"type\" is id, then?\n21\n\n\fUsing parametrically polymorphic functions\n● Rust lets you either explicitly or implicitly specify the type arguments.\n\nlet x: i32 = id::<i32>(10);\nlet y: i32 = id(20);\nlet z = id(30);\n\nthe ::<> operator is\ncalled \"turbofish.\" 🐟💨\n\nin the second line, the compiler is able to infer\nwhat type should be passed as the type argument.\nin the third line, it infers the type of the variable too!\nall three lines use the same implementation of id, id::<i32>.\n22\n\n\fMonomorphization\n● but CPUs don't understand polymorphic code.\n● monomorphization converts each use of a polymorphic function\ninto a concrete, monomorphic (\"one-shape\") version of itself.\n\nlet x = id(10i32);\nlet y = id(3.5f64);\nlet z = id(\"hello\");\n\nid::<i32>\nid::<f64>\nid::<&str>\n\nthe compiler looks at every use of the function across the\nentire program to determine the implementations needed.\nit will reuse implementations if used in multiple locations\n(like if id::<i32> is used 100 times).\nessentially, it's automatically generating\nall the ad-hoc overloads for us!\n\n23\n\n\fGenerics without monomorphization\n● monomorphization gives you the best possible performance…\no but it can slow compilation, and produce a larger executable.\n● so, in Java, generics are not monomorphized!\no if you have ArrayList<Integer> and ArrayList<String>, the\ncompiler treats them as separate types, but…\no both of those use the exact same code at runtime.\no under the hood, all versions of ArrayList just use Object\nvariables and downcasts and instanceof to make it work.\n● this has advantages in compilation time and code size… but it comes\nat two big costs.\no one, you cannot use non-object types in Java generics.\n▪ this is why ArrayList<int> is invalid!\no two, the performance is mediocre.\n▪ but that's not really a main goal for Java so whatever!\n24\n\n\fBounded Quantification\n(or, \"making Parametric Polymorphism actually useful\")\n\n25\n\n\f∀ only gets you so far.\n● remember, a type has a set of values and a set of operations.\n● what operations can you do with a variable that can be any type?\no can you add it? index it? even print it out?\no well… no. because you don't know if the type will support those\noperations.\no about all you can do is copy values around (put them in variables,\npass them as arguments, return them) and that's not too useful.\n● well if we can make universal types, what about ∃ existential types?\n● an existential type (or bounded quantification) gives you the\nflexibility to accept multiple types…\no while saying, \"they need to support at least this set of operations.\"\n● it's a little easier to demonstrate than explain.\n\n26\n\n\fBounded quantification in Java\n● remember extends in generics?\n\n<T extends Comparable<T>> T max(T a, T b) {\nreturn (a.compareTo(b) > 0) ? a : b;\n}\nthis works for any type T as long as that type implements the\nComparable interface (against other values of the same type).\n\nSystem.out.println(max(3, 4));\n\/\/ prints 4\nSystem.out.println(max(\"a\", \"b\")); \/\/ prints b\nwithout the extends Comparable<T>, we wouldn't\nbe able to call .compareTo() in the method.\n27\n\n\fBounded quantification in Rust\n● Rust's traits do the same job as Java's interfaces here.\n\nfn max<T: PartialOrd>(a: T, b: T) -> T {\nif a > b { a } else { b }\n}\nPartialOrd is the closest thing Rust has to Java's Comparable,\nand the resulting function can be used the same way:\n\nprintln!(\"{}\", max(3, 4));\n\/\/ prints 4\nprintln!(\"{}\", max(3.3, 4.4)); \/\/ prints 4.4\n\n28\n\n\fBut what about using subtype polymorphism?\n● couldn't we just make our Java function take two Comparables?\n● well yes, but…\n\nComparable max(Comparable a, Comparable b) {\nreturn (a.compareTo(b) > 0) ? a : b;\n}\nnow this nonsense code compiles and runs,\nand we get a runtime error instead:\n\nSystem.out.println(max(3, \"a\"));\nparametric polymorphism moves the type\nchecking into the compiler, allowing us to catch\nerrors without having to run the code, and without\nthe performance penalty of runtime type-checking*.\n29\n\n\fSummary\n\n30\n\n\fThe kinds of polymorphism\n● ad-hoc polymorphism is also called function overloading.\no it lets you define multiple versions of the same function with\ndifferent type signatures.\no the version that will be used is selected at compile time.\n● parametric polymorphism is also called generics or templates.\no it lets you define \"fill-in-the-blanks\" classes, structs, functions etc.\no the blanks can be filled in with any type… or with bounded\nquantification, only types which satisfy some condition.\no the types are filled in and checked at compile time.\no the generated code may or may not be monomorphized.\n● finally, subtype polymorphism is a feature of classes and interfaces.\no a derived value can be used anywhere a base type is required.\no when a virtual method is called, the version that is used is selected\nat runtime, based on the type of the object.\n31\n\n\fThe limits of typing and type systems\n● it doesn't matter how fancy your type system is…\no it will never allow all correct programs.\nwe saw this previously when talking about Box in Rust.\n\nthis seemingly-simple object graph is impossible to directly\nrepresent in Rust because of its type system.\nbut that's the point. type systems limit what we can do,\nbecause unlimited ability leads to mistakes.\n32\n\n\fColoring inside the lines\n● type systems force us to work within a set of limitations, which can\nsometimes be annoying…\no but it can also lead to simpler, more elegant solutions.\n● because static type systems are automatically checked by the\ncompiler, playing by their rules can avoid many silly mistakes.\no programming is hard. you need all the help you can get!\n● and with that…\no we're done with the material before the exam ö\n\n33\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":230,"segment": "unlabeled", "course": "cs1550", "lec": "lec01", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Lab 0 due this Friday (soft deadline; not graded)\n\n• Homework 1 will be posted this Friday\n• Recitations start this week\n• VS Code setup tutorial on Piazza (also linked from\nCanvas)\n\n• Draft Slides linked from Canvas\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fAgenda\n• Main tasks of an operating system\n\n• System Calls\n• What an interrupt is\n• What happens when an interrupt occurs\n• What a system call is\n\n• How system calls implemented\n• Effect of OS structure on system calls\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fWhat is an Operating System?\nA program that acts as an intermediary between a user\nof a computer and the computer hardware\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fWhat does an OS do?\n• Manages (controls and arbitrates) resources\n• Processors, Memory, Input\/output devices,\nCommunication devices, Storage, Software applications\n• Conflicting goals:\n•\n\ne.g., performance vs. utilization\n\n•\n\nSeparation of policy and mechansim\n\n• Provides abstractions to application programs\n• Ease of use\n• Virtualization\n\n• Protects resources\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fInterrupts\nHardware or software:\n• Hardware interrupt by one of the devices\n• Software interrupt (exception or trap):\n• Software error (e.g., division by zero)\n• Other process problems include processes trying to modify each\nother’s or the operating system’s memory (e.g., segmentation\nfault)\n• Request for operating system service (i.e., system call)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fInterrupt Descriptor Table\n• Interrupt transfers control to the interrupt service\nroutine (ISR)\n• ISRs are segments of code that determine what\naction should be taken for each type of interrupt\n• part of the OS kernel\n\n• An interrupt vector contains the address of the ISR\nfor one interrupt\n• An interrupt vector table is an array of interrupt\nvectors\n• also known as interrupt descriptor table (IDT)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDual-mode Operation\n• Dual-mode operation allows OS to protect itself and\nother system components\n• At least two modes: user mode and kernel mode\n• Mode bit(s) provided by hardware (inside CPU registers)\n• Provides ability to distinguish when system is running user code\nor kernel code\n• Some instructions designated as privileged, only executable in\nkernel mode\n• Some memory addresses designated as privileged, only\naccessible in kernel mode\n•\n\nTherefore, we get segmentation fault on null (i.e., 0) pointer dereference\n\n• Interrupts change mode to kernel\n•\n\nreturn from interrupt resets mode back to user\n\n• Increasingly CPUs support multi-mode operations\n• virtual machine manager (VMM) mode for guest VMs\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fWhat happens when an interrupt occurs?\nThe CPU transitions from User Mode to Kernel Mode\n\nan interrupt\noccurs\n\nreturn from ISR\n\nexecute ISR\n\nTime\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fHow does an OS (roughly) work?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fWhat happens on a hardware interrupt?\n6\n\n1\n\n4\n\n5\n\n9\n\n3\n\n8\n\n2\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fSystem Calls\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSystem Calls\n• Programming interface to OS services\n• Typically written in a high-level language (C or C++)\n• Mostly accessed by programs via a high-level\nApplication Programming Interface (API) rather\nthan direct system call use\n• Win32 API for Windows\n• POSIX API for POSIX-based systems (including virtually\nall versions of UNIX, Linux, and Mac OS X), and\n• Java API for the Java virtual machine (JVM)\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSystem Call Implementation\n• Typically, there is a number associated with each system\ncall\n• Each system call has a corresponding system call\nimplementation function (part of the OS kernel)\n• System-call table indexed according to these numbers\n•\n\nEach entry in the table contains the address of the\ncorresponding system call implementation function\n\n• The system call interface is the ISR corresponding to the\nsyscall software interrupt\n•\n•\n•\n\ninvokes the intended system call in OS kernel,\npasses arguments if needed, and\nreturns status of the system call and any return values\n\n• The caller need know nothing about how the system call\nis implemented\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fAPI – System Call – OS Relationship\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fStandard C Library Example\n• C program invoking printf() library call, which calls\nwrite() system call\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fWhat happens on a syscall?\n2\n\n1\n6\n\n7\n12\n\n1 eax <-- 1\nint 64\n\n10\n\n3\n\n4\n\n11\n\n5\n\n8\n\n9\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSystem Call Parameter Passing\n• Three general methods used to pass parameters to\nthe OS\n• Simplest: pass the parameters in registers\n•\n\nIn some cases, may be more parameters than registers\n\n• Parameters stored in a block, or table, in memory, and\naddress of block passed as a parameter in a register\n• This approach taken by Linux and Solaris\n\n• Parameters placed, or pushed, onto the stack by the\nprogram and popped off the stack by the operating\nsystem\n•\n\nXV6\n\n• Block and stack methods do not limit the number or\nlength of parameters being passed\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fHow to add a system call to an OS?\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fXv6 Code Walkthrough\n• IDT table initialization\n\n• Syscall table\n• How a syscall is invoked\n• Syscall implementation\n• Parameter passing into a syscall\n• In Lab 1 you will add a system call to Xv6\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fTraditional UNIX System Structure\nBeyond simple but not fully layered\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fMicrokernel System Structure\nApplication\nProgram\n\nFile\nSystem\n\nmessages\n\nInterprocess\nCommunication\n\nDevice\nDriver\n\nuser\nmode\n\nCPU\nscheduling\n\nkernel\nmode\n\nmessages\n\nmemory\nmanagment\n\nmicrokernel\n\nhardware\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":231,"segment": "unlabeled", "course": "cs1550", "lec": "lec17", "text":"Introduction to Operating Systems\nCS 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Quiz 2: due on 3\/25\n• Homework 9: due on 3\/28\n• Lab 3: due on 4\/1\n• Project 3: due on 4\/11\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• Page replacement algorithms\nAlgorithm\n\nComment\n\nOPT (Optimal)\n\nNot implementable, but useful as a benchmark\n\nNRU (Not Recently Used)\n\nCrude\n\nFIFO (First-In, First Out)\n\nMight throw out useful pages\n\nSecond chance\n\nBig improvement over FIFO\n\nClock\n\nBetter implementation of second chance\n\nLRU (Least Recently Used)\n\nExcellent, but hard to implement exactly\n\nNFU (Not Frequently Used)\n\nPoor approximation to LRU\n\nAging\n\nGood approximation to LRU, efficient to implement\n\nWorking Set\n\nSomewhat expensive to implement\n\nWSClock\n\nImplementable version of Working Set\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (Memory)\n• Difference between NRU and NFU\n• For NRU, what is the way in which it separates the pages into\nclasses in order to pick a victim from a group of the lowest nonempty class\n• On the clock for the clock algorithm, what is t and why does it\nchange to 32 on the referenced pages?\n• What are we adding to the shift register in the working set\nreplacement algorithm?\n• Why is the tracking of a process’ working set necessary?\n• Which page replacement algorithm is the best (generally)?\n• The question on the homework that has the numerical entry\nanswer. I'm not quite sure how to adapt what we discussed in\nclass to that question.\n• How the eviction happens in the pages\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (Memory)\n• How do we determine tau again?\n• On the last slide about working set page, does the page\ntable get updated every clock tick? Or do we only care\nabout evey clock interrupt here? I am a little confused\nabout how is the clock interrupt concept applied to the\nworking set page replacement algorithm.\n• What is k in the working set graph?\n• in second chance and clock replacement, why do\nreferenced pages have their ref bit set back to 0?\n• Do most algorithms need to iterate through all pages\n(O(N) runtime)?\n\n• I am confused on the details of thrashing\n• LRU page replacement\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (Misc.)\n• i had a bit of a hard time keeping up with pretty much\neverything but i think that's just a side effect of\ncoming back from break\n• what is it that makes the TLB access fast while being\nslower for memory PTE access\n• when is the midterm grades posted also will there be\nmakeup to get up to 30% back?\n• When are TLB entries replaced\n• Is \"Frame\" the same thing as a physical page?\n• How is the page table indexed?\n\n• redo a walk through the address translation process\nwith the CPU, MMU, & TLB caching\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fProblem of the Day\n• How to simulate page replacement algorithms\n• FIFO\/Clock\n• LRU, OPT\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\f•\n\n•\n\nHow is modeling done?\nGenerate a list of references\n•\n\nArtificial (made up)\n\n•\n\nTrace a real workload (set of processes)\n\nUse an array (or other structure) to track the pages in physical memory\nat any given time\n•\n\nMay keep other information per page to help simulate the algorithm (modification\ntime, time when paged in, etc.)\n\n•\n\nRun through references, applying the replacement algorithm\n\n•\n\nExample: FIFO replacement on reference string 0 1 2 3 0 1 4 0 1 2 3 4\n•\n\nPage replacements highlighted in yellow\nPage referenced\n0 1 2 3 0 1 4 0 1 2 3 4\nYoungest page\n\n0 1 2 3 0 1 4 4 4 2 3 3\n0 1 2 3 0 1 1 1 4 2 2\n\nOldest page\n\n0 1 2 3 0 0 0 1 4 4\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fInteractive Simulation Tool\n• https:\/\/sim-50.github.io\/cs-tools\/\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fFIFO with 3 frames\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fFIFO Example 1\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fFIFO with 4 frames\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\f•\n\n•\n\n•\n\nBelady’s anomaly\nReduce the number of page faults by supplying more memory\n•\n\nUse previous reference string and FIFO algorithm\n\n•\n\nAdd another page to physical memory (total 4 pages)\n\nMore page faults (10 vs. 9), not fewer!\n•\n\nThis is called Belady’s anomaly\n\n•\n\nAdding more pages shouldn’t result in worse performance!\n\nMotivated the study of paging algorithms\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fCLOCK Simulation\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\fModeling more replacement algorithms\n• Paging system characterized by:\n• Reference string of executing process\n• Page replacement algorithm\n• Number of page frames available in physical memory (m)\n\n• Model this by keeping track of all n pages referenced\nin array M\n• Top part of M has m pages in memory\n• Bottom part of M has n-m pages stored on disk\n\n• Page replacement occurs when page moves from top\nto bottom\n• Top and bottom parts may be rearranged without causing\nmovement between memory and disk\nCS 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\f•\n\nExample: LRU\nModel LRU replacement with\n•\n\n8 unique references in the reference string\n\n•\n\n4 pages of physical memory\n\n•\n\nArray state over time shown below\n\n•\n\nLRU treats list of pages like a stack\n0 2 1 3 5 4 6 3 7 4 7 3 3 5 5 3 1 1 1 7 1 3 4 1\n0 2 1 3 5 4 6 3 7 4 7 3 3 5 5 3 1 1 1 7 1 3 4 1\n0 2 1 3 5 4 6 3 7 4 7 7 3 3 5 3 3 3 1 7 1 3 4\n0 2 1 3 5 4 6 3 3 4 4 7 7 7 5 5 5 3 3 7 1 3\n0 2 1 3 5 4 6 6 6 6 4 4 4 7 7 7 5 5 5 7 7\n0 2 1 1 5 5 5 5 5 6 6 6 4 4 4 4 4 4 5 5\n0 2 2 1 1 1 1 1 1 1 1 6 6 6 6 6 6 6 6\n0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nCS 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fStack algorithms\n• LRU is an example of a stack algorithm\n• For stack algorithms\n• Any page in memory with m physical pages is also in\nmemory with m+1 physical pages\n• Increasing memory size is guaranteed to reduce (or at\nleast not increase) the number of page faults\n\n• Stack algorithms do not suffer from Belady’s anomaly\n• Distance of a reference == position of the page in the\nstack before the reference was made\n• Distance is  if no reference had been made before\n• Distance depends on reference string and paging\nalgorithm: might be different for LRU and optimal (both\nstack algorithms)\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fPredicting page fault rates using distance\n• Distance can be used to predict page fault rates\n• Make a single pass over the reference string to\ngenerate the distance string on-the-fly\n• Keep an array of counts\n• Entry j counts the number of times distance j occurs in the\ndistance string\n\n• The number of page faults for a memory of size m is\nthe sum of the counts for j>m\n• This can be done in a single pass!\n• Makes for fast simulations of page replacement\nalgorithms\n\n• This is why virtual memory theorists like stack\nalgorithms!\nCS 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fLRU\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n19\n\n\fOPT\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n20\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":232,"segment": "unlabeled", "course": "cs1541", "lec": "lec3.4_virtual_memory","text":"Virtual Memory\nand Caching\nCS 1541\nWonsun Ahn\n\n\fVirtual Memory and Caching\n● So what does virtual memory have to do with caching?\n● A lot actually.\n● But first let’s do a quick review of virtual memory\no To warm up your cache with CS 449 info\n\n2\n\n\fVirtual Memory Review\n\n3\n\n\fVirtual Memory: Type of Virtualization\n● Virtualization: hiding the complexities of hardware to software\n● Virtual Memory: hides the fact that physical memory (DRAM) is\nlimited and shared by multiple processes\nPhysical Memory\nProcess 1\n0xFFFF\n\n0xFFFF\n\nMemory\n\n0x8000\n\nCode\n\n...\n\nProcess 2\n0xFFFF\n\nProcess 1’s\nand\nProcess 2’s\nmemory?\n\nMemory\n\n0x8000\n\nCode\n\nClearly this is impossible.\nBut programs see this view of memory.\n\n0x8000\n...\n\n4\n\n\fVirtual Memory: Behind the Scenes\n● Pages of memory are mapped to either physical memory or disk\no Look familiar? Physical memory acts as a cache for disk storage\n\nValid\n1\n0\n1\n1\n0\n1\n1\n0\n1\n1\n0\n1\n\nPage table\n\nPhysical memory\n\nVirtual memory\nDisk storage\n(swap space)\n\n5\n\n\fHow virtual to physical address translation happens\n1. CPU extracts virtual page number from virtual address\n2. CPU locates page table pointed to by page table register\n3. Page table is indexed using virtual page number\nPage table register\nVirtual address\n\n31 30 29 28 27\n\n15 14 13 12 11 10 9 8\n\nVirtual page number\nVirtual address\n31 30 29 28 27\n\n15 14 13 12 11 10 9 8\n\nVirtual page number\n\npage offset\n\n20\n\n3210\n\nValid\n\npage offset\n\n3 2 1 0\n\n12\n\nPhysical page number\n\nPage table\n29 28 27\n\n15 14 13 12 11 10 9 8\n\nPhysical page number\nPhysical address\n\n3210\n\npage table\n\npage offset\nIf 0 then page is\nnot in memory\n29 28 27\n\n18\n\n15 14 13 12 11 10 9 8\n\nPhysical page number\n\n3 2 1 0\n\npage offset\n\nPhysical address\n\n6\n\n\fDRAM as Cache\n\n7\n\n\fPhysical Memory as a Cache\n● Relationship between DRAM « Disk is same as Cache « DRAM\no DRAM is fast but small and expensive\no Disk is slow but big and cheap\n● If you view DRAM as cache, some design decisions become obvious\no Size of block: 4 KB pages. Why?\n§ For spatial locality. Capacity is less of a problem for DRAM.\no Associativity: Fully-associative (can map page anywhere). Why?\n§ A miss (page fault) is expensive. You need to read from disk!\n§ But now page hits become expensive due to lookup cost\no Block replacement scheme: LRU, or some approximation. Why?\n§ Did I say a page fault is expensive?\no Write policy: Write-back (a.k.a. page swapping). Why?\n§ Bandwidth for write-through to disk is too much for I\/O bus\n8\n\n\fPhysical Memory as a Cache\n● If you treated each page as a cache block, what would be the tag?\nPage offset (12 bits)\no 32-bit address: Tag (20 bits): page number\no Fully-associative, so row bits and 4 KB pages, so 12 bits for offset\n● How would the page table for searching physical memory look?\n\nCPU\n(PID 1)\nCPU\n(PID 2)\n.\n.\n.\n\nAccess\n\nPID\n\n20-bit tag\n(page number)\n\n1\n\n10110…111\n\n2\n\n10010…001\n\n1\n\n01010…100\n\n…\n\n…\n\n[Page Table]\n\nPhysical\nMemory\n[Pages]\n\nMiss\nDisk storage\n(swap space)\n\n[Disk]\n9\n\n\fInverted Page Table: tags for physical pages\n● This type of page table is called an inverted page table.\nAssociative search\n\nVirtual Page\nNumber\n\nPID\n\n20-bit tag\n(page number)\n\n1\n\nVirtual Page 42\n\n2\n\nVirtual Page 100\n\n1\n\nVirtual Page 123\n\n…\n\n…\n\n[Page Table]\n\nPhysical\nMemory\n[Pages]\n\nMiss\nDisk storage\n(swap space)\n\n[Disk]\n\n● Called inverted because table contains virtual page numbers\n(Unlike regular page tables which contains physical page numbers)\n● Pro: Page table only as big as physical mem (low space complexity)\n● Con: Associative search of page table (high time complexity)\n→ Often hashing used to direct map pages. Causes conflict misses.\n10\n\n\fHow Often do Lookups Happen?\n● Programs use virtual addresses to refer to code and data\no E.g. If program has jump to method address, it’s a virtual address\n● DRAM and Caches use physical addresses\n● At every lw or sw MEM stage a lookup needs to happen\n● At FETCH stage of every instruction a lookup needs to happen!\nProcess\n\nVirtual\nMemory\nCode\n\nVirtual\nAddresses\n\n?\nCPU ? Cache\n?\n\nPhysical\nAddresses\n\nPhysical\nMemory\n\n11\n\n\fAddress Lookup Using (Regular) Page Table\n● Lookup is done by indexing page table using virtual page number.\n● Every memory access requires one extra access to read page table.\nNow table must cover entire virtual memory!\nVirtual\nAddress\n\nCPU\n\nValid\n\nPhysical Page\nNumber (20 bits)\n\n1\n\nPhysical Page 42\n\n0\n\nPhysical Page 10\n\n0\n\nPhysical Page 7\n\n1\n\nPhysical Page 1337\n\n…\n\n…\n\nPhysical\nAddress\n\nDRAM\n\n12\n\n\fHow big is the Page Table?\n● 32-bit addresses with 4KiB (212 B) pages means 220 (1M) PTEs.\n● 64-bit addresses with 4KiB pages means 252 (4 quadrillion) PTEs.\n● We can use hierarchical page tables as a sparse data structure.\nAddress\n\nPTR\n\n10 bits (“directory”)\n\n10 bits (“table”)\n\n12 bits (“offset”)\n\n00 0010 11\n\n10 0011 00\n\n0010 0001 0000\n\nindex...\n\nindex...\n\nV Table Addr\n\nV D R Page Addr\n\nP\n\n…\n\n...\n\n…… …\n\n...\n\n...\n\n1\n\n0004C000\n\n10 1\n\n03BFA000\n\nRX\n\n…\n\n...\n\n…… …\n\n...\n\n...\n\nPA!\n\n13\n\n\fPage Table Lookup Cost\n● Let’s say we have a lw $t0, 16($s0)\n\nPTR\n\nCPU\n\nV Table Addr\n\nV D R Page Addr\n\nP\n\n…\n\n...\n\n…… …\n\n...\n\n...\n\n1\n\n0004C000\n\n10 1\n\n03BFA000\n\nRX\n\n…\n\n...\n\n…… …\n\n...\n\n...\n\nhit!\n\nPA!\n\nCache\n\n● Must perform two memory accesses to hierarchical page table\no May miss in cache and even cause page faults themselves!\n14\n\n\fThe real picture looks more like this\n● Alpha 21264 CPU with 3-level page table:\n\nIn the end, the PTE (Page Table Entry)\nis all you need for a translation.\n\nPtr to level 2\nPtr to level 3\n\nHow can I make access to it faster?\nWhere have I heard that before...\nmaking accesses faster… I wonder…\n\n15\n\n\fThe TLB:\nA Cache for Page Tables\n\n\fTLB (Translation Lookaside Buffer\n● TLB: A cache that contains frequently accessed page table entries\n● TLB just like other caches\nresides within the CPU\n● On a TLB hit:\no No need to access page\ntable in memory\n● On a TBL miss:\no Load PTE from page table\no That means “walking” the\nhierarchical page table\n\n17\n\n\fPage Table Walking\n● On a TLB miss, the CPU must “walk” the page table:\n● Two options:\n1. Software option\no Miss raises OS exception\no OS exception handler\nfills the TLB with PTE\n\nPtr to level 2\nPtr to level 3\n\n2. Hardware option\no CPU has special circuitry\nto walk page table\n(the page table walker)\n→ Faster than SW option\n18\n\n\fMemory Access Flowchart\nVirtual page\nnumber\n\nPage offset\n\nTLB\n\n• Page table walk to get the PT\nentry into the TLB (SW or HW)\n• If PT walk indicates page is\nnot in memory, then service\npage fault (OS handler)\n\nPhysical\naddress\n\nCache\nTo memory\n\nNote that there cannot be a\npage fault in case of a TLB hit\n– when page is swapped to\ndisk, the TLB is flushed\n\nMay need to write\nback a dirty block\n\nor,\ndepending\non being\nwrite back\nor write\nthrough\n\n19\n\n\fClose-up on the TLB\n● The TLB holds PTEs – mappings from VAs to PAs, along with other\ninfo used for protection and paging.\n\nVA\n\nVA Page (Tag)\n\nV\n\nD\n\nPres Ref Prot PA Page\n\n00008\n\n1\n\n0\n\n0\n\n1\n\nRX\n\n03BFA\n\nFFFF3\n\n1\n\n1\n\n1\n\n1\n\nRW\n\n19400\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\n…\n\nPA\n\n0 valid bit triggers TLB Miss.\n0 present bit triggers Page Fault.\nD-Read, D-Write, or I-Fetch? Exception if invalid!\n20\n\n\fTLBs in Real Processors\n\n21\n\n\fCaching Makes Everything Faster\nCPU\nVirtual address from lw\/sw instructions or\nfrom program counter (PC)\n\nVA Page#\n\npage\npage\n\nPage offset\n\npage\nBlock of a page\n\nTLB\n\nPhysical\naddress\n\nPhysical\nMemory\n\nCache\n\nPTE\n\nTLB miss\nPT walker\n\nPage fault\n\nPTE into TLB\n\nVirtual\nAddress\nspace\n\nHDD\/SSD\n\nPage\ntable\n\nOh no!\n\nOS Page Fault Handler\n\n22\n\n\fOverall Memory System Design\n● Fast memory access is possible through SW \/ HW collaboration:\n\nAddress\nTranslation\n(HW \/ SW)\n\nVA\n\nTLB\n\nCaching\n(HW)\n\nPA\n\nDatapath\n\nPA\nCaches\n\nWords\n\/bytes\n\nPaging\n(SW)\n\nDRAM\n\nBlocks\n\nPT\n\nPages\n\nHDD\/SSD\n\n23\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":233,"segment": "unlabeled", "course": "cs1550", "lec": "lec09", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines\n• Homework 4 is due next Monday 2\/14\n• Project 1 due on 2\/18\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n2\n\n\fPrevious lecture …\n• Readers-Writers problem\n• Semaphore solution\n• Condition Variable solution\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n3\n\n\fMuddiest Points (Semaphores solution)\n• reader writer queue\n• Why the read section of the Reader code includes\nmutex.down(). Why is it not included in the ReaderDone\nsection?\n• Why mutex need to up and down two times?\n• I confused about finally which of writing or reading has\npriority?\n• Starting off the readers writers problem how do you know\nthe amount of m, w, and nr ?\n• could you reiterate the reasoning for why a mutex doesn't\nwork for the writing semaphore? I think I understood it but I\njust want to be sure\n• does writing start at 0 or 1?\nCS 1550 – Operating Systems – Sherif Khattab\n\n4\n\n\fMuddiest Points (Condition Variable Solution)\n• Which solution is more optimal for the readers and\nwriters problem... CV or semaphores?\n\n• Ideas for more efficient implementation of condition\nvariable\/mutex code for reader\/writer problem\n• In the \"real world\" what is more commonly used, CV or\nsemaphore ?\n\n• can you post the cv example?\n• Why was the code at the end different from the code we\nwent over during lecture?\n• Understanding when a reader or writer is asked to wait!\n\n• Does the last example of the code in class prevent\nstarvation of writers?\nCS 1550 – Operating Systems – Sherif Khattab\n\n5\n\n\fMuddiest Points (misc.)\n• Can you expand on project 1's writeup?\n\n• I was confused on the producer consumer buffer and\nhow the producers go to sleep when waiting to enter.\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n6\n\n\fProblem of the Day\n•\n\n•\n\nDining Philosophers\n\nN philosophers around\na table\n•\n\nAll are hungry\n\n•\n\nAll like to think\n\nN chopsticks available\n•\n\n1 between each pair of\nphilosophers\n\n•\n\nPhilosophers need two\nchopsticks to eat\n\n•\n\nPhilosophers alternate\nbetween eating and\nthinking\n\n•\n\nGoal: coordinate use of\nchopsticks\nCS 1550 – Operating Systems – Sherif Khattab\n\n7\n\n\fDining Philosophers: solution 1\n•\n\nUse a semaphore for each chopstick\n\n•\n\nA hungry philosopher\n\n•\n\nGets the chopstick to his left\n\nShared variables\n\n•\n\nGets the chopstick to his right\n\n•\n\nEats\n\nconst int n;\n\/\/ initialize to 1\nSemaphore chopstick[n];\n\n•\n\nPuts down the chopsticks\n\n•\n\nPotential problems?\n•\n\nDeadlock\n\n•\n\nFairness\n\nCode for philosopher i\nwhile(1) {\nchopstick[i].down();\nchopstick[(i+1)%n].down();\n\/\/ eat\nchopstick[i].up();\nchopstick[(i+1)%n].up();\n\/\/ think\n}\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n8\n\n\fTracing: Sequence 1\n• P0 picks left\n\n• P0 picks right\n• P3 picks left\n• P3 picks right\n• P3 eats\n• P0 eats\n• P3 puts down\n• P0 puts down\n\nShared variables\nconst int n;\n\/\/ initialize to 1\nSemaphore chopstick[n];\n\nCode for philosopher i\nwhile(1) {\nchopstick[i].down();\nchopstick[(i+1)%n].down();\n\/\/ eat\nchopstick[i].up();\nchopstick[(i+1)%n].up();\n\/\/ think\n}\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n9\n\n\fTracing: Sequence 2\n• for(i=0; i<6; i++)\n• Pi picks left\n\n• P3 eats\n• P0 eats\n• P3 puts down\n\n• P0 puts down\n\nShared variables\nconst int n;\n\/\/ initialize to 1\nSemaphore chopstick[n];\n\nCode for philosopher i\nwhile(1) {\nchopstick[i].down();\nchopstick[(i+1)%n].down();\n\/\/ eat\nchopstick[i].up();\nchopstick[(i+1)%n].up();\n\/\/ think\n}\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n10\n\n\fWhat is a deadlock?\n• Formal definition:\n“A set of processes is deadlocked if each process in\nthe set is waiting for an event that only another\nprocess in the set can cause.”\n• Usually, the event is release of a currently held\nresource\n\n• In deadlock, none of the processes can\n• Run\n• Release resources\n• Be awakened\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n11\n\n\fHow to solve the Deadlock problem?\n• Ignore the problem\n\n• Detect and react\n• Prevent (intervene at design-time)\n• Avoid (intervene at run-time)\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n12\n\n\fThe Ostrich Algorithm\n• Pretend there’s no problem\n\n• Reasonable if\n• Deadlocks occur very rarely\n• Cost of prevention is high\n\n• UNIX and Windows take this approach\n• Resources (memory, CPU, disk space) are plentiful\n• Deadlocks over such resources rarely occur\n• Deadlocks typically handled by rebooting\n\n• Trade off between convenience and correctness\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n13\n\n\fDeadlock Detection\n\nHow can the OS detect a deadlock?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n14\n\n\f•\n\n•\n•\n\n•\n\nResource allocation graphs\nResource allocation modeled by directed\ngraphs\n\nA\n\nExample 1:\n•\n\nR\n\nResource R assigned to process A\n\nB\n\nExample 2:\n•\n\nProcess B is requesting \/ waiting for resource S\n\nS\n\nExample 3:\n•\n\nProcess C holds T, waiting for U\n\n•\n\nProcess D holds U, waiting for T\n\n•\n\nC and D are in deadlock!\n\nT\n\nC\n\nD\nU\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n15\n\n\fDeadlock Prevention\n\nHow an application\/system designer prevent\ndeadlocks?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n16\n\n\fDining Philosophers: solution 2\n•\n\nUse a semaphore for each\nchopstick\n\n•\n\nA hungry philosopher\n\nCode for philosopher i\n\n•\n\nGets lower, then higher numbered\nchopstick\n\n•\n\nEats\n\n•\n\nPuts down the chopsticks\n\nint i1,i2;\nwhile(1) {\nif (i != (n-1)) {\ni1 = i;\ni2 = i+1;\n} else {\ni1 = 0;\ni2 = n-1;\n}\nchopstick[i1].down();\nchopstick[i2].down();\n\/\/ eat\nchopstick[i1].up();\nchopstick[i2].up();\n\/\/ think\n}\n\n•\n\nPotential problems?\n•\n\nDeadlock\n\n•\n\nFairness\n\nShared variables\nconst int n;\n\/\/ initialize to 1\nSemaphore chopstick[n];\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n17\n\n\fDeadlock Avoidance\n\nHow can the OS intervene at run-time to avoid\ndeadlocks?\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n18\n\n\fDeadlock detection algorithm\n\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) { \/\/reached end of loop\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\n4\n\n0 4\n\n1\n\n1\n\nNote: want[j], hold[j], current, avail are arrays!\n\nA B C D\nAvail\n\n2 3 0 1\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n\nCS\/COE 1550 – Operating Systems – Sherif Khattab\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":234,"segment": "unlabeled", "course": "cs1541", "lec": "lec2.1_processor_review","text":"Processor Review\nCS 1541\nWonsun Ahn\n\n\fClocking Review\nStuff you learned in CS 447\n\n\fLogic components\n● Do you remember what all these do?\nNOT gate\n\nMultiplexer\n(MUX)\n32\n\nAND gate\n\n32\nDecoder\n\nOR gate\n\nXOR gate\n\nALU\n\n32\n\nThese wires\ncarry several\nbits at once.\n\nBlue wires are control signals.\n\n3\n\n\fUses of a Decoder\n● Translates a set of input signals to a bunch of output signals.\no E.g. a binary decoder:\nTruth Table for Decoder\nQ0\nA\n\nQ1\n\nB\n\nQ2\nQ3\n\nA\n\nB\n\nQ0\n\nQ1\n\nQ2\n\nQ3\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\no You can come up with any truth table and make a decoder for it!\n\n4\n\n\fUses of a Multiplexer\n● No problem in fanning out one signal to two points\nA\n\n+\n\n=\nB\n\nA\n\nB\n\n● Cannot connect two signals to one point\no Must use a multiplexer to select between the two\n+\n\nPath A\n\n=\n\nPath B\n\nMux control =\n0 for path A\n1 for path B\n\n5\n\n\fGates are made of transistors (of course)\n\n• NOT gate\n\n• NAND gate\n\n6\n\n\fThe clock signal\n● The clock is a signal that alternates regularly between 0 and 1:\n\n1\n\n0\n\ntime\n\n● Bits are latched on to registers and flip-flops on rising edges\n● In between rising edges, bits propagate through the logic circuit\no Composed of ALUs, muxes, decoders, etc.\no Propagation delay: amount of time it takes from input to output\n7\n\n\fCritical Path\n● Critical path: path in a circuit that has longest propagation delay\no Determines the overall clock speed.\n5 ns\n\nA\nD Q\n\nEN\n\nB\nD Q\n\n1\n\n+\n\nIN\n\n5 ns\n\nOUT\n\nEN\nSelect\n\no The ALU and the multiplexer both have a 5 ns delay\n● How fast can we clock this circuit?\no Is it 1 \/ 5 ns (5 × 10-9s) = 200 MHz?\no Or is it 1 \/ 10 ns (10 × 10-9s) = 100 MHz?\n\n8\n\n\fMIPS Review\nStuff you learned in CS 447\n\n\fThe MIPS ISA - Registers\n● MIPS has 32 32-bit registers, with the following usage conventions\n\no But really, all are general purpose registers (nothing special about them)\nName\n$zero\n$at\n$v0-$v1\n$a0-$a3\n$t0-$t7\n$s0-$s7\n$t8-$t9\n$k0-$k1\n$gp\n$sp\n$fp\n$ra\n\nRegister number\nUsage\n0\nthe constant value 0 (can't be written)\n1\nassembler temporary\n2-3\nvalues for results and expression evaluation\n4-7\nfunction arguments\n8-15\nunsaved temporaries\n16-23\nsaved temporaries (like program variables)\n24-25\nmore unsaved temporaries\n26-27\nreserved for OS kernel\n28\nglobal pointer\n29\nstack pointer\n30\nframe pointer\n31\nreturn address\n10\n\n\fThe MIPS ISA - Memory\n● MIPS is a RISC (reduced instruction set computer) architecture\n● It is also a load-store architecture\no All memory accesses performed by load and store instructions\n● Memory is a giant array of 232 bytes\nAddr\n\nData\n\nAddr\n\nData\n\nAddr\n\nData\n\n0\n\n0x3F\n\n0\n\n0x3F00\n\n0\n\n0x3F002A08\n\n1\n\n0x00\n\n2\n\n0x2A08\n\n4\n\n0x47F426B9\n\n2\n\n0x2A\n\n4\n\n0x47F4\n\n...\n\n...\n\n3\n\n0x08\n\n6\n\n0x26B9\n\n4\n\n0x47\n\n...\n\n...\n\n5\n\n0xF4\n\n6\n\n0x26\n\n7\n\n0xB9\n\n...\n\n...\n\n• The same memory viewed as bytes, 16-bit\nhalfwords, and 32-bit words (using big-endian)\n• All addresses are aligned (multiples of data size)\n11\n\n\fThe MIPS ISA - Memory\n\n• Loads move data from\n\nmemory into the registers.\n\nlw $t0, 8($s4)\n\n0x0000CAFE\n0x0000BEEF\n\ns4\n\n0x00000004\n\nRegisters\n\nregisters into memory.\n\nsw $t0, 12($s4)\n\nThis is the address, and it\nmeans \"the value of $s4 + 8.\"\n\nt0\n\n• Stores move data from the\n$t0 is the SOURCE!\n\n0\n\n0x3F002A08\n\n4\n\n0x47F426B9\n\n8\n\n0x00000000\n\n$s4 + 8\n\n12\n\n0x0000BEEF\n\n$s4 + 12\n\n16\n\n0x0000DEAD\n0x0000BEEF\n\n...\n\n...\n\nlw\n\nsw\n\nMemory\n12\n\n\fThe MIPS ISA – Flow control\n● Jump and branch instructions change the flow of execution.\n\n_top:\n# ....\n# lots o' code\n# ....\nj _top\n\nli\n$s0, 10\n_loop:\n# ....\naddi $s0, $s0, -1\nbne $s0, $zero, _loop\njr\n$ra\n\nj : jumps unconditionally\n• jumps to _top\n\nbne : jumps conditionally\nIf $s0 != $zero, jumps to _loop\nIf $s0 == $zero, continues to jr\n\n$ra\n\n13\n\n\fPhases of instruction execution\n● In most architectures, there are five phases:\n1. IF (Instruction Fetch) – get next instruction from memory\n2. ID (Instruction Decode) – figure out what instruction it is\n3. EX (Execute – ALU) – do any arithmetic\n4. MEM (Memory) – read or write data from\/to memory\n5. WB (Register Writeback) – write any results to the registers\n\n● Sometimes these phases are chopped into smaller stages\n\n14\n\n\fA simple single-cycle implementation\n\nInstruction\nMemory\n\nControl\n\nPC\n\nRegister\nFile\n\nIF\n\nID\n\nWB\n\nALU\n\nEX\n\nData\nMemory\n\nMEM\n\n● An instruction goes through IF\/ID\/EX\/MEM\/WB in one cycle\n\n15\n\n\f\"Minimal MIPS\"\n\n16\n\n\fIt’s a “subset” of MIPS\n● For pedagogical (teaching) purposes\n\n● Contains only a minimal number of instructions:\no lw, sw, add, sub, and, or, slt, beq, and j\no Other instructions in MIPS are variations on these anyway\n● Let's review the Minimal MIPS CPU focusing on the control signals\no Again, these control signals are decoded from the instruction\n\n17\n\n\fThe Minimal MIPS single-cycle CPU\n● A more detailed view of the 5-phase implementation\n\nInstruction\nMemory\n\nIns. Decoder\n\nPC\n\nPCSrc\n\n+\n\n4\n\n+\n\nimm field\n\ndst\nsrc1\nsrc2\n\nRegDataSrc\n\nData\nMemory\n\nRegister\nFile\n\nRegWrite\n\nimm field\n\nsxt\n\nMemWrite\n\nALUSrc\n\nALUOp\n\n18\n\n\fControl signals\n● Registers\no RegDataSrc: controls source of a register write (ALU \/ memory)\no RegWrite: enables a write to the register file\no src1, src2, dst: the register number for each respective operand\n● ALU\no ALUSrc: whether second operand of ALU is a register \/ immediate\no ALUOp: controls what the ALU will do (add, sub, and, or etc)\n● Memory\no MemWrite: enables a write to data memory\n● PC\no PCSrc: controls source of next PC (PC + 4 \/ PC + 4 + imm)\n→ All these signals are decoded from the instruction!\n19\n\n\fHow an add\/sub\/and\/or\/slt work\n\nInstruction\nMemory\n\nIns. Decoder\n\nPC\n\nPCSrc\n\n+\n\n4\n\nadd t0, t3, s0\n\n+\n\nimm field\n\nnext instruction\nt0 dst\nt3 src1\ns0 src2\nRegister\n\nData\nMemory\nMemWrite\n\nFile\n\nRegDataSrc\nfrom ALU\n\nRegWrite\nenable\n\nimm field\n\nsxt\n\ndisable\n\nALUSrc\n\nfrom reg\n\nALUOp\nadd\n\n20\n\n\fHow an lw works\n\nInstruction\nMemory\n\nIns. Decoder\n\nPC\n\nPCSrc\n\n+\n\n4\n\nlw s4, 12(s0)\n\n+\n\nimm field\n\nnext instruction\ns4 dst\ns0src1\nx src2\nRegister\n\nData\nMemory\nMemWrite\n\nFile\n\nRegWrite\nRegDataSrc\nenable\nfrom Mem\n12\nsxt\nimm field\n\ndisable\n\nALUSrc\n\nfrom imm\n\nALUOp\nadd\n\n21\n\n\fHow an sw works\n\nInstruction\nMemory\n\nIns. Decoder\n\nPC\n\nPCSrc\n\n+\n\n4\n\nsw t3, 8(sp)\n\n+\n\nimm field\n\nnext instruction\nx dst\nspsrc1\nt3 src2\nRegister\n\nData\nMemory\nMemWrite\n\nFile\n\nRegWrite\nRegDataSrc\ndisable\nx\n8\nsxt\nimm field\n\nenable\n\nALUSrc\n\nfrom imm\n\nALUOp\nadd\n\n22\n\n\fWhat about beq?\n● Compares numbers by subtracting and see if result is 0\no If result is 0, we set PCSrc to use the branch target.\no Otherwise, we set PCSrc to PC + 4.\nisBEQ\nisZero\n• Instruction decoder outputs isBEQ\n• 1: When instruction is beq\n• 0: When instruction not beq\n\nALU\n\nPCSrc\n\n• When PCSrc is 1,\nPC = PC + 4+ imm\n(relative branch target)\n• When PCSrc is 0,\nPC = PC + 4\n\n23\n\n\fHow a beq works\n\nbeq t0, t1, loop\n\nInstruction\nMemory\n\nIns. Decoder\n\nPC\n\nPCSrc\n\n+\n\n4\n\nTake green PC path when t0 == t1\nTake red PC path when t0 != t1\n\n+\n\nimm field\nloop\n\nx dst\nt0 src1\nt1 src2\n\nRegDataSrc\nx\n\nData\nMemory\n\nRegister\nFile\n\nRegWrite\ndisable\n\nimm field\n\nsxt\n\nMemWrite\n\ndisable\n\nALUSrc\n\nfrom reg\n\nALUOp\nsub\n\n24\n\n\fWhat about j?\n● We have to add another input to the PCSrc mux.\n\nj\n\ntop\n\nPC+4\nPC+4+imm\njump target\n(now 2 bits)\n\nPCSrc\n25\n\n\fA Single-cycle Implementation is not Optimal\n\nInstruction\nMemory\n\nControl\n\nPC\n\nRegister\nFile\n\nIF\n\nID\n\nWB\n\nALU\n\nEX\n\nData\nMemory\n\nMEM\n\n● Why? Since the longest critical path must be chosen for cycle time\no And there is a wide variation among different instructions\n26\n\n\fA Single-cycle Implementation is not Optimal\n● In our CPU, the lw instruction has the longest critical path\no Must go through all 5 stages: IF\/ID\/EX\/MEM\/WB\no Whereas add goes through just 4 stages: IF\/ID\/EX\/WB\n● If each phase takes 1 ns each, cycle time must be 5 ns:\no Because it needs to be able to handle lw, which takes 5 ns\no add also takes 5 ns when it could have been done in 4 ns\n\nQ) If lw is 1% and add is 99% of instruction mix,\nwhat is the average instruction execution time?\nA) Still 5 ns! Even if lw is only 1% of instructions!\n\n27\n\n\fA Multi-cycle Implementation\n● It takes one cycle for each phase through the use of internal latches\n\nMemory\n\nID\nIns. Decoder\n\nIF\n\nRegister\nFile\n\nEX\n\nALU\n\nMEM\n\nMemory\n\nWB\n28\n\n\fA Multi-cycle Implementation is Faster!\n● Now each instruction takes different number of cycles to complete\no lw takes 5 cycles: IF\/ID\/EX\/MEM\/WB\no add takes 4 cycles: IF\/ID\/EX\/WB\n● If each phase takes 1 ns as before:\no lw takes 5 ns and add takes 4 ns\nQ) If lw is 1% and add is 99% of instruction mix,\nwhat is the average instruction execution time?\nA) 0.01 * 5 ns + 0.99 * 4 ns = 4.01 ns (25% faster than single cycle)\n* Caveat: delay due to the added latches not shown, but net win\n29\n\n\fAnd we can do even better!\n● Did you notice?\no When an instruction is on a particular phase (e.g. IF) …\no … other phases (ID\/EX\/MEM\/WB) are not doing any work!\n● Our CPU is getting chronically underutilized!\no If CPU is a factory, 80% (4\/5) of the workers are idling!\n● Car factories create an assembly line to solve this problem\no No need to wait until a car is finished before starting on next one\no Our CPU is going to use a pipeline (similar concept)\n\n30\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":235,"segment": "unlabeled", "course": "cs1550", "lec": "lec10", "text":"Introduction to Operating Systems\nCS\/COE 1550\nSpring 2022\n\nSherif Khattab\nksm73@pitt.edu\n(Some slides are from Silberschatz, Galvin and Gagne ©2013)\n\n\fAnnouncements\n• Upcoming deadlines:\n• Project 1: due on 2\/18\n• Homework 5: due 2\/21\n• Lab 2: due on 2\/28\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fPrevious lecture …\n• Dining philosophers\n\n• Deadlock prevention\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fMuddiest Points\n• Checked on Tophat\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fQuestions of the Day\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fBanker’s Algorithm\n\nWe can use the same algorithm for both detecting and\navoiding deadlocks\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\nAvail\n\n2 3 0 1\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent 2 3 0 1\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n2 3 0 1\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n3 3 1 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n3 3 1 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n3 3 1 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n3 3 1 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n3 6 1 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n3 6 1 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n5 8 4 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n5 8 4 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n5 10 5 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fDeadlock detection algorithm\n\nA B C D\ncurrent\n\n5 10 5 2\n\nHold\n\nProcess A B C D\n1\n\n0 3\n\n0\n\n0\n\n2\n\n1 0\n\n1\n\n1\n\n3\n\n0 2\n\n1\n\n0\n\n4\n\n2 2\n\n3\n\n0\n\nWant\n\nProcess A B C D\n1\n\n3 2\n\n1\n\n0\n\n2\n\n2 2\n\n0\n\n0\n\n3\n\n3 5\n\n3\n\n1\n\n4\n\n0 4\n\n1\n\n1\n\ncurrent=avail;\nfor (j = 0; j < N; j++) {\nfor (k=0; k<N; k++) {\nif (finished[k])\ncontinue;\nif (want[k] <= current) {\nfinished[k] = 1;\ncurrent += hold[k];\nbreak;\n}\n}\nif (k==N) {\nprintf “Deadlock!\\n”;\n\/\/ finished[k]==0 means process is in\n\/\/ the deadlock\nbreak;\n}\n}\n\nNote: want[j], hold[j], current, avail are arrays!\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fBanker’s Algorithm Insights\n• It is possible that some event sequences lead a\ndeadlock\n• What we are looking for is at least one event\nsequence that can make all processes finish\n• If such sequence exists, the state is safe\n\n• The Banker’s algorithm finds such sequence if it\nexists\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fUsing the Banker’s Algorithm for Deadlock Avoidance\n• Call the algorithm on the following ``What-if” state\ninstead of the current state\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fProof sketch for Deadlock Prevention\n\n• If resources are ordered and resource requests within\neach process follow the resource ordering, the\nresource allocation graph will have no downward\narrows.\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fProblem of the Day: Sleepy Barbers\n• We have two sets of processes\n•\n•\n\nWorker processes (e.g., barbers)\nCustomer processes\n\n• Customer processes may arrive at anytime\n• Worker processes check in when they are not serving any\ncustomers\n• Each worker process must wait until it gets matched with a\ncustomer process\n• Each customer process must wait until it gets matched with a\nworker process\n• The customer process cannot leave until the matched worker\nprocess finishes the work\n• The worker process cannot check in for the next customer until\nthe matched customer process leaves\n• Many applications in the real-world\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fRendezvous Pattern\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSolution Using Semaphores: Take 1\n• One pair of semaphores per rendezvous\n• RV1a and RV1b\n• RV2a and RV2b\n\n• Notice the flipped order of the down and up calls in\nthe two processes\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\fSolution Using Semaphores: Take 1\n• This solution doesn’t work for multiple workers and\nmultiple customers\n• A customer can leave before its associated worker\nfinishes\n\nCS 1550 – Operating Systems – Sherif Khattab\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":236,"segment": "unlabeled", "course": "cs1567", "lec": "lec02_introduction_ros", "text":"Introduction to\nRobot Operating System (ROS)\nThumrongsak Kosiyatrakul\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fRobot Operating System (ROS)\nROS is not an operating system. It is a meta-Operating\nSystem\nROS is running on top of another operating system (Linux)\n\nContains a collection of software, packaging, and building\ntools\nSuitable for distributed inter-process\/inter-machine\ncommunication\nCan be used across networks (TCP\/IP)\n\nProvides tools for data analysis and debugging during run-time\nAllows debugging under a distributed environment\n\nSupport various languages such as C++ and Python\nWe are going to use Python in this class\nFor CS, it should take about 5 hours to learn enough Python\nfor this class\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fRobot Operating System (ROS)\n\nROS is not an actual OS but has the same functionalities as\nan OS\nSupply interprocess communication\nSignal\/interrupt to processes\n\nAll major functionalities are broken up into a number of nodes\nEach node runs as a separate process\nNodes communicate with each other using messages\n\nMatchmaking between nodes is done by the ROS Master\n(roscore)\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fRobot Operating System (ROS)\n\nAt the lowest level (main):\nGeneral tools for distributed computing\nPackaging\nBuilding tools\nAPI\nLanguage binding\n\nThis part is maintained by Willow Garage, Inc and some other\ndeveloper\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fRobot Operating System (ROS)\n\nAt higher level (universe):\nProvides algorithms, hardware abstractions, applications, etc\nApplication Level: playing football\nCapability Level: Navigation\nLibraries: OpenCV, Drivers, etc\n\nDeveloped and maintained by ROS community\nThe main web site www.ros.org\nQuestions\/Answers Community Support\n\nThumrongsak Kosiyatrakul\n\nanswers.ros.org\n\nIntroduction to Robot Operating System (ROS)\n\n\fROS Core\n\nROS Master\nA centralized Remote Procedure Call (RPC) server\nNegotiates communication connections between nodes\nRegisters names\n\nrosout: network standard output\nCan show output of nodes running on different machines over\nnetworks\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fROS Communication Protocol\n\nROS Topics\nAsynchronous communication (stream)\nStrongly Typed\nHelp hiding underly byte stream\nThink about struct in C\n\nCan have one or more publishers in the same topic\nCan have one or more subscribers in the same topic\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fHow ROS Works?\n\ncmvision\nnode\n\ncontrol\nnode\n\n2\n\n3\nROS\nMaster\n\n1\n\n4\n\ncamera\nnode\n\nkobuki\nnode\n\nCamera\n\nComputer\nUSB\n\nThumrongsak Kosiyatrakul\n\nKobuki\nUSB\n\nIntroduction to Robot Operating System (ROS)\n\n\fHow ROS Works?\n\ncamera node\nPerforms hardware abstraction to support all kinds of USB\ncameras\nTurn the data from camera into a standard image format\n\nTells the ROS Master\nI wants to publish images on topic image\n\ncmvision node\nPerforms objects detection based on blob of colors\nTurn an image into an array of coordinate of detected objects\n\nTells the ROS Master\nI want to subscribe to receive images from the topic image\nI also want to publish blobs on topic blobs\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fHow ROS Works?\n\ncontrol node\nMake decisions to turn left\/right based on an array of\ncoordinates\nTells the ROS Master\nI want to subscribe to receive blobs from the topic blobs\nI also want to publish velocities on topic velocity\n\nkobuki node\nPerforms hardware abstractions\nMotor controller, motor feedback, sensors\n\nTells the ROS Master\nI want to subscribe to receive velocities from the topic velocity\n\nNote: the order does not matter\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fHow ROS Works?\n\ncmvision\nnode\n\ncontrol\nnode\n\nROS\nMaster\n\ncamera\nnode\n\nkobuki\nnode\n\nCamera\n\nComputer\nUSB\n\nKobuki\nUSB\n\nROS Master sets up communication among nodes based on their\nrequests.\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fROS Nodes\n\nA node is a process that performs some computations.\nWe typically divide the entire software functionality into\ndifferent modules\nSingle Node\nColor image to array of coordinate in one program\n\nMultiple Nodes:\nColor image to black and white\nBlack and white to coordinate\n\nNodes are combined together into a graph and communicate\nwith one another using streaming topics\nThese nodes are meant to operate at a fine-grained scale\nA robot control system usually comprise of many nodes.\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fROS Node Examples\ngscam node:\nCommunicates with integrated web camera at the hardware\nlevel\nCaptures images and publishes on topic image\nNote that a video is simply a series of images\n\ncmvision node:\nSubscribes to the topic image\nTries to find blobs of specific color for every image received\nPublishes locations and sizes of blobs on topic blobs\n\ncontrol node:\nSubscribes to the topic blob\nDecides what to do\nPublishes velocities to move robot\n\nkobuki node:\nSubscribes to the topic velocity\nTells kobuki robot what to do based on velocities received\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fROS Topics\n\nTopics are named buses over which nodes exchange messages\nThere can be hundreds of topics at a time\n\nTopics have anonymous publish\/subscribe semantics\nA subscriber does not care which node published the data it\nreceives\nA publisher does not care which node subscribes to the data it\npublishes.\n\nThere can be multiple publishers and subscribers to a topic\nEach topic is strongly typed by the ROS message it transports\nTransport is done using TCP or UDP\nCan be between multiple machines over networks\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\fROS Messages\nNodes communicate with each other by publishing messages\nto topics\nPublishers publish data to topics\nSubscribers consume data from topics\n\nA message is a simple data structure, comprising typed fields.\nSimilarly to data type in programming:\nstd msgs\/Int32\nstd msgs\/Float32\nstd msgs\/String\nstd msgs\/Empty\n\nThe numbers at the end indicates number of bits\nReference to standard messages (std msgs) can be found at\nhttp:\/\/wiki.ros.org\/std_msgs\n\nThumrongsak Kosiyatrakul\n\nIntroduction to Robot Operating System (ROS)\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":237,"segment": "unlabeled", "course": "cs1541", "lec": "lec3.6_simd_and_gpus","text":"SIMD and GPUs\nCS 1541\nWonsun Ahn\n\n\fSIMD Architectures\n\n2\n\n\fISA not optimized for data parallel workloads\n● This loop does multiply accumulate (MAC):\nfor (int i = 0; i < 64; i++) {\ny[i] = a * x[i] + y[i]\n}\no A common operation in digital signal processing (DAXPY)\n● Note how we apply the same MAC operation on each data item\no This is how many data parallel workloads look like\n● A conventional ISA (likes MIPS) is not optimal for encoding this\no Results in wasted work and suboptimal performance\no Let’s look at the actual MIPS translation\n\n3\n\n\fMIPS code for 𝒚 𝒊 = 𝒂 ∗ 𝒙 𝒊 + 𝒚 𝒊\nl.d\n$f0,0($sp)\n;$f0 = a\naddi $s2,$s0,512\n;64 elements (64*8=512 bytes)\nloop: l.d\n$f2,0($s0)\n;$f2 = x(i)\nmul.d $f2,$f2,$f0\n;$f2 = a * x(i)\nl.d\n$f4,0($s1)\n;$f4 = y(i)\nadd.d $f4,$f4,$f2\n;$f4 = a * x(i) + y(i)\ns.d\n$f4,0($s1)\n;y(i) = $f4\naddi $s0,$s0,8\n;increment index to x\naddi $s1,$s1,8\n;increment index to y\nsubu $t0,$s2,$s0\n;evaluate i < 64 loop condition\nbne\n$t0,$zero,loop ;loop if not done\n● Blue instructions don’t do actual computation. There for indexing and loop control.\no Is there a way to avoid? Loop unrolling yes. But that causes code bloat!\n● Red instructions do computation. But why decode them over and over again?\no Is there a way to fetch and decode once and apply to all data items?\n4\n\n\fSIMD (Single Instruction Multiple Data)\n● SIMD (Single Instruction Multiple Data)\no An architecture for applying one instruction on multiple data items\no ISA includes vector instructions for doing just that\n▪ Along with vector registers to hold multiple data items\n\n● Using MIPS vector instruction extensions:\nl.d\n$f0,0($sp)\nlv\n$v1,0($s0)\nmulvs.d $v2,$v1,$f0\nlv\n$v3,0($s1)\naddv.d $v4,$v2,$v3\nsv\n$v4,0($s1)\n\n;$f0 = scalar a\n;$v1 = vector x (64 values)\n;$v2 = a * vector x\n;$v3 = vector y (64 values)\n;$v4 = a * vector x + vector y\n;vector y = $v4\n\no Note: no indexing and loop control overhead\no Note: each instruction is fetched and decoded only once\n\n5\n\n\fSIMD Processor Design\n● How would you design a processor for the vector instructions?\ndata\n\n1. One processing element (PE)\no Fetch and decode instruction once\no PE applies op on each data item\n▪ Item may be in vector register\nprogram\n▪ Item may be in data memory\n2. Multiple PEs in parallel\nprogram\no Fetch and decode instruction once\no PEs apply op in parallel\n▪ In synchronous lockstep\nPE\n→ The more PEs, the faster!\ndata\n\ncontrol\n\nPE\n\ncontrol\n\nPE\n\nPE\n\nPE\n\ndata\n\ndata\n\ndata\n6\n\n\fExample: Adding Two Vectors\n● Instead of having a single FP adder work on each item (a)\n● Have four FP adders work on items in parallel (b)\n● Each pipelined FP unit is in charge of pre-designated items in vector\no For full parallelization, put as many FP units as there are items\n\n7\n\n\fVector Load-Store Unit\n● Striding lets you load\/store non-contiguous data from memory at\nregular offsets. (e.g. the first member of each struct in an array)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0\n\n4\n\n8\n\nC\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\n● Gather-scatter lets you put pointers in a vector, then load\/store\nfrom arbitrary memory addresses. (gather = load, scatter = store)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n2\n\nE\n\n7\n\n4\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n8\n\n\fHow does Gather-Scatter work?\n\n9\n\n\fWhen does Gather-Scatter make sense?\n● Often data for scientific or AI applications are sparse.\no Time-sampled points where a sensor measurement changes\no In social networking, connections in a N x N friendship matrix\no In neural networks, weights in a filter layer that are non-zero\n● To save memory, sparse data is stored in sparse format:\n\nHow a sparse filter layer is stored in a\nConvolutional Neural Network (CNN)\n\n10\n\n\fWhen does Gather-Scatter make sense?\n● Convolution works by applying filter like a shifting window:\nrow column weight\n\nImage\n\nConvolved Feature\n\n0\n\n0\n\n1\n\n0 1 2 3\n\n0\n\n2\n\n1\n\n1 1 0 1\n\n1\n\n1\n\n1\n\n2 0 1 0\n\n2\n\n0\n\n1\n\n3 1 0 1\n\n2\n\n2\n\n1\n\nFilter (sparse matrix format)\n\n● When applying filter on image, a gather needs to take place\n1. Gather values in image in corresponding rows and columns\n2. Create an image vector out of those gathered values\n3. Do a dot (●) product between image vector and filter vector\n11\n\n\fSIMD instructions in real processors\n● x86 vector extensions\no Historically: MMX, SSE, AVX, AVX-2\no Current: AVX-512 (512-bit vector instructions)\n● ARM vector extensions\no Historically: VFP (Vector Floating Point)\no Current: Neon (128-bit vector instructions)\n● Vector instructions have progressively become wider historically\no Due to increase of data parallel applications\no Due to their power efficiency\n▪ Compared to fetching, decoding, scheduling multiple instructions\n▪ Good way to increase FLOPS while staying within TDP limit\n● Enter GPUs for general computing (circa 2001)\n13\n\n\fGPUs:\nGraphical Processing Units\n\n14\n\n\fHistory of GPUs\n• VGA (Video graphic array) has been around since the early 90’s\n• A display generator connected to some (video) RAM\n• By 2000, VGA controllers were handling almost all graphics computation\n• Programmable through OpenGL, Direct 3D API\n• APIs allowed accelerated vertex\/pixel processing:\n• Shading\n• Texture mapping\n• Rasterization\n• Gained moniker Graphical Processing Unit\n• 2007: First general purpose use of GPUs\n• 2007: Release of CUDA language\n• 2011: Release of OpenCL language\n15\n\n\fGPU is Really a SIMD Processor\nGPU\n\nStreaming Multi-processor (SM)\n\nStreaming\nProcessor (SP)\n\nIF\/ID\n\nIF\/ID\n\nIF\/ID\n\nL1 cache\n\nL1 cache\n\nL1 cache\n\nShared\nmemory\n\nShared\nmemory\n\nShared\nmemory\n\nCPU\nL2 cache\n\nCPU (Host)\nmemory\n\nL2 cache\nPCIe bus\n\nGlobal (GPU) memory\n\n● Logically, a GPU is composed of SMs (Streaming Multi-processors)\no An SM is a vector unit that can process multiple pixels (or data items)\n● Each SM is composed of SPs which work on each pixel or data item\n16\n\n\fCPU-GPU architecture\nGPU\n\nStreaming Multi-processor (SM)\n\nStreaming\nProcessor (SP)\n\nIF\/ID\n\nIF\/ID\n\nIF\/ID\n\nL1 cache\n\nL1 cache\n\nL1 cache\n\nShared\nmemory\n\nShared\nmemory\n\nShared\nmemory\n\nCPU\nL2 cache\n\nCPU (Host)\nmemory\n\nL2 cache\nPCIe bus\n\nGlobal (GPU) memory\n\n● Dedicated GPU memory separate from system memory\n● Code and data must be transferred to GPU memory for it to work on it\no Through PCI-Express bus connecting GPU to CPU\n17\n\n\fModern GPU architecture\nSM=streaming\nmultiprocessor\n\nTPC = Texture\nProcessing\nTPC = texture Cluster\nprocessing cluster\n\nROP = raster operations pipeline\n\nSFU = special\nfunction unit\n\n\fGPU Programming Model\nCopy data from CPU\nmemory to GPU memory\n\nCPU\n\nIF\/ID\n\nCPU\nmemory\n\nLaunch the kernel\n\nCPU\n\nCPU\n\nCPU\nmemory\n\nIF\/ID\n\nGlobal (GPU) memory\n\nIF\/ID\n\nCPU\nmemory\n\nCopy data from GPU\nmemory to CPU memory\n\nIF\/ID\n\nIF\/ID\n\nIF\/ID\n\nGlobal (GPU) memory\n\nIF\/ID\n\nIF\/ID\n\nIF\/ID\n\nGlobal (GPU) memory\n19\n\n\fGPU Programming Model\nCPU program\n(serial code)\n\ncudaMemcpy ( … )\n\nCopy data from CPU\nmemory to GPU memory\n\nFunction <<<nb,nt >>>Launch kernel on GPU\n\ncudaMemcpy ( … )\n_global_ Function ( … )\n\nCopy results from GPU\nmemory to CPU memory\nImplementation of GPU kernel\nkernel: Function executed on the GPU\n\n20\n\n\fGPU Programming Model: Copying Data\n\/* malloc in GPU global memory *\/\ncudaMalloc (void **pointer, size_t nbytes);\n\/* free malloced GPU global memory *\/\ncudaFree(void **pointer) ;\n\/* initialize GPU global memory with value *\/\ncudaMemset (void *pointer, int value, size_t count);\n\/* copy to and from between CPU and GPU memory *\/\ncudaMemcpy(void *dest, void *src, size_t nbytes, enum cudaMemcopyKind dir);\nenum cudaMemcpyKind\n• cudaMemcpyHostToDevice\n• cudaMemcpyDeviceToHost\n• cudaMemcpyDeviceToDevice\n\nCPU\nmemory\n\nPCIe bus\n\nGPU Global\nmemory\n\n21\n\n\fExample: Copying array a to array b using the GPU\n\n22\n\n\fGPU Programming Model: Launching the Kernel\nCPU program\n(serial code)\n\ncudaMemcpy ( … )\n\nCopy data from CPU\nmemory to GPU memory\n\nFunction <<<nb,nt >>>Launch a kernel with nb\nblocks, each with nt threads\n\ncudaMemcpy ( … )\n_global_ Function ( … )\n\nCopy results from GPU\nmemory to CPU memory\nImplementation of kernel\n(the function run by each GPU thread)\n\n23\n\n\fThe Execution Model\nIF\/ID\n\nL1 cache\n\nShared\nmemory\n\n• The thread blocks are dispatched to SMs\n• The number of blocks dispatched to an\nSM depends on the SM’s resources\n(registers, shared memory, …).\nBlocks not dispatched initially are dispatched\nwhen an SM frees up after finishing a block\n• When a block is dispatched to an\nSM, each of its threads executes on\nan SP in the SM.\n\nIF\/ID\n\nL1 cache\nShared\nmemory\n24\n\n\fA thread block is executed in warps\n● Each block (up to 1K threads) is divided into groups of 32\nthreads (called warps) – empty threads are used as fillers.\n● A warp executes as a SIMD vector instruction on the SM.\n● Depending on the number of SPs per SM:\n0 1\n\n30 31\n\no If 32 SP per SM → 1 thread of a warp executes on 1 SP\n(32 lanes of execution, one thread per lane)\n0 1\n\n30 31\n\n01\n\n30 31\n\no If 16 SP per SM → 2 threads are time multiplexed on 1 SP (16\nlanes of execution, 2 threads per lane)\n0\n\n15\n\n0123\n\n31\n\n0\n\no If 8 SP per SM → 4 threads are time multiplexed on 1 SP\n(8 lanes of execution, 4 threads per lane)\n\n7\n25\n\n\fA SM is composed of one or more warp schedulers\n● In this SM, there are 4 warp schedulers.\n\n● Warps in thread block are divided\nstatically among warp schedulers.\n● E.g. for 4 schedulers:\no Scheduler 1: Warp 0, Warp 4, …\no Scheduler 2: Warp 1, Warp 5, …\no Scheduler 3: Warp 2, Warp 6, …\no Scheduler 4: Warp 3, Warp 7, …\n● Round robin assignment to ensure\nequal distribution of warps\n26\n\n\fWarp scheduling enables latency hiding\n● Warp scheduler can hide bubbles (just like a superscalar scheduler)\no But without an instruction queue and out-of-order execution\n● How?\no In-order execution.\no Switch to different warp\nwhen a bubble is\nabout to form.\n● Warp can come from any\nthread block in SM\no More thread blocks can\nlead to higher utilization.\n27\n\n\fAll threads execute the same code\n● Launched using Kernel <<<1, 64>>> : 1 block with 64 threads\nthreadIdx.x 0\n\n1\n\n2\n\n3\n\n60\n\n61\n\n62\n\n63\n\nint i = threadIdx.x;\nB[i] = A[63-i];\nC[i] = B[i] + A[i]\n\nA[0,…,63]\nB[0,…,63]\nC[0,…,63]\nGPU memory\n\n● Each thread in a thread block has a unique “thread index” → threadIdx.x\n● The same sequence of instructions can apply to different data items.\n28\n\n\fBlocks of Threads\n● Launched using Kernel <<<2, 32>>> : 2 blocks of 32 threads\nblockIdx.x = 1\n\nblockIdx.x = 0\n\nthreadIdx.x 0\n\n1\n\n30\n\n31\n\n0\n\n1\n\n30\n\n31\n\nint i = 32 * blockIdx.x + threadIdx.x;\nB[i] = A[63-i];\nC[i] = B[i] + A[i]\n\nA[0,…,63]\nB[0,…,63]\nC[0,…,63]\nGPU memory\n\n● Each thread block has a unique “block index” → blockIdx.x\n● Each thread has a unique threadIdx.x within its own block\n● Can compute a global index from the blockIdx.x and threadIdx.x\n29\n\n\fTwo-dimensions grids and blocks\n● Launched using Kernel <<<(2, 2), (4, 8)>>> : 2X2 blocks of 4X8 threads\nblockIdx.x = 0 blockIdx.x = 1\nblockIdx.y = 0 blockIdx.y = 0\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\nx\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\ny\n\nblockIdx.x = 0 blockIdx.x = 1\nblockIdx.y = 1 blockIdx.y = 1\n\n● Each block has two indices (blockIdx.x, blockIdx.y)\n● Each thread in a thread block has two indices (threadIdx.x, threadIdx.y)\n30\n\n\fExample: Computing the global index\nthreadIdx.x\nvoid main ()\n{ cudaMalloc (int* &a, 20*sizeof(int));\nblockIdx.x\nblockIdx.x\nblockIdx.x\nblockIdx.x\ncudaMalloc (int* &b, 20*sizeof(int));\n=0\n=1\n=2\n=3\ncudaMalloc (int* &c, 20*sizeof(int));\n0 1 2 3 4\n0 1 2 3 4\n0 1 2 3 4\n0 1 2 3 4\n…\nkernel<<<4,5>>(a, b, c) ;\n…\n}\nNOTE: Each block will consist of one warp –\n_global_ void kernel(int *a, *b, *c)\n{ int i = blockIdx.x * blockDim.x + threadIdx.x ; only 5 threads in warp will do useful work.\n(Other 27 threads will execute no-ops.)\na[i] = i ;\nb[i] = blockIdx.x;\nc[i] = threadIdx.x;\n}\n\na[]\nGlobal\nMemory b[]\nc[]\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9 10 11 12 13 14 15 16 17 18 19\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n3\n\n3\n\n3\n\n3\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n31\n\n\fExample: Computing 𝒚 𝒊 = 𝒂 ∗ 𝒙 𝒊 + 𝒚 𝒊\nC program (on CPU)\n\nCUDA program (on CPU+GPU)\n\nvoid saxpy_serial(int n, float a, float\n*x, float *y)\n{\nfor(int i = 0; i<n; i++)\ny[i] = a * x[i] + y[i];\n}\n\n_global_ void saxpy_gpu(int n, float a, float *x,\nfloat *y)\n{\nint i = blockIdx.x*blockDim.x +\nthreadIdx.x;\nif (i < n ) y[i] = a * x[i] + y[i];\n}\n\nvoid main ()\n{\n…\nsaxpy_serial(n, 2.0, x, y);\n…\n}\n\nvoid main ()\n{…\n\/\/ cudaMalloc arrays X and Y\n\/\/ cudaMemcpy data to X and Y\nint NB = (n + 255) \/ 256;\nsaxpy_gpu<<<NB, 256>>>(n, 2.0, X, Y);\n\/\/ cudaMemcpy data from Y\n}\n32\n\n\fExample: Computing 𝒚 𝒊 = 𝒂 ∗ 𝒙 𝒊 + 𝒚 𝒊\n● What happens when n = 1?\n_global_void saxpy_gpu(int n, float a, float *X, float *Y)\n{\nint i = blockIdx.x*blockDim.x + threadIdx.x;\nif (i < n ) Y[i] = a * X[i] + Y[i];\n}\n…..\nsaxpy_gpu<<<1, 256>>>(1, 2.0, X, Y); \/* X and Y are both sized 1! *\/\n\n● “if (i < n)” condition prevents writing beyond bounds of array.\n● But that requires some threads within a warp not performing the write.\no But a warp is a single vector instruction. How can you branch?\no “if (i < n)” creates a predicate “mask” vector to use for the write\no Only thread 0 has predicate turned on, rest has predicate turned off\n33\n\n\fGPUs Use Predication for Branches\n\n● Each thread computes own predicate for condition threadIdx.x < 4\n● Taken together, 32 threads of a warp create a 32-bit predicate mask\n● Mask is applied to warps for A, B, X, and Y.\n● Just like for VLIW processors, this can lead to low utilization.\n34\n\n\fGPU Performance\n\n35\n\n\fLesson 1:\nParallelism is Important\n\n36\n\n\fThread Level Parallelism\n● Superscalars and VLIWs are useful only if…\no Program exhibits ILP (Instruction Level Parallelism) in the code\n● GPUs are useful only if…\no Program has TLP (Thread Level Parallelism) in the code\no TLP can be expressed as the number of threads in the code\n● How that TLP is laid out in the kernel is also important\no How many threads are in a thread block\n▪ If less than threads in warp, some SPs may get unused\no How many thread blocks are in the grid\n▪ If less than number of SMs, some SMs may get unused\n→ If not careful, your GPU may get underutilized\n37\n\n\fExample: Kernels with Bad Layout\n● Suppose there are 4 SMs in GPU with 32 SPs in each SM.\no Case 1, 2 below have enough TLP (1024 threads) but bad layout.\no Utilized threads are marked in red. Rest are unused.\n● Case 1: Not enough threads\nkernel<<<1024, 1>>(…) ;\n● Case 2: Not enough blocks\nkernel<<<1, 1024>>(…) ;\n● Balanced threads and blocks\nkernel<<<32, 32>>(…) ;\nkernel<<<16, 64>>(…) ;\nkernel<<<4, 256>>(…) ;\n\nSM 0\n\nSM 1\n\nSM 2\n\nSM 3\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n0 1 2 … 31\n\nSM 0\n\nSM 1\n\nSM 2\n\nSM 3\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n0 1 2 … 31\n\nSM 0\n\nSM 1\n\nSM 2\n\nSM 3\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n0 1 2 … 31\n\n38\n\n\fLesson 2:\nBandwidth is Important\n\n39\n\n\fExample: Computing 𝒚 𝒊 = 𝑨 𝒊, 𝒋 ∗ 𝒙 𝒋\nC program (on CPU)\nvoid mv_cpu(float* y, float* A,\nfloat* x, int n) {\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\ny[i] += A[i*n + j] * x[j];\n}\n\nvoid main ()\n{\n…\nmv_cpu(y, A, x, n);\n…\n}\n\nCUDA program (on CPU+GPU)\nvoid mv_gpu(float* y, float* A, float* x, int n) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\nif (i < n) {\nfor (int j = 0; j < n; j++)\ny[i] += A[i*n + j] * x[j];\n}\n}\nvoid main ()\n{\n…\nint nblocks = (n + block_size - 1) \/ block_size;\nmv_gpu <<<nblocks, block_size>>> (y, A, x, n);\n…\n}\n40\n\n\fPerformance Results for 𝒚 𝒊 = 𝑨 𝒊, 𝒋 ∗ 𝒙 𝒋\n● Guess what? CPU is faster than GPU!\n\nBut even comparing pure compute\ntime, CPU is still faster than GPU.\nWhat the…?\n\n* Run on netlab-1.cs.pitt.edu with n=8192:\n- Intel Core i7-4770 CPU\n- NVIDIA GF119-825-A1 GPU\n\nA lot of time is spent copying back and\nforth between CPU and GPU memories.\n\n41\n\n\fPerformance Results for 𝒚 𝒊 = 𝑨 𝒊, 𝒋 ∗ 𝒙 𝒋\n● Was it because the GPU was wimpy and can’t do enough FLOPS?\n\n● NVIDIA GF119-825-A1 is a Fermi GPU Capability 2.1\no Clock rate: 1046 MHz (X 2 for warp execution)\no Number of SMs: 1\no Number of SPs per SM: 48\no Max FLOPS = 1046 MHz * 2 * 1 * 48 = 100.4 GFLOPS\n● What was the FLOPS achieved?\no y[i] += A[i*n + j] * x[j] = 2 FP ops each iteration for n * n iterations\no n = 8192, so FP ops = 8192 * 8192 * 2 = 134 M\no Time = 0.27 seconds (shortest at 32 thread block size)\no FLOPS = 134 M \/ 0.27 = 496 MFLOPS\no Not even close to the limit!\n42\n\n\fPerformance Results for 𝒚 𝒊 = 𝑨 𝒊, 𝒋 ∗ 𝒙 𝒋\n● Could it be that the GPU didn’t have enough memory bandwidth?\n\n● NVIDIA GF119-825-A1 is a Fermi GPU Capability 2.1\no Memory Type: DDR3\no Memory Bandwidth: 14.00 GB\/s\n● GPUs also have Performance Monitoring Units (PMUs)\no NVIDIA Profiler (nvprof) provides an easy way to read them:\nhttps:\/\/docs.nvidia.com\/cuda\/profiler-users-guide\/index.html\no Let’s use the PMU to profile the following:\n▪ DRAM Transfer Rate (GB\/s)\n▪ L1 Hit Rate (%)\n▪ L2 Hit Rate (%)\n43\n\n\fMemory Wall Hits Again\n\n44\n\n\fMemory Wall Hits Again\n\nDRAM bandwidth not saturated:\nBenefits from more parallelism\ndue to larger thread block sizes\n\nDRAM bandwidth saturated:\nLarger thread blocks → increased working set size →\nhigher cache miss rates → worse bandwidth problem\nDRAM\nBandwidth\nLimit (14 GB\/s)\n\n45\n\n\fIs there a way we can reach max FLOPS?\n● Let’s take a look at the GPU design metrics again:\no Max FLOPS = 100.4 GFLOPS\no Memory Bandwidth: 14.00 GB\/s\n● To sustain max FLOPS, you need to do a lot of work per byte\no 100.4 GFLOPS \/ 14.00 GB\/s = 7.17 FP ops \/ byte\no Or, about 28 FP ops \/ float (4 bytes) fetched from memory\no Otherwise, the memory bandwidth cannot sustain the FLOPS\n● All GPUs have this problem with memory bandwidth:\no It’s easy to put in more SMs using transistors for Moore’s Law\no Your memory bandwidth is limited due to your DDR interface\n\n46\n\n\fArithmetic Intensity: A property of the program\n● How many FP ops \/ float for our mat-vec multiplication?\no y[i] += A[i*n + j] * x[j] each iteration with n * n iterations\no FP ops = 2 * n * n (one multiply and one add)\no Float accesses = n * n + 2n (1 matrix and 2 vector accesses)\n▪ That’s counting only cold misses but could be even more\no So approx. 2 FP ops \/ float (a far cry from 28 FP ops \/ float)\no This metric is called arithmetic intensity\n\n● Arithmetic intensity is a property of the program needed by GPUs\no Just like TLP (thread-level-parallelism) is needed by GPUs\no Matrix-vector multiplication has low intensity\n→ Fundamentally not suited for fast GPU computation\n\n47\n\n\fArithmetic Intensity: A property of the program\n\n* Courtesy of Lawrence Berkeley National Laboratory:\nhttps:\/\/crd.lbl.gov\/departments\/computer-science\/par\/research\/roofline\/introduction\/\n\n● BLAS: Basic Linear Algebra Subprograms\no BLAS 1: Vector operations only (e.g. saxpy) → Bad intensity\no BLAS 2: General Matrix-Vector Multiplication (GeMV) → Bad intensity\no BLAS 3: General Matrix Multiplication (GeMM) → Good intensity\n48\n\n\fMatrix-Matrix Multiply: Good Arithmetic Intensity\n● Matrix-multiplication:\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\nfor (int k=0; k<n; k++)\nC[i*n + j] += A[i*n + k] * B[k*n + j];\n● What’s the arithmetic intensity for this program?\no FP ops = 2 * n * n * n (one multiply and one add)\no Float accesses = 3 * n * n (3 matrix accesses)\n▪ If we only have cold misses and no capacity misses\no Arithmetic intensity = 2 * n \/ 3 = 0.66 * n = O(n)\no Implication: The larger the matrix size, the better suited for GPUs!\n▪ Important result for deep learning and other apps\n49\n\n\fExample: Computing 𝑪 𝒊, 𝒋 = 𝑨 𝒊, 𝒌 ∗ 𝑩 𝒌, 𝒋\nC program (on CPU)\nvoid mm_cpu(float* C, float* A, float* B,\nint n) {\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\nfor (int k=0; k<n; k++)\nC[i*n + j] += A[i*n + k] * B[k*n + j];\n}\n\nvoid main ()\n{\nmm_cpu(C, A, B, n);\n}\n\nCUDA program (on CPU+GPU)\nvoid mm_gpu(float* C, float* A, float* B, int n) {\nfloat Cvalue = 0;\nint i = blockIdx.y * blockDim.y + threadIdx.y;\nint j = blockIdx.x * blockDim.x + threadIdx.x;\nfor (int k = 0; k < n; ++k)\nCvalue += A[i * n + k] * B[k * n + j];\nC[i * n + j] = Cvalue;\n}\nvoid main ()\n{\ndim3 dimBlock(block_size, block_size);\ndim3 dimGrid(n \/ dimBlock.x, n \/ dimBlock.y);\nmm_gpu <<<dimGrid, dimBlock>>> (C, A, B, n);\n}\n50\n\n\fPerformance Results for 𝑪 𝒊, 𝒋 = 𝑨 𝒊, 𝒌 ∗ 𝑩 𝒌, 𝒋\n\nNow GPU is much faster\nthan CPU given enough TLP\n\nWhat’s this? This version of\nmatrix-multiply reduces\ncache capacity misses using\nshared memory in the GPU.\n\n51\n\n\fCapacity Misses can Reduce Arithmetic Intensity\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\nfor (int k=0; k<n; k++)\nC[i*n + j] += A[i*n + k] * B[k*n + j];\n\n● The ideal arithmetic intensity for this program was:\no FP ops = 2 * n * n * n (one multiply and one add)\no Float accesses = 3 * n * n (3 matrix accesses)\no Arithmetic intensity = 2 * n \/ 3 = 0.66 * n = O(n)\n● Only if we have no capacity misses. What if we did have them?\no For B[k*n + j], reuse distance is the entire matrix of B\no If B[k*n + j] always misses, memory accesses is in the order of n3!\n52\n\n\fSo what is Shared Memory?\nIF\/ID\n\nIF\/ID\n\nIF\/ID\n\nL1 cache\n\nL1 cache\n\nL1 cache\n\nShared\nmemory\n\nShared\nmemory\n\nShared\nmemory\n\nL2 cache\n\nGlobal (GPU) memory\n\n● Shared Memory: memory shared among threads in a thread block\no Variables declared with __shared__ modifier live in shared memory\no Is same as L1 cache in terms of latency and bandwidth!\no Storing frequently used data in shared memory can save on bandwidth\n53\n\n\fLoop Tiling with Shared Memory\n● Store a “tile” within matrix in shared memory while operating on it\no Can reduce accesses to DRAM memory\n● Code in: https:\/\/docs.nvidia.com\/cuda\/cuda-c-best-practicesguide\/index.html#shared-memory-in-matrix-multiplication-c-ab\n__shared__ float aTile[TILE_DIM][TILE_DIM],\nbTile[TILE_DIM][TILE_DIM];\nint row = blockIdx.y * blockDim.y + threadIdx.y;\nint col = blockIdx.x * blockDim.x + threadIdx.x;\nfloat sum = 0.0f;\naTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];\nbTile[threadIdx.y][threadIdx.x] = b[threadIdx.y*N+col];\n__syncthreads();\nfor (int i = 0; i < TILE_DIM; i++)\nsum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];\nc[row*N+col] = sum;\n● Assumption: TILE_DIM = w. What if w > TILE_DIM?\n\n54\n\n\fLoop Tiling with Shared Memory\n● TILE_DIM is limited by amount of shared memory. What if w > TILE_DIM?\no Now must load A and B tiles w \/ TILE_DIM times per thread block\n● Now code will look like:\nN\n__shared__ float aTile[TILE_DIM][TILE_DIM],\nB\nbTile[TILE_DIM][TILE_DIM];\nfloat sum = 0.0f;\nfor (int t = 0; t < w \/ TILE_DIM; t++) {\n…\naTile[threadIdx.y][threadIdx.x] = …;\nA\nC\nbTile[threadIdx.y][threadIdx.x] = …;\n__syncthreads();\nN\nfor (int i = 0; i < TILE_DIM; i++)\nsum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];\nw\n}\nc[row*N+col] = sum;\n55\n\nw\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":238,"segment": "unlabeled", "course": "cs1622", "lec": "lec12","text":"Dynamic Memory\nManagement\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● we're gonna talk about some stuff a bit abstractly today\no but I think it's a really important topic\no dynamic memory management has a H U G E impact on language\nand compiler design\no and it will also be important to know about this for when we start\ntalking about code generation and the runtime library!\n\n2\n\n\fAllocation, Deallocation,\nLifetime, and Ownership\n\n3\n\n\fMaking room\n● every value in your program takes up space in memory.\n● allocation sets aside a piece of memory as a value's \"home.\"\n\nlet g = 10;\nlet h = 20;\nfn main() {\ng = g + h;\n}\n\nglobal variables are the easiest to\nallocate, because the compiler knows\nhow many globals there are.\nit does static allocation: it gives each\nglobal a unique location, which is encoded\nin the output machine code.\n\nstatic allocation is also used for certain kinds of\nconstants, like \"quoted string literals\".\n4\n\n\fBut that's boring\n● what about locals? they \"appear\" and \"disappear\" with function calls.\n● that means we have to do dynamic allocation: we come up with the\nlocations of variables at runtime.\n\nfn r(x: int) {\nthis is easier than it sounds at\nif x > 1 {\nfirst, because every function has a\nfixed number of variables.\nreturn x + r(x-1);\n} else {\nreturn x;\nbecause of how function calls work, the\nvariables can be allocated using a stack.\n}\n}\nso, each local variable gets a statically-determined\nlocation within the stack frame, relative to the sp.\n5\n\n\fDeallocation\n● memory is a finite resource. we have to reuse it when we can.\n● deallocation marks a previously-used location as ready for reuse.\nstacks are natural recyclers.\n3\n\n3\n\n3\n\n3\n\n10\n\n10\n\n10\n\n10\n\nas we push values,\nthey are placed in\nnew locations.\n\n99\n\n99\n\n433\n\n4\n\n4\n\n4\n\n163\n\n163\n\n163\n\nwhen they're popped, the\nvalues are deallocated, but\nthe locations stick around… and can be easily reused\nby the next push.\n6\n\n\fLifetime and Ownership\n● a value's lifetime is the span of time from allocation to deallocation.\n● its owner is what decides when it's safe to deallocate it.\n\nlet g = 10;\nfn main() {\nlet x = g + 5;\nprintln_i(x);\n}\n\nglobal variables are owned by the\nprogram: they can't be deallocated\nuntil the program ends.\nlocal variables are owned by the\nenclosing function: they can be\ndeallocated when it returns.\n\n…but this isn't the whole story, is it?\n7\n\n\fIndirection\ntime check < 20 min\n\n8\n\n\fIndirection, the concept\n● indirection means using a symbol to stand in for something, instead\nof using that thing directly.\no yes, this is really abstract, but it's an abstract concept!\nlinguistic symbols – spoken or written\n– all stand in for something else.\n\n👉\nif I point at a cat, my\nfinger is the symbol\nwhich refers to the cat.\n\nif I want to refer to houses on my street, I\ncan refer to them by number.\n121\n\n123\n\n125\n\n127\n\n129\n\n9\n\n\fIt works for houses, and it works for objects\n● if I have a program which deals with very large objects (kB-MB?),\no I can put them in an array and access them by their index.\n\n64kB 64kB 64kB 64kB 64kB 64kB 64kB\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nthe number 3 is small enough to fit into a register.\nof course, memory itself is an array (of bytes), so we can refer\nto anything in memory by its index: its memory address.\n\n01\n\n00\n\n00\n\n80\n\nF8\n\nFF\n\nFF\n\n8000 8001 8002 8003 8004 8005 8006\n\n10\n\n\fPointers and references\n● programs manipulate values, like ints, strings, objects, etc.\n● a pointer is a value which is the memory address of another value.\no \"reference\" is another name for a pointer, with a \"safer\" connotation.\n\nlet x = 1; 07FF8FFC\nlet y = 5; 07FF8FF8\nlet r = &x; 07FF8FF4\n\n00000001\n\n00000005\n07FF8FFC\n\nwe say,\n\"r points to x.\" r is\nthe pointer, and x\nis its referent.\n\nwhen you run this code, the variables\nmight look something like this in memory.\n\nif I now write this:\n\nlet z = *r;\n\nthe asterisk follows the arrow from r to x,\nand loads the value stored there. so z will be…?\n11\n\n\fDereferencing\n● dereferencing means “accessing the referent of a reference.”\n● it does not mean “to deallocate a piece of memory.”\n\nA a = new A();\na.x = 10;\na.method();\narr[i] = a;\n\nlet mut x = 1;\nlet r = &mut x;\nlet z = *r;\n*r = 10;\n\nin Java, the dot and square\nbracket operators perform\ndereferencing, to access\nvalues and methods within\nan object or array.\n\nRust has those, as well as the\nprefix asterisk operator, which\ncan get or set the value at the\nother end of the reference.\n\n12\n\n\fThe necessity of indirection\n● when you write a program, you have a fixed number of variables…\no but the size of most data structures is dynamic.\no you don't know how many values there will be until runtime.\n● so, we have to dynamically allocate memory:\nint[] a = new int[100]; \/\/ gimme 100 new variables\n● but we won't know the array's address until runtime!\no so the only way to access it is indirectly.\n● the variable above is a pointer to an value on the heap, the area of\nmemory where dynamic allocation happens.\no from now on, I'll use the term object to refer to heap-allocated\nvalues, but not necessarily in the \"object-oriented\" sense.\n\n13\n\n\fThe power of the heap\n● the heap lets us create data structures of any practical size at runtime.\n● the lifetime of heap-allocated values (objects) is also dynamic.\n\nfn f(): Cat {\nreturn new Cat();\n}\nfn main() {\nlet c = f();\nc.meow();\n}\n\nan object can outlive the\nfunction which allocated it!\nbut this presents a problem:\nwhen does the lifetime of\nan object end?\n(who's the owner??)\n\n14\n\n\fHeap memory\nmanagement\ntime check < 40 min\n\n15\n\n\fMemory safety (slightly animated)\n● a program is memory-safe if, when you follow an arrow:\no there is exactly one possible value on the other end; and\no that value is guaranteed to be alive.\nfn main() {\nlet a = new A();\na.method();\na = new A();\na.method();\n}\n\na\n\nStack\n\nHeap\n\n00000000\n00804000\n00804018\n\ninstance of A\ninstance of A\n\nat all times, a is pointing to a live object,\nso calling a method on it is safe.\n\nfor memory safety to hold, as long as at least one arrow is\npointing to an object, it must not be deallocated.\nbut that sounds hard to prove…\n16\n\n\fSo let's not prove it!\n● one way to ensure arrows always point to live objects is…\no you never deallocate them!\n● what?? didn't we say memory was finite, and we have to reuse it?\no well, yeah, but not every program needs to reuse memory.\n● if you have a short-lived program which allocates memory, does its\nwork, and exits, there may be no need to deallocate at all.\no for example, a program that reads some text, parses it into a tree,\ndoes some processing on that tree, and spits out different text.\n▪ what could I be talking about? :^)\n\n● this idea can be applied at smaller scales within programs by using\narena allocation: tying heap allocation to the stack by deallocating a\ngroup of objects when a function returns.\n● buuuuut obviously this isn't a general solution.\n17\n\n\fManual memory management\n● since the programmer decides when object lifetimes begin…\n● let's also have them decide when their lifetimes end!\no\n\nhahaha what could possibly go wrong?\n\nthis is not a solution to the problem.\nwe've just pushed it somewhere else.\n\n\/\/ C++\nvoid main() {\nauto a = new A();\na->method();\ndelete a;\neasy peasy!\na->method(); oh fu---}\n\nhumans suck at knowing when is the right time\nto deallocate an object. what makes it hard?\n18\n\n\fIt's those dang arrows (slightly animated)\n● lots of languages make it really, really easy to create arrows…\no but they offer little or no help in ensuring that those arrows are\nvalid (that is, they point to a live object)\n\/\/ C\nint* p = malloc(40);\nint* q = p + 256;\np\n*q = 5000; \/\/ UB\nq\nfree(p);\nx\n*p = 10;\n\/\/ UB\nint x = 3;\np = &x;\nfree(p);\n\/\/ UB\n(UB = Undefined Behavior)\n\nHeap\n\nStack\n\n07FF8FF8\n00804000\n00804400\n\n40 bytes\n???????\n(10 ints)\n\n00000003\n\n???????\n\nit gets worse.\n19\n\n\fAliasing\n● this is when two or more arrows point to the same value.\n● it's very powerful: it lets you create any directed graph (digraph).\nStack\n\n1\n\n1\n\n1\n\nlists…\n\n1\n\n1\n\ntrees…\n\n1\n1\n\n2\n\n2\n\ndoubly-linked lists…\n\n1\n3\n\n1\n\ntrees with\nparent links…\n\n1\n2\n\n1\n1\n\ndirected acyclic\ngraphs (DAGs)…\n\na node's in-degree is how many arrows point to it.\n\n20\n\n\fWITH GREAT POWER etc.\n● unrestricted pointer aliasing can cause a lot of problems.\n\np\nt1\n\ndelete p;\n\nt2\nq\n\ntwo threads of execution\nboth accessing the same\nobject at the same time\ncan cause race conditions,\ndeadlocks, and more.\n\nif the programmer can\ndeallocate objects, two pointers\npointing at the same thing can\nlead to dangling pointers.\n\nother sources of dangling\/invalid pointers are\npointer arithmetic and pointers to stack values.\n21\n\n\fAutomatic heap\nmemory management\ntime check ≤ 70 min\n\n22\n\n\fHard for humans, easy for computers (animated)\n● at a high level, the problem is not that complicated.\nStack\n\nobjects and pointers form graphs.\n\nGlobals\n\nthe roots are the\nstack(s) and globals.\n\nreachable objects are pointed\nto – directly or transitively –\nfrom the roots.\nany objects that become\nunreachable can never be used\nby the program again, and are\ntherefore safe to deallocate.\nthis gives us a simple* rule: an\nobject's lifetime ends when its\nin-degree reaches 0.\n\n23\n\n\fCounting the arrows by… counting them (animated)\n● reference counting (refcounting) explicitly tracks the number of\narrows pointing to an object at any given time.\nwhen an object is\nwhenever a reference to it is\nallocated, its refcount is 0.\ncreated, the count is incremented.\nAnt a = new Ant();\nAnt b = a;\na = new Ant();\nb = null;\na = null;\n\na\nb\n\nwhen a variable is reassigned, the\nprevious object's count is decremented.\n\nvtbl: …\nrefs: 1\n0\n2\nspecies: \"ant\"\n\nvtbl: …\nrefs: 0\n1\nspecies: \"ant\"\n\nwhen the count reaches 0, the object is deallocated.\n24\n\n\fThe compiler's job\n● the compiler knows when the references are created and destroyed.\n● so, around each assignment, it inserts those 'crements.\nAnt a = new Ant();\ntricky details to consider:\nincRef(a);\nwhen a local variable goes out of scope,\nAnt b = a;\nwe need to decRef() it.\nincRef(b);\ndecRef(a);\nreturning a reference or passing one as\na = new Ant();\nan argument will increment the refcount.\nincRef(a);\ndecRef(b);\nbefore an object is deallocated, any\nb = null;\nreferences it has to other objects must\ndecRef(a);\nbe recursively decremented first.\na = null;\n25\n\n\fSo what's the catch? (animated)\n● \"when an object's in-degree reaches 0\" may never happen.\n\n1\n2\n\nnow what????????\n\n1\n\nthese objects are unreachable and should\nbe deallocated. but they won't be.\n\nthis is a memory leak: these objects take\nup space that can never be deallocated.\nit's 2!\n\n2!!!\n\n1!!!\n\nt1\n\n:c\n\nit's also bad for multithreaded\nprograms, since the threads will\nfight over the refcount.\n\nit's 1!\n\nt2\n\nknowing the in-degree is necessary for automatic memory\nmanagement, but it isn't sufficient: it's not all you need to know.\n26\n\n\fTracing Garbage\nCollection\ntime check ≤ 90 min\n\n27\n\n\fThe idea (animated)\n● same ideas as before: roots, graph, reachability.\nStack\n\nGlobals\n\n✅\n✅\n\n✅\n✅\n\nyour program runs normally, with\nno reference counting. this is\ncalled the mutation phase.\nperiodically, your program is paused\nand the collection phase begins.\nthe collector starts at the roots, and\nfollows every arrow it can find, marking\neach object it encounters as live.\nwhen there are no more arrows to\nfollow, the remaining objects are\ngarbage and are swept away.\n\n28\n\n\fThe correctness\n● this algorithm is mark-and-sweep and is the basis of tracing GC.\n● it works because an object is not garbage when its in-degree is 0…\no it's garbage when it is unreachable from the roots.\n● but wait, how does the collector get to the dead objects?\no simple: you are no longer the objects' owner. the collector is.\n● every time you allocate an object, the collector remembers it.\no the collector has a list of every object on the heap.\no in the mutation phase, it adds to this list when you new.\no the collection phase, it uses this list to find disconnected objects.\n● this works great for:\no cycles, cause the collector doesn't care about in-degree.\no multithreading, cause there are no ref counts to argue over, and\nthe collector considers all threads' stacks as roots.\n29\n\n\fConcessions we make\n● for the tracing algorithm to work well, there are some requirements.\nlet x = 0x08004030;\nlet a = new A();\nx 08004030\na 08004030\n\nwhat if this happened by\nchance? how can the collector\nknow a is a pointer but x is not?\n\nthe compiler must produce information for\nevery global, local, and class field saying\nwhether or not something is a pointer.\n\nint x = 0x08004030;\nint *p = (int*)x;\n\ntype safety is crucial. being able to\nfreely change the types of values will\nthrow the collector way off.\n\npointer arithmetic is a no-go. nuh uh. no way.\n30\n\n\fWhat's the downside?\n● well, the mark-and-sweep algorithm I showed is… too simple.\no it doesn't have great performance.\n● getting good performance means making it way more complex.\no concurrent collectors do some\/most of their work during the\nmutation phase, making the collection phase shorter.\no multithreaded collectors extend that to multiple mutators.\no generational collectors take advantage of the fact that most objects\nare deallocated very quickly after they're allocated, and focus more\neffort on newer objects.\no copying collectors reduce the amount of memory needed by\nmoving objects around.\n\n31\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":239,"segment": "unlabeled", "course": "cs1622", "lec": "lec04","text":"CFGs and ASTs\nCS\/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n● did you start the project yet?\no a fair number of you haven’t accepted the assignment on Github…\no do iiiiiiit >:V\n\n2\n\n\fBeyond Regular\nGrammars\n\n3\n\n\fRegular?\n● lexing grammars describe regular languages. what's that mean?\nProgram: (WS? Token)* WS? '<eof>'\nWS:\n' ' | '\\t' | '\\n'\nit's a little nonobvious, but if we\nToken:\n'(' | ')' | Id | IntLit start at the start rule, Program,\nId:\nIdStart IdCont*\nthe right-hand sides of every\nIdStart: <alphabetic> | '_'\nrule never take us in a loop to\nIdCont: IdStart | <digit>\nthe same rule twice.\nIntLit: <digit>+\n\nthis puts strict limitations on the kinds of strings\nthese grammars can recognize…\n…but it does make these languages easier to\nreason about and implement.\n4\n\n\fAnother way to look at it\n● if we look at the dependency graph of these rules:\nProgram\n\nToken\n\nId\n\nIdStart\n\nWs\n\nIntLit\n\nIdCont\n\nnow it becomes more obvious that\nthere are no cycles in this graph.\n(this is called a DAG: a directed acyclic graph.\nit's like a tree, but nodes can share parents.)\n\nbut… what happens if we remove that requirement?\n5\n\n\fA whole new world\n● let's say our alphabet is A = { '(', ')', 'e' }.\n● and our grammar rule is Exp: 'e' | '(' Exp ')'\n● valid strings would be e, (e), ((e)), etc. for any number of parens.\none l o o p y b o i is all it\ntakes to give us this power.\n\nExp\n\nbut power comes at a cost: this kind of language is strictly\nmore complex than regular languages and requires more\ncomplex algorithms to implement.\nonce you allow these kinds of recursive rules, you are\nnow in the land of context-free languages!\n6\n\n\fSo what can they do?\n● context-free grammars (CFGs) can represent nesting of any depth.\n● this happens to map very nicely onto our intuition (instinct?) about how\nlanguage works in general.\nin human languages, we can nest phrases inside other ones.\n\nthe cat (on the bed (in that room))\nwith CFGs, we can nest\nexpressions…\nf(x * (y + 3))\n\nand statements!\n\nif x == 10 {\nif y == 20 {\nf();\n}\n}\n7\n\n\fBut what can't they do?\n● there are a lot of kinds of languages they can't represent.\n● but for most computer languages, this isn't really a problem.\n● unlike natural human languages, computer languages…\no are designed up front, rather than emerging naturally\no are meant to be unambiguous\no are meant to be easy to read and maintain\n● so something simpler than human languages is a good thing.\n● THERE'S STILL THIS THING THOUGH\n\nint y = x >> 3;\nList<List<String>> l;\n\nto properly parse this requires a\ncontext-sensitive grammar (CSG)…\nbut the ambiguity here is simple\nenough that we can fake it using a\nCFG and some parsing kludges.\n8\n\n\fASTs\n\n9\n\n\fFrom lexical to syntactic\n● the lexer was responsible for splitting the source into \"words.\"\n● the parser looks at those words and extracts the syntactic structure\nfrom them, building a tree representation of your program.\nsource\ncode\n\nLexer\n\ntokens\n\nParser\n\ntree\n\nthe parser typically implements a CFG whose alphabet (the set of\nterminals) is the tokens that were produced by the parser.\nin doing so it checks your syntax – to make sure you\nwrote something that, well, looks like a program.\n10\n\n\fA tree of the syntax, which is abstract\n● the abstract syntax tree (AST) is the structure the parser builds.\n● trees are much easier to work with than lists of characters or tokens.\n\nif(x) y(2);\nelse return;\n\nIfElseStmt\ncond\n\nIdentExp\nname\n\nT\n\nF\n\nCallExp\n\n\"x\"\n\ncallee\n\nReturnStmt\n\nargs[]\n\nvalue\n\nø\nIdentExp\nname\n\n\"y\"\n\nIntExp\nvalue\n\n2\n11\n\n\fWhy is it called \"abstract\"?\n● because we can change the syntax of our language but the AST\nrepresentation doesn't have to change. it decouples them.\nJava\/C if(x) y(2); else return;\n\nIfElse\n\nRust if x { y(2); } else { return }\n\nLua if x then y(2) else return end\nRuby if x\nPython if x:\ny 2\ny(2)\nelse\nelse:\nreturn\nreturn\nendif\n\nIdent\n\nCall\n\nIdent\n\nReturn\nInt\n\nof course, these languages work differently,\nbut the point is that the syntax is just fluff.\n12\n\n\fValues, expressions, and statements\n● computation is the act of transforming values into different values.\no values are just things like ints, strings, objects, etc.\nwhen you execute an\nexpression, you get a value.\n2.8\n\nx + y\n\nsin(2 * ang)\narr[i] + \";\"\n\nobj.field\n\nwhen you execute a\nstatement, you… don't.\n\nreturn;\n\nif(…) …\nelse …\n\nprintf(\"hi!\\n\");\n{ …statements… }\n\nbut there are many exceptions, and not\nevery language makes this distinction.\nit's good to know the terms though, cause\nthey come up a lot in parsing and semantics.\n\n13\n\n\fWhat about Rust?\n● actually, Rust only has expressions. everything can give a value.\nlet x = if y { 10 } else { 20 };\nfn radix(c: char) -> Base {\nprintln!(\"in radix\");\nmatch c {\nthis demonstrates a sort of\n'x' | 'X' => Base::Hex,\nconfusing rule: you can omit\n'b' | 'B' => Base::Bin,\nreturn if you're in the last\n_\n=> Base::Dec,\nexpression in the function.\n}\n}\nthis match is the last expression, so whatever\nvalue it gives becomes the return value!\nbut this is confusing, so I won’t use it this term. :)\n14\n\n\fTrees in Rust\n\n15\n\n\fIn Java…\n● you may have learned about trees (or their one-dimensional cousins,\nlinked lists) and seen them written like this:\nclass Node<T> {\nNode<T> left, right;\nT value;\nNode(T t) { value = t; }\n}\n\nNode<Integer> a = new Node<>(5);\na.left = new Node<>(2);\na.right = new Node<>(7);\n\nleft and right have two special properties that\nmake this representation work:\n1. they are references: they refer indirectly to\nNode objects.\n2. they can optionally hold null to indicate that\nthey are not pointing to anything.\n\n5\n2\n\n7\n\n16\n\n\fDoing it in Rust?\n● if we try to do this, we get an error:\nstruct Node<T> {\nleft: Node<T>,\nright: Node<T>,\nvalue: T,\n}\n\nit says this type has\ninfinite size, but it\ngives us a hint:\nstruct Node<T> {\nleft: Box<Node<T>>,\nright: Box<Node<T>>,\nvalue: T,\n}\n\ngreat, it compiles! what the heck does it mean tho\n17\n\n\fI would not eat them with a fox\n● a Box is the simplest kind of reference in Rust: it allocates an object\non the heap (just like Java's new) and points to it indirectly.\n● you create a boxed object with Box::new():\nlet x = 10;\n\/\/ an int on the stack\nlet b = Box::new(20); \/\/ an int on the heap\nCall Stack\n\nHeap\n\nx\n\nthe Box\n\nb\n\n10\n\n20\n\nI mean, normally you wouldn't box an\nint, but it's just for the example.\n\nokay, let's make a\nNode<T> then!\n\n18\n\n\fCurses! Foiled again!\n● when you make a struct, you have to give values for all the fields.\nstruct Node<T> {\nleft: Box<Node<T>>,\nright: Box<Node<T>>,\nvalue: T,\n}\n\nlet a = Node {\nvalue: 5,\nleft: Box::new(Node {\nvalue: 2,\nleft: ………uhhhhhhhh\n\n5\n2\n\n7\n\nwhat do we write for left, here? 2 has no left child.\nwhat we've made here is a type that is impossible to construct!\n\nso we need something to satisfy the other important property\nof the Java Node<T> variables: the possibility of being null.\nthe solution is to use Option<Box<Node<T>>>.\n19\n\n\fOption<Box<Node<T>>>? That's… verbose\n● a little, yeah. but this comes up less often than you’d think.\n● add a constructor and it becomes easy to use:\nstruct Node<T> {\nleft: Option<Box<Node<T>>>,\nright: Option<Box<Node<T>>>,\nvalue: T,\n}\n\nlet mut a = Node::new(5);\na.left = Some(Node::new(2));\na.right = Some(Node::new(7));\n\nand now we have something very similar to the\noriginal Java, except we have to use Some(..) to\nindicate the opposite of None (aka null).\n\n5\n2\n\n7\n\nthis code is in the rust_trees example.\n20\n\n\fA brief digression on Option\n● let’s look at another example, rust_option!\n● Option gives you something like Java’s null, but it’s opt-in, and\nworks on any type, not just object references.\n● there are a few ways of “getting the value out of” an Option, but…\no .unwrap() is the most risky, cause it could crash your program!\n\n21\n\n\fDefining and using an\nAST\n\n22\n\n\fBefore we get to a real language…\n● let's start with something simple.\n● maybe we're writing some calculator software and want to represent\nwhat the user types in symbolically, instead of just evaluating it.\n×\n\nhow we get from what was\ntyped in at the bottom to\nthe tree up top is a topic for\nan upcoming lecture, but…\n\n÷\n\n+\n2\n\n3\n\n5\n\n-40\n\nif the expression is a tree,\nwe can have fun with it!\n\n(2 \/ -(3 + 5)) * -40\n23\n\n\fRepresenting this tree\n● we've got a few kinds of AST node here:\no constants (2, 3, 5, -40)\no negation (-x)\no addition, subtraction, multiplication, division\n▪ we can group these under a single kind: binary operators\n▪ not binary as in the base, binary as in \"has two operands\"\n\n● when you have choices of a type, the thing to use is an enum.\n● this enum appears in the ast_math example:\nenum AstNode {\nConst { val: f64 },\nNegate { lhs: Box<AstNode> },\nBinary { op: BinOp, lhs: Box<AstNode>, rhs: Box<AstNode> },\n}\n\nlook, no Options!\n24\n\n\fRecursion Refresher\n● recursion is a programming technique where a function calls itself.\n● there is an important fact about recursion that a lot of people are\nhazy on, even as juniors and seniors:\n\nlocal variables are duplicated for each recursive call.\nfn sum(x: i32) -> i32 {\nif x <= 1 {\nreturn x;\n} else {\nreturn x + sum(x – 1);\n}\n}\nif I call sum(5), every recursion gets its\n\nown copy of x, and the in-progress\nrecursions do not “share” that variable.\n\nStack\nsum(5) x = 5\nsum(4) x = 4\nsum(3) x = 3\nsum(2) x = 2\nsum(1) x = 1\n\n25\n\n\fTrees ♥ Recursion\n● forget Fibonacci: recursion is exactly what you need for trees.\n● we have this tree, and a variable pointing to the root.\nhow do you evaluate a multiplication?\n×\n\n÷\n\n-40\n\nthe first two steps are done as recursive\ncalls to evaluate the two children.\n\n-\n\n2\n\n+\n3\n\n1. evaluate the left-hand side (LHS)\n2. evaluate the right-hand side (RHS)\n3. multiply them together\n\n5\n\nthat's it. that's how you write a recursive\nalgorithm over a tree. (see AstNode::eval)\n26\n\n\fWhat else could you do with them?\n● you could apply transformations to the tree.\n● maybe we have a calculator feature to get the reciprocal.\ndoing \"one over\" means\nadding two new nodes and\nmaking the division the root.\n\n÷\n+\n3\n\n7\n\n1\n\n+\n3\n\nof course, if the input is a\ndivision, we can special-case it.\n\n÷\n\n÷\n2\n\n7\n\n5\n\n5\n\nthis is what AstNode::recip in the example does!\n\n2\n27\n\n\fWhat if the programmer could do that?\n● some languages have macros: things that look like functions, but\nwhich operate on the AST at compile-time.\no that’s really what println!() is!\no the details of how println!() works are beyond the scope of this\nlecture but that's why it yells: it's a macro, not a function. it\ngenerates different ASTs based on what arguments you give it.\n● if you've experienced C's preprocessor macros (#define), those are\nsimilar, but they operate on tokens instead of the AST.\no still, you can do some impressive stuff with them!\n\n28\n\n\fOk, but, how does the AST get built\n● uhhhhhhhhhhhhhhhh wellll\no parsing!!!!!!\no parsing is definitely more complicated than lexing\no which is why we have the next two lectures dedicated to it.\n● but that's all for today. good luck on the project, have a nice break,\nand see you in a week!\n\n29\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":240,"segment": "unlabeled", "course": "cs1622", "lec": "lec01","text":"Intro and Rust\nCS 1622\nJarrett Billingsley\n\n\fClass announcements\n● be sure to check the notes on each of my slides\no for extra explanations, examples, snarky comments etc.\no as well as answers to the questions on the slides so you can study.\n● also, be sure to use Powerpoint to view my slides\no not Keynote, not Google Drive; they absolutely butcher alignment\nand diagrams and arrows and I use a lot of those. it’s bad enough\nthat diagrams can become incorrect\/misleading.\no if you use a note-taking app, open the pptx in Powerpoint and\nexport it as PDF, then use that. it will look much better than\nopening the pptx directly in your note-taking app.\n\n2\n\n\fAdministrivia\n\n3\n\n\fhi\n● you can just call me Jarrett\n\n●jarrett@cs.pitt.edu\n●sites.pitt.edu\/~jfb42\n\n● Office hours: Mon\/Wed 1:15-3:30 in 6509 SENSQ\n● Religious absences: contact me ASAP\n● Students with disabilities: contact the DRS ASAP\n● Everything in this section is also on my site in \"course info\"!\no I use red on the slides for really important things\n\n4\n\n\fCommunication\n● I send announcements through Canvas which come thru email\no it is not my responsibility to make sure you get them, it’s yours\n● announcements\/grades are the only things I use Canvas for\no (well recording class too)\no everything else is on my site.\no including the course schedule\/syllabus.\n● Discord hours: most days after 12PM and before 9PM EDT\no\n\nbut I don't really have a healthy work\/life separation so\n\n● For more \"official\" communications, or if you need my attention\nimmediately, use email.\no otherwise, Discord is preferred – much better for sharing code,\nscreenshots, etc. in a much quicker way.\n\n5\n\n\fTextbooks???\n● I'm not really a book person.\n● none of these are required and the first two are free (see course info)\n\n6\n\n\fGrading\n● Projects (x4): 60%\no each is worth 15%\no there is a late submission policy, see the course info page\n● Exams (x2): 40%\no each worth 20%\no exam 2 is not cumulative! it’s just on the stuff after exam 1.\no on both exam days, we will do a short review session beforehand.\n● There are no labs or recitations or anything.\n● Attendance is not graded for lectures…\no but people who don't come and interact don't do well.\n\n7\n\n\fTeaching philosophy\n● I'm more of a \"big picture\" teacher\no I care about high-level concepts and problem-solving\no you won't get exam questions about function parameters\n● You are a student. you are supposed to be confused.\no I wanna help you understand!\no never say \"this is a dumb question\" cause there aren't any!\n\noDON’T STRUGGLE IN SILENCE!!!! EVER!!\n\n● What is a university setting good for today?\no focus, practice, and access to people who know stuff\no your friends do not know as much about this stuff as I do\no but also… your friends can be very helpful when studying or in\npractical matters (installing stuff etc)!\no studying together is not cheating! do it!\n8\n\n\fI can tell when you cheat :^)\n● If you cheat, you fail the course. Period.\n● Don't post your code on github, baidu etc.\n● Don't \"help\" your friends by giving them code.\n\n9\n\n\fAcademic Integrity\n● If you \"help\" a friend, you are only delaying their failure until a\nlater point when it will be too late to fix the problems.\no a 0 will hurt way, way more than a 60, and it'll hurt you both.\n● Don't cheat if your enrollment is contingent upon your GPA.\no or your ROTC or your student visa or your graduation or whatever.\n● Generally speaking, I trust you!\no I try to be accommodating about extensions and such\no I don’t think most cheaters are being lazy\n● But you’re an adult, and are responsible for your actions.\no I hate being in a position where I have your future in my hands…\no especially when you're the one who put it there.\no Do not take advantage of my trust.\n● If you're confused, don't cheat, ask for help. yes, even on the\nexams. yes, even an hour before the project is due.\n10\n\n\fIntro\n\n11\n\n\fWhat, Why, and How\n● for every topic, ask yourself (and me!) these questions:\n\nWhy do we use it?\n\n\"splitting code into functions makes it\neasier to read, understand, and reuse.\"\n\nWhat is it?\n\n\"a function is a named piece of code.\"\n\"in Java, you write the return type,\n\nHow do we do it?\n\nthen the name, then a left parenthesis,\nthen any arguments, with the type and name,\n\nseparated by commas, then a right parenthesis, then…\n\nwhat and why are almost always more important than how.\non the exams, be sure you are answering the right question.\nif I ask \"what is X,\" don't explain how to X.\n12\n\n\fWhat is language?\n● a system of encoding information by using arbitrary symbols\no \"symbols\" can also include sounds, objects, actions, etc.\nhuman languages are massively\ncomplex, like anything involving brains.\nwithin the mess, there are kernels of\nmathematical and logical truth…\n\n∃𝑥∈𝑆 ∀𝑦≠𝑥 𝑃 𝑦 → 𝑄(𝑥)\n…upon which computer\nlanguages were designed.\n\n13\n\n\fComputer languages\n● modeled after human languages, these are much simpler languages\nused to encode information in a way that is useful for computing.\ndeclarative languages\nencode data and rules.\n\nprogramming\nlanguages encode\ninstructions for the\ncomputer to\nexecute.\n\ninterchange and binary file\nformats encode domain- or\napplication-specific data.\neven things like network\nprotocols are a kind of language!\n14\n\n\fProgramming languages\n● one definition is that programs are sequences of instructions…\n● but programs are also proofs.\n\nfn main() {\nfor i in 0 .. 10 {\nprintln!(\"i = {}\", i);\n}\n}\n\nthe Curry-Howard correspondence\nshows that programs and proofs\nare two ways of expressing the\nsame mathematical objects.\n\nthere are proof languages and proof assistants which can\nfunction as both programming languages and proof checkers.\n\n15\n\n\fThe Origin of (high-level programming) Languages\n● as you learned in 447, CPUs execute machine code…\no and assembly language is a textual representation of that.\nmain:\nbut even the simplest tasks take a lot of code in\nli\ns0, 0\nassembly, and more code = more mistakes.\n_loop:\nla\na0, msg\nli\nv0, 4\nwe invented high-level languages (HLLs) to let\nsyscall\nus write programs in shorter, human-friendly ways.\nmove a0, s0\nli\nv0, 1\nfn main() {\nsyscall\nfor i in 0 .. 10 {\nli\na0, '\\n'\nli\nv0, 11\nprintln!(\"i = {}\", i);\nsyscall\n}\nadd s0, s0, 1\n}\nblt s0, 10, _loop\n16\n\n\fPerformance ∝ 1 \/ Abstraction\n● HLLs allow us to focus more on solving problems than on holding\nthe CPU's hand through every step of computation\n● but most abstractions come at a performance cost\n\nfor(i = 0; i < n; i++) {\nA[i] = B[i] + C[i];\n}\n\nA = B + C\n\nlower-level languages like C\nprovide abstractions that better\nmatch* what the underlying\nhardware provides.\n\nhigher-level languages like\nPython give you more powerful\nabstractions, at the cost of lower\nperformance (usually).\n\nbut it all depends on the CPU design and the\nquality of the language implementation.\n17\n\n\fCompilers\n\n18\n\n\fWhat's a compiler?\n● it's a program that translates one programming language (the\nsource language) to another (the target language).\n● typically this is from an HLL to a machine language.\n\nJava\n\ntranspilers convert\nbetween HLLs.\n\njavac\n\nC\ngcc\n\nrecompilers convert\nbetween machine languages.\n\nJVM Bytecode\n\nand decompilers\nattempt to reconstruct\nHLL code from\nmachine language!\ne.g. ghidra,\nhex-rays\n\nx86 Machine Code\njava\n19\n\n\fAhead-of-time vs. Just-in-time (AOT vs JIT)\n● AOT compilers produce a file that contains the native machine code.\n● JIT compilers produce native machine code right before it's needed.\n\nC\n\nJava\n\ngcc\n\nCPU\n\nhello.exe\n\nJVM\njavac\n\nhello.class\n\njava\n\n…can use an\ninterpreter to run\nthe bytecode, or JIT\nit to native code.\n20\n\n\fOkay, okay: HOW does a compiler WORK\n● there are kind of two broad stages:\nthe frontend reads the\nsource code and checks it\nfor \"correctness.\"\n\nthe backend produces target\ncode from the compiler's\n\"idea\" of your program.\n\nFrontend\n\nBackend\nhello.exe\n\nhello.c\n\nthis \"idea\" is the intermediate representation (IR):\na sort of third language which \"bridges the gap.\"\n\n21\n\n\fFrontend step 1: Lexical Analysis (Lexing (or Scanning))\n● lexing splits the source text into tokens: words, symbols, etc.\n● it's a straightforward string processing algorithm.\n\nThis is English.\n\nThis, is, English, .\n\nvoid main(){}\n\nvoid, main, (, ), {, }\n\n\"hello!\n\nerror: unclosed string literal\n\nif<class)++\n\nif, <, class, ), ++\n\nthis last one is nonsense, but the lexer doesn't know that.\nthat's the responsibility of…\n22\n\n\fFrontend step 2: Syntactic Analysis (Parsing)\n● parsing takes the lexed tokens and extracts structure from them.\n● a language's grammar defines the rules of these structures.\nIF-ELSE\n\nS\nVP\nNP\nART\n\nCALL\n\nPP\nN\n\nV\n\nPREP PRN\n\nThe cat stares at me.\n\nID\n\nID\n\n[]\n\nCALL\nID\n\n[]\n\nif(x) y(); else z();\n\nthis is where you get syntactic errors, like \"missing\nsemicolon\" or \"unexpected closing paren\" or whatever.\n\nthe result is an AST: Abstract Syntax Tree.\nbut this just looks like a program. is it one?\n\n23\n\n\fFrontend step 3: Semantic Analysis\n● this checks for \"correctness\" according to the rules of the language.\no things like type checking, name usage, privacy…\n\nColorless green dreams sleep furiously.\n(syntactically correct, but nonsense.)\n\nx = 10;\n\nerror: undefined reference to 'x'\n\nint x = 1.0;\n\nerror: possible loss of precision\n\nint x = 1;\n\n👍\n\nI keep quoting \"correctness\" because it's absolutely possible\nto write incorrect programs that pass these checks.\nmuch PL research is about making these checks better!\n24\n\n\fBackend: optimization and code generation\n● optimization rewrites the program to do the same things but faster.\n● codegen produces the target language code from the IR.\nfor(i = 0; i < 4; i++)\nA[i] = B[i] + C[i];\n\nA[0] = B[0] + C[0];\nA[1] = B[1] + C[1];\nA[2] = B[2] + C[2];\nA[3] = B[3] + C[3];\n\nla\nla\nla\nlw\nlw\nadd\nsw\nlw\nlw\nadd\nsw\nlw\nlw\nadd\nsw\nlw\nlw\nadd\nsw\n\nt0, A\nt1, B\nt2, C\nt3, 0(t1)\nt4, 0(t2)\nt3, t3, t4\nt3, 0(t0)\nt3, 4(t1)\nt4, 4(t2)\nt3, t3, t4\nt3, 4(t0)\nt3, 8(t1)\nt4, 8(t2)\nt3, t3, t4\nt3, 12(t0)\nt3, 12(t1)\nt4, 12(t2)\nt3, t3, t4\nt3, 12(t0)\n\nand boom... we\nhave a program!\n\n25\n\n\fWhat we'll talk about along the way\n● grammars and abstract syntax trees!\n● type theory and type systems!\no static and dynamic typing!\no strong and weak typing!\no parametric types and generics!\n● runtime representation!\n● memory management!\n● ABIs and linking!\n● and more!\n\n26\n\n\fRust\n\n27\n\n\fIf you want to write a compiler…\n● different problems require different tools.\n● when writing compilers, some language features will be very useful:\nJava\n\nC\n\nRust\n\nGood string manipulation\n\n4\/5\n\n0\/5\n\n5\/5\n\nAutomatic memory management\n\n5\/5\n\n0\/5\n\n5\/5\n\nGenerics (types and code)\n\n4\/5\n\n1\/5\n\n4\/5\n\nFunctional programming style\n\n3\/5\n\n0\/5\n\n4\/5\n\nAlgebraic data types\n\n1\/5\n\n1\/5\n\n5\/5\n\nLess repetitive boilerplate\n\n0\/5\n\n0\/5\n\n4\/5\n\nStrong, expressive type system\n\n2\/5\n\n0\/5\n\n5\/5\n\nratings are my opinion based on experience. this is a limited view of their strengths\/weaknesses.\n\n28\n\n\fWhat is Rust?\n● it's a systems language like C, but designed to not be terrible.\n● it's rigorously defined and borrows many features from PL theory…\n● …but also tries to be a practical language for writing real programs.\nyou might have heard that\nit's complex and confusing.\nand it sure can be!\n\nfn err_context<'a, O, F>(kind: LexErrorKind, parser: F) ->\nimpl Fn(Span<'a>) -> LResult<'a, O>\nwhere\nF: Fn(Span<'a>) -> IResult<Span<'a>, O, LexError>,\n\nfn main() {\nlet nums = vec![1, 3, 5, 7, 10];\nfor (i, n) in nums.iter().enumerate() {\nprintln!(\"nums[{}] = {}\", i, n);\n}\n\nbut it can also be simple,\nelegant, and readable.\n\nlet sum: i32 = nums.iter().sum();\nprintln!(\"sum of nums = {}\", sum);\n}\n29\n\n\fBasic tools and vocabulary\n● rustup is the tool to install the Rust toolchain.\n● cargo is the \"swiss army knife\" for creating and building programs.\no it lets you create new Rust projects with some basic features.\no it acts as a build tool like make.\no it lets you run tests, build documentation, and so on.\no it lets you specify dependencies on external libraries (crates).\no it automatically downloads, builds, and installs those libraries.\n● a crate is like a Java package: a self-contained program or library.\no crates.io is the website where you can search for crates.\no each Rust program you make is its own crate.\n\n30\n\n\fRust by Comparison\n● here is a simple program written in Java and Rust.\npublic class Code {\npublic static void main(String[] a) {\nfor(int i = 1; i <= 100; i++) {\nif((i % 3) == 0)\nSystem.out.print(\"Fizz\");\nif((i % 5) == 0)\nSystem.out.print(\"Buzz\");\nif((i % 3) != 0 && (i % 5) != 0)\nSystem.out.print(i);\nSystem.out.println();\n}\n}\n}\n\nfn main() {\nfor i in 1 ..= 100 {\nif (i % 3) == 0 {\nprint!(\"Fizz\");\n}\nif (i % 5) == 0 {\nprint!(\"Buzz\");\n}\nif (i % 3) != 0 && (i % 5) != 0 {\nprint!(\"{}\", i);\n}\nprintln!();\n}\n}\n\nwhat are some differences?\n(see slide notes for some answers)\n\n31\n\n\fMore Comparison\n● it really isn't THAT scary. (class\/main\/System.out omitted for space)\nint[] nums = new int[]{1, 3, 5, 7, 10};\n\nlet nums = vec![1, 3, 5, 7, 10];\n\nfor(int i = 0; i < nums.length; i++) {\nfor (i, n) in nums.iter().enumerate() {\nprintf(\"nums[%d] = %d\\n\", i, nums[i]);\nprintln!(\"nums[{}] = {}\", i, n);\n}\n}\nint sum = Arrays.stream(nums).sum();\nprintf(\"sum of nums = %d\", sum);\n\nlet sum: i32 = nums.iter().sum();\nprintln!(\"sum of nums = {}\", sum);\n\ntakeaways:\n- variables declared with let\n- arrays called \"vectors\"*\n- .iter() seems to do Fun Iteration Things\n- i32 instead of int\n- print formatting done with {}\n32\n\n\foh no I'm not good with computer what does this me\n● let's try something.\nlet nums = vec![1, 3, 5, 7, 10];\nfor n in nums { println!(\"{}\", n); }\nfor (i, n) in nums.iter().enumerate() {\nprintln!(\"nums[{}] = {}\", i, n);\n}\n\nRust's compiler errors tend\nto be very verbose, but it's\nbecause it actually tries to\nhelp you fix the error!\n\nfollowing its help works here, and often does!\n33\n\n\fWhat I want you to do!\n\n34\n\n\fFor next class…\n● On the course info page, find the Rust Programming Language book.\n● follow along with chapters 1-3 to install and play around with Rust.\no Windows users, use PowerShell or WSL. cmd.exe is dead.\n● you're already programmers so a lot of things will be obvious to you!\n● try this: just read the code examples and if you're confused, read\nthe text before it. that way you can skip the easy stuff.\n\n35\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":241,"segment": "unlabeled", "course": "cs1567", "lec": "lec03_publisher_and_subscriber", "text":"Publisher and Subscriber\nThumrongsak Kosiyatrakul\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fPublisher\n\nA publisher publishes data to topics\nA publisher has to have a name (for debugging purpose)\nA topic is a named bus (need a name as well)\nA topic must have a type associated with it (strongly typed)\nDo not need to care who are subscribers (anonymous)\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\ftalker.py\n#!\/usr\/bin\/env python\nimport rospy\nfrom std_msgs.msg import String\ndef talker():\npub = rospy.Publisher('chatter', String, queue_size=1)\nrospy.init_node('talker', anonymous=True)\nrate = rospy.Rate(10) # 10hz\nwhile not rospy.is_shutdown():\nhello_str = \"hello world %s\" % rospy.get_time()\nrospy.loginfo(hello_str)\npub.publish(hello_str)\nrate.sleep()\nif __name__ == '__main__':\ntry:\ntalker()\nexcept rospy.ROSInterruptException:\npass\n\nDo not forget to make this file executable (chmod +x talker.py)\nYou only need to set it once\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\ftalker.py\n#!\/usr\/bin\/env python\n\nIf you try to create a node using Python, the above line must\nbe at the top\nIt let the OS knows that this program should be passed to the\nPython interpreter\nimport rospy\n\nThis line should be in all Pyhton node\nIt imports all of the basic functionality that we will need\nfrom std_msgs.msg import String\n\nWe are going to publish messages of type String\nWe need to import String from the standard message\nstd msg.msg.\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\ftalker.py\npub = rospy.Publisher('chatter', String, queue_size=1)\n\nThis create an object of type Publisher and can be referred\nto by the variable named pub\nrospy.Publisher() is a constructor:\nFirst Argument: The topic name (chatter)\nSecond Argument: The type of message (String)\nThird Argument: The maximum number of message that will\nbe stored in case any subscriber is not receiving them fast\nenough\nrospy.init_node('talker', anonymous=True)\n\nInitialize this node with the name talker\nShould be anonymous\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\ftalker.py\nrate = rospy.Rate(10) # 10hz\n\nCreate an object of type Rate and can be referred to by the\nvariable named rate\nrospy.Rate() is a constructor\nThe argument 10 specifies the frequency in Hertz\n\nrate.sleep() will be used later which will put this node to\nsleep for 1\/frequency second (0.1 second in this case).\nwhile not rospy.is_shutdown():\n\nA loop is a fairly standard rospy construct.\nThis node should stop if it gets a signal to shutdown.\nrospy.is shutdown() return True if this node gets the\nsignal to shutdown.\nThe other way to simply shut this node down is by pressing\nCtrl-c on the console that this node is executing.\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\ftalker.py\n\nhello_str = \"hello world %s\" % rospy.get_time()\n\nCreate a string named hello str.\nrospy.get time() returns the current time.\nrospy.loginfo(hello_str)\n\nPrint the message on the console screen that this node is\nexecuting.\nStore the message into node’s log file\nCan also be viewed from any console using rqt console\ncommand.\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\ftalker.py\npub.publish(hello_str)\n\nPublish the message\nrate.sleep()\n\nPut this node to sleep for 0.1 second (10hz).\nif __name__ == '__main__':\ntry:\ntalker()\nexcept rospy.ROSInterruptException:\npass\n\nInterrupt may be generated by rospy.Rate.sleep()\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fSubscriber\n\nA subscriber subscribes to receive data from topics\nA subscriber has to have a name (for debugging purpose)\nA topic is a named bus (need a name as well)\nA topic must have a type associated with it (strongly typed)\nDo not need to care who are publishers (anonymous)\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\flistener.py\n\n#!\/usr\/bin\/env python\nimport rospy\nfrom std_msgs.msg import String\ndef callback(data):\nrospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\ndef listener():\nrospy.init_node('listener', anonymous=True)\nrospy.Subscriber(\"chatter\", String, callback)\nrospy.spin()\nif __name__ == '__main__':\nlistener()\n\nDo not forget to make this file executable (chmod +x talker.py)\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\flistener.py\nrospy.init_node('listener', anonymous=True)\n\nInitialize this node with the name listener\nShould be anonymous\nrospy.Subscriber(\"chatter\", String, callback)\n\nSubscribes to the topic named chatter.\nThe type of message is String (must be the same as the\npublisher)\nEvery time a message is received, call the function\ncallback()\nrospy.spin()\n\nKeeps the node from exiting until the node has been\nshutdown.\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fThe callback() Function\n\nThis callback() function will be called every time it receives\na message on the topic chatter:\ndef callback(data):\nrospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\n\nThe argument data has type std msgs\/String\nA data of this type consists of one component of type string\nnamed data\n\nTo access the actual string, we use data.data like C structure\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fmessenger.py\n\nSometimes your ROS node may behave like a middle man\nSubscribes to a topic to receive data\nProcesses data\nPublishes new data to a new topic\n\nExample: Blob Detection\nSubscribes to a topic to receive a series of image\nFor each image received, find blobs of a specific color, and\ncalculate the middle point of each blob\nPublishes an array of coordinates of blobs\n\nThis type of node subscribes as well as publishes data\nExample:\nmessenger node subscribe to the topic chatter1 and publish\nthe modified message to the topic chatter2\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fmessenger.py\n#!\/usr\/bin\/env python\nimport rospy\nfrom std_msgs.msg import String\npub = rospy.Publisher('chatter2', String, queue_size=10)\ndef chatterCallback(data):\nglobal pub\nmessenger_str = \"Messenger heard %s\" % data.data\npub.publish(messenger_str);\ndef messenger():\nrospy.init_node('messenger', anonymous=True)\nrospy.Subscriber(\"chatter1\", String, chatterCallback)\nrospy.spin()\nif __name__ == '__main__':\nmessenger()\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fNotes about Distributed Computing\nROS environment is similar to distributed environment.\nRequires a number of processes to work together to finish a\njob.\nIf a process dies, the whole thing will not work properly.\nWe need to make sure that each process is alive until the job is\ndone.\n\nGenerally, if a program finish, its process dies (exit).\nIn ROS, there are two main methods to keep a process alive.\nLoop until a job is done.\nwhile not rospy.is_shutdown():\n:\n\nUse rospy.spin() which is almost the same as\nwhile not rospy.is_shutdown():\npass\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fLoop vs rospy.spin()\n\nYou should use either loop or rospy.spin() but not both in\nthe same program.\nUse loop when your program has to continuously checking\nand doing something.\nContinuously publishes messages.\nContinuously monitors variable values.\n\nUse rospy.spin() when you program can go to sleep until\nsomething happen.\nProcess an image or data whenever it is available (subscriber),\npublish just one message, and go back to sleep.\n\nOften time, the choice of loop vs rospy.spin() is not clear.\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fCleanup\nA process (node) can be killed by pressing Ctrl-c or receiving\nthe kill signal from ROS master\nSometime, a process needs to clean up before exit.\nIf robot is currently moving, stop it first and exit\nOtherwise, robot may keep moving non-stop\n\nCleanup can be done by registering shutdown hooks:\ndef cleanup():\n# stop the robot\nrospy.on_shutdown(cleanup)\n\nWhen the program receive the kill signal, it will call the\nfunction cleanup() first and then exit\nA node can manually shut itself down.\nrospy.signal_shutdown(\"I am done.\")\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fAbout Callback Functions\nRecall the listener node.\n#!\/usr\/bin\/env python\nimport rospy\nfrom std_msgs.msg import String\ndef callback(data):\nrospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\ndef listener():\nrospy.init_node('listener', anonymous=True)\nrospy.Subscriber(\"chatter\", String, callback)\nrospy.spin()\nif __name__ == '__main__':\nlistener()\n\nThe function callback() will be called every time a message\nfrom the topic chatter is received.\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fAbout Callback Functions\nRule of Thumb:\nA callback function should be as short as possible\nIf you need to process a large data (e.g., image), the\nprocessing part should not be in the callback function\nThe callback should simply update global variables and let the\nprogram process it when it is ready to process.\n:\nfrom copy import deepcopy\nimage = None\ndef aCallback(data):\nimage = data\ndef mainProgram():\n:\nwhile not rospy.is_shutdown():\n:\ncopyImage = deepcopy(image)\n# process copyImage\n:\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\frostopic\nrostopic is a tool related to topics in ROS environment\nIt allows us to perform the following:\nList the current active topics\nShow messages publish to a specific topic on screen\nPublish a message to a specific topic\nShow information about a specific topic\nShow type of a specific topic\n\nSuppose we run the talker and listener node using the\nfollowing commands in three separate console screen:\nroscore\nrosrun mypackage talker.py\nrosrun mypackage listener.py\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\frostopic\nUse\nrostopic list\n\nto list the active topics on the console screen.\n\/chatter\n\/rosout\n\/rosout_agg\n\nUse\nrostopic echo chatter\n\nto show messages published to the topic chatter\ndata: hello from talker 1463668746.93\n--data: hello from talker 1463668747.03\n--data: hello from talker 1463668747.23\n--data: hello from talker 1463668747.33\n--:\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\frostopic\nUse\nrostopic info chatter\n\nto show information about the topic chatter\nType: std_msgs\/String\nPublishers:\n* \/talker_4544_1463668570440 (http:\/\/r2-d2:56659\/)\nSubscribers:\n* \/listener_4723_1463668575785 (http:\/\/r2-d2:35408)\n\nUse\nrostopic type chatter\n\nto show the type of the topic chatter\nstd_msgs\/String\n\nTo publish the string \"Blah Blah Blah\" to the topic\nchatter\nrostopic pub \/chatter std_msgs\/String \"Blah Blah Blah\"\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\fWorkspace Directory\n\nYour workspace directory is\n\/home\/[username]\/cs1567\/src\/mypackage\n\nAll Python program should be in\n\/home\/[username]\/cs1567\/src\/mypackage\/scripts\n\nor any subdirectory under the above directory.\nTo quickly change to ..\/mypackage directory, you can simply\ntype\nroscd my[Tab]\n\nwhere [Tab] is the Tab button on your keyboard.\n\nThumrongsak Kosiyatrakul\n\nPublisher and Subscriber\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":242,"segment": "unlabeled", "course": "cs1622", "lec": "lec09","text":"Typing\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n● I may hold some Extra Office Hours on Friday… what time would be\nbest for those of you who think you may need them?\n\n2\n\n\fA little Type Theory\n\n3\n\n\fRemember what a type is?\n● a type is:\no a set of valid values, plus:\no a set of valid operations on those values\n● think about ints:\no there's a range of valid values of an int variable\no and a set of operators and other things you can use on them\n● type theory is a branch of mathematics\no it might seem kind of abstract and confusing (as most math does)\no but it has had profound impacts on the design and behaviors of\nprogramming languages, especially in the past 30 years\n● without rigorous type theory, you get C 😬\n\n4\n\n\fI'll Curry YOUR Howards, isomorphically\n● remember from the beginning of the term: programs are proofs.\n● type systems are essentially proof justifications.\nthese justifications are saying,\n\"look, I'm using only valid\noperations on these values!\"\nsimilarly, when you say \"this\nvariable is an int,\" the compiler\nuses that information to justify\nthe steps of your program.\nif it can't… that's a type error!\n5\n\n\fType Safety = Progress + Preservation\n● type safety means that you know exactly what will happen at every\nstep of your program, because you know the types of everything.\nprogress means that at every step of computation:\n…or there is exactly one\nthing that can happen.\neither the program halts…\n\nint[] a = { 1, 2, 3 };\na[4] = 10; \/\/ halts\n\nint[] a = { 1, 2, 3 };\na[0] = 10; \/\/ assigns\n\npreservation means that you know the type of every value, at\nevery step of computation. (you never \"leave the walls\" of the type system.)\nJava is a type-safe language, so I can't give an example of that.\nbut what about the most popular language used for programming our operating\nsystems, device drivers, embedded device firmware etc.? 🙃\n\n6\n\n\fA very unsafe language\n● in C, the type system was an afterthought.\n\nfloat f = 3.4;\nint a[10];\na[10] = 0xDEADBEEF;\nprintf(\"%f\\n\", f);\n\nthis line of code violates both\nprogress and preservation.\n\nit violates progress because accessing an array out-of-bounds is\n\"undefined behavior\" (UB), which means \"anything could happen.\"\nit violates preservation because one of the things that could happen\nis that this line overwrites the value in f, violating its type.\ndepending on the platform and compilation settings, this printf\nmight show 3.4 or -6259853398707798016.000000!\n7\n\n\fSafety has to be built-in\n● once you step into UB, you cannot guarantee anything.\no it's like writing a proof with an incorrect step.\no all the steps after may or may not be invalid!\no HLLs are a set of abstractions, and type-unsafety lets you ruin\nthose abstractions entirely.\n● you can't make C safe. this has big consequences:\no you can't do automatic memory management, because that\nrequires you to know the types of every value in the program.\no you can't prove that a C program is correct, because there are\nmany \"correct\" programs which actually violate type safety.\n● you can't be superhuman. sorry, but no amount of \"just write the\ncode correctly\" is going to lead to correct C code. mistakes happen.\no this is why languages like Java and Rust exist!\n8\n\n\fDecidability\n● decidable means: \"can this question be answered in finite time?\"\n● type-checking is an algorithm, so it can be undecidable.\no we can make our type system so powerful that we'd have to solve\nthe halting problem to check our program's types!\n▪ which maybe seems like a bad thing, right?\n● there's this other related thing that I think is related to decidability?\no you can't predict the future.\no that means sometimes you can't know what type something is\nuntil runtime, no matter how clever your type system is.\n● so if you want programmers to have that (very useful!) ability, you\nhave to design your type system to allow for it in a safe way.\no we'll see some examples shortly.\n\n9\n\n\fType systems\n\n10\n\n\fThe primitive types\n● a language's problem domain is the problems it's designed to solve.\no most languages try to be \"general purpose\" and therefore give you\nrelatively few, simple primitive types, like int, char, bool, etc.\n● the hardware also dictates what primitives are available.\no e.g. on GPUs, 4-element vectors and 4x4 matrices are primitives!\n● domain-specific languages (DSLs) are designed to do one job well.\no these might have many primitive types, including types which don't\nseem very \"primitive\" at all.\no for example, a shell scripting language may have primitive types\nlike files, processes, and terminal emulators.\n● the set of primitive types is really a product of what problems you\nwant your language to be able to solve easily.\n\n11\n\n\fStatic vs. Dynamic typing\n● remember: static =\"before runtime;\" dynamic = \"at runtime.\"\nstatically typed languages\nassociate types with variables.\n\ndynamically typed languages\nassociate types with values.\n\nlet mut a = 10;\na = 20;\n\/\/ ok\na = \"hello\"; \/\/ bad\na = a \/ 2;\n\/\/ ok\n\nlet a = 10; \/\/ JS\na = 20;\n\/\/ ok\na = \"hello\"; \/\/ ok!\na = a \/ 2;\n\/\/ bad\n\nin dynamically typed languages, type errors\ncannot be found until you run the program.\nbut this is a spectrum: many statically-typed\nlanguages include dynamically-typed features too!\n12\n\n\fDynamic types in static languages\n● sometimes it's useful not to know the type until runtime.\nclass A { String toString() { return \"A\"; } }\nclass B { String toString() { return \"B\"; } }\n\nObject o;\nif(user types A)\no = new A();\nelse\no = new B();\no.toString();\n\nthis works because of Java's subtyping\nrules and virtual methods.\n\ncalling toString() is a valid operation on any\nObject, and every class instance is an Object.\nthis is a form of existential polymorphism: we\ndon't know what type it is until runtime, but we\nat least know that it will support this operation!\n13\n\n\fType coercion (\"punning\") and conversion\n● type coercion is when the language will let you use the \"wrong\"\ntype and it will automatically turn it into the \"right\" one.\n● type conversion creates a new value of a different type.\no some type coercions do a type conversion; some don't!\nimplicit conversions like these can\n\/\/ Java\nmake code shorter and more readable…\nint x = 10;\nbut they can be confusing too:\nfloat f = x;\n\/\/ now f == 10.0\n\/\/ JavaScript\nint x = 10;\nlet x = 10 + \"20\";\nString s = \"x = \" + x;\nlet y = 10 – \"20\";\n\/\/ now s is \"x = 10\"\n\/\/ x == \"1020\"\n\/\/ y == -10 ?!\n14\n\n\fStrong vs. Weak typing\n● different people use “strong” and “weak” to mean different things.\none definition is basically the\nsame as type safety: it says\nthat weakly-typed languages\nlet you “step around” the type\nsystem, while strongly-typed\nlanguages keep you in.\n\/\/ C\nbool b = true;\nint* p = (int*)&b;\n*p = 700; \/\/ wat.\n\nanother definition is that weaklytyped languages do lots of\nimplicit conversions and almost\nnever give type errors, instead\nchoosing to “do something else.”\n\/\/ JavaScript again\nalert([] + []); \/\/ \"\"\nalert({} + {}); \/\/ NaN\nalert({} + []); \/\/ 0\nalert([] + {}); \/\/ {}\n\nI like to think ”weakly-typed” means “the language doesn’t\ncare about types, for some value of ‘doesn’t care’” lol\n\n15\n\n\fThe typing alignment chart\n● if we treat strong\/weak and static\/dynamic as axes on a spectrum…\nstrong\nPython\nRust Fortran\nJava\nTypeScript\ndynamic\n\nstatic\n\nC++\n\nJS\n\nPHP\nweak\n\nC\n16\n\n\fType inference\n● type inference automatically determines what type something is.\n● you may not know it, but you've been using it for years.\n\nint x = 10;\nint y = x + 5;\n\nthe compiler implicitly determines the\ntypes of the parts of the expressions, and\nwhich addition operation to do.\n\nimagine having to write this…\n\nint x = 10:int;\nint y = x:int +:int 5:int;\ntype inference can go further, too, like variables in Rust!\n\nlet x = 10;\n\/\/ x: i32\nlet y = Some(x); \/\/ y: Option<i32>\n17\n\n\fDefining Type Systems\n\n18\n\n\fTerms and types\n● a term is a thing that has a value.\no it's either a value itself, or it can be evaluated to get one.\n▪ (that sounds a lot like an expression…)\n● every value has a type, and therefore every term has a type.\nPL theory writes this relation like so:\nterm:Type\nlike:\nx:i32\nand now you know why Rust uses this syntax.\n\n19\n\n\fInference rules\n● a type system's inference rules explain how to give a type to a term.\n● some rules are axiomatic: they are the \"ground truth\" of the types.\n● others say how to determine the type of a more complex term.\nhere's an axiomatic rule that says all\ninteger literals are of type int:\n\nand here's a rule for\ninteger addition:\n\n<IntExp>:int\n\nt:int u:int\nt + u :int\n\n(where IntExp is the kind of AST\nnode that represents integer literals.)\n\n\"if t is an int and u is an int,\nthen (t+u) is an int.\"\n\nour type checking algorithm will be driven by these rules.\nif no rule can be applied, it's a type error!\n20\n\n\fThe type context\n● in addition to the axiomatic rules, we also need some context to\nassign types to some terms, like variables.\n\nint x = 10;\nhow do we know what type x is?\nint y = x + 5;\n\nwe have to look back to its declaration.\n\nfortunately, name resolution has already\ndetermined which symbol x refers to!\nthe type context is another mapping: it associates\nsymbols in the symbol table with types.\nhere, int x explicitly associates x with the type int…\nbut in languages with type inference, the type of the\ninitializer would have to be determined first.\n21\n\n\fTruss's type system\n● let's examine the type system of this toy language to get familiar\nwith how these concepts play out.\n● first, it has a set of four primitive types.\no int: a signed 32-bit integer\no bool: a truth value, true or false\no string: an immutable sequence of 0 or more codepoints\no (): the absence of a type, called void\n▪ this is only used for function return types.\n● it might not look like it, but we've sort of implicitly defined the sets\nof valid values for each type.\no there are 232 valid values for int…\no 2 valid values for bool…\no and so on.\n22\n\n\fFunction Types\n● any language that has functions will also need a type for them.\n● or to be more accurate, a type constructor for them.\nTruss writes its function types like: fn(A0, A1, …): R,\nwhere A0, A1 etc. are the arguments and R is the return type.\nfunctions can have any number of arguments, which is\nwhy the argument types are written like that.\n\nhere are some valid function types:\nfn(): ()\nfn(int): int\nfn(string, string): bool\nfn(fn():()): ()\n\nthe last one is a higher\norder function: it takes a\nfunction as an argument!\n\nTruss also has structs, but we'll come back for those later…\n\n23\n\n\fInference rules\n● we need rules for every operation in the language. examples:\nt:int u:int\nfor any op in { +, -, *, \/, % }\nt op u :int\nt:string u:string\n(this is for string concatenation!)\nt + u :string\nt:int u:int\nfor any op in { ==, !=, <, <=, >, >= }\nt op u :bool\nt:bool u:bool\nfor any op in { and, or }\nt op u :bool\nf:fn(A𝟎 ,A𝟏 ,… ):R a𝟎 :A𝟎 a𝟏:A𝟏\nf(a𝟎 , a𝟏, …):R\n\n…\n\nthis says, \"the type of a function call is the function's return type.\"\n24\n\n\fWhat about statements?\n● statements have gotten left out of the discussion, because they\ndon't evaluate to a value, and therefore have no type.\n● but statements can interact with the type system nonetheless.\nwhat type does the condition have to be, in Java?\n\nif(x < 10)\nprintln(\"yep\");\n\nboolean.\n\nsimilar things apply for loops, switches (and Rust matches),\nreturn statements, and so on.\n(we could also say that statements have a type like () or void,\nbut it isn't strictly necessary to do this.)\n25\n\n\fImplementation Details\n\n26\n\n\fData structures needed\n● we'll need to represent types in the compiler.\no this can be as simple as a Rust enum, very similar to how we\nrepresent the kinds of AST nodes.\n● we'll also need the type context like we said before.\no we said we needed this to map from symbols to types…\no but we're going to extend this to all AST nodes that can have\ntypes, because subsequent compiler passes will need this info.\no this will get filled in as we do type checking.\n● finally, we need a way to represent the terms…\no wait, don't we have that already?\n\n27\n\n\fOH, right, the AST!\n● the AST encodes terms as trees.\n● this implies a relatively straightforward way to check for types:\n1. do a depth-first traversal\nto typecheck the leaves,\nusing the axiomatic rules\nand type context;\n2. typecheck the internal\nnodes according to the\ntypes of their children,\nusing the inference rules.\n\nCallExp\n\ncallee\n\nargs[]\n\nIdentExp\nname\n\nf(y+2)\n:()\n\nAddExp\n\n\"f\"\n\nlhs\n\nrhs\n\ny+2\n:int\n\nf:fn(int):()\nIdentExp\nname\n\n\"y\"\n\ny:int\n\nIntExp\nvalue\n\n2\n\n2:int\n\n28\n\n\fImplementing the inference rules\n● the inference rules are easier to implement than they might seem.\n● most of the checks boil down to: is this type equal to that type?\n● for instance in this rule:\nt:int u:int\no\nfor any op in { ==, !=, <, <=, >, >= }\nt op u :bool\n● if I am looking at an AST node for a ==, the check goes like this:\n1. check that LHS's type equals int, and give an error if not.\n2. check that RHS's type equals int, and give an error if not.\n3. set this == AST node's type to bool in the type context.\n● that's it. some of the rules are more involved, but it's rarely much\nmore complex than just checking type equality.\n\n29\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":243,"segment": "unlabeled", "course": "cs1622", "lec": "lec07","text":"Semantic Analysis\nCS\/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n●ö\n\n2\n\n\fSemantics\n\n3\n\n\fWhat does it all MEAN\n● semantics means… meaning.\n● a language's semantics are the rules that make it that language.\n\nint x = 10;\nprintf(\"x = \" + x);\n\nthis code lexes and parses exactly\nthe same in both C and Java.\n\nbut the semantics differ wildly.\n\nin Java, this says: \"create a new string object that is the\nconcatenation of \"x = \" and the result of calling x.toString().\"\nin C, this says: \"calculate a pointer starting at the address of \"x = \"\nand adding the value in x times the size of one char.\"\nthe semantics define what each piece of the language does.\n4\n\n\fStatic vs. Dynamic\n● you'll see these words used a lot in talking about semantics\n● if we look at a timeline of the process of compiling and running…\nexecution begins here!\ncompilation, linking, loading…\n\nprogram is running\n\nstatic means something that is\ndone before execution begins,\nand it's only done once.\n\ndynamic means something that\nis done during execution,\npossibly over and over.\n\nyou'll also hear it called\n\"compile-time.\"\n\nyou'll also hear it called\n\"runtime.\"\n\n(or \"link-time\" or \"load-time\")\n\n5\n\n\fAbstraction\n● HLLs provide programmers with useful sets of abstractions.\n● semantics are about enforcing the rules of those abstractions…\no even if the target CPU doesn't make a distinction!\n\nint i\n= 0;\nObject o = null;\nchar c\n= '\\0';\nboolean b = false;\n\nin this Java, all four lines here will\nprobably compile down to the\nexact same code, something like:\n\nsw\n\n$zero, i\n\nthe CPU doesn't know or care about the difference\nbetween these types, but the language does.\n6\n\n\fCompile-time and runtime guarantees\n● the compiler is responsible for enforcing many of the language's\nrules, but those rules can extend beyond the compiler's reach!\nint[] a = new int[5];\na[10] = 0; \/\/ oops\n\na Java compiler might be able to\ncatch this contrived case.\n\nbut in the general case, this error can't be detected until runtime.\nstatic checks done by the compiler try to\ncatch as many mistakes as possible…\n…but they can't catch every mistake. doing so\nwould require solving the halting problem.\nstill, doing what static checks we can is better than\nleaving all the semantic analysis until runtime!\n7\n\n\fSemantic analysis can get tricky\n● in most nontrivial languages, semantic analysis is not just \"one step.\"\no there may be several phases or passes of semantic analysis…\no and sometimes, those passes can be run multiple times, or be\nmutually recursive with one another!\n● the toy language we're working with won't get too scary to analyze…\no but it's good to keep in mind that even seemingly-simple features\ncan cause the complexity of semantic analysis to explode.\n● one of the most powerful tools we have to define the rules and\nboundaries of a language is…\n\n8\n\n\fTypes\n\n9\n\n\fTypes and typing\n● HLLs give us several kinds of values to work with.\n● we classify these values according to their type.\na type is a set of valid values…\n\n{ 0, 1, -1, 2, -2, ... }\n…along with a set of operations that can be performed on them.\n\n{ +, -, *, \/, %, parse, ...}\ntypes set the boundaries on what you\ncan and can't do within the language.\n10\n\n\fYou have to listen to the notes they don't play\n● down at the CPU level, types don't really exist.\no you can do almost anything with anything! the CPU doesn't care!\n● we say that assembly is untyped.\nli\nsw\nli\njr\n\nt0, 10\nzero, (t0)\nt0, 'a'\nt0\n\n# oh, t0 is an int\n# wait we're using it as an address?\n# uhh now it's a character?\n# and now we're jumping to it??\n\n● in an untyped language, the programmer can do anything.\no but is that really a good idea…?\n\n11\n\n\fA vast sea of garbage\n● a program is any sequence of instructions.\n● so if we consider all possible sequences of instructions…\nalmost all of them do nothing useful at all.\nthis is Useful Program Island.\nType System\nTown\n\n×\n\ntype systems keep you\nsafely on the island…\n…at the cost of preventing you from being able\nto write all useful programs. like this one.\nbut maybe these kinds of programs are too hard for\nhumans to understand anyway, so no big loss?\n12\n\n\fThe soul of a programming language\n● the type system affects every operation in the language.\n● the type system decides…\no what kinds of variables can exist\no what operators and methods you can use on them\no how those operators do their work\no how control structures work\n● the main difference between C and Java is their type systems.\no even though their syntax can look superficially similar…\no the boundaries drawn by their type systems are completely\ndifferent.\n\n13\n\n\fType systems\n\n14\n\n\fPrimitive types\n● every language considers some types to be primitive, or\nfundamental. they cannot be broken into any simpler parts.\nsome, like ints and floats, are lw $t0, int_var\nderived from the CPU's abilities. l.s $f0, float_var\n\nothers, like bools and chars, are simple\nabstractions on top of those…\nbut they help us encode intent into our\nprograms, and intent helps us better understand\nour own code (and the code that others write).\nint isDone = 0; \/\/ ??\n\nbool isDone = false; \/\/ better.\n15\n\n\fCompound types\n● compound types are composed of two or more primitive types.\no you can think of primitive and compound types like atoms and\nmolecules: multiple atoms make up a molecule.\nstruct Point {\nx: i32,\nstructs and classes are common ways to let the\ny: i32,\nprogrammer define their own compound types.\n}\nlet p: (i32, i32) = (4, 5);\ntuples are a little obscure, but work similarly to structs.\nlet a: [i32; 3] = [1, 2, 3];\n\narrays also count, but they're a little more interesting…\n16\n\n\fGeneric types\n● a generic type is a special kind of function which takes types as its\ninputs, and produces a type as its output.\no we call that a type constructor.\n● if that sounds really out there, well, how about ArrayList?\no it takes type arguments in angle brackets: ArrayList<Integer>\n● and if you give it different type arguments, you get a different type.\no ArrayList<Integer> is different from ArrayList<Double>.\n● another example in Java is the square brackets: []\no you can't just have a []. it has to be an array of something.\n● you can put any type before [] and get a new type, so it's a type\nconstructor.\no int[] is a new type. int[][] is another new type. and so on,\nforever!\n17\n\n\fAnd further still…\n● there are even type constructors that can take values as arguments.\n● there's a very concrete example for that: arrays\no see, an array has a length\no but… what if that length were part of the type? like int[10]?\n● could you make a language where you could never have an\nArrayIndexOutOfBoundsException?\no yes!\no it's possible to track the length of every array and the range of\nevery integer at compile time, and ensure they can't happen.\n● but there's a big downside to this:\no in the general case, these type systems are undecidable\n(meaning you have to solve the halting problem).\n▪ but in some limited cases, it is decidable, and maybe useful?\n18\n\n\fNaming\n\n19\n\n\fHello, my name is…\n● computers don't know anything about names, language, or strings…\no but humans love words. we can't get enough of em.\n● one of the reasons people very quickly invented assembly language\nwas to be able to name things in their programs, because dealing\nwith addresses is confusing, tedious, and error-prone.\no can you imagine having to refer to everything by a number?\no and where changing one part of the program changed the\nnumbers of everything after it, and so you had to go back and\nchange all the references to all those numbers…\n▪ it was not doable.\n\n20\n\n\fScoping and name resolution\n● name resolution matches names to the things they refer to.\n● scoping determines which names are candidates during resolution.\nvoid main() {\nint m = 5;\nf();\n}\n\nyou know that this is wrong, but not\nevery language works like this!\n\nvoid f() {\nprintf(\"m = %d\\n\", m);\n}\n\nin some languages, you can\naccess the local variables of any\nfunction on the call stack.\n\nthis used to be somewhat common in older languages, but eventually we realized \"wow that's confusing as hell\"\n\nwe've kind of settled on a common set of name resolution\nrules, but some languages can still surprise you.\n21\n\n\fStatic vs. dynamic name resolution\n● some names' referents can't be decided until runtime.\nclass A { String toString() { return \"A\"; } }\nclass B { String toString() { return \"B\"; } }\nObject[] objs = new Object []{ new A(), new B() };\nfor(Object o : objs)\nSystem.out.println(o.toString());\nwhich toString() does o.toString() refer to?\n\non the other hand, all the class names,\nvariable names etc. are statically resolved.\nin some languages, all names are dynamically resolved.\n22\n\n\fRound and round\n● name resolution can create a complex or even cyclical graph of\ndependencies between the parts of a program.\nclass A {\nvoid print(B b) { println(b.value()); }\nint value()\n{ return 10; }\n}\nA depends on B, and B depends on A.\nclass B {\nvoid print(A a) { println(a.value()); }\nint value()\n{ return 20; }\n}\nI can't check that all of A's code is correct until I check B's…\nbut I can't check that B's is correct until I check A's…???\n\nwell. it's possible to do things in phases, like I said before.\n23\n\n\fScoping\n\n24\n\n\fWhat is it?\n● the programmer can declare named things like variables, etc.\n● the scope is where those names can be seen in the program.\nif(x == 10) {\nint y = 20;\nprintf(\"%d\", y); this is alright!\n}\n\nwell, that's how you\nlearned it. not every\nlanguage has the\nsame rules.\n\nprintf(\"%d\", y); this is not.\n\n# Python\nif x == 10:\ny = 20\nprint(y) # works!\nprint(y)\n\n# works!\n25\n\n\fStatic vs. dynamic\n● the majority of languages use static scope: the syntactic structure\nof the language defines where a name can be seen.\n● Python and many other dynamically-typed languages are instead\ndynamically scoped: the variable is not looked up until runtime.\ndynamic scope can lead to really\nconfusing problems; the same piece\nof code can either work or not work\nbased on what variables are\nf()\n# runtime error!\ndeclared around it.\ng = 10 # g is a global\nf()\n# prints 10\n# Python\ndef f():\nprint(g)\n\nbut I think most dynamic languages do it like\nthis cause it's easy to implement!\n26\n\n\fShadowing\n● names aren't always unique.\n● sometimes this is clearly a mistake, but in other cases…\nclass A {\nin this case we say that the local\nint x;\nvariable shadows the member variable.\nA(int x) {\n\/\/ which x?\nx = 5;\n} this refers to…\n}\nx\nthat argument.\n\n27\n\n\fShadowing isn't always a mistake\n● sometimes it just makes things more convenient.\no if you write a piece of code, and then change the environment\naround it (like adding a global), you don't want it to suddenly\nchange behavior or get new compiler errors!\n● or in Rust, where variables are immutable by default, this is a\ncommon pattern:\nlet ast = parse(lex(input)?)?;\nlet types = typecheck(ast)?;\nlet ast = const_fold(ast, types)?;\nlet ast = inline_calls(ast, types)?;\ncodegen(ast, types)?;\n● subsequent declarations of ast shadow the previous ones.\n\n28\n\n\fEvaluation and Execution\n\n29\n\n\fEvaluation and Side Effects\n● evaluation means figuring out the value that an expression has.\no it might be trivial, like for literals: 10 evaluates to… 10!\n● but it might require execution of an unknown amount of code…\n● and in the process, that code may also produce side effects:\nchanges to things outside of the code's local variables, like:\no modifying global variables\no modifying objects that were passed to it by reference\no performing input or output\no making your computer explode\n● side effects make it hard for us to reason about code…\no they can cause the same piece of code to do different things\ndepending on how many times you run it.\n● but without side effects, code can't do anything useful!\no imagine having no input or output!\n30\n\n\fEvaluation Order\n● here's some Java code:\nint f(int x) { println(\"got \" + x); return x; }\nint add(int a, int b) { return a + b; }\nprintln(\"3 + 5 = \" + add(f(3), f(5)));\n● which one does it print?\ngot 3\ngot 5\n3 + 5 = 8\n\ngot 5\ngot 3\n3 + 5 = 8\n\nwe have an expectation that code executes left-to-right, so\nwe expect this left one… and this is correct. for Java.\nevaluation order is defined by a language's semantics, and Java\nhappens to define this order, but not all languages agree!\n31\n\n\fEager vs. Lazy\n● a less common distinction is eager versus lazy evaluation.\nint x = someLongComputation();\nSystem.out.println(\"done.\");\nSystem.out.println(\"x = \" + x);\n● in the above Java code, someLongComputation() is called, and its\nreturn value is assigned into x after it returns.\no this is eager evaluation, in which the function call occurs at the\npoint where you write it.\n● but another way is lazy evaluation, in which the variable declaration\ndoesn't call the function, the use of x on the last line does.\no that might sound bizarre, but is actually very popular in functional\nlanguages, where side-effect-free functions are the norm!\n● eager evaluation is by far the more popular strategy today, but…\n32\n\n\fYou never knew you were using it\n● in this code:\nif(condOne() && condTwo()) { println(\"woo\"); }\n● if condOne() returns false, does condTwo() ever run?\no nope.\n● this is lazy evaluation.\no in all C-style languages, && and || lazily evaluate their second\noperand: they only execute that code if needed.\n● lazy evaluation can be really useful when using higher-order\nfunctions (passing functions to other functions), too.\n● so don't write it off as academic nonsense!\n\n33\n\n\fLanguages without\nSemantic Analysis?\n\n34\n\n\fWho needs semantic analysis anyway\n● depending on how the language is designed, we may not have to\ndo any semantic analysis in the compiler.\ndef test():\nPython is a dynamically-typed language.\nx = 10\nx = \"hello\" that means the semantic rules of its type system\nare checked at runtime, as the code executes.\nx = print\nx(\"hi!\")\nthis is totally valid Python code, but there's no\ntest()\nway for the compiler to tell. so, it… doesn't.\ncompilers for dynamic languages can be pretty\nsimple: lex, parse, and then… well, right to execution!\n35\n\n\fExecuting the AST\n● we saw this already with the ast_math example!\no once you build the AST, you just write some recursive methods to\nvisit all the AST nodes in order, evaluating them as you go.\no it's not hard to extend this concept to AST nodes for loops,\nconditionals, function calls etc.\n● this is an AST interpreter, and is a really quick-and-dirty way to get\nsomething running.\no from what I understand, Ruby used exactly this execution strategy\nfor over 10 years!\n● but as you might imagine, it is not fast at all.\no consider all the interpreter code that needs to run just to, say, add\na couple numbers together.\n● but this topic is better left for after the midterm…\n36\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":244,"segment": "unlabeled", "course": "cs1622", "lec": "lec03","text":"Lexing and Grammars\nCS\/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n● project 1 is out if you didn't see the announcement(s)\no remember it's due by this Saturday evening\no or Sunday for late credit (-10%)\n● finally today we're getting into the COMPILER STUFF!\n● couple new examples in the examples repo for today too!\n\n2\n\n\fLexing\n\n3\n\n\fWhat is it?\n● when a compiler reads the source code, it's just a long string.\n● lexing (or \"scanning\") is the process of splitting that string into small,\nmeaningful pieces in order to simplify the next step, parsing.\n\n\"3*x + (y \/ 1.9)\"\n\nthe lexer's input is\nthe source code.\n\nLexer\n\nIntLit(3, Dec), Times,\nId(\"x\"), Plus, ...\n\nthe lexer's output\nis a list of tokens.\n\neach token is like one \"word\" in the source\nlanguage – the smallest unit of meaning.\n4\n\n\fTokens, and… not-tokens\n● tokens have meaning to the programming language. but not\neverything you type in the source code is meaningful.\nif x < 10 {\nf();\n}\nif x<10{f();}\nif x < 10\n{\n\/\/ call\nf ( );\n}\n\nall three of these would lex to the same\nsequence of tokens. but what differs?\nspacing and indentation – together called\nwhitespace – is meaningless in most modern\nlanguages, and the lexer strips it out.\ncomments are another common kind of\nmeaningless text that can be ignored.\n5\n\n\fNot-so-whitespace\n● whitespace can't be ignored in all situations, though.\n\"hello, world\"\n\"hello,\nworld\"\n\nthese are different strings, right?\n\nso we have to remember whether or not we're inside \"quotes\".\nalso, fortress and for tress mean different things.\nPython uses indentation to structure code, instead of { braces }.\nif x < 10:\nf()\nprint(\"done\")\n\nthe print call is outside of the if. the only thing\nthat indicates that is the indentation.\n\nand JavaScript (aka ECMAScript) does weird things with newlines\nso you can avoid writing semicolons…\n6\n\n\fWhere are we?\n● a secondary (but super important) job of the lexer is to produce\nlocation info so the compiler can give good error messages.\nline 1, column 1\n\n1:13\n\nline 1, col 4\n\n1:19\n1:16\n\nif num_cats == 10 {\nprintln(\"yay!\");\n2:5\n\n2:12\n2:13\n\n2:19\n2:20\n\nthe line number is\nbased on how many\nnewlines it's seen.\nthe column number is\nbased on how many\ncharacters it's seen\nsince the last newline.\n\nthis information can be carried forward through the\nrest of the compilation process for a number of uses.\n7\n\n\fLexing from Intuition\n\n8\n\n\fGetting a feel for it\n● a lot of problem-solving works by getting a feel for what a solution\nmight look like, then formalizing that intuition into an algorithm.\n● let's start by looking at what our input language looks like.\nstruct S {\nx: int,\ny: bool,\n}\n\nthis is the toy language we'll be using for\nprojects and examples, called Truss.\n\nfn main() {\nprintln_s(\"hi!\");\nlet s = new S();\ns.x = 10;\ns.y = true;\nprintln_i(s.x);\n}\n\nit has similarities to both Rust and Java,\nbut is a lot simpler in many ways.\nwhat are some kinds of tokens you\ncan pick out from this example?\n\n9\n\n\fCommon classes of tokens\n● most languages today use similar rules for their classes of tokens.\nthere are tokens that look like words.\nstruct int\nreturn while\n\nsome have special meaning in the\nlanguage. these are called keywords.\n\nS num_cats\nprintln main\n\nothers are written by the programmer to\nname things. these are called identifiers.\n\nthere are often\nmany symbols.\n\nand there are literals: a way of embedding\nconstant values directly into your code.\n\n+\n\n-\n\n* \/\n\n=\n\n. , ; { } ==\n&& || +=\n\n\"hello,\\nworld!\"\n'c'\ntrue\n0xDEADBEEF\n1.9e6\n345\n10\n\n\fA first attempt\n● we might come up with some simple rules for these tokens:\no identifiers are a sequence of letters, underscores, and digits.\n▪ e.g. x, x2, _lift, THIS_IS_A_TEST, fort, o_o\no keywords are a fixed subset of identifiers.\n▪ e.g. if, else, int, private, for\no symbols are a fixed set of sequences of symbol characters.\n▪ e.g. +, +=, =, ==, &, &=, &&\n● but then we get to the literals, and it gets harder.\no here are some floats: 1.2, 10e9, 6.28E+23, 4., 5f\no do you think you could come up with a concise rule for that?\no and then strings… \"hello\\nworld\"\n● oh, but it gets worse!\n\n11\n\n\fWe keep running into each other\n● let's look at some awkward situations!\n\nabc123\n123abc\nx.y\n4.y\nx.5\n4.5\n1.2.3\n\nabc123\n123abc? or 123, abc? or error?\nx, ., y\n4, ., y or 4., y ?\nx, ., 5 or x, .5 ?\n4.5 or 4., 5 or 4, ., 5 ?\nor\nor\nor\nor\n\n1.2, ., 3\n1.2, .3\n1., 2.3\n1., 2., 3\n1, ., 2, ., 3\n\n12\n\n\fuHH\n● even with such a simple set of tokens, we're already running into\nproblems, and it's not entirely clear where we went wrong.\n● well, let's first learn a bit about grammars, which can give us some\ntools to talk about these things more rigorously.\n\n13\n\n\fGrammars\n\n14\n\n\fLanguages and alphabets\n● a language is a set of strings. (most useful languages are infinite sets.)\n● each string is a sequence of symbols chosen from an alphabet.\n● for example, here's a really dumb language and its alphabet:\n\nL = { \"hi\", \"bye\" }\nA = { 'a', 'b', …, 'z' }\n\neach symbol in the alphabet\nis called a terminal.\n\nif we generate strings from this alphabet, it's\nkind of obvious whether or not they are in L.\n\n\"hi\" ∈ L\n\"hii\" ∉ L\n\"cat\" ∉ L\n\n…ok, what about a\nmore complicated\nexample?\n15\n\n\fGrammars\n● for more complex or infinite languages, you need a grammar: a set\nof rules to decide if a string is in the language.\n\nL = { \"a\", \"aa\", \"aaa\", … }\nA = { 'a', 'b', …, 'z' }\n\nwhat is\/are the rule(s)\nfor this language?\n\n\"1 or more 'a's\"\n\nokay. but can we write that grammar rule in a more rigorous way?\n\nL: 'a'+\nthis is a nonterminal. it\nisn't part of the alphabet;\nit stands in for some\ncombination of terminals.\n\nthis + says \"repeat the\nprevious thing 1 or\nmore times.\"\n16\n\n\fA language for specifying languages\n● we usually specify the lexical and syntactic form of a language with a\nmetalanguage: a concise way of specifying rules.\n● there are some common patterns for these rules:\n\nL: A B\nR: A | B\nX: A*\nY: A+\nU: A?\nD: (A B)+\n\nA followed by B. (sequencing)\nan A or a B. (alternation)\n\nzero or more As. (repetition)\none or more As. (repetition… again)\nzero or one A, or \"an optional A\".\nparens can group things together.\n\nin these examples, A and B can be terminals (from the\nalphabet) or nonterminals (names of other rules).\n17\n\n\fLexing a programming language\n● the alphabet is the character set that the source code is in.\no in our case, that's Unicode!\n● the language isn't the entire programming language…\no instead, it's the tokens we want to produce.\n● so, we'll write a rule for each kind of token.\n\nSymbol: '+' | '-' | '{' | '}' | '=' | ('=' '=')\n'quoted' things are terminals, picked from the alphabet.\n\nKeyword: ('i' 'f') | ('e' 'l' 's' 'e') |...\nthat's ugly, but we can make our metalanguage look any way we like.\n\nKeyword: \"if\" | \"else\" | \"fn\" | \"return\"\nthat's better!\n\n18\n\n\fGetting more complex\n● rules can refer to other rules. like here:\n\nId:\nIdStart IdCont*\nIdStart: Alphabetic | '_' | '$'\nIdCont: IdStart | Digit\nDigit:\n'0'|'1'|'2'|'3'|'4'|'5'|'6'|'7'|'8'|'9'\nAlphabetic: (a whole bunch of characters)\nso, an identifier starts with a letter or underscore or dollar sign; and\nthat is followed by zero or more of those, plus digits. neat.\nand we could keep going for all the tokens!\n\n19\n\n\fLiterals!\n● numeric and string literals can get pretty complicated, but let’s keep\nthem simple for now:\n\nIntLit: Digit+\nStrLit: '\"' StrChar* '\"'\nStrChar: <any character except '\"'>\ninteger literals are sequences of digits, and string literals are\nsequences of characters surrounded by double quotes. also it’s fine\nto be a bit hand-wavey like in StrChar, as long as the intent is clear.\nbut: what if the input contains the number\n100000000000000000000? isn’t it too big…?\ndefining the valid ranges of numbers within the metalanguage\nisn’t possible. so we could define that as an additional rule.\n20\n\n\fUh oh, whitespace.\n● whitespace isn't a token, but we do have to deal with it somehow.\n● we can write rules for it, like a real token:\n\nWhitespace: ' ' | '\\t' | '\\n' | Comment\nComment:\n\"\/\/\" CommentChar* CommentEnd\nCommentChar: <any character except '\\n' or Eof>\nCommentEnd: ('\\n' | Eof)non-consuming\nwoah what the heck is that\n\nthis is a lookahead: it checks if the next character is a newline\nor EOF, but it does not make it part of the Comment.\nit feels a little kludgey, but sometimes\nlookaheads are just what you need.\n\njust… don’t use them too much.\n\n21\n\n\fWrapping it up and putting a bow on top\n● once we have a rule for each kind of token, we can do this:\n\nToken:\nSymbol | Keyword | Id | IntLit | StrLit\nProgram: (Whitespace? Token)* Whitespace? Eof\nhere, Program is a special rule: it's our top-level or start rule.\nif we want to translate this grammar into code that\nlexes, it's where we start – it's the outermost loop.\n\nbut wait. if we spent all this time writing the\nlexical rules in a formalized, rigorous way…\ncouldn't we just have a program compile\nthe grammar itself into a lexer?\n22\n\n\fCOMPILER COMPILERS 🤯\n● yes, this is A Thing.\n● there are many tools which will take some kind of metalanguage as\nthe input, and produce a lexer program as an output.\no (or a parser, but we haven't gotten to those yet)\n\n● however…\no most are tightly tied to the language whose code they output.\no they're really complex, to be as flexible as possible.\no …but, it can be awkward to fit your grammars into their rules.\no they can give really confusing\/terrible error messages on invalid\nlexer input.\n● and honestly, writing your own lexer is not that complicated.\no and, yanno, it's a compilers course, so you have to do it. (:\n\n23\n\n\fDealing with Ambiguity\n\n24\n\n\fDealing with ambiguity\n● ambiguity is when you have > 1 possible \"correct\" tokenizations.\n● consider these snippets of Java.\n\nif(x == 3) is this [==] or [=, =]?\n3.4\n\nis this [3.4] or [3., 4]?\n\nabc123\n123abc\n\nis this [abc123] or [abc, 123] or [ab, c123] or…?\nwhy do we think this is an error when the above is not?\n\non this line, >> is a right shift…\nint y = x >> 3;\nList<List<String>> l; but here, it's two right angle brackets?\nthere are three ways I can think of to resolve ambiguity.\n25\n\n\fApproach 1: Maximal Munch\n● maximal munch is a strategy that says: always try to make the\nbiggest token possible, going left-to-right.\n\nif(x == 3) this is [==].\n3.4\n\nthis is [3.4].\n\nabc123\n123abc\n\nthis is [abc123].\nthis is an error because we start lexing a number, and\nthen hit a non-numerical character.\nalthough, it doesn't have to be; we could say \"3x\" is another way of writing \"3 * x\", couldn't we?\n\nint y = x >> 3;\nList<List<String>> l; but this still presents an issue…\n26\n\n\fApproach 2: Deal with it later!\n● we can have the parser (the next step) disambiguate things that\nwould be impossible to detect in the lexer.\nlexers are usually specified as regular languages,\nwhich cannot detect arbitrarily nested brackets.\n\nList<List<String>> l;\nthe lexer cannot know that >> is a pair of closing brackets\nwithout knowing that there are two unpaired open brackets.\n\nso, we have the parser say \"oh, I currently have two open brackets;\nthis >> must be a pair of closing brackets and not a right-shift.\"\nbut why bother making things harder for ourselves?\n27\n\n\fApproach 3: \"Doctor, it hurts when I do this\"\n● we could just define our rules so they're unambiguous.\nList!(List!(String)) l;\n\nin the D language, templates\n(generics) use parens instead.\n\nthe parser then knows !( is a \"template open paren\".\nl :: List List String\nm :: List (List String)\n\nin Haskell, generics don't require\nany symbols, but can use parens.\n\nx shr 3\n\nor, we could redefine right shift to\nuse some spelling other than >>.\n\n28\n\n\fReally bad cases of ambiguity\n● if you aren't careful, you can end up with nasty situations.\n\nLexer\n\nParser\n\nSemantic\nAnalyzer\n\nC and C++ have this situation, where you cannot correctly lex\nthe source code until you've done (some) semantic analysis.\n\nthis sucks! it makes the compiler slower,\nharder to write, and harder to reason about.\nfortunately, we have the hindsight of decades\nof language design and can avoid this stuff :)\n29\n\n\fThe examples\n● first, there’s an example showing how Rust’s match statement works,\ncalled rust_match.\n● there's a small example, lexing_toy, that shows how simple it can be\nto make a lexer.\no it only has a few kinds of tokens, and is written in a different style\nfrom the project, cause I don’t wanna give too much away (:\no but it shows the general “shape” of a lexing algorithm.\n\n30\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":245,"segment": "unlabeled", "course": "cs1541", "lec": "lec1.2_technology_advances","text":"CS 1541 Introduction\nTechnology Advances\n\nWonsun Ahn\nDepartment of Computer Science\nSchool of Computing and Information\n\n\fTechnology Advances\n\n\fAdvances in Technology\n◼ Technology has been advancing at lightning speed\n◼ Architecture and IT as a whole were beneficiaries\n◼ Technology advance is summarized by Moore’s Law\n⚫ You probably heard of it at some point. Something about …\n⚫ “X doubles every 18-24 months at constant cost”\n\n◼ Is X:\n⚫ CPU performance?\n⚫ CPU clock frequency?\n⚫ Transistors per CPU chip?\n⚫ Area of CPU chip?\n\n\fMoore’s Law\n\n2X transistors\nevery 2 years!\n\n\fMiniaturization of Transistors\n\nData source: Radamson, H.H.; He, X.; Zhang, Q.; Liu, J.; Cui, H.; Xiang, J.; Kong, Z.; Xiong, W.; Li, J.; Gao, J.; Yang, H.;\nGu, S.; Zhao, X.; Du, Y.; Yu, J.; Wang, G. Miniaturization of CMOS. Micromachines 2019, 10, 293.\n\n◼ Moore’s Law has been driven by transistor miniaturization\n⚫ CPU chip area hasn’t changed much\n\n\fFuture of Moore’s Law\n◼ The semiconductor industry has produced roadmaps\n⚫ Semiconductor Industry Association (SIA):\n1977~1997\n⚫ International Technology Roadmap for Semiconductors (ITRS):\n1998~2016\n⚫ International Roadmap for Devices and Systems (IRDS):\n2017~Present\n\n◼ IRDS Lithography Projection (2020)\nYear of Production\nTechnology Node (nm)\n\n2018 2020 2022 2025 2028 2031 2034\n7\n\n5\n\n3\n\n2.1\n\n1.5\n\n1.0\n\n0.7\n\n⚫ Looks like Moore’s Law will continue into foreseeable future\n⚫ IRDS does not project significant increase in CPU chip size\n⚫ Increases in transistors will come from transistor density\n\n\fIRDS isn’t Perfect\n◼ ITRS (predecessor of IRDS) has made corrections before\n\n⚫ After all, you are trying to predict the future\n⚫ But architects rely on the roadmap to design future processors\n\n\fMoore’s Law and Performance\n◼ Million-dollar question:\nDid Moore’s Law result in higher performance CPUs?\n◼ We will do a Zoom breakout room session\n1. Get to know each other (5 mins):\n⚫ Introduce yourself and say one fun thing you did over winter\n\n2. And then answer the following questions (5 mins):\n⚫ When you decide on a CPU for your laptop, what number(s) do\nyou look at to measure how fast the CPU is?\n⚫ Are CPUs getting faster using that measure?\n\n3. After 10 minutes, we will share discussions with class\n\n\fAre CPUs getting Faster?\nMeasure of Performance\n\nIs it Getting Better?\n\nResponse time (for an app)\n\nDepends (generally increasing)\n\nNumber of cores\n\nGenerally upwards trajectory\n\nPower draw \/ Thermals\n\nGetting lower\n\nClock speed\n\nSame\n\nCache size\n\nIncreasing\n\n\fComponents of Execution Time\n◼ Processor activity happens on clock “ticks” or cycles\ntime\n⚫ On each tick, bits flow through logic gates and are latched\n\n◼ Execution time =\nseconds\n=\nprogram\n\ncycles\nprogram\n\nseconds\nprogram\n\nX\n\n= instructions X\nprogram\n\nseconds\ncycle\ncycles\ninstruction\n\nX\n\nseconds\ncycle\n\n\fImproving Execution Time\ncycles\nseconds\nX\ncycle\ninstruction\nseconds\n◼ Improving cycle :\ncycles\nseconds\n⚫ Clock frequency =\n= reverse of\nsecond\ncycle\n\ninstructions\nX\nprogram\n\n⚫ Higher clock frequency (GHz) leads to shorter exec time\n\ncycles\n◼ Improving instruction :\n\n⚫ Also known as CPI (Cycles Per Instruction)\ninstructions\ncycles\n⚫ IPC (Instructions Per Cycle) =\n= reverse of\ncycles\ninstructions\n⚫ Higher IPC leads to shorter execution time\ninstructions\n◼ Improving program :\n\n⚫ Less instructions leads to shorter execution time\n⚫ ISAs that do a lot of work with one instruction shortens time\n\n\fMoore’s Law and Performance\n◼ Million-dollar question:\nDid Moore’s Law result in higher performance CPUs?\n◼ Law impacts both architecture and physical layers\n\nInstruction Set Architecture\nProcessor Organization\n\nComputer\nArchitecture\n\nTransistor Implementation\n\nPhysical Layer\n\n⚫ Processor Organization: many more transistors to use in design\n⚫ Transistor Implementation: smaller, more efficient transistors\n\n\fMoore’s Law Impact on Architecture\n◼ So where did architects use all those transistors?\n◼ Well, we will learn this throughout the semester ☺\n⚫ Pipelining\n⚫ Parallel execution\n⚫ Prediction of values\n⚫ Speculative execution\n⚫ Memory caching\n⚫ In short, they were used to improve frequency or IPC\n\n◼ Let’s go on to impact on the physical layer for now\n\n\fMoore’s Law Impact on Physical Layer\n◼ CPU frequency is also impacted by transistor speed\n⚫ As well as how many transistors are in between clock ticks\n(which is determined by processor organization)\n\n◼ So did Moore’s Law result in faster transistors?\n⚫ In other words, are smaller transistors faster?\n\n\fSpeed of Transistors\n◼ Transistor 101: Transistors are like faucets!\n\n◼ To make a transistor go fast, do one of the following:\n⚫ Reduce distance from source to sink (channel length) \n⚫ Reduce bucket size (capacitance) \n⚫ Increase water pressure (supply voltage) \n\n\fSmaller Transistors are Faster!\n◼ Transistor 101: Transistors are like faucets!\n\n◼ When a transistor gets smaller:\n⚫ Channel length (channel resistance) is reduced \n⚫ Capacitance is reduced \n\n◼ So, given the same supply voltage, smaller is faster!\n◼ So, did Moore’s Law enjoy faster and faster frequencies?\n\n\fYes, for a while …\n\nExponential\nincrease!\n\nSource: Computer Architecture, A Quantitative Approach (6th ed.) by John Hennessy and David Patterson, 2017\n\n◼ Improvements in large part due to transistors\n⚫ Processor design also contributed but we’ll discuss later\n\n\fBut not so much lately\n\nWhat happened?\n\nSource: Computer Architecture, A Quantitative Approach (6th ed.) by John Hennessy and David Patterson, 2017\n\n◼ Suddenly around 2003, frequency scaling stops\n\n\fDent in CPU Performance\nInflection Point\n\nSource: https:\/\/preshing.com\/20120208\/\na-look-back-at-single-threaded-cpu-performance\/\n\n◼ This caused a big dent in CPU performance at 2003\n◼ Improvements henceforth only came from architecture\n⚫ From improvements to IPC (instructions per cycle)\n\n\fSo What Happened? TDP.\n◼ TDP (Thermal Design Power):\n⚫ Maximum power (heat) that CPU is designed to generate\n⚫ Capped by the amount of heat cooling system can handle\n⚫ Cooling system hasn’t improved much over generations\n\n◼ CPU Power = A * N * CFV2 must be < TDP\n⚫ A = Activity factor (% of transistors with activity)\n⚫ N = Number of transistors\n⚫ C = Capacitance\n⚫ F = Frequency\n⚫ V = Supply Voltage\n\n◼ What happens to each factor with Moore’s Law?\n\n\fTDP and Moore’s Law\n◼ CPU Power ∝ A * N * CFV2 with Moore’s Law\n⚫ A = Activity factor\n⚫ N = Number of transistors (∝ 1\/transistor size2)  \n⚫ C = Capacitance (∝ transistor size) \n⚫ F = CPU frequency (∝ 1\/transistor size) \n⚫ V = CPU Supply Voltage\n\n◼ Decrease in C cannot offset increases in N and F\n⚫ Power increases quadratically with reductions in transistor size\n⚫ That means F (frequency) needs to be decreased to meet TDP\n\nQ) So how did CPU frequency keep increasing up to 2003?\nA) By maintaining power through reductions in Voltage \n\n\fDennard Scaling\n◼ By reducing CPU Supply Voltage ∝ transistor size\n◼ CPU Power ∝ A * N * CFV2 with Moore’s Law\n⚫ A = Activity factor\n⚫ N = Number of transistors (∝ 1\/transistor size2)  \n⚫ C = Capacitance (∝ transistor size) \n⚫ F = CPU frequency (∝ 1\/transistor size) \n⚫ V = CPU Supply Voltage (∝ transistor size)  \n\n◼ Factors balance each other out to keep power constant\n⚫ Note that reducing V (∝ transistor size) has a quadratic effect\n\n◼ Dennard Scaling: Above recipe for scaling up frequency,\nwhile reducing supply voltage to keep power constant\n\n\fDennard Scaling and Vth\n◼ So, it’s that easy? Just reduce V until you meet TDP?\n◼ No, it’s not that simple .\n\n◼ Reducing Vdd (supply voltage) affects CPU operation\n⚫ As Vdd is reduced, CPU becomes slower and slower\n⚫ Eventually, CPU stops working altogether\n\n◼ CPU (specifically transistors) needs redesigning\n⚫ Vth (threshold voltage) needs to be reduced along with Vdd\n⚫ To understand this, we need a 101 on MOSFETs\n\n\fMOSFET 101\n◼ MOSFET (Metal Oxide Silicon Field Effect Transistor)\n[A MOSFET transistor switched off] [A MOSFET transistor switched on]\n\n◼ Gate is switched on when VG reaches a threshold Vth\n⚫ By creating a channel in depletion region through field effect\n⚫ Vth: threshold voltage (minimum voltage to create channel)\n\n\fMOSFET 101\n◼ RC charging curve of VG\nVdd\nVG\nVth\n\nTon\n◼ Speed (Ton) is determined by Vdd if Vth is fixed\n⚫ Vdd is the CPU supply voltage (the water pressure)\n⚫ If Vdd is lower, VG will reach Vth more slowly (low pressure)\n\n\fMOSFET 101\n◼ RC Charging Curve of VG\nVdd\nVG\nVG’\n\nVdd’\nVth\nVth’\n\nTon\n◼ Speed (Ton) is maintained while reducing Vdd to Vdd’,\nonly if Vth is also reduced to Vth’\n\n\fEnd of Dennard Scaling\n\nVth stopped scaling\n\n◼ And around 2003 is when Dennard Scaling ended\n\n\fLimits to Dropping Vth\n◼ Subthreshold leakage\n⚫ Transistor leaks current even when gate is off (VG = 0)\n\n⚫ This leakage current translates to leakage power\n⚫ Leakage worsens when Vth is dropped (related to oxide thickness)\n\n\fLeakage Power across Generations\n◼ Leakage power has increased across technology nodes\n\nSource: L. Yan, Jiong Luo and N. K. Jha, \"Joint dynamic voltage scaling and adaptive body biasing for\nheterogeneous distributed real-time embedded systems,\" in IEEE Transactions on Computer-Aided Design of\nIntegrated Circuits and Systems, vol. 24, no. 7, pp. 1030-1041, July 2005\n\n\fEnd of Dennard Scaling\n◼ Previous power calculation was incomplete\n⚫ CPU power is the sum of both dynamic and leakage power\n\n◼ PowerCPU ∝ Powerdynamic + Powerleakage\n\n⚫ Powerdynamic ∝ A * N * CFVdd2\n⚫ Powerleakage ∝ f(N, Vdd, Vth) ∝ N * Vdd * e-Vth\n⚫ Leakage worsens exponentially when Vth is dropped\n⚫ Catch-22: when dropping Vth, Powerdynamic  but Powerleakage \n\n◼ Vth can’t be reduced further, so Vdd can’t be reduced\n◼ Dennard Scaling relies on reducing Vdd, so it’s the end\n\n\f“Dark Silicon” Rears its Head\n◼ What happens to frequency without Dennard Scaling?\n◼ Powerdynamic (∝ A * N * CFV2) + Powerleakage (∝ N * V * e-Vth)\n⚫ A = Activity factor\n⚫ N = Number of transistors (∝ 1\/transistor size2)  \n⚫ C = Capacitance (∝ transistor size) \n⚫ V = CPU Supply Voltage  (Due to fixed Vth)\n⚫ F = CPU frequency ???\n\n◼ To offset N, you actually have to decrease F\n◼ Otherwise, if you want to maintain F, must decrease N\n⚫ That is, you cannot power on all the transistors at any given point\n⚫ Dark silicon: situation where chip is only partially powered\n\n\fFree Ride is Over\n◼ “Free” speed improvements from transistors is over\n◼ Now it’s up to architects to improve performance\n⚫ Moore’s Law is still alive and well (although slowing down)\n⚫ Architects are flooded with extra transistors each generation\n⚫ But it’s hard to even keep them powered without reducing F!\n\n◼ Now is a good time to discuss technology constraints\n⚫ Since we already mentioned a big one: TDP\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":246,"segment": "unlabeled", "course": "cs1541", "lec": "lec3.1_memory_hierarchy","text":"Memory Hierarchy\nCS 1541\nWonsun Ahn\n\n\fUsing the PMU to\nUnderstand Performance\n\n2\n\n\fExperiment on kernighan.cs.pitt.edu\n● The source code for the experiments are available at:\nhttps:\/\/github.com\/wonsunahn\/CS1541_Spring2022\/tree\/main\/res\nources\/cache_experiments\n● Or on the following directory at linux.cs.pitt.edu:\n\/afs\/cs.pitt.edu\/courses\/1541\/cache_experiments\/\n● You can run the experiments by doing ‘make’ at the root\no It will take a few minutes to run all the experiments\no In the end, you get two plots: IPC.pdf and MemStalls.pdf\n\n3\n\n\fFour benchmarks\n● linked-list.c\no Traverses a linked list from beginning to end over and over again\no Each node has 120 bytes of data\n● array.c\no Traverses an array from beginning to end over and over again\no Each element has 120 bytes of data\n\n● linked-list_nodata.c\no Same as linked-list but nodes have no data inside them\n● array_nodata.c\no Same as array but elements have no data inside them\n4\n\n\fCode for linked-list.c\n\/\/ Define a linked list node type with data\ntypedef struct node {\nstruct node* next; \/\/ 8 bytes\nint data[30];\n\/\/ 120 bytes\n} node_t;\n…\n\/\/ Create a linked list of length items\nvoid *create(void *unused) {\nfor(int i=0; i<items; i++) {\nnode_t* n = (node_t*)malloc(sizeof(node_t));\nif(last == NULL) { \/\/ Is the list empty? If so, the new node is the head and tail\nhead = n;\nlast = n;\n} else {\nlast->next = n;\nlast = n;\n}\n}\n}\n5\n\n\fCode for linked-list.c\n#define ACCESSES 1000000000\n\n\/\/ MEASUREMENT BEGIN\n\/\/ Traverse list over and over until we’ve visited `ACCESSES` nodes\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\n\/\/ reached the end\nelse current = current->next;\n\/\/ next node\n}\n\/\/ MEASUREMENT END\n● Note: executed instructions are equivalent regardless of list length\n● So we expect performance to be same regardless of length. Is it?\n6\n\n\fCode for array.c\n\/\/ Define a linked list node type with data\ntypedef struct node {\nstruct node* next; \/\/ 8 bytes\nint data[30];\n\/\/ 120 bytes\n} node_t;\n…\n\/\/ Create a linked list but allocate nodes in an array\nvoid *create(void *unused) {\nhead = (node_t *) malloc(sizeof(node_t) * items);\nlast = head + items - 1;\nfor(int i=0; i<items; i++) {\nnode_t* n = &head[items];\nn->next = &head[items+1]; \/\/ Next node is next element in array\n}\nlast->next = NULL;\n}\n7\n\n\fCode for array.c\n#define ACCESSES 1000000000\n\n\/\/ MEASUREMENT BEGIN\n\/\/ Traverse list over and over until we’ve visited `ACCESSES` nodes\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\n\/\/ reached the end\nelse current = current->next;\n\/\/ next node\n}\n\/\/ MEASUREMENT END\n● Note: same exact loop as the linked-list.c loop.\n● So we expect performance to be exactly the same. Is it?\n8\n\n\fkernighan.cs.pitt.edu specs\n● Two CPU sockets. Each CPU:\no Intel(R) Xeon(R) CPU E5-2640 v4\no 10 cores, with 2 threads per each core (SMT)\no L1 i-cache: 32 KB 8-way set associative (per core)\no L1 d-cache: 32 KB 8-way set associative (per core)\no L2 cache: 256 KB 8-way set associative (per core)\no L3 cache: 25 MB 20-way set associative (shared)\n● Memory\no 128 GB DRAM\n● Information obtained from\no “cat \/proc\/cpuinfo” on Linux server\no “cat \/proc\/meminfo” on Linux server\no https:\/\/en.wikichip.org\/wiki\/intel\/xeon_e5\/e5-2640_v4\n9\n\n\fExperimental data collection\n● Collected using CPU Performance Monitoring Unit (PMU)\no PMU provides performance counters for a lot of things\no Cycles, instructions, various types of stalls, branch mispredictions,\ncache misses, bandwidth usage, …\n\n● Linux perf utility summarizes this info in easy to read format\no https:\/\/perf.wiki.kernel.org\/index.php\/Tutorial\n\n10\n\n\fCPI (Cycles Per Instruction) Results\nWhy the three “plateaus”?\n\nWhy increase in CPI with larger size?\n\nWhy is array\nfaster than\nlinked list?\n\nWhy constant IPC, regardless of size?\n\n11\n\n\fMemory Stall Cycle Percentage\nThe three “plateaus” are in memory stalls too!\n\nIPC decrease due to memory stall increases!\n\nArray has less\nmemory stalls\nthan linked list\n\nNo IPC decrease because no increase in memory stalls!\n\n12\n\n\fData Structure Performance ∝ Memory Stalls\n\n● Data structure performance is proportional to memory stalls\no Applies to other data structures such as trees, graphs, …\n● In general, more data leads to worse performance\no But why? Does more data make MEM stalls longer? (Hint: yes)\no And why is an array not affected by data size? (I wonder …)\n● You will be able to answer all these questions when we are done.\n13\n\n\fMemory Technologies\n\n14\n\n\fStatic RAM (SRAM)\n● SRAM uses a loop of NOT gates to\nstore a single bit\n● This is usually called a 6T SRAM cell\nsince it uses... 6 Transistors!\n● Pros:\no Very fast to read\/write\n● Cons:\no Volatile (loses data without power)\no Relatively many transistors needed\n-> expensive\n\n15\n\n\fDynamic RAM (DRAM)\n● DRAM uses one transistor and one capacitor\no The bit is stored as a charge in the capacitor\no Capacitor leaks charge over time\n-> Must be periodically recharged (called refresh)\n-> During refresh, DRAM can’t be accessed\no Accesses are slower\n-> Small charge must be amplified to be read\n-> Also after read, capacitor needs recharging again\no Reading a DRAM cell is slower than reading SRAM\n● Pros:\no Higher density -> less silicon -> much cheaper than SRAM\n● Cons:\no Still volatile (even more volatile than SRAM)\no Slower access time\n16\n\n\fSpinning magnetic disks (HDD)\n● Spinning platter coated with a ferromagnetic\nsubstance magnetized to represent bits\no Has a mechanical arm with a head\no Reads by placing arm in correct cylinder,\nand waiting for platter to rotate\n● Pros:\no Nonvolatile (magnetization persists without power)\no Extremely cheap (1TB for $50)\n● Cons:\no Extremely slow (it has a mechanical arm, enough said)\n\n17\n\n\fOther technology\n● Flash Memory\no Works using a special MOSFET with “floating gate”\no Pros: nonvolatile, much faster than HDD\no Cons:\n▪ Slower than DRAM\n▪ More expensive than HDDs (1TB for $250)\n▪ Writing is destructive and shortens lifespan\n\n● Experimental technology\no Ferroelectric RAM (FeRAM), Magnetoresistive RAM (MRAM),\nPhase-change memory (PRAM), carbon nanotubes ...\no In varying states of development and maturity\no Nonvolatile and close to DRAM speeds\n18\n\n\fMemory\/storage technologies\nVolatile\n\nNonvolatile\n\nSRAM\n\nDRAM\n\nHDDs\n\nFlash\n\nSpeed\n\nFAST\n\nOK\n\nSLOW\n\nPretty good!\n\nPrice\n\nExpensive\n\nOK\n\nCheap!\n\nMeh\n\nPower\n\nGood!\n\nMeh\n\nBad\n\nOK\n\nDurability\n\nGood!\n\nGood!\n\nGood!\n\nOK\n\nReliability\n\nGood!\n\nPretty good!\n\nMeh\n\nPretty good!\n\nI’m using Durability to mean “how well it holds data after repeated use.”\nI’m using Reliability to mean “how resistant is it to external shock.”\n\n19\n\n\fDo you notice a trend?\n● The faster the memory the more expensive and lower density.\n\n● The slower the memory the less expensive and higher density.\n● There exists a hierarchy in program data:\no Small set of data that is accessed very frequently\no Large set of data that is accessed very infrequently\n\n● Thus, memory should also be constructed as a hierarchy:\no Fast and small memory at the upper levels\no Slow and big memory at the lower levels\n\n20\n\n\fLarger capacity memories are slower\n\nThoziyoor, Shyamkumar & Muralimanohar, Naveen & Ahn, Jung Ho & Jouppi,\nNorman. (2008). CACTI 5.1.\n\n21\n\n\fDRAM faster than SRAM at high capacity due to density\n\n● Access Time = Memory Cell Delay + Address Decode Delay + Wire Delay\n● With high capacity, Wire Delay (word lines + bit lines) starts to dominate\n● Wire Delay ∝ Memory Structure Area\n● DRAM density > SRAM density → DRAM Wire Delay < SRAM Wire Delay\n22\n\n\fThe Memory Hierarchy\n\n23\n\n\fSystem Memory Hierarchy\n● Use fast memory (SRAM) to store frequently used data inside the CPU\n● Use slow memory (e.g. DRAM) to store rest of the data outside the CPU\n\nCPU\n\nPipeline\n\nMemory bus\ndelay\n\nSRAM\n(regs)\n\nPCIe bus\ndelay\n\nDRAM\n(memory)\n\nHDD\/SDD\n(files, swapped\nout memory)\n\n● Registers are used frequently for computation so are stored in SRAM\n● Memory pages used frequently are stored in DRAM\n● Memory pages used infrequently are stored in HDD\/SDD (in swap space)\n● Note: Memories outside CPU suffers from bus delay as well\n24\n\n\fSystem Memory Hierarchy\n● Use fast memory (SRAM) to store frequently used data inside the CPU\n● Use slow memory (e.g. DRAM) to store rest of the data outside the CPU\n\nCPU\n\nPipeline\n\nMemory bus\ndelay\n\nSRAM\n(regs)\n\nPCIe bus\ndelay\n\nDRAM\n(memory)\n\nHDD\/SDD\n(files, swapped\nout memory)\n\n● Drawback: Memory access is much slower compared to registers\n● Q: Can we make memory access speed comparable to register access?\n\n25\n\n\fSystem Memory Hierarchy\n● Use fast memory (SRAM) to store frequently used data inside the CPU\n● Use slow memory (e.g. DRAM) to store rest of the data outside the CPU\n\nCPU\nSRAM\n(registers)\n\nDRAM\n(memory)\n\nPipeline\nSRAM\n(cache)\n\nHDD\/SDD\n(files, swapped\nout memory)\n\n● Drawback: Memory access is much slower compared to registers\n● Q: Can we make memory access speed comparable to register access?\no How about storing frequently used memory data in SRAM too?\no This is called caching. The hardware structure is called a cache.\n26\n\n\fCaching\n● Caching: keeping a temporary copy of data for faster access\n● DRAM is in a sense also caching frequently used pages from swap space\no We are just extending that idea to bring cache data inside the CPU!\n● Now instructions like lw or sw never directly access DRAM\no They first search the cache to see if there is a hit in the cache\no Only if they miss will they access DRAM to bring data into the cache\n\nCPU\nSRAM\n(registers)\n\nDRAM\n(memory)\n\nPipeline\nSRAM\n(cache)\n\nHDD\/SDD\n(files, swapped\nout memory)\n\n27\n\n\fCache Flow Chart\n● Cache block: unit of data used to cache data\no What page is to memory paging\no Cache block size is typically multiple words\n(e.g. 32 bytes or 64 bytes. You’ll see why.)\n\n● Good: Memory Wall can be surmounted\no On cache hit, no need to go to DRAM!\n● Bad: MEM stage has variable latency\no Typically, only a few cycles if cache hit\no More than a 100 cycles if cache miss!\n(Processor must go all the way to DRAM!)\no Makes performance very unpredictable\n28\n\n\fCache Locality: Temporal and Spatial\n\n• Temporal Locality\n\n• Spatial Locality\nCache Block\nData\nItem 0\n\nData\nItem\n\nMiss!\n\nHit!\nTime\n\nHit!\n\nMiss!\n\nData\nItem 1\n\nHit!\n\nData\nItem 2\n\nHit!\nTime\n\n29\n\n\fCache Locality: Temporal and Spatial\n● Caching works because there is locality in program data accesses\no Temporal locality\n▪ Same data item is accessed many times in succession\n▪ 1st access will miss but following accesses will hit in the cache\no Spatial locality\n▪ Different data items spatially close are accessed in succession\n▪ 1st access will miss but bring in an entire cache block\n▪ Accesses to other items within same cache block will hit\n▪ E.g., fields in an object, elements in an array, …\n● Locality, like ILP, is a property of the program\n\n30\n\n\fCold Misses and Capacity Misses\n● Cold miss (a.k.a. compulsory miss)\no Miss suffered when data is accessed for the first time by program\no Cold miss since cache hasn’t been warmed up with accesses\no Compulsory miss since there is no way you can hit on the first access\no Subsequent accesses will be hits since now data is fetched into cache\n▪ Unless it is replaced to make space for more frequently used data\n● Capacity miss\no Miss suffered when data is accessed for the second or third times\no This miss occurred because data was replaced to make space\no If there had been more capacity, miss wouldn’t have happened\no Capacity decides how much temporal locality you can leverage\n\n31\n\n\fReducing Cold Misses and Capacity Misses\n● Reducing capacity misses is straightforward\no Increase capacity, that is cache size!\n● But how do you reduce cold misses? Is it even possible?\no Yes! By taking advantage of spatial locality.\no Have a large cache block so you bring in other items on a miss\no Those other items may be accessed for the first time but will hit!\n● Large cache blocks can …\no Potentially reduce cold misses (and\/or reduce capacity misses)\n(given some spatial locality, can bring in more data on a miss)\no Potentially increase capacity misses\n(with no spatial locality, can store less data items in same capacity)\n→ Each program has a sweet spot. Architects choose a compromise.\n32\n\n\fSo how big do we want the cache to be?\n● Uber big!\n\nCaches\n\n● On the right is a diagram of\nthe Xeon Broadwell CPU used\nin kernighan.cs.pitt.edu.\no Caches take up almost as\nmuch real estate as cores!\no A cache miss is that painful.\n● But having a big cache comes\nwith its own set of problems\no Cache itself gets slower\n\n33\n\n\fBigger caches are slower\n● Below is a diagram of a Nehalem CPU (an older Intel CPU)\n● How long do you\nthink it takes for\ndata to make it from\nhere...\n● ...to here?\n● It must be routed\nthrough all this.\n● Can we cache the\ndata in the far away\n“L3 Cache” to a\nnearby “L2 Cache”?\n34\n\n\fMulti-level Caching\n● This is the structure of the kernighan.cs.pitt.edu Xeon CPU:\nL1 I-Cache\n\nL1 D-Cache\n\nL2 Cache\nL3 Cache\n\n● L1 cache: Small but fast. Interfaces with CPU pipeline MEM stage.\no Split to i-cache and d-cache to avoid structural hazard\n● L2 cache: Middle-sized and middle-fast. Intermediate level.\n● L3 cache: Big but slow. Last line of defense against memory access.\n● Allows performance to degrade gracefully\n35\n\n\fRevisiting Our Experiments\n\n36\n\n\fRevisiting our CPI Results with the new perspective\nCorrespond to multiple levels of memory\n\nBut this is just conjecture.\nIs it actually true?\n\nCPI increases with bigger size: more of\ndata structure is stored in lower memory\n\n37\n\n\fkernighan.cs.pitt.edu cache specs\n● On a Xeon E5-2640 v4 CPU (10 cores):\no L1 i-cache: 32 KB 8-way set associative (per core)\no L1 d-cache: 32 KB 8-way set associative (per core)\no L2 cache: 256 KB 8-way set associative (per core)\no L3 cache: 25 MB 20-way set associative (shared)\nRef: https:\/\/en.wikichip.org\/wiki\/intel\/xeon_e5\/e5-2640_v4\n\n● Access latencies (each level includes latency of previous levels):\no L1: ~3 cycles\no L2: ~8 cycles\no L3: ~16 cycles\no DRAM Memory: ~67 cycles\n\nRef: https:\/\/www.nas.nasa.gov\/assets\/pdf\/papers\/NAS_Technical_Report_NAS-2015-05.pdf\n\n38\n\n\fCache Specs Reverse Engineering\n● Why do I have to refer to a NASA technical report for latencies?\n\no Ref: https:\/\/www.nas.nasa.gov\/assets\/pdf\/papers\/NAS_Technical_Report_NAS-2015-05.pdf\n\no Because Intel doesn’t publish detailed cache specs on data sheet\n● In the technical report (does the step function look familiar?):\n\nWhy just Loads and not Stores?\n\n39\n\n\fLoads have more impact on performance\n● Suppose we added a store to our original loop:\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\n\/\/ reached the end\nelse {\ncurrent->data[0] = 100;\n\/\/ store to node data\ncurrent = current->next;\n\/\/ load next node address\n}\n\n● Which would have more impact on performance? The load or the store?\no A: The load because it is on the critical path.\n…\n\ncurrent = current->next\n\ncurrent = current->next\n\ncurrent = current->next\n\n…\n\ncurrent->data[0] = 100\n\ncurrent->data[0] = 100\n\ncurrent->data[0] = 100\n\n…\n40\n\n\fLoads have more impact on performance\n● Loads produce values needed for computation to proceed\no Stalled loads delays computation and possibly the critical path\n● Stores write computation results back to memory\no As long as the results are written back eventually, no big hurry\no If store results in a cache miss,\nstore is marked as “pending” and CPU moves on to next computation\no Pending stores are maintained in a write buffer hardware structure\n\n● What if the next computation reads from a pending store?\no First check the write buffer and read in the value if it’s there\no Again, performing the store is not on the critical path\n\n41\n\n\fHow Write Buffer Maintains Pending Stores\n● sw p0,4(p1) is about to commit. 4(p1) == 0xdeadbeef, p0 == 10\n● Unfortunately, address 0xdeadbeef is not in the L1 d-cache and it misses\nL1 d-cache\n…\n… Miss!\n…\n…\n\nStore Queue\nAddress Value\n0xdeadbeef 10\n…\n…\n…\n…\n…\n…\n\ncommit in order\n\nInstruction Decoder\n\nWrite Buffer\nAddress Value\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\nInstruction Queue\ninstruction dest done?\nsw p0,4(p1)\nY\n…\n…\n…\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n42\n\n\fHow Write Buffer Maintains Pending Stores\n● sw p0,4(p1) commits successfully anyway\n● The store is moved to the Write Buffer and stays there until store completes\nL1 d-cache\n…\n…\n…\n…\n\nStore Queue\nAddress Value\n…\n…\n…\n…\n…\n…\n…\n…\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\n\ncommit in order\n\nInstruction Decoder\n\nWrite Buffer\nAddress Value\n0xdeadbeef 10\n\nInstruction Queue\ninstruction dest done?\n…\n…\n…\n…\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n43\n\n\fHow Write Buffer Maintains Pending Stores\n● Later, when lw p3,0(p2) comes along, it checks Write Buffer first\n● If 0(p2) == 0xdeadbeef, Write Buffer provides value instead of memory\nL1 d-cache\n…\n…\n…\n…\n\nStore Queue\nAddress Value\n…\n…\n…\n…\n…\n…\n…\n…\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n…\n…\n…\n\ncommit in order\n\nInstruction Decoder\n\nWrite Buffer\nAddress Value\n0xdeadbeef 10\n\nInstruction Queue\ninstruction dest done?\nlw p3,0(p2) t0\nN\n…\n…\n…\n\nLoad\/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n44\n\n\fSo are stores never on the critical path?\n● If we had an infinitely sized write buffer, no, never.\n\n● In real life, write buffer is limited and can suffer structural hazards\no If write buffer is full of pending stores, you can’t insert more.\n→ That will prevent a missing store from committing from i-queue\n→ That will prevent all subsequent instructions from committing\n→ That will eventually stall the entire pipeline\n● But with ample write buffer size, happens rarely\no And if it does happen can be detected using PMUs\n● Hence, we will focus on loads to analyze performance\n\n45\n\n\fLinked List Cache Load Miss Rates\n\nBut what do these lines\nactually mean?\n\n46\n\n\fLinked List Cache Load Miss Rates\n\nA few L2 cold misses result in high miss rate.\nBut very few L2 accesses to begin with.\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nDo the miss rates correspond to CPI results?\nLet’s compare with our own eyes!\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\nMiss! Miss! Miss! Hit!\n\nL1 Cache\n\nMiss! Miss!\n\nL2 Cache\n\nMiss!\nHit!\n\nHit!\n\nHit!\n\nL3 Cache\nDRAM Memory\n47\n\n\fLinked List Cache Load Miss Rates vs CPI\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\n48\n\n\fLinked List Cache Load Miss Rates – Other Questions\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\n● Why the step up in L1 cache misses between 500 – 1000 nodes?\n● Why the step up in L2 cache misses between 1000 – 5000 nodes?\n● Why the step up in L3 cache misses between 100 k – 500 k nodes?\n● Also, why do cache miss increases look like step functions in general?\n49\n\n\fWorking Set Overflow can cause Step Function\n● The size of a node is 128 bytes:\ntypedef struct node {\nstruct node* next;\n\/\/ 8 bytes\nint data[30];\n\/\/ 120 bytes\n} node_t;\n● Working set: amount of memory program accesses during a phase\no For linked-list.c, working set is the entire linked list\n▪ Program accesses entire linked list in a loop over and over again\no If there are 8 nodes in linked list, working set size = 128 * 8 = 1 KB\n● When working set overflows cache capacity, start to see cache misses\no Miss increase can be drastic, almost like a step function\no Suppose cache size is 1 KB and nodes increase from 8 → 9\n▪ When 8 nodes: always hit (since entire list in contained in cache)\n▪ When 9 nodes: always miss (if least recent node is replaced first)\n50\n\n\fLinked List Cache Load Miss Rates – Other Questions\n● Why the step up in L2 cache misses between 1000 – 5000 nodes?\no L2 cache size is 256 KB\no Number of nodes that can fit = 256 KB \/ 128 = 2048\n● Why the step up in L3 cache misses between 100 k – 500 k nodes?\no L3 cache size is 25 MB\no Number of nodes that can fit = 25 MB \/ 128 ≈ 200 k\n● Why the step up in L1 cache misses between 500 – 1000 nodes?\no L1 d-cache size is 32 KB\no Number of nodes that can fit = 32 KB \/ 128 = 256\no So, in theory you should already see a step up at 500 nodes\no Apparently, CPU doesn’t use least-recently-used (LRU) replacement\no According to another reverse engineering paper, Intel uses PLRU\n\nRef: “CacheQuery: Learning Replacement Policies from Hardware Caches” by Vila et al.\nhttps:\/\/arxiv.org\/pdf\/1912.09770.pdf\n\n51\n\n\fLinked List Cache Load Miss Rates – Other Questions\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\n● Why did L1 cache miss rate saturate at around 20%?\no Shouldn’t it keep increasing with more nodes like L2 and L3?\n\n52\n\n\fLinked List Cache Load Miss Rates – Other Questions\n[linked-list.c]\nvoid *run(void *unused) {\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\nelse current = current->next;\n}\n}\n\n[objdump -S linked-list]\n0000000000400739 <run>:\n...\n400741: mov 0x200920(%rip),%rax # %rax = head\n400748: mov %rax,-0x8(%rbp) # current = %rax\n40074c: movl $0x0,-0xc(%rbp) # i = 0\n400753: jmp 400778 # jump to i < ACCESSES comparison\n400755: cmpq $0x0,-0x8(%rbp) # current == NULL?\n40075a: jne 400769 # jump to else branch if not equal\nWithin a typical iteration in for loop:\n40075c: mov 0x200905(%rip),%rax # %rax = head\n4 blue loads that hit in L1:\n400763: mov %rax,-0x8(%rbp) # current = %rax\n• 2 loads each of local vars current, i 400767: jmp 400774 # jump to end of if-then-else\n• Read frequently so never replaced\n400769: mov -0x8(%rbp),%rax # %rax = current\n1 red load that misses in L1:\n40076d: mov (%rax),%rax # %rax = current->next\n• current->next (next field of node)\n400770: mov %rax,-0x8(%rbp) # current = %rax\n• Node may not be in cache and miss 400774: addl $0x1,-0xc(%rbp) # i++\n(e.g. due to a capacity miss)\n400778: cmpl $0x3b9ac9ff,-0xc(%rbp) # i < ACCESSES ?\n 1 miss \/ 5 loads = 20% miss rate\n40077f: jle 400755 # jump to head of loop if less than\n53\n\n\fHow about Linked List with No Data?\n\n● Linked list, no data suffered almost no CPI degradation. Why?\n\n54\n\n\fLinked List w\/ Data vs. w\/o Data\n● Linked list with data\n\n● Linked list with no data\n\n55\n\n\fLinked List w\/ Data vs. w\/o Data. Why?\n● The size of a node with no data is only 8 bytes:\ntypedef struct node {\nstruct node* next;\n\/\/ 8 bytes\n\/\/ no data\n} node_t;\n● Compared to 128 bytes with data, can fit in 16X more nodes in cache\no Temporal locality: More likely that a node will be present in cache\n\n● How about L1 cache miss rate that hovers around 10% instead of 20%?\no By 107 nodes, there is no temporal locality with respect to the L1 cache\no Spatial locality must be responsible for the reduction in miss rate\n\n56\n\n\fLinked List w\/ Data vs. w\/o Data. Why?\n● Nodes of the linked list are malloced one by one in a loop:\nfor(int i=0; i<items; i++) {\nnode_t* n = (node_t*)malloc(sizeof(node_t));\n…\n}\no I have no idea where glibc malloc decides to allocate each node\n● But knowing each cache block is 64 bytes long in the Xeon E5 processor\no Let’s say multiple nodes are allocated on same cache block:\nnode 1\n\nmeta-data meta-data\n\nnode 37 meta-data meta-data node 23 meta-data\n\no Then even if access to node 1 misses, due to a capacity miss,\naccesses to nodes 37 and 23 that soon follow will hit!\no This is assuming there is some spatial locality in how malloc allocates\n57\n\n\fData structure with most spatial locality: Array\n● Elements of an array are guaranteed to be in contiguous memory:\nvoid *create(void *unused) {\nhead = (node_t *) malloc(sizeof(node_t) * items);\n…\n}\n● Each cache block is 64 bytes long in the Xeon E5 processor\no Now 8 elements are guaranteed to be on same cache line, in order:\nnode 0 node 1 node 2 node 3 node 4 node 5 node 6 node 7\n\no Even with cold cache, only 1 out of 8 nodes miss (1\/8 = 12.5% miss rate)\no Assuming that nodes are accessed sequentially\n▪ If accessed in random order, no spatial locality, even with array\no True regardless of capacity (even if cache contained only a single block)\n58\n\n\fLet’s look at the CPI of arrays finally\n\n● Array, no data did very well as expected\no The most spatial locality of the four benchmarks (contiguous nodes)\no Smallest memory footprint so can also leverage temporal locality the best\n● Array performed the same as array, no data. How come?\no No spatial locality since each node takes up two 64-byte cache blocks\no Has much larger memory footprint compared to array, no data\no This is the real mystery. We will learn more about it as we go on. ☺\n59\n\n\fImpact of Memory\nOn Performance\n\n60\n\n\fImpact of Memory on Performance\n● CPU Cycles = CPU Compute Cycles + Memory Stall Cycles\no CPU Compute Cycles = cycles where CPU is not stalled on memory\no Memory Stall Cycles = cycles where CPU is waiting for memory\n● Why do we need to differentiate between the two?\no Because each are improved by different design features!\n● HW\/SW design features that impact CPU Compute Cycles:\no HW: Pipelining, branch prediction, wide execution, out-of-order\no SW: Optimizing the computation in the program\n● HW\/SW design features that impact Memory Stall Cycles:\no HW: Caches, write buffer, prefetcher (we haven’t learned this yet)\no SW: Optimizing memory access pattern of program\n\n(Taking into consideration temporal and spatial locality)\n\n61\n\n\fHow about overclocking using DVFS?\n● CPU Time = CPU Cycles * Cycle Time\n= (CPU Compute Cycles + Memory Stall Cycles) * Cycle Time\n● What if we halved the Cycle Time using DVFS?\no Memory Stall Cycles could increase by up to 2X!\no Why? DRAM speed remains the same, so you need twice the cycles.\n▪ The bus (wire) that connects CPU to DRAM is not getting any faster\n▪ The DRAM chip itself is not getting clocked any faster\no How about caches? Same number of cycles regardless of DVFS.\n● If most of your time is spent accessing DRAM, then overclocking is useless\no Memory Stall Time (Memory Stall Cycles * Cycle Time) is mostly constant\n\n62\n\n\fMemory Latency vs. Memory Bandwidth\n● Memory stall cycles comes from two sources:\no Memory latency: seconds to handle a single memory request\n\nvs.\no Memory bandwidth: maximum requests handled per second\n\nvs.\n\n63\n\n\fMemory bandwidth puts a ceiling on performance\n● Memory latency can be surmounted by smart scheduling\no Either by compiler or by instruction queue scheduling by CPU\n● No real way to surmount memory bandwidth limit\no No matter how much scheduling you do, same bandwidth\n● When you get a traffic jam, performance is dictated by bandwidth\no How quickly you pull data in, rather than how fast you process it\no Operational intensity = work (FLOP) per memory access (byte)\no Performance = work \/ second\n= work \/ byte * byte \/ second\n= operational intensity * memory bandwidth\n→ Linear relationship between performance and operational intensity\n64\n\n\fThe Roofline Model: A bird’s eye view of performance\n● Roofline: theoretical performance achievable given an intensity\no Formed by memory bandwidth bounds + peak CPU performance\n\nWhere do these gaps come from?\n\n65\n\n\fMemory bound vs. compute bound apps\n● I = intensity, β = peak bandwidth, π = peak performance\n\n66\n\n\fEffect of Cache on Roofline Model\n● With caching, apps shift upwards and rightwards\n\nReduction in memory latency makes CPU efficient\nReduction in memory accesses increases operational intensity\n\n67\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":247,"segment": "unlabeled", "course": "cs1541", "lec": "lec1.1_introduction","text":"CS 1541 Introduction\nInstructor Introduction\nCourse Introduction\n\nWonsun Ahn\nDepartment of Computer Science\nSchool of Computing and Information\n\n\fInstructor Introduction\n\n\fMy Technical Background\n◼ Wonsun Ahn\n⚫ First name is pronounced one-sun (if you can manage)\n⚫ Or you can just call me Dr. Ahn (rhymes with naan)\n\n◼ PhD in CPU Design and Compilers\n⚫ University of Illinois at Urbana Champaign\n\n◼ Industry Experience\n⚫ Software engineer, field engineer, technical lead, manager\n⚫ Bluebird Corporation (70-person startup company)\nManufactures industrial hand-held devices from top to bottom\nMe: Built software stack based on Windows Embedded\n\n⚫ IBM Research (thousands of people)\nDoes next-gen stuff like carbon nanotubes, quantum computers\nMe: Designed supercomputers for ease of parallel programming\n\n\fMy World View\n◼ Everything is connected\n⚫ Pandemic: If my neighbors catch the virus, so will I\n⚫ Environment: If my neighbors pollute, I will feel the effects\n⚫ Economy: Think of how the subprime mortgage crisis spread\n\n◼ Zero-sum thinking (old way of thinking)\n⚫ “If you get a larger slice of the pie, I get a smaller slice.”\n⚫ Therefore, if you lose, I win (and vice versa)\n\n◼ Zero-sum thinking no longer works\n⚫ If you catch the virus, do I become safer from the virus?\n\n◼ Collaboration is replacing competition\n\n\fCollaboration is Replacing Competition\n◼ Is happening in all spheres of life\n◼ Collaboration is also happening in the IT industry\n⚫ The open source movement\n⚫ Increasing importance of the software\/hardware ecosystem\n⚫ Increasing importance of the developer community\n\n◼ Collaboration is also important for learning\n⚫ During my undergrad years, what do I remember best?\n⚫ Stuff that I explained to my classmates\n⚫ Stuff that my classmates taught me\n\n\fSupporting Collaborative Learning\n◼ I do not grade on a curve\n⚫ You will not be competing against your classmates\n⚫ You are graded on your own work on an absolute scale\n\n◼ You will be a member of a Team\n⚫ You are already a member of the class on Microsoft Teams\n⚫ I encourage you to be on Teams at most times (I will too)\nYou can install app on both laptop and cell phone\n\n⚫ If you have a question, you can ask in the Team “Posts” tab\nEither your classmate or your instructor will answer\n\n⚫ You can chat with any individual on the Team\n“Manage Team” item in the “…” Team context menu\n\n\fSupporting Collaborative Learning\n◼ You will be a member of a Group\n⚫ On Teams, you will be part of a chat group of 8 members\n⚫ Your instructor is also a member of each Group\n⚫ It is a smaller support group where you can talk more freely\n\n◼ You are allowed to discuss TopHat lecture questions\n⚫ The goal is no-student-left-behind\n⚫ Discuss answers on Teams even before submitting them\n⚫ Form a basis of knowledge for doing homeworks and exams\n\n\fCourse Introduction\n\n\fStructure of the Course\n◼ (45% of grade) Two midterms\n◼ (20% of grade) Two projects\n⚫ Implementing a CPU simulator using C programming language\n\n◼ (20% of grade) Four homeworks\n◼ (15% of grade) Participation\n⚫ Attendance, TopHat lecture questions, Teams participation\n\n◼ Class resources:\n⚫ Canvas: announcements, Zoom meetings, recorded lectures\n⚫ GitHub: syllabus, lectures, homeworks, projects\n⚫ Tophat: online lecture questions\n⚫ GradeScope: homework \/ projects submission, grading and feedback\n⚫ Microsoft Teams: all out-of-class communication\n\n\fTextbook (You Probably Have it)\n◼ “Computer\nOrganization and\nDesign - The\nHardware\/Software\nInterface” by David\nPatterson and John\nHennessy Fifth\nEdition - Morgan &\nKaufmann.\n\n\fFor More Details\n◼ Please refer to the course info page:\nhttps:\/\/github.com\/wonsunahn\/CS1541_Spring2022\n\/blob\/main\/course-info.md\n\n◼ Please follow the course syllabus schedule:\nhttps:\/\/github.com\/wonsunahn\/CS1541_Spring2022\n\/blob\/main\/syllabus.md\n\n\fWhat is Computer Architecture?\n12\n\n◼ At a high-level: how a computer is built\n⚫ Computer here meaning the processor (CPU)\n\n◼ You probably heard of a similar term before: ISA\n⚫ ISA (Instruction Set Architecture)\n\n◼ Review: what is defined by an ISA?\n⚫ Set of instructions usable by the computer\n⚫ Set of registers available in the computer\n⚫ Other functional attributes\n\n◼ What is not defined by an ISA?\n⚫ Speed of computer\n⚫ Energy efficiency of computer\n⚫ Reliability of computer\n⚫ Other performance attributes\n\n\fComputer Architecture Defined\nApp 1\n\nApp 2\n\nApp 3\n\nOperating System\n\nSoftware Layer\n\nInstruction Set Architecture\nProcessor Organization\n\nComputer\nArchitecture\n\nTransistor Implementation\n\nPhysical Layer\n\n◼ Computer Architecture = ISA + Processor Organization\n⚫ Processor organization is also called Microarchitecture\n\n◼ Given an ISA, performance is decided by:\n⚫ Processor organization (internal design of the processor)\n⚫ Transistor implementation (semiconductor technology)\n\n\fScope of Class\n◼ Physical layer is beyond\nthe scope of the class\n\n◼ We will focus mostly on\nprocessor organization\n⚫ And how performance\ngoals are achieved\n\n\fScope of Class\n◼ Computer architecture is part of system architecture\n\n◼ Other components beside processor is beyond the scope\n\n\fTwo Forces on Computer Architecture\n1. Application pull\n\n2. Technology push\n\n◼ Market forces pull\n◼ Advances in silicon\narchitecture towards\ntechnology push\npopular applications\narchitecture to change\n\n\fApplication Pull\n◼ Different applications pull in different directions\nApplication pull\n\nApp 1\n\nApp 2\n\nApp 3\n\nOperating System\n\nSoftware Layer\n\nInstruction Set Architecture\nProcessor Organization\n\nComputer\nArchitecture\n\nTransistor Implementation\n\nPhysical Layer\n\n⚫ Real-time app (e.g. Game): Short latency\n⚫ Server app: High throughput\n⚫ Mobile app: High energy-efficiency (battery life)\n⚫ Mission critical app: High reliability\n\n◼ An app typically has multiple goals that are important\n\n\fApplication Pull\n◼ Some goals can be incompatible\n⚫ E.g. Speed and energy-efficiency are incompatible\nRunning is faster than walking but uses more energy\nA Ferrari is faster than a Prius but has worse fuel efficiency\n\n⚫ E.g. Reliability is incompatible with many other goals\nIf you use redundancy, you use twice the amount of energy\n\n◼ Even when sharing a goal, apps have unique needs\n⚫ Scientific apps need lots of floating point units to go fast\n⚫ Database apps need lots of memory cache to go fast\n\n◼ An architecture is a compromise among all the apps\n⚫ When app achieves market critical mass, designs diverge\n(Mobile chips \/ Server chips \/ GPUs \/ TPUs diverged)\n⚫ Sometimes even ISAs diverge (GPUs and TPUs)\n\n\fTechnology Push\n◼ Trends in technology pushes architecture too\nApp 1\n\nTechnology push\n\nApp 2\n\nApp 3\n\nOperating System\n\nSoftware Layer\n\nInstruction Set Architecture\nProcessor Organization\n\nComputer\nArchitecture\n\nTransistor Implementation\n\nPhysical Layer\n\n⚫ Trends can be advances in technology\n⚫ Trends can be constraints technology couldn’t overcome\n\n “Technology” in CPU design refers to the physical layer\n⚫ Manufacturing technology used for transistor implementation\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":248,"segment": "unlabeled", "course": "cs1567", "lec": "lec14_pid_control","text":"PID Control\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fTracking an Object\n\nImagine that there is an object somewhere in front of our\nrobot\n\nCamera\nField−of−View\n\nHow to turn the robot such that it points directly to the\nobject?\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fTracking an Object\nAgain, top view\n\nCamera\nField−of−View\n\nHere is from the camera point-of-view\n\nThe object detection node may say it sees an object at\n(150, 160)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fTracking an Object\nThe goal is to turn the robot such that it points directly to\nthe object the object\n\nHere is from the camera point-of-view\n\nThe object detection node may say it sees an object at\n(320, 160)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fTracking an Object\n\nThe simplified goal is to make the object appear at the center\nof the camera (x-coordinate is at 320)\nExecution (same as one of our lab)\nIf x-coordinate of the object is in between 315 to 325, stop\nmoving\nlinear.x = 0.0 and angular.z = 0.0\n\nIf x-coordinate of the object is less than 315 (on the left side),\ntell the robot to make a stationary turn left at speed 0.5\nlinear.x = 0.0 and angular.z = 0.5\n\nIf x-coordinate of the object is greater than 325 (on the right\nside), tell the robot to make a stationary turn right at speed\n0.5\nlinear.x = 0.0 and angular.z = -0.5\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fTracking an Object\nThis simplified method is suitable for tracking a stationary\nobject\nAn object that does not move or move very slowly\n\nIf an object moves around the robot in circle:\nAt speed greater than 0.5 radian per second\nEventually the robot will loose track of the object because the\nobject moves out of the camera view\n\nAt speed exactly 0.5 radian per second\nThe robot will not be able to point itself directly at the object\nThe robot will not loose track of the object but will not be\nable to catch up with it\n\nAt speed less than 0.5 radian per second\nThe robot will be able to catch up with the object eventually\nThe robot will stutter (not smooth) especially when the object\nis at the center a move slightly out of center\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fControl System\n\nA control system is a device, or set of devices, that manages,\ncommands, directs or regulates the behavior of other devices\nor systems\nIn robot application, the control system is the software that\nmanage\/regulate commands that control the behavior of\nrobots\nTells a robot to move faster or slower\nTells a robot to turn left or right\n\nType of Control Systems:\nOpen Loop: Output is generated purely based on the current\ninput\nClosed Loop: Current output is taken into consideration and\ncorrections are made based on feedback\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fOpen Loop Examples\n\nProject 1 (Sennott Rover): We adjust the command\nlinear.x and angular.z based only on the odometry data.\nSpeed\n1.0\n\n0.5\n\n0.5\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n1.0\n\n1.5\n\nPID Control\n\nDistance\n\n\fOpen Loop Examples\nLab (Blob Tracker): We adjust the command angular.z\nbased only on the pointer location and the odometry data\n315\n\n480\n\nangular.z\n0.5\n\n325\n\nangular.z\n0.0\n\n640\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\nangular.z\n−0.5\n\n\fOpen Loop Examples\nLab (Blob Tracker): For better follower, you may have\nvarious speed based on the pointer location and the odometry\ndata\n315\n\n480\n\nangular.z\n0.8\n\nangular.z\n0.5\n\nangular.z\n0.3\n\n325\n\nangular.z\n0.0\n\n640\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\nangular.z\n−0.3\n\nangular.z\n−0.5\n\nangular.z\n−0.8\n\n\fOpen Loop Examples\nLab (Mouse Tracker): For even better result, the turn speed\nshould be proportional to the distance from center\n315\n\n480\n\nangular.z\n(315 − center) * delta\n\n325\n\nangular.z\n0.0\n\n640\n\ncenter is the x-coordinate of the object\ndelta ≥ 0\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\nangular.z\n(315 − center) * delta\n\n\fProblems with Open Loop Control\n\nProblem with object tracking and line following application\nOscillation:\nHard to see when the robot moves at slow speed\nAt faster speed, the robot may oscillate trying to keep at center\n\nLost track:\nThe robot may not turn fast enough\n\nOvercompensate:\nOver shoot the target (cause oscillations)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fClosed Loop Control\n\nClosed Loop Control\nInput\n\nController\n\nOutput\n\nError\n\nFeedback\n\nThe goal is to generate output signal to minimize the error\nExample: In object tracking application\nInput the location of an object (according to the image)\nThe error is the number of pixels of the object from the center\nof the screen\nThe output is the command (Twist)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fPID Control\n\nProportional-Integral-Derivative (PID) Controller is a closed\nloop control\nLet\nz(t) be the control value at time t (will be turned in to\nangular.z),\ne(t) be the error at time t (e.g., number of pixel of the center\nof the object to the center of the screen), and\nKp , Ki , Kd are non-negative coefficients\n\nZ t\nz(t) = Kp e(t) + Ki\n\ne(t)dt + Kd\n0\n\nde\ndt\n\nThe meaning of z(t), e(t), e(t)dt, and de\ndt depend on\napplication\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fPID Control\n\nPID:\n\nZ t\nz(t) = Kp e(t) + Ki\n\ne(t)dt + Kd\n0\n\nde\ndt\n\nFrom the above equation z(t) is adjusted based on the\nfollowing:\n1\n2\n3\n\nProportional: magnitude of the error (e(t))\nRt\nIntegral: accumulated magnitude of the error ( 0 e(t)dt), and\nDifferential: speed of error ( de\ndt )\n\nSpecify values of Kp , Ki , and Kd are difficult\nThese values depend on type of application and characteristic\nof the robot\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fProportional\n\nProportional portion of PID\nKp e(t)\nThis term allows you to adjust the speed based on the\nmagnitude of the error\nThe larger the error, the faster the robot should turn\nThe sign of the error effects the sign of the output (direction)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fDifferential\nDifferential portion of PID\nKd\n\nde\ndt\n\nde\nis the rate of the error\ndt\nThis can be as simple as e(t) − e(t − 1)\n\nIf the robot turn fast enough, the sign of the rate of the error\nwill be different than the sign of the magnitude of the error\ne(t)\nTo prevent over compensate, we should slow the speed down a\nlittle bit\n\nThis also help when the robot turns too slow, the sign of the\nerror rate will be the same as the magnitude of the error\nwhich makes the robot turn even faster\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fPID Control\n\nSometime P and D portions are not enough\nMoving object:\nThe P part makes the robot turn at the exact speed of the\nobject movement\nThe D part will be 0\nNever be able to catch up with it\n\nBalancing application (Segway)\nThe P and D parts can make it stands on its two wheels\nBut it may slowly drift forward or backward\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fIntegral\n\nIntegral portion of PID\nZ t\nKi\n\ne(t)dt\n0\n\nThis is the accumulating of error over time\nThis helps catching up with moving object\nThis also helps with drifting\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\fFind the values of Kp , Ki , and Kd\n\nPID Control\nZ t\nz(t) = Kp e(t) + Ki\n\ne(t)dt + Kd\n0\n\nde\ndt\n\nThe values of Kp , Ki , and Kd generally based on robot\napplication\nA way to find one these values is simply test it\n\nFirst, find Kp by setting Ki and Kd to 0s\nSlowly adjust Kp until the robot start oscillating\n\nSecond, fix Kp from the first step, setting Ki to 0\nSlowly adjust Kd until the robot stop oscillating\n\nThen, adjusting Ki .\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPID Control\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":249,"segment": "unlabeled", "course": "cs1502", "lec": "lec08_finite_automata_07","text":"Finite Automata 07\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\n\nShow that C = {w | w has an equal number of 0s and 1s} is not\nregular.\nAssume that C is regular\nSince C is regular, the Pumping lemma says that for any\nstring s ∈ C of length at least p, s can be divided into\ns = xyz satisfying the following conditions:\n1\n2\n3\n\nxy i z ∈ C for any i ≥ 0\n|y| > 0\n|xy| ≤ p\n\nLet s = (01)p\ns ∈ C — good\n|s| = 2p ≥ p — good\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\nShow that C = {w | w has an equal number of 0s and 1s} is not\nregular.\ns = (01)p\nLet x = ε, y = 01, and z = (01)p−1 and check all three\nconditions:\n1\n\nε(01)i (01)p−1 ∈ C for any i ≥ 0\n\n2\n\n|01| = 2 > 0\n|ε01| = 2 ≤ p\n\nEvery time you insert a y, you add equal number of 0 and 1\n3\n\nAll three condition can be true (no contradiction)\nImportant: No contradiction means nothing\nYou also cannot conclude that C is regular\n\nBut if we pick s = 0p 1p , we will get a contradiction\nSame kind of proof as in previous example but focus on the\nnumber of 0s and 1s (no pattern)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fShow that C = {w | w has an equal number of 0s and 1s}\nis not regular\nAssume that C is regular. Since C is regular, the Pumping lemma says that for any\nstring s ∈ C of length at least p, s can be divided into s = xyz satisfying the following\nconditions:\n1\n\nxy i z ∈ C for any i ≥ 0\n\n2\n\n|y| > 0\n\n3\n\n|xy| ≤ p\n\nLet s = 0p 1p . Since s starts with p 0s, to satisfy the third condition, x and y are\nstrings that contain nothing but 0s. In other words, x = 0j for any j ≥ 0, and y = 0k\nfor any k > 0. Note that k must be greater than 0 because |y| = |0k | = k, and the\ncondition 2 says that |y| > 0. Since x = 0j and y = 0k , z = 0p−(j+k) 1p . Let i = 2.\nWe have\nxy i z = xy 2 z\n= xyyz\n= 0j 0k 0k 0p−(j+k) 1p\n= 0p+k 1p\nFor the string 0p+k 1p to be in C, the number of 0s must be equal to the number of\n1s. In other words, p + k must be equal to p. This requires k to be 0. But since k\nmust be greater than 0, xy 2 z 6∈ B — contradiction. Therefore, C is not regular.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\nShow that D = {w | w has more number of 0s than number of 1s}\nis not regular.\nAs usual, assume that D is regular and followed by the\nstatement from the Pumping lemma\nIf you pick s = 02p 1p , you will not get a contradiction\nx = 0j for any j ≥ 0\ny = 0k for any k > 0\nz = 02p−(j+k) 1p\nFor i ≥ 2 in xy i z, you add more 0s which makes the result\nstring still have more 0s than 1s\nFor i = 0\nxy 0 z = xz = 0j 02p−(j+k) 1p = 02p−k 1p\nIf k is 1, 2p − 1 > p (the number of 0s is greater than the\nnumber of 1s) — no contradiction\nIf k is p − 1, 2p − (p − 1) = p + 1 > p — no contradiction\nNote that 0 < k ≤ p from (2) and (3)\nIf there is a k that works, no contradiction\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\nLet’s try s = 1p 02p . Note that s ∈ D and |s| ≥ p.\nAgain, to satisfy (2) and (3), we have\nx = 1j for any j ≥ 0\ny = 1k for any k > 0\nz = 1p−(j+k) 02p\n\nWe have xy i z = 1j (1k )i 1p−(j+k) 02p = 1p+ki−k 02p\nIf we increase i, we increase the number of 1s\nTo get a contradiction, we need the number of 1s to be\ngreater than or equal to the number of 0s\nIn other words, p + ki − k ≥ 2p\np + ki − k ≥ 2p\nki − k ≥ p\nk(i − 1) ≥ p\ni − 1 ≥ p\/k\ni ≥ (p\/k) + 1\nxy (p\/k)+1 z = 12p 02p 6∈ D — contradiction\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fShow that\nD = {w | w has more number of 0s than number of 1s} is\nnot regular\nAssume that D is regular. Since D is regular, the Pumping lemma says that for any\nstring s ∈ D of length at least p, s can be divided into s = xyz satisfying the\nfollowing conditions:\n1 xy i z ∈ D for any i ≥ 0\n2 |y| > 0\n3 |xy| ≤ p\nLet s = 1p 0p+1 . Since s starts with p 1s, to satisfy the third condition, x and y are\nstrings that contain nothing but 1s. In other words, x = 1j for any j ≥ 0, and y = 1k\nfor any k > 0. Note that k must be greater than 0 because |y| = |1k | = k, and the\ncondition 2 says that |y| > 0. Since x = 1j and y = 1k , z = 1p−(j+k) 0p+1 . Let\ni = 2. We have\nxy i z = xy 2 z = xyyz\n= 1j 1k 1k 1p−(j+k) 0p+1\n= 1p+k 0p+1\nFor the string 1p+k 0p+1 to be in D, the number of 0s must be greater than the\nnumber of 1s. In other words, p + 1 must be greater than p + k. This requires k to be\n0. But since k must be greater than 0, xy 2 z 6∈ D — contradiction. Therefore, D is\nnot regular.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fRule of Thumb\n\nIf a condition of the language is about inequality (<, ≤, >,\n≥), pick a string that is right at the border line to break the\ncondition\nD = {w | w has more number of 0s than number of 1s}\n1p 0p+1 needs xy 2 z to add at least one 1\n0p 1p−1 needs xy 0 z to take out at least one 0\nNo need to find a large value of i\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\n2\n\nShow that E = {0(i ) | i ≥ 0} is not regular.\nLet’s try to understand this language first\n2\n\nIf i = 0, 0(0 ) = 00 = ε\n2\nIf i = 1, 0(1 ) = 01 = 0\n2\nIf i = 2, 0(2 ) = 04 = 0000\n2\nIf i = 3, 0(3 ) = 09 = 000000000, and so on\n\nThus, we have\nE = {w | w contains nothing but 0s and\nthe number of 0s is i2 for some i ≥ 0}\nImportant: You cannot pick s = 0p\nThere is nothing to guarantee that p = i2 for some i ≥ 0\n2\nWe need to pick s = 0(p )\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\n\n2\n\nShow that E = {0i | i ≥ 0} is not regular.\nAs usual, assume that E is regular and followed by the\nstatement from the Pumping lemma\n2\n\nLet s = 0(p )\nNote that s ∈ E and\n|s| = p2 ≥ p.\n\nFrom the second and third conditions (|y| > 0 and |xy| ≤ p),\nwe have\n0 < |y| ≤ p\nNote that since s = xyz and |s| = p2 , |xyz| = p2 .\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\fExample\n\nLet’s do some analysis about xy 2 z\np2 = |xyz|\n|xyz| < |xy 2 z| = |xyyz| because |y| > 0\n|xyyz| = |xyz| + |y| = p2 + |y|\np2 + |y| ≤ p2 + p because |y| ≤ p\np2 + p < p2 + 2p + 1 = (p + 1)2\n\nNote that the string xy 2 z can in E if |xy 2 z| = q 2 for some q\nAbove analysis shows that p2 < |xy 2 z| < (p + 1)2\nBut there is no q such that p2 < q 2 < (p + 1)2\nThus, xy 2 z 6∈ E — contradiction\nE is not regular\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 07\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":250,"segment": "self_training_2", "course": "cs1502", "lec": "lec14_decidability_02","text":"Decidability 02\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidable Languages\n\nEDFA\nLet EDFA = {hAi | A is a DFA and L(A) = ∅}. Show that EDFA\nis decidable.\nHow do we know a DFA accepts no string?\nThis machine has no accept state (F = ∅) or\nThis machine has some accept states but are not reachable\nfrom the start state\n\nTo show that EDFA is decidable, we need a Turing machine T\nsuch that\nIf hAi ∈ EDFA , T accepts hAi\nIf hAi 6∈ EDFA , T rejects hAi\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fNot a Decider Example\nConsider this TM S:\nS = “On input hAi where A is a DFA:\n1\n2\n3\n\nFor all strings w:\nRun TM M on input hA, wi\nIf M accepts hA, wi, reject”\n\nDo you see the problem of TM S?\nTM S is not a decider\nGiven hAi where A is a DFA and L(A) = ∅, S will run\nindefinitely\n\nFor this problem:\nWe may have to play around with its state diagram\nCreate a Turing machine that analyze a DFA by marking all\nstates reachable from the start state. If no accept state is\nmarked, the DFA accepts no string\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidable Languages\n\nConsider the following TM T :\nT =“On input hAi, where A is a DFA:\n1\n2\n3\n\n4\n\nMark the start state of A.\nRepeat until no new states get marked:\nMark any state that has a transition coming into it from\nany state that is already marked.\nIf no accept state is marked, accept; otherwise, reject.”\n\nWe should not need to go down to the state diagram of a\nDFA unless it is necessary\nRely on graph theory\nTo prove, it must be based on graph theory\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidable Languages\nEQDFA\nLet EQDFA = {hA, Bi | A and B are DFAs and L(A) = L(B)}.\nShow that EQDFA is decidable.\nGiven two DFAs A and B, is there any algorithm to check\nwhether L(A) = L(B)?\nIn other words, can we have a Turing machine F such that\nIf hA, Bi ∈ EQDFA (L(A) = L(B)), F will accept hA, Bi and\nIf hA, Bi 6∈ EQDFA (L(A) 6= L(B)), F will reject hA, Bi\n\nAgain, we cannot simply run both DFAs on all strings until\none accept but the other reject\nThis may loop indefinitely\n\nSince L(A) and L(B) are sets, we need to use the set theory\nL(A) = L(B) iff L(A) ⊆ L(B) and L(B) ⊆ L(A)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidable Languages\nL(A) ⊆ L(B)\nL(B)\nL(A)\nL(B)\n\nFrom the above diagram L(A) ∩ L(B) = ∅\nL(B) ⊆ L(A)\nL(A)\nL(B)\nL(A)\n\nFrom the above diagram L(A) ∩ L(B) = ∅\nL(A) = L(B) iff (L(A) ∩ L(B)) ∪ (L(A) ∩ L(B)) = ∅.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fThe set of regular languages\nClosed Under Complement:\nGiven a DFA A = (Q, Σ, δ, q0 , F ) we can construct a DFA A0\nwhere L(A0 ) = L(A).\nA0 = (Q, Σ, δ, q0 , Q − F )\n\nClosed Under Union:\nGiven two DFAs A = (QA , Σ, δA , qA , FA ) and\nB = (QB , Σ, δB , qB , FB ) we can construct a DFA\nC = (QC , Σ, δC , qC , FC ) where L(C) = L(A) ∪ L(B).\nQC = QA × QB\nδC ((qi , qj ), x) = (δA (qi , x), δB (qj , x)) where qi ∈ QA and\nqj ∈ QB .\nqC = (qA , qB )\nFC = {(qi , qj ) | qi ∈ FA or qj ∈ FB }.\n\nClosed Under Intersection:\nSame as Closed Under Union except that\nFC = {(qi , qj ) | qi ∈ FA and qj ∈ FB }.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidable Language\nTo show that\n(L(A) ∩ L(B)) ∪ (L(A) ∩ L(B)) = ∅\nWe need to construct a DFA C such that\nL(C) = (L(A) ∩ L(B)) ∪ (L(A) ∩ L(B))\nand check that L(C) = ∅ (hCi ∈ EDFA )\nThis can be done by:\n1\n2\n3\n4\n5\n6\n\nConstruct DFA D such that L(D) = L(B)\nConstruct DFA E such that L(E) = L(A) ∩ L(D)\nConstruct DFA F such that L(F ) = L(A)\nConstruct DFA G such that L(G) = L(F ) ∩ L(B)\nConstruct DFA C such that L(C) = L(E) ∪ L(G)\nRun T (a decider for EDFA ) on input hCi\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidable Languages\n\nFor simplicity, we can simply construct a DFA C such that\nL(C) = (L(A) ∩ L(B)) ∪ (L(A) ∩ L(B))\nAgain, we need to show that C accepts no string (L(C) = ∅)\nThis is an example of a TM:\nF =“On input hA, Bi, where A and B are DFAs:\n1\n\nConstruct DFA C such that\nL(C) = (L(A) ∩ L(B)) ∪ (L(A) ∩ L(B))\n\n2\n3\n\nRun TM T from Theorem 4.4 on input hCi.\nIf T accepts hCi, accept. If T rejects hCi, reject.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fProve that EQ DFA is Decidable\nProve that EQ DFA = {hA, Bi | A and B are DFAs and L(A) = L(B)} is decidable.\nSolution: Construct a TM F as follows:\nF =“On input hA, Bi, where A and B are DFAs:\n1\n\nConstruct DFA C such that L(C) = (L(A) ∩ L(B)) ∪ (L(A) ∩ L(B))\n\n2\n\nRun TM T from Theorem 4.4 on input hCi.\n\n3\n\nIf T accepts hCi, accept. If T rejects hCi, reject.\n\nNow, we need to prove that F is a decider for EQ DFA .\n1\n\nAssume that hA, Bi ∈ EQ DFA . Since hA, Bi ∈ EQ DFA , A and B are DFAs\nand L(A) = L(B). Since L(A) = L(B), according to the definition of the\nlanguage of DFA C, L(C) = ∅. Since L(C) = ∅, hCi ∈ EDFA . By running T\non input hCi, T will accept hCi. Since T accepts hCi, F accepts hA, Bi.\n\n2\n\nAssume that hA, Bi 6∈ EQ DFA . Since hA, Bi 6∈ EQ DFA , A and B are DFAs\nand L(A) 6= L(B). Since L(A) 6= L(B), according to the definition of the\nlanguage of DFA C, L(C) 6= ∅. Since L(C) 6= ∅, hCi 6∈ EDFA . By running T\non input hCi, T will reject hCi. Since T rejects hCi, F rejects hA, Bi.\n\nThis shows that F is a decider for EQ DFA . Therefore, EQ DFA is decidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fDecidability\nSo far we have the following Turing machines:\n1\n\nTM M that decides ADFA where\nADFA = {hB, wi | B is a DFA that accepts w}\n\n2\n\nTM N that decides ANFA where\nANFA = {hB, wi | B is an NFA that accepts w}\n\n3\n\nTM P that decides AREX\nAREX = {hR, wi | R is a regular expression that generates w}\n\n4\n\nTM T that decides EDFA\nEDFA = {hAi | A is an DFA and L(A) = ∅}\n\n5\n\nTM F that decides EQ DFA\nEQ DFA = {hA, Bi | A and B are DFAs and L(A) = L(B)}\n\nWe can use these TMs as helper machines\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fONE DFA\nHow do we know that a DFA accepts exactly one string?\nThis problem corresponds with the language ONE DFA as\nfollows:\nONE DFA = {hAi | A is a DFA and |L(A)| = 1}\nHow to show that ONE DFA is decidable?\nConstruct a TM M 0 such that\n1\n2\n\nIf hAi ∈ ONE DFA , M 0 accepts hAi\nIf hAi 6∈ ONE DFA , M 0 rejects hAi\n\nNote that we cannot run a DFA A on all string until it\naccepts a string because it may loop indefinitely\nBut if L(A) 6= ∅, it will not loop indefinitely\nSo, we need to check that the language of a given DFA is not\nempty first\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fONE DFA\n\nConstruct a TM M 0 as follows:\nM 0 = “On input hAi where A is a DFA:\n1\n2\n3\n\n4\n5\n6\n\nRun T on input hAi\nIf T accepts hAi, reject.\nIf T rejects hAi, run A on all inputs until it accepts an input\nstring w\nConstruct a DFA B such that L(B) = {w}\nRun F on input hA, Bi.\nIf F accepts hA, Bi, accept. Otherwise, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\fONE DFA\nProve that if hAi ∈ ONE DFA , M 0 accepts hAi:\nAssume that hAi ∈ ONE DFA . Since hAi ∈ ONE DFA , A is a DFA and\n|L(A)| = 1. Since |L(A)| = 1, L(A) 6= ∅. Since L(A) 6= ∅, hAi 6∈ EDFA . Since\nT is a decider for EDFA , by running T on input hAi, T will reject hAi. Since\n|L(A)| = 1, by running A on all strings, it will eventually accept a string w.\nSince |L(A)| = 1 and A accepts w, L(A) = {w} which is the same as the\nlanguage of B. Since L(A) = L(B), hA, Bi ∈ EQ DFA . By running F on input\nhA, Bi, F will accept hA, Bi. Since F accepts hA, Bi, M 0 accepts hAi.\nProve that if hAi 6∈ ONE DFA , M 0 rejects hAi:\nAssume that hAi 6∈ ONE DFA . Since hAi 6∈ ONE DFA , A is a DFA and\n|L(A)| 6= 1. In other words, L(A) = ∅ or |L(A)| > 1.\nIf L(A) = ∅, hAi ∈ EDFA . Since T is a decider for EDFA , by running T on\ninput hAi, T will accept hAi. Since T accepts hAi, M 0 rejects hAi.\nIf |L(A)| > 1, by running A on all strings, it will eventually accept a string w.\nSince |L(A)| > 1 and A accepts w, L(A) 6= {w} which is not the same as the\nlanguage of B. Since L(A) 6= L(B), hA, Bi 6∈ EQ DFA . By running F on input\nhA, Bi, F will reject hA, Bi. Since F rejects hA, Bi, M 0 rejects hAi.\nThis shows that M 0 is a decider for ONE DFA . Therefore, ONE DFA is decidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 02\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":251,"segment": "unlabeled", "course": "cs1502", "lec": "lec25_reducibility_08","text":"Reducibility 08\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fMapping Reducibility\nDefinition 5.20\nLanguage A is mapping reducible to language B, written\nA ≤m B, if there is a computable function f : Σ∗ → Σ∗ , where for\nevery w,\nw ∈ A ↔ f (w) ∈ B.\nThe function f is called the reduction from A to B.\n\nA\n\nB\nf\n\nf\n\nFunction f reducing A to B\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fReductions\nCrucial Features in a Reduction:\n1\n\nFor every instance w of the first problem that we start with,\nwe must be able to obtain the instance f (w) of the second\nproblem algorithmically\nIn our discussion, this reduction f : Σ∗ → Σ∗ will be a TM\nthat produces an output string\n\n2\n\nThe answer to the second question for the instance f (w) must\nbe the same as the answer to the original question for w\nw ∈ A ↔ f (w) ∈ B.\nIn other words, for every w,\nIf f (w) ∈ B, w must be in A\nIf f (w) 6∈ B, w must not be in A\n\n3\n\nThe reduction does not have to be one-to-one or onto as long\nas it satisfies the above property\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fMapping Reducibility\n\nTheorem 5.22\nIf A ≤m B and B is decidable, then A is decidable.\nProof:\nAssume that A ≤m B and B is decidable\nSince A ≤m B, there exists a reduction in the form of a TM\nF satisfying\nw ∈ A ↔ F (w) ∈ B\nSince B is decidable, there exists a TM MB that decides B\nWe just need to use the same method as in Chapter 4 to show\nthat A is decidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fMapping Reducibility\nConstruct a decider N (to decide A) as follows:\nN =“On input w:\n1\n2\n3\n\nRun TM F (a reduction from A to B) on w to produce F (w).\nRun MB (a decider for B) on F (w)\nIf MB accepts F (w), accept. If MB rejects F (w), reject.”\n\nProve that if w ∈ A, N accepts w:\nAssume that w ∈ A. Since w ∈ A, from the property of the reduction F ,\nF (w) ∈ B. Since F (w) ∈ B and MB is a decider for B, by running MB on\ninput F (w), MB will accept F (w). Since MB accepts F (w), N accepts w.\n\nProve that if w 6∈ A, N rejects w:\nAssume that w 6∈ A. Since w 6∈ A, from the property of the reduction F ,\nF (w) 6∈ B. Since F (w) 6∈ B and MB is a decider for B, by running MB on\ninput F (w), MB will reject F (w). Since MB rejects F (w), N rejects w.\n\nThis shows that N is a decider for A\nTherefore A is decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fMapping Reducibility\n\nTheorem 5.22\nIf A ≤m B and B is decidable, then A is decidable.\nLet the following:\nP ≡ A ≤m B\nQ ≡ B is decidable\nR ≡ A is decidable\n\nSince (P ∧ Q) → R ≡ (P ∧ ¬R) → ¬Q, we have\nCorollary 5.23\nIf A ≤m B and A is undecidable, then B is undecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fMapping Reducibility\nCorollary 5.23\nIf A ≤m B and A is undecidable, then B is undecidable.\nWe already know from the fact that ATM is undecidable\nBecause of the above Corollary, to show that a language B is\nundecidable, we just need to show that ATM ≤m B\nATM can be replaced by other undecidable language such as\nETM\nTo show that ATM ≤m B, we need to construct a TM F that\nbehaves like a reduction from ATM to B satisfying\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ B\nAlso need to prove that F actually satisfies the above property\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fHALT TM is undecidable\nATM and HALTTM\nATM = {hM, wi | M is a TM and M accepts w}\nHALT TM = {hM, wi | M is a TM and M halts on input w}\nTo use mapping reducibility to show that HALT TM is\nundecidable:\nWe need to show that ATM is mapping reducible to HALT TM\n(ATM ≤m HALT TM )\nSince ATM ≤m HALT TM and ATM is undecidable, from\nCorollary 5.23, HALT TM is undecidable\n\nTo show that ATM ≤m HALT TM , we need a computable\nfunction F : Σ∗ → Σ∗ (TM F ) such that\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ HALT TM\nNote that F (hM, wi) must be in the form of hM 0 , w0 i (the\nformat of HALT TM )\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fHALT TM is undecidable\n\nA computable function F where F (hM, wi) = hM 0 , w0 i is a\nTM\nA TM that takes hM, wi as its input on its tape and produces\nthe output hM 0 , w0 i on its tape after it is halted\n\nNote that TM F must be a reduction from ATM to HALT TM :\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ HALT TM\nwhere F (hM, wi) = hM 0 , w0 i\nhM, wi ∈ ATM → F (hM, wi) ∈ HALT TM\nM accepts w → M 0 halts on input w0\nhM, wi 6∈ ATM → F (hM, wi) 6∈ HALT TM\nM does not accept w → M 0 does not halt on input w0\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fHALT TM is Undecidable\nMachine F computes a reduction f :\nF =“On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct the following machine M 0 :\nM 0 =“On input x:\n1\n2\n\n2\n\nRun M on w.\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nOutput hM 0 , εi.”\n\nNext, we need to check whether TM F satisfies\nhM, wi ∈ ATM ↔ hM 0 , w0 i ∈ HALT TM\nwhere hM 0 , εi = F (hM, wi) (the output of TM F )\nTo do so, we need to show the following:\n1\n2\n\nIf hM, wi ∈ ATM , hM 0 , εi ∈ HALT TM\nIf hM, wi 6∈ ATM , hM 0 , εi 6∈ HALT TM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fHALT TM is Undecidable\nShow that\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ HALT TM\nwhere F (hM, wi) = hM 0 , εi\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M is a\nTM that accepts w. Since M accepts w, from the definition of\nTM M 0 , M 0 halts (accepts) on all strings including ε. Since\nM 0 halts on ε, hM 0 , εi ∈ HALT TM . Since\nhM 0 , εi = F (hM, wi), F (hM, wi) ∈ HALT TM .\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M is a\nTM that does not accept w. Since M does not accept w, from\nthe definition of TM M 0 , M 0 does not halt on all strings\nincluding ε. Since M 0 does not halt on ε, hM 0 , εi 6∈ HALT TM .\nSince hM 0 , εi = F (hM, wi), F (hM, wi) 6∈ HALT TM .\n\nThis show that TM F is a reduction from ATM to HALT TM\nIn other words, ATM ≤m HALT TM\nSince ATM is undecidable, HALT TM is undecidable (Corollary\n5.23)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fHALT TM is Undecidable\nLet’s compare two methods:\nMethod #1: We construct a TM S to decide ATM as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct a TM M 0 as follows:\nM 0 = “On input x:\n1\n2\n\n2\n3\n\nRun M on input w.\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nRun R (a decider for HALT TM ) on input hM 0 , εi.\nIf R accepts hM 0 , εi, accept. If R rejects hM 0 , εi, reject.”\n\nMethod #2: Construct a reduction F from ATM to\nHALT TM as follows:\nF = “On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct a TM M 0 as follows:\nM 0 = “On input x:\n1\n2\n\n2\n\nRun M on input w.\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nOutput hM 0 , εi.”\n\nDid you see the similarity?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fETM is Undecidable\nTo show that ETM is undecidable using the mapping\nreducibility, we need to show that ATM ≤m ETM\nIn other word, we need to find a computable function\nF : Σ∗ → Σ∗ (TM F ) such that\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ ETM\nSuppose we construct the following TM F :\nF = “On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct TM M 0 as follows:\nM 0 = “On input x:\n1\n2\n\n2\n\nRun M on input w.\nIf M accepts w, accept. If M rejects w, reject.”\n\nOutput hM 0 i.”\n\nThe above TM F does not satisfy the above property\nIt satisfies\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ ETM\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fETM is Undecidable\n\nTM F is a reduction from ATM to ETM\nThis shows that ATM ≤m ETM\nBy Corollary 5.23, since ATM is undecidable, ETM is\nundecidable\nNote that if ETM is decidable, ETM is decidable\nWe discussed this toward the end of Chapter 4\n\nTherefore, ETM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fREGULAR TM is Undecidable\nUse the mapping reducibility to show that\nREGULAR TM = {hM i | M is a TM and L(M ) is regular} is undecidable.\nSolution: We need to construct a reduction F from ATM to REGULAR TM satisfying\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ REGULAR TM\nConstruct a TM F as follows:\nF = “On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct a TM M 0 as follows:\nM 0 = “On input x:\n1\n2\n\nIf x is in the form of 0n 1n , accept.\nIf x is not in the form of 0n 1n , run M on input w. If M accepts w,\naccept; otherwise, reject.”\n\n2\n\nOutput hM 0 i.”\n\nNext we need to show that the above TM F satisfies the property:\nhM, wi ∈ ATM ↔ hM 0 i ∈ REGULAR TM\nwhere hM 0 i = F (hM, wi) (output of TM F ):\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fREGULAR TM is Undecidable\nContinue:\nIf hM, wi ∈ ATM , F (hM, wi) ∈ REGULAR TM :\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M is a TM that accepts w.\nSince M accepts w, from the definition of TM M 0 , L(M 0 ) = Σ∗ . Since\nL(M 0 ) = Σ∗ , L(M 0 ) is regular. Since L(M 0 ) is regular,\nhM 0 i ∈ REGULAR TM . Since F (hM, wi) = hM 0 i,\nF (hM, wi) ∈ REGULAR TM .\nIf hM, wi 6∈ ATM , F (hM, wi) 6∈ REGULAR TM :\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M is a TM that does not\naccept w. Since M does not accept w, from the definition of TM M 0 ,\nL(M 0 ) = {0n 1n | n ≥ 0}. Since L(M 0 ) = {0n 1n | n ≥ 0}, L(M 0 ) is not\nregular. Since L(M 0 ) is not regular, hM 0 i 6∈ REGULAR TM . Since\nF (hM, wi) = hM 0 i, F (hM, wi) 6∈ REGULAR TM .\nThis show that TM F is a reduction from ATM to REGULAR TM . Therefore\nATM ≤m REGULAR TM . Since ATM is undecidable, by Corollary 5.23,\nREGULAR TM is undecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fEQ TM is Undecidable\nWe need to show that ETM ≤m EQ TM\nWe need to find a computable function F : Σ∗ → Σ∗ such\nthat\nhM i ∈ ETM ↔ F (hM i) ∈ EQ TM .\nwhere F (hM i) = hM1 , M2 i for some TMs M1 and M2\nHere is the TM F :\nF = “On input hM i where M is a TM:\n1\n\nConstruct a TM ME as follows:\nME = “On input x:\n\n2\n\nOutput hM, ME i.”\n\n1\n\nreject.”\n\nAfter proofs, we can make a conclusion that F is a reduction\nfrom ETM to EQ TM\nTherefore, ETM ≤m EQTM\nSince ETM is undecidable, by Corollary 5.23, EQ TM is\nundecidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fUndecidability of the Post Correspondence Problem\nEarlier, we construct an instance P 0 of MPCP that has a\nmatch if and only if M accepts w\nFormally,\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ MPCP\nwhere F (hM, wi) = P 0\nThis shows that ATM ≤m MPCP\nSince ATM is undecidable, MPCP is undecidable\n\nThen we construct an instance P of PCP that has a match if\nand only if P 0 has a matchFormally,\nP 0 ∈ MPCP ↔ F 0 (P 0 ) ∈ PCP .\nwhere F 0 (P 0 ) = P\nThis shows that MPCP ≤m PCP\nSince MPCP is undecidable, PCP is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\fConclusion\nThe mapping reducibility can be used to show that a\nlanguage is undecidable\nCorollary 5.23\nIf A ≤m B and A is undecidable, then B is undecidable\nSince ATM is undecidable, to show that a language B is\nundecidable, simply show that ATM ≤m B\nNeed to show that a reduction F satisfying\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ B\nexists\nInstead of ATM , other known undecidable language can be\nused\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 08\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":252,"segment": "self_training_2", "course": "cs1502", "lec": "lec28_time_complexity_03","text":"Time Complexity 03\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fHamiltonian Path\nA Hamiltonian path in a directed graph G is a directed path\nthat goes through each node exactly once\nFormally,\nHAMPATH = {hG, s, ti | G is a directed graph with a\nHamiltonian path from s to t}\n\ns\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nt\n\nTime Complexity 03\n\n\fVerifier\nSuppose a path is represented by a sequence of pairs of nodes\nFor example: ((s, a), (a, c), (c, d), (d, r), (r, t))\nGiven a path c, we can verify whether it is a Hamiltonian path\nof graph G from s to t in polynomial time\nGiven an input hhG, s, ti, ci, where G is a directed graph, s\nand t are nodes, and c is a path, a TM can verify whether c is\na Hamiltonian path of graph G from s to t in polynomial time\n\nSimilarly, given a natural number c, we can verify whether c is\na factor of a composite number x in polynomial time\nGiven an input hx, ci where x and c are natural numbers, a\nTM can verify whether c is a factor of x in polynomial time\n\nThis feature is called polynomial verifiability\nRecall that both problems need exponential time to solve\nVerifying is easier than finding\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fVerifier\nRecall the language COMPOSITES :\nCOMPOSITES = {hxi | x = pq for natural numbers p, q > 1}\n\nWe can define the identical set using a different predicate\nCOMPOSITES = {hxi | x is divisible by c a natural number c > 1}\n\nCheck whether x is divisible by c can be easily done by this\nTM V :\nV = “On input hx, ci where x and c are natural numbers\ngreater than 1:\n1\n2\n\nCalculate y = x mod c\nIf y = 0, accept. If y 6= 0, reject.”\n\nWith the TM V above, now we have\nCOMPOSITES = {hxi | V accepts hx, ci for some c}\n\nThis TM V is called a verifier\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fVerifier\nA verifier for a language A is an algorithm V , where\nA = {w | V accepts hw, ci for some string c}\nIn our discussion, an algorithm V is a TM\nIf TM V runs in polynomial time, it is called a polynomial\ntime verifier\nA language A is polynomially verifiable if it has a polynomial\ntime verifier\n\nc is called a certificate or proof of membership in A\nExamples:\nA certificate of HAMPATH problem for hG, s, ti is simply a\nHamiltonian path from s to t\nA certificate of COMPOSITES problem for a number x is one\nof its divisors\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fClass NP\n\nDefinition 7.19\nNP is the class of languages that have polynomial time verifiers.\nNP comes from Nondeterministic Polynomial time\nNP -Problems are set of problems in NP\nEvery problems in P also have polynomial time verifier\nTherefore, P ⊆ NP\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fNondeterministic\nRecall that NP stands for Nondeterministic Polynomial\nThe word nondeterministic has been discussed int\nNondeterministic Finite Automaton (NFA)\nNondeterministic Turing Machine (NTM)\n\nWe have not yet seen a nondeterministic Turing machine yet\nExample:\nt\n\n→ 0, R\n\nq0\n\nt\n\n→ 0, R\n\nq1\nt\n\n→ 1, R\n\nt\n\n→ 0, R\n\nq2\nt\n\n→ 1, R\n\nq3\nt\n\n→ 1, R\n\nWhen it read the blank symbol, it splits to two copies, same\ncurrent state, but different tape contents\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fNondeterministic\nt\n\n→ 0, R\n\nq0\n\nt\n\n→ 0, R\n\nq1\nt\n\n→ 1, R\n\nt\n\n→ 0, R\n\nq2\nt\n\n→ 1, R\n\nq3\nt\n\n→ 1, R\n\nWhat would happen if we run the above machine on input ε\nfor three steps?\nConsists of 8 copies\nThey are all in state q3\nTape contents are unique:\n000, 001, 010, 011, 100, 101, 110, and 111\n\nThis is how we generate all possible strings over Σ = {0, 1} of\nlength exactly 3\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fNondeterministic\nt\n\n→ 0, R\n\nq0\n\nt\n\n→ 0, R\n\nq1\nt\n\n→ 1, R\n\nt\n\n→ 0, R\n\nq2\nt\n\n→ 1, R\n\nq3\nt\n\n→ 1, R\n\nBut how to express the above TM in a high-level description?\nM = “On input w:\n1\n\n2\n\nWrite a string of length 3 where each symbol on the string is\nnondeterministically selected to be either 0 or 1\n...\n\nThis type of NTM is suitable for describing brute-force\nalgorithm\nA brute-force algorithm generally generates all possible of\nsomething\nThen search for an element in the set of possibilities\nIf it finds an element that satisfies the criteria, done\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fNondeterministic TM for HAMPATH\n\nA nondeterministic TM N1 that decides HAMPATH :\nN1 = “On input hG, s, ti, where G is a directed graph with\nnodes s and t:\n1\n\n2\n3\n4\n\nWrite a list of m numbers p1 , . . . pm , where m is the number\nof nodes in G. Each number in the list is nondeterministically\nselected to be between 1 and m.\nCheck for repetitions in the list. If any are found reject.\nCheck whether s = p1 and t = pm . If either fail, reject.\nFor each i between 1 and m − 1, check whether (pi , pi+1 ) is\nan edge of G. If any are not, reject. Otherwise, all tests have\nbeen passed. so accept.\n\nThe nondeterministic part of N1 is in step 1\nIt generates all permutations of m nodes\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fNondeterministic TM for HAMPATH\nWhat is the run-time of NTM N1 ?\nStep 1 takes only m steps to write m numbers\nCheck for repetition take O(m2 )\nGet the first number and compare with the rest is O(m)\nGet the second number and compare with the rest is O(m)\nand so on (m times)\n\nCheck whether the first number is s and the last number is t is\nO(m)\nStep 4 takes O(m2 )\nGet the first pair and scan for the edge is O(m)\nGet the second pair and scan for the edge is O(m)\nand so on (m − 1 times)\n\nNote that all copies run steps 2, 3, and 4 simultaneously\nThe above TM runs in polynomial time\nFrom Theorem 7.11, this algorithm will run in exponential time\non a single-tape Turing machine\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fLanguage in NP\n\nIs there a relation between problem in NP and NTM?\nYes:\nTheorem 7.20: A language is in NP iff it is decided by some\nnondeterministic polynomial time Turing machine.\nBut the NP is the class of languages that have polynomial\ntime verifiers\nProof Idea:\nShow how to convert a polynomial time verifier to an\nequivalent polynomial time nondeterministic Turing machine\n(decider)\nShow how to convert polynomial time nondeterministic Turing\nmachine (decider) to an equivalent polynomial time verifier\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fVerifier to NTM\nAssume that A ∈ NP\nBy definition, there exists a TM V (a polynomial time verifier)\nfor the language A that can verify an input of length n in nk\nsteps for some k\nRecall that a verifier for a language A is an algorithm V ,\nwhere\nA = {w | V accepts hw, ci for some string c}\n\nConstruct a NTM N as follows:\nN = “On input w of length n:\n1\n2\n3\n\nNondeterministically select string c of length at most nk .\nRun V on input hw, ci.\nIf V accepts, accept; otherwise, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fNTM to Verifier\n\nAssume that A can be decided by NTM N in polynomial time\nConstruct a polynomial time verifier V as follows:\nV = “On input hw, ci, where w and c are strings:\n1\n\n2\n\nSimulate N on input w, treating each symbol of c as a\ndescription of the nondeterministic choice to make at each step\nIf this branch of N ’s computation accepts, accept; otherwise,\nreject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fLanguage in NP\nDefinition 7.21\nNTIME(t(n)) = {L | L is a language decided by an O(t(n)) time\nnondeterministic Turing machine}.\nCorollary 7.22\nNP =\n\n[\n\nNTIME(nk )\n\nk\n\nTo show that a language A is in NP :\nConstruct a nondeterministic TM M that decides A\nVerify that M runs in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fCLIQUE\nA clique in an undirected graph is a subgraph where every\ntwo nodes in the subgraph are connected by an edge\nA k-clique is a clique that contains k nodes\nA graph with 5-clique is shown below:\n\nFormally,\nCLIQUE = {hG, ki | G is an undirected graph with a k-clique}\n\nIs CLIQUE in NP ?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fCLIQUE ∈ NP\n\nTo show that CLIQUE ∈ NP , we can perform one of the\nfollowing:\nShow a polynomial time verifier V for CLIQUE or\nShow an NTM N that decides CLIQUE in polynomial time\n\nThe following TM V verifies CLIQUE in polynomial time:\nV = “On input hhG, ki, ci:\n1\n2\n3\n\nTest whether c is a subgraph with k nodes in G.\nTest whether G contains all edges connecting nodes in c.\nIf both pass, accept; otherwise, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fCLIQUE ∈ NP\n\nAnalysis of TM V :\nEach step will be executed only once\nStep 1: Need to go back and forth k times\nEach time is O(n) step\nThus step 1 is k × O(n) = O(n)\n\nStep 2: for each node a ∈ c, it needs to find either (a, b) or\n(b, a) for every b ∈ c and b 6= a\nFor each node, it needs to scan back and forth k − 1 times\nand each time is O(n)\nThere are k nodes in c. Thus, step 2 is k × (k − 1) × O(n) =\nO(n)\n\nThus, V runs in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fCLIQUE ∈ NP\n\nThe following NTM N can decide CLIQUE in polynomial\ntime:\nN = “On input hG, ki, where G is a graph:\n1\n2\n3\n\nNondeterministically select a subset c of k nodes of G.\nTest whether G contains all edges connecting nodes in c.\nIf yes, accept; otherwise, reject.”\n\nAnalysis of NTM N :\nEach step will be execute only once\nStep 1: To generate a subset c of k nodes of G requires k steps\nStep 2: This step is the same as Step 2 of V which is O(n)\nsteps\nStep 3: Simply O(1) or O(n) depending on implementations\nThus N can decides CLIQUE in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fSUBSET −SUM\nConsider a set of numbers S = {x1 , x2 , . . . , xk } and a target\nnumber t\nIs\nPthere exists a set Y = {y1 , y2 , . . . , yi } ⊆ S such that\ni yi = t?\nFormally:\nSUBSET −SUM = {hS, ti | S = {x1 , x2 , . . . , xk }, and for some\n{y1 , y2 , . . . , yi } ⊆ {x1 , x2 , . . . , xk },\nX\nwe have\nyi = t}\ni\n\nFor example h{4, 11, 16, 21, 27}, 25i ∈ SUBSET −SUM\nbecause 4 + 11 = 25\nNote that for SUBSET −SUM , S and Y are multisets which\nallow duplicate elements\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\fSUBSET −SUM ∈ N P\nTo show that SUBSET −SUM ∈ NP , we can perform one of\nthe following:\n1\n2\n\nShow a polynomial time verifier V for SUBSET −SUM\nShow an NTM N that decide SUBSET −SUM in polynomial\ntime\n\nThe following TM V verifies SUBSET −SUM in polynomial\ntime:\nV = “On input hhS, ti, ci:\n1\n2\n3\n\nTest whether c is a collection of numbers that sum to t.\nTest whether S contains all the numbers in c.\nIf both pass, accept; otherwise, reject.”\n\nThe following NTM N decides SUBSET −SUM in\npolynomial time:\nN = “On input hS, ti:\n1\n2\n3\n\nNondeterministically select a subset c of the number in S.\nTest whether c is a collection of numbers that sum to t.\nIf the test passes, accept; otherwise, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 03\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":253,"segment": "self_training_2", "course": "cs1502", "lec": "lec03_finite_automata_02","text":"Finite Automata 02\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fFinite Automata\nThe computational model called finite automata can be used\nto simulate a set of simple algorithms\nCheck whether a string starts with 010\nCheck whether a string ends with 111\nCheck whether a string contains 0101 as a substring\nCheck whether a string contains substrings 000 and 111 where\n000 comes before 111\n\nIt is a powerful tool in compiler\nAccept or reject your source code based on a programming\nsyntax\nExample: the for statement:\nstarts with for\nfollowed by (\nfollowed by assignment statement(s)\nfollowed by ;\nfollowed by conditional statement(s)\nfollowed by ;\nfollowed by assignment statement(s)\nfollowed by )\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fFormal Definition of Computation\n\nLet M = (Q, Σ, δ, q0 , F ) be a finite automaton and let\nw = w1 w2 . . . wn be a string where each wi is a member of\nthe alphabet Σ.\nM accepts w if a sequence of states r0 , r1 , . . . , rn in Q exists\nwith three conditions:\n1\n2\n3\n\nr0 = q 0\nδ(ri , wi+1 ) = ri+1 , where i = 0, . . . , n − 1\nrn ∈ F\n\nThink in terms of processing the input string w\nw\n\nw\n\nw\n\nw\n\nw\n\nwn−1\n\nw\n\nr0 →1 r1 →2 r2 →3 r3 →4 r4 →5 . . . → rn−1 →n rn\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fRegular Languages\nA language L over an alphabet Σ is said to be a regular\nlanguage if some finite-state automaton recognizes it.\nConsider the following machine M :\n0\n\n0\n1\n\nq0\n\nq1\n\n1\n\nWhat is the language of this machine?\nL(M ) = {w | w contains an odd number of 1s}\n\n“The set of all strings consisting of an odd number of 1s” is a\nregular language\nL(M ) is a regular language\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fRegular Languages\n\nWhy regular language is important in our discussion?\nDefinition: A language is regular if some finite-state machines\nrecognize it.\nIf we can prove that a language is regular\nWe must be able to construct a finite-state machine to\nrecognize it\nIt maybe hard to build but I know that it exists\n\nIf we can prove that a language is not regular\nWe cannot construct a finite-state machine to recognize it\n\nThis is an example of a limitation of this computational model\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fProblem and Language\nIn theory of computation, a problem is represented as a\nlanguage\nA problem of determining whether a string contains 011 as a\nsubstring\nL(M ) = {x | x contains 011 as a substring}\nWe already see a Deterministic Finite Automaton (DFA) M\nthat accepts all strings that contains 011 as a substring and\nreject those that does not contain 011 as a substring\nIt means this problem is solvable by the algorithm captured by\nthe previous DFA\nIn case of algorithm in a form of DFA (not all algorithms)\nif L(M ) is regular, the problem represented by L(M ) is\nsolvable\nif L(M ) is not regular, no DFA exists, the problem\nrepresented by L(M ) is unsolvable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fProblems and Languages\nSolvable problems that we see so far\nThe problem of determining whether a string ends with a 1\n{x | x ends with a 1}\nThe problem of determining whether a string is an empty\nstring or ends in a 0\n{x | x is an empty string or ends in a 0}\nThe problem of determining whether a string starts and ends\nwith the same symbol\n{x | x starts and ends with the same symbol}\nThe problem of determining whether a string contains either\n11 or 00 as a substring\n{x | x contains either 11 or 00 as a substring}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fProblems and Languages\nSolvable problems that we see so far (continue)\nThe problem of determining whether a string contains 011 as a\nsubstring\n{x | x contains 011 as a substring}\nThe problem of determining whether a string ends with 0110\n{x | x ends with 0110}\nThe problem of determining whether a string contains an odd\nnumber of 1s\n{x | x contains an odd number of 1s}\n\nEach of the above languages is regular since we can construct\na DFA that recognizes it.\nBut if a language is very complicate, it will be difficult to\nconstruct a DFA that recognizes it\n\nWe need tools to help us to determine whether a language is\nregular or not\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fThe Regular Operations\nIn arithmetic:\nObjects are numbers\nTools are operations for manipulating numbers (e.g., + and ×)\n1 + 1 gives you a new number which is 2\n\nIn the theory of computation,\nObjects are languages (sets of strings)\nTools are operations for manipulating languages\n\nDefinition 1.23\nLet A and B be languages. We define the regular operations as\nfollows:\nUnion: A ∪ B = {x | x ∈ A or x ∈ B}\nConcatenation: A ◦ B = {xy | x ∈ A and y ∈ B}\nStar: A∗ = {x1 x2 . . . xk | k ≥ 0 and each xi ∈ A}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fExamples (Union)\n\nLet Σ = {0, 1}\nConsider the following languages A and B\nA = {00, 11}\nB = {010, 101}\n\nThe union operations is identical to the set’s union operation:\nA ∪ B = {x | x ∈ A or x ∈ B}\nFrom the above definition:\nA ∪ B = {00, 11, 010, 101}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fExamples (Concatenation)\n\nLet Σ = {0, 1}\nConsider the following languages A and B\nA = {00, 11}\nB = {010, 101}\n\nThe definition of concatenation is defined as\nA ◦ B = {xy | x ∈ A and y ∈ B}\nFrom the above definition:\nA ◦ B = {00010, 00101, 11010, 11101}\nFor simplicity, sometimes we write AB instead of A ◦ B\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fExamples (Star)\nLet Σ = {0, 1}\nConsider the following language A\nA = {00, 11}\n\nThe definition of start is defined as\nA∗ = {x1 x2 . . . xk | k ≥ 0 and xi ∈ A}\nIf k = 0, the above definition becomes\n{ | 0 ≥ 0} = {ε}\nIf k = 1, the above definition becomes\n{x1 | 1 ≥ 0 and xi ∈ A} = {00, 11}\nIf k = 2, the above definition becomes\n{x1 x2 | 2 ≥ 0 and xi ∈ A} = {0000, 0011, 1100, 1111}\nIf k = 3, the above definition becomes\n{x1 x2 x3 | 3 ≥ 0 and xi ∈ A} = {000000, 000011, . . . , 111111}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fExamples (Star)\nLet Σ = {0, 1}\nSuppose A = {00, 11}, what is A∗ ?\nA∗ = {ε, 00, 11, 0000, 0011, 1100, 1111, 000000, . . . }\nSuppose A = {011}, what is A∗ ?\nA∗ = {ε, 011, 011011, 011011011, 011011011011, . . . }\nSuppose A = {0, 1}, what is A∗ ?\nA∗ = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, . . . }\nThis is the set of all strings over {0, 1}\nSuppose A = ∅, what is A∗ ?\nA∗ = {ε}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fDefinition of Closed Under Operations\n\nLet A be a set of objects (a collection of object)\nWe say that A is closed under operation 4 if for any x ∈ A\nand y ∈ A, x4y is also in A.\nExample: Let N be the set of natural number\nN is closed under addition\nFor any two natural numbers x and y, x + y is a natural\nnumber\n\nN is closed under multiplication\nFor any two natural numbers x and y, x × y is a natural\nnumber\n\nN is not closed under subtraction\n5 − 7 is not a natural number\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fDefinition of Closed Under Operations\n\nLet L be the set of all regular languages\nThis is a set of sets\n\nRecall that we have three operations, union, concatenation,\nand star\nIs L closed under union operation?\nFor any regular languages A and B, is A ∪ B a regular\nlanguage?\n\nIs L closed under concatenation operation?\nFor any regular languages A and B, is A ◦ B a regular\nlanguage?\n\nIs L closed under star operation?\nFor any regular language A, is A∗ a regular language?\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\nLet A be a set of strings over {0, 1} that contain a 00 as a\nsubstring\nIs A a regular language?\nCan you construct a DFA that recognizes the language A?\nOne of the machine that recognizes A can be as follows:\n1\n\n0,1\n\n0\n\nqa\n\n0\n\nqb\n\nqc\n\n1\n\nBecause there exists a DFA that recognizes A, A is a regular\nlanguage\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\n\nLet B be a set of strings over {0, 1} that end with a 1\nIs B a regular language?\nCan you construct a DFA that recognizes the language B?\nOne of the machine that recognizes B can be as follows:\n0\n\n1\n1\n\nq0\n\nq1\n0\n\nBecause there exists a DFA that recognizes B, B is a regular\nlanguage\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\nWe have A = {x | x contains 00 as a substring} is regular\nWe have B = {x | x ends with a 1} is regular\nHow about A ∪ B?\nA ∪ B = {x | x contains 00 as a substring or x ends with a 1}\nIt is quite straightforward to construct a machine that\nrecognizes A ∪ B (try to build one yourself)\n1\n\n0,1\n\n0\n1\n1\n0\n\n0\n\nThis does not prove that if A and B are regular, A ∪ B is\nregular\nThis is just one example out of infinite may instances of\nregular languages\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\nWe need to show that for any two regular languages A and B,\nA ∪ B is regular\nGiven a regular language A over a Σ, what do we know about\nthe language A?\nThere exists a DFA MA that recognizes A (L(MA ) = A)\nMA = (QA , Σ, δA , qA , FA ) for some QA , δA , qA , and FA\n\nSimilarly, given a regular language B over a Σ:\nThere exists a DFA MB that recognizes B (L(MB ) = B)\nMB = (QB , Σ, δB , qB , FB ) for some QB , δB , qB , and FB\n\nTo show that A ∪ B is regular for any regular languages A\nand B, we need to construct a DFA that recognizes A ∪ B\nfrom MA and MB\nTo understand the process, we are going to work on a specific\nexample\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\nRecall the previous two regular languages and its DFAs where\nΣ = {0, 1}\nA = {x | x contains 00 as a substring}\n1\n\n0,1\n\n0\n\nqa\n\n0\n\nqb\n\nqc\n\n1\n\nMA = (QA , Σ, δA , qA , FA ) and L(MA ) = A\nB = {x | x ends with a 1}\n0\n\n1\n1\n\nq0\n\nq1\n0\n\nMB = (QB , Σ, δB , qB , FB ) and L(MB ) = B\n\nGiven a string w and these two DFAs, how to check whether\nw is in A ∪ B?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\nRecall that A = L(MA ) and B = L(MB )\nThus, A ∪ B = L(MA ) ∪ L(MB )\n\nw ∈A∪B\niff w ∈ A or w ∈ B\niff w ∈ L(MA ) or w ∈ L(MB )\niff MA accepts w or w ∈ L(MB )\niff MA accepts w or MB accepts w\n\nIn other words,\nw ∈ A ∪ B if and only if MA accepts w or MB accepts w\nTo check whether w ∈ A ∪ B:\nRun both MA and MB on input w\nIf one of them or both accepts w, w ∈ A ∪ B\nIf both reject w, w 6∈ A ∪ B\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fL is regular under union operation\nWe can run both machines simultaneously\n1\n\n0,1\n\n0\n\n1\n1\n\n0\n\nqa\n\n0\n\nqb\n\nqc\n\nq0\n\nq1\n\n1\n0\n\nLet state (p, q) represents the situation where\nThe current state of MA is p\nThe current state of MB is q\n\nWith the new notion of states, we have\n0\n\n(qa , q0 )\n1\n1\n1\n\n(qa , q1 )\n\n0\n\n(qb , q0 )\n0\n\n0\n\n1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n(qb , q1 )\n\nFinite Automata 02\n\n(qc , q0 )\n0\n\n0\n1\n\n(qc , q1 )\n\n1\n\n\fL is regular under union operation\nLet MA recognizes A, where MA = (QA , Σ, δA , qA , FA )\nLet MB recognizes B, where MB = (QB , Σ, δB , qB , FB )\nMachine M = (Q, Σ, δ, q0 , F ) that recognizes A ∪ B can be\nconstructed as follows:\n1\n2\n\nQ = {(r1 , r2 ) | r1 ∈ QA and r2 ∈ QB }\nFor each (r1 , r2 ) ∈ Q and a ∈ Σ\nδ((r1 , r2 ), a) = (δA (r1 , a), δB (r2 , a))\n\n3\n4\n\nq0 = (qA , qB )\nF = {(r1 , r2 ) | r1 ∈ FA or r2 ∈ FB }\n\nTo recognize A ∩ B, simply change the set of accept states to\nF = {(r1 , r2 ) | r1 ∈ FA and r2 ∈ FB }\nIf A and B are regular languages, A ∪ B is regular\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\fConclusions\n\nA language is regular if it is recognized by some finite-state\nmachines\nIf you can prove that a language is regular:\nthere exists a finite-state machine that recognizes it\n\nIf you can prove that a language is not regular:\nthere is no finite-state machine that recognizes it\n\nIn formally, we show that if A and B are regular languages,\nA ∪ B is a regular language\nTo prove the closure of concatenation and star operators, we\nneed a sightly different computational model\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 02\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":254,"segment": "unlabeled", "course": "cs1502", "lec": "lec27_time_complexity_02","text":"Time Complexity 02\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fPolynomial Time vs Exponential Time\n\nPolynomial time is considered small but exponential time is\nconsidered large\nConsider a problem size n = 1000\nAssume that algorithms A and B requires to execute n2 and\n2n instructions respectively\nAssume that each instruction takes 1 nanosecond\n\nAlgorithm A takes\n10002 × 10−9 = 106 × 10−9 = 10−3 = 0.001second\nAlgorithm B takes\n21000 × 10−9 ≈ 10301 × 10−9 = 10292 seconds ≈ 10284 years\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fClass P\n\nDefinition 7.12\nP is the class of languages that are decidable in polynomial time\non a deterministic single-tape Turing machine. In other words,\n[\nP =\nTIME(nk )\nk\n\nRecall that TIME(nk ) is a set of languages that can be\ndecided in O(nk ) time Turing machine\nP is a class of problems that are realistically solvable on a\ncomputer\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fClass P\n\nConsider the following Turing machine:\nM = “On input x:\n1\n2\n3\n\nDo this\nWhile this is true\nDo that\n\nThere are two meanings of the word “step” of a TM M :\nThe above TM has three steps, step 1, step 2, and step 3\nEach step takes a number of steps to execute\nOne step results in a configuration\n\nA TM runs in polynomial time if\n1\n\n2\n\nThe number of times each step is executed (including\nrepeated) is a polynomial (nk for some k)\nEach step takes a polynomial time (O(nk ) for some k)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fAnalyzing Algorithms\n\nRecall our Turing machine M1 that decides\nA = {0k 1k | k ≥ 0}\nM1 = “On input string w:\n1\n\n2\n3\n4\n\nScan across the tape and reject if a 0 is found to the right of a\n1.\nRepeat if both 0s and 1s remain on the tape:\nScan across the tape, crossing off a single 0 and a single 1.\nIf 0s still remain after all the 1s have been crossed off, or if 1s\nstill remain after all the 0s have been crossed off, reject.\nOtherwise, if neither 0s nor 1s remain on the tape, accept.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fAnalyzing Algorithm\nInput 0k 1k where k = n\/2\nNumber of times each step is executed:\nStep 1 will be executed 1 time\nStep 2 and 3 will be executed n\/2 times\nStep 4 will be executed 1 time\n\nNumber of steps of each step\nStep 1 takes O(n) steps\nStep 2 and 3 take O(n) steps\nStep 4 takes O(n) steps\n\nFrom the analysis:\nThe number of times each step is executed (including repeat)\nis polynomial\nEach step takes polynomial time\nTherefore, M1 runs in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fExamples of Problems in P\nLet\nPATH = {hG, s, ti | G is a directed graph that has a\ndirected path from s to t}\nG\n\ns\n\nt\n\nIs PATH ∈ P ?\nIf PATH can be decided by a TM in polynomial time,\nPATH ∈ P\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fBrute-Force Algorithm for PATH\nSuppose a directed graph G consists of m nodes\nA brute-force algorithm:\nCreating all possible paths of length at most m\nSearch for a direct path from s to t\n\nExample: All possible paths of length at most 3:\n\n1\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\n2\n\n3\n\n1\n\n2\n\n1\n\n1\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n2\n\n2\n\n3\n\n3\n\n3\n\n3\n\n1\n\n2\n\n1\n\n3\n\n1\n\n2\n\n3\n\n2\n\n3\n\n1\n\n2\n\n3\n\n3\n\n1\n\n2\n\n1\n\n3\n\n1\n\nMaximum Paths with 3 nodes\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n3\n\nTime Complexity 02\n\n2\n\n2\n\n3\n\n1\n\n2\n\n3\n\n3\n\n1\n\n2\n\n3\n\n\fBrute-Force Algorithm for PATH\n\nThe number of possible paths is roughly mm\nFor each path, you need to check whether it is a path from s\nto t\nThere are roughly mm paths\nTo search all possible paths is O(mm )\nO(mm ) = O((2log2 m )m ) = O(2m log2 m )\nThis will take an exponential time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fBreadth-First Search for PATH\nUse breadth-first search to achieve polynomial time for PATH:\nM = “On input hG, s, ti, where G is a directed graph with\nnodes s and t:\n1\n2\n3\n\n4\n\nPlace a mark on node s.\nRepeat the following until no additional nodes are marked:\nScan all the edges of G. If an edge (a, b) is found going\nfrom a marked node a to an unmarked node b, mark node b.\nIf t is marked, accept. Otherwise, reject.”\n\nFor simplicity of analysis, we are going to assume the\nfollowing:\nThe number of nodes is n\nThe number of edges is proportional to the number of nodes\nkn edges for some k\nThe string representation of a graph is proportional to the\nnumber of nodes and the number of edges\n\nIn doing so, we can simply use n (number of nodes) as the\nproblem size instead of the length of the input string\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fBreadth-First Search for PATH\nAnalysis where input string is hG, s, ti where G is a directed\ngraph and s and t are nodes\nStep 1: Place a mark on node s.\nThis step will be executed one time and the number of steps\nis O(n)\n\nStep 4: If t is marked, accept. Otherwise, reject.\nThis step will be executed one time and the number of steps\nis O(n)\n\nSteps 2 and 3:\nRuns at most n times (mark one additional node every\nrepetition)\nRecall that there are kn edges\nFor each execution of step 3, it needs O(kn) = O(n) steps\n\nConclusion:\nNumber of times each step will be executed is polynomial\nEach step is polynomial\nTherefore, this algorithm M is polynomial\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fRelatively Prime\nTwo numbers are relatively prime if 1 is the largest integer\nthat evenly divides them both\nFormally,\nRELPRIME = {hx, yi | x and y are relatively prime}\nIs RELPRIME ∈ P ?\nBrute-force algorithm:\nFind all possible divisors of both numbers and accept if none\nare greater than 1\nIf a number is represented by n-bit binary, the number of\npossible value is 2n\nThus, to find all possible divisor of x and y is O(2n )\n(exponential)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fEuclidean Algorithm for Greatest Common Divisor\n\nWe can check whether x and y are relatively prime by\ncomputing their Greatest Common Divisor (GCD)\nIf gcd(x, y) = 1, x and y are relatively prime\n\nComputing a gcd can be done using Euclidean Algorithm in a\nform of a TM as follows:\nE = “On input hx, yi, where x and y are natural numbers:\n1\n2\n3\n4\n\nRepeat until y = 0:\nAssign x ← x mod y.\nExchange x and y.\nOutput x.”\n\nWhen we run TM E on input hx, yi where x and y are natural\nnumbers, it simply output their GCD\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fEuclidean Algorithm for Greatest Common Divisor\n\nThis algorithm R solves RELPRIME\nR = “On input hx, yi, where x and y are natural numbers in\nbinary:\n1\n2\n\nRun E on hx, yi.\nIf the result is 1, accept. Otherwise, reject.\n\nAnalysis:\nThere is no loop in TM R\nStep 1 will be executed one time\nStep 2 will be executed one time\n\nFor the number of steps in each step:\nFor step 1, it depends on the number of steps of TM E\nFor step 2, at most O(n)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fRELPRIME ∈ P\n\nAnalysis of Algorithm E:\nGiven two numbers x and y, there are two possibilities:\n1\n2\n\nx\/2 ≥ y or\nx\/2 < y\n\nIf x\/2 ≥ y, x mod y < y < x\/2\nIf x\/2 < y, x mod y = x − y < x\/2\nIn other words, performing x = x mod y reduces the value of\nx roughly in half\nThus, steps 2 and 3 will be repeated roughly 2 log2 x or\n2 log2 y (proportional to the length of either x or y)\nMultiplied by 2 because of the swap in step 3\n\nAlgorithm E runs in polynomial time\n\nRELPRIME ∈ P\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fHamiltonian Path\nA Hamiltonian path in a directed graph G is a directed path\nthat goes through each node exactly once\nFormally,\nHAMPATH = {hG, s, ti | G is a directed graph with a\nHamiltonian path from s to t}\n\ns\n\nt\n\nIs HAMPATH ∈ P ?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fPolynomial Verifiability\n\nUnfortunately, we need the brute-force algorithm:\nFirst, we generate all possible paths\nThen check each path whether it is a Hamiltonian path from s\nto t\n\nThis requires exponential time since there are roughly mm\npossible paths\nNo one know an algorithm that can solve Hamiltonian path in\npolynomial time yet\nNo one can prove that the Hamiltonian path problem cannot\nbe solved in polynomial time yet\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fPATH vs HAMPATH\n\nRecall languages PATH and HAMPATH\nPATH = {hG, s, ti | G is a directed graph that has a\ndirected path from s to t}\nHAMPATH = {hG, s, ti | G is a directed graph with a\nHamiltonian path from s to t}\nGiven hG, s, ti:\nto decide whether it is in PATH can be done in polynomial\ntime\nto decide whether it is in HAMPATH (as of now) cannot be\ndone in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\fCOMPOSITES\n\nAnother problem is called COMPOSITES\nFormally,\nCOMPOSITES = {x | x = pq, for integers p, q > 1}\nGiven a composite number x = pq where p and q are large\nprime numbers, it takes a very long time to find p and q\nThe RSA algorithm is based on the fact that COMPOSITES\nrequires an exponential time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 02\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":255,"segment": "unlabeled", "course": "cs1502", "lec": "lec09_turing_machine_01","text":"Turing Machine 01\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fHuman Computer\nImagine a human computer working with a pencil and paper:\nThe only things written on the paper are symbols from some\nfixed finite alphabet\nEach step taken by the computer depends only on the symbol\nhe\/she is currently examining and on his “state of mind” at\nthe time\nHis state of mind may change as a result of his examining\ndifferent symbols, only a finite number of distinct states of\nmind are possible\n\nThis sounds like a finite-state machine that has its input\nstring written on the paper\nPrimitive Steps of Human Computation:\nExamining an individual symbol on the paper\nErasing a symbol or replacing it by another\nTransferring attention from one symbol to another nearby\nsymbol\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fTuring Machines\nYou can think of a Turing machine a finite state machine\nwith unlimited amount of memory\nControl\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\nTape\n\nA Turing machine has the following:\nA control (state diagram\/transition functions)\nAn infinite long tape\nA tape head that can move around on the tape\nA TM can read input symbols from its tape\nA TM can write output symbols to its tape\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fTuring Machines\nAt first, tape contains the input string follows by infinite blank\n(t) symbols\nExample: Input: 101\n1\n\n0\n\n1\n\nExample: Input: 011010010\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\nExample: Input: ε\n\nThe machine can store information by writing symbols onto\nthe tape\nThe output of a machine is either accept or reject\nOutput symbols\/strings can also be on the tape\n\nA Turing machine may run forever\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fTuring Machine\nControl\n\na\n\n1\n\n2\n\n3\n\n4\n5\n\nb\n\na\n\nb\n\nA Turing machine can read a symbol from the tape under the\ntape head\nA Turing machine can write a symbol to the tape under the\ntape head\nThe tape head can move to the left and to the right one\nsquare at a time\nThe tape is infinite\nThe special states for rejecting and accepting take effect\nimmediately (qreject and qaccept )\nUnlike DFA that needs to process the last input symbol before\naccepting or rejecting an input string\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fExample\n\nLet Σ = {0, 1, #}\nConsider the language B = {w#w | w ∈ {0, 1}∗ }\nExample of strings in B are\n01101#01101\n#\n01#01\n\nThis language is not regular\nNeed an infinite states to to remember all symbols in w\nWe can use the Pumping lemma to prove that it is not regular\n\nIf the number of # symbol in a string is not exactly 1, the\nstring is not in the language B\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fExample\n\nLet Σ = {0, 1, #} and B = {w#w | w ∈ {0, 1}∗ }\nImaging what would you do if I give you a very long piece of\npaper and it contains a string of the form x#y where\nx, y ∈ {0, 1}∗ and they are so long that you cannot remember\nall its symbols\nOne way is to go back and forth across the # and compare\nsymbols at the same position one symbol at a time\nYou may need to cross off those that have been compared\n0110110101011010111010100101011001100101010#0110110101011010111010100101011001100101010\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fFormal Definition of a Turing Machine\n\nA Turing machine is a 7-tuple, (Q, Σ, Γ, δ, q0 , qaccept , qreject ),\nwhere Q, Σ, Γ are all finite sets and\n1\n\nQ is a set of states,\n\n2\n\nΣ is the input alphabet not containing the blank symbol t,\n\n3\n\nΓ is the tape alphabet, where t ∈ Γ and Σ ⊆ Γ,\n\n4\n\nδ : Q × Γ → Q × Γ × {L, R} is the transition function,\n\n5\n\nq0 ∈ Q is the start state,\n\n6\n\nqaccept ∈ Q is the accept state, and\n\n7\n\nqreject ∈ Q is the reject state.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fTransition Function of a Turing Machine\n\nThe transition function of a TM is defined as:\nδ : Q × Γ → Q × Γ × {L, R}\nAn assignment δ(q, a) = (r, b, L) means if the machine is at\nstate q and the tape head is over a square containing a, the\nmachine write symbol b (replacing a), change its state to r\nand move the tape head to the left one square\nThis transition can be represented in a state diagram as shown\nbelow:\na → b, L\nq\nr\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fComputation of a Turing Machine\n\nThe input string w = w1 w2 . . . wn ∈ Σ∗ is on the leftmost n\nsquare of the tape and the rest are filled with blank symbols\nThe tape head starts at the leftmost square of the tape\nThe machine processes input according to its transition\nfunction\nIf the tape head is at the leftmost square and the transition\nfunction indicates L, the tape head stays at the same place\nThe machine continues until it enter the qaccept or qreject state\nThe machine may run forever\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fTracing a Turing Machine\nTo trace whether a DFA accept or reject a string is easy\nTo trace whether an NFA accept or reject a string is harder\nAn NFA can split into multiple copies\nEach copy has its own current state\nWe use a computational tree to keep track of all copies\n\nTo trace whether a TM accept or reject a string is even harder\nWe need to keep track or the current state (similar to DFA or\nNFA)\nWe also need to keep track of the content of the tape\nThe content of the tape is changed over time (TM can write\nonto the tape)\n\nWe also need to keep track of the location of its tape head\nNeed to know the symbol under the tape head\nThe tape head needs to move to either left or right direction\nat every step\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fConfiguration\n\nWhen a machine process an alphabet, three things change:\nthe state of the machine,\nthe content of the tape, and\nthe location of the tape head\n\nThe above three items can be represented by a configuration\nA configuration is in the form of u q v\nu and v are strings that can be empty\nq is a state that represents the current state of TM\nThe content of the tape is the string uv\nThe tape head is on the first alphabet of the string v\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fConfiguration (Examples)\n\nExample: The configuration 1011q7 01111 corresponds to a\nmachine as shown below:\nq7\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n1\n\nExample: The configuration q3 101101111 corresponds to a\nmachine as shown below:\nq3\n1\n\n0\n\n1\n\n1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0\n\n1\n\n1\n\n1\n\nTuring Machine 01\n\n1\n\n\fConfiguration (Examples)\n\nExample: The configuration 10110111q8 1 corresponds to a\nmachine as shown below:\nq8\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n1\n\nExample: The configuration 101101111q4 corresponds to a\nmachine as shown below:\nq4\n1\n\n0\n\n1\n\n1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0\n\n1\n\n1\n\n1\n\nTuring Machine 01\n\n1\n\n\fConfiguration\nWe say that the configuration C1 yields configuration C2 if\nthe machine can legally go from C1 to C2 in one step\nFor example, suppose a TM has the following transition\nfunction:\nδ(qi , 0) = (qj , 1, L)\nConsider the configuration 010010qi 0101\nFor the above configuration, u = 010010, q = qi , and v = 0101\nThe current state is qi\nThe content of the tape is 0100100101\nThe tape head is on top of the symbol 0 (the first symbol of v)\n\nAccording to the above transition function, the next\nconfiguration would be\n\n01001qj 01101\nWe says that the 010010qi 0101 yields 01001qj 01101\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fConfiguration\n\nFormally:\nLet a, b, c ∈ Γ and u, v ∈ Γ∗\na, b, and c are symbols\nu and v are strings over Γ\nBy concatenating the symbol a to the end of the string u, we\nget the string ua\n\nWe say that\nua qi bv\n\nyields\n\nu qj acv\n\nif δ(qi , b) = (qj , c, L)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fStarting Configuration\n\nGiven a string w = w0 w1 w2 . . . wn\nSuppose the start state of a TM M is q0\nWhen M is about to process the string w:\nq0\n\nw0 w1 w2\n\nwn\n\nThe starting configuration of M on input w is q0 w\nExample: The starting configuration of M on input 01101 will\nbe q0 01101\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fConfiguration\n\nIf the state in a configuration is qaccept , the configuration is\ncalled accepting configuration\n0101qaccept 101\nqaccept 1111\n\nSimilarly, if the state in a configuration is qreject , it is called\nrejecting configuration\nqreject 010100\n011qreject 01\n\nOnce a machine yields either the accepting or rejecting\nconfiguration, the machine will not yield any other\nconfiguration (halting configuration).\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fLanguage of a TM\nA machine M accepts a string w if the sequence of\nconfiguration C1 , C2 , . . . Ck exists, where\n1\n2\n3\n\nC1 is the starting configuration of M on input w,\neach Ci yields Cj , and\nCk is an accepting configuration\n\nThe set of all strings A accepted by M is the language of M\nM recognizes A or\nL(M ) = A\n\nNote: If L(M ) = A, it does not mean that M rejects all\nstrings that are not in the language A\nGiven a string s 6∈ A, M may loop indefinitely on input s\nUnlike a DFA D, if L(D) = B, D rejects all strings not in the\nlanguage D\nDFA D cannot loop indefinitely on any string\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\fLanguage of a TM\nA language B is called Turing-recognizable if some Turing\nmachine recognize it\nSometimes we call B is recognizable\n\nGiven a string w and a TM M there are three possibilities:\n1\n2\n3\n\nM accepts w\nM rejects w\nM loops indefinitely on input w\n\nTuring machines that never loop indefinitely are called\ndeciders\nThese type of TMs will always halt on all inputs\n\nA decider that recognizes a language is said to decide that\nlanguage\nA language is called Turing-decidable if some Turing\nmachines decide it\nSuppose TM M is a decider and L(M ) = C, we say that\nM decides C\nC is decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 01\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":256,"segment": "self_training_2", "course": "cs1502", "lec": "lec13_decidability_01","text":"Decidability 01\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidability\n\nA language is decidable if there is a decider decides it\nA language is a set of strings\nA decider is a Turing machine that halts on all inputs\nA Turing machine halts when it enters either the accept state\n(qaccept ) or the reject state (qreject )\nTo show that a language A is decidable:\nConstruct a TM M that you think it decides the language A\nand prove that\n1\n2\n\nFor every s ∈ A, M must accept s and\nFor every s 6∈ A, M must reject s\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidability\n\nWe constructed M and proved that\n1\n2\n\nFor every s ∈ A, M must accept s and\nFor every s 6∈ A, M must reject s\n\nThis allows us to conclude the following:\n1\n\nL(M ) = A\nM accepts all strings in A and does not accept all strings not\nin A\n\n2\n\nM is a decider (always halts)\nGiven a string s, it is either in A or not in A\nIf s ∈ A, M accepts s (halts)\nIf s 6∈ A, M rejects s (halts)\n\n3\n\nA is decidable\nThe decider M decides A\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\n\nProblem\nThe problem of determining whether a DFA accepts a string.\nLet hxi be a string representation of the object x\nThe above problem can be converted into a membership\nproblem (language) ADFA\nADFA = {hB, wi | B is a DFA that accepts input string w}\nNow, the problem whether DFA B accepts the string w\nbecomes the problem whether hB, wi is in ADFA\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\nThe language ADFA\nADFA = {hB, wi | B is a DFA that accepts input string w}\nAbout the above language:\nIf F is a DFA and it accepts 01101, hF, 01101i is in ADFA\nhF, 01101i can be the formal definition of DFA F , followed by\n#, and followed by 01101\nhF, 01101i = \"({q0 , . . . }, {0, 1}, . . . )#01101\"\n\nIf G is a DFA and it rejects ε, hG, εi is not in ADFA\n\nRecall a set definition defined by a predicate:\nWhat can we conclude if hB, wi ∈ ADFA ?\nB is a DFA that accepts input string w\n\nWhat can we conclude if hB, wi 6∈ ADFA ?\nB is not a DFA,\nw is not a string, or\nB is a DFA that rejects input string w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\n\nThe language ADFA\nADFA = {hB, wi | B is a DFA that accepts input string w}\nTo show that ADFA is decidable, we must show that there\nexists a decider M that decides ADFA\nConstruct a TM M that we think it decides A and prove that\n1\n\n2\n\nIf hB, wi ∈ ADFA (DFA B accepts the string w), M accepts\nhB, wi\nIf hB, wi 6∈ ADFA (DFA B rejects the string w), M rejects\nhB, wi\n\nWe learn an algorithm how to simulate a DFA on an input\nstring in Chapter 1\nTuring machine can do the same\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\nThe language ADFA\nADFA = {hB, wi | B is a DFA that accepts input string w}\nAn example of a Turing machine M is as follows:\nM =“On input hB, wi, where B is a DFA and w is a string:\n1\n2\n\nSimulate B on input w.\nIf the simulation ends in an accept state, accept. If it ends in a\nnonaccepting state, reject.”\n\nNotes about the above TM\nInput format hB, wi is the same as format of strings in ADFA\nThe where clause filters out the following cases:\nB is not a DFA\nw is not a string\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\n\nAn example of a Turing machine M is as follows:\nM =“On input hB, wi, where B is a DFA and w is a string:\n1\n2\n\nSimulate B on input w.\nIf the simulation ends in an accept state, accept. If it ends in a\nnonaccepting state, reject.”\n\nWe do not know whether M is a decider for ADFA yet\nWe need to show that M is a decider for the language ADFA\nby proving that\n1\n2\n\nIf hB, wi ∈ ADFA , M accepts hB, wi\nIf hB, wi 6∈ ADFA , M rejects hB, wi\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fIs M a Decider for the Language ADFA\nAgain, to show that the Turing machine M is a decider for\nthe language ADFA , we have to show that\n1\n2\n\nIf hB, wi ∈ ADFA , M accepts w\nIf hB, wi 6∈ ADFA , M rejects w\n\nProofs:\nCase 1: Assume that hB, wi ∈ ADFA . Since hB, wi ∈ ADFA ,\nby the definition of ADFA , B is a DFA that accepts the string\nw. Since B accepts w, by simulating B on input w in step 1,\nthe simulation will end in an accept state. Since the simulation\nends in an accept state, M accepts hB, wi.\nCase 2: Assume that hB, wi 6∈ ADFA . Since hB, wi 6∈ ADFA ,\nby the definition of ADFA , B is a DFA that rejects the string\nw. Since B rejects w, by simulating B on input w in step 1,\nthe simulation will end in a non-accept state. Since the\nsimulation ends in a non-accept state, M rejects hB, wi.\n\nWe just show that M accepts all strings in ADFA and rejects\nall strings not in ADFA\nM is a decider for ADFA . Therefore, ADFA is decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fProve that ADFA is Decidable\nProve that\nADFA = {hB, wi | B is a DFA that accepts w}\nis decidable.\nSolution: Construct a TM M as follows:\nM =“On input hB, wi, where B is a DFA and w is a string:\n1\n\nSimulate B on input w.\n\n2\n\nIf the simulation ends in an accept state, accept. If it ends in a nonaccepting\nstate, reject.”\n\nNext, we need to prove that M decides ADFA :\nCase 1: Assume that hB, wi ∈ ADFA . Since hB, wi ∈ ADFA , by the definition\nof ADFA , B is a DFA that accepts the string w. Since B accepts w, by\nsimulating B on input w in step 1, the simulation will end in an accept state.\nSince the simulation ends in an accept state, M accepts hB, wi.\nCase 2: Assume that hB, wi 6∈ ADFA . Since hB, wi 6∈ ADFA , by the definition\nof ADFA , B is a DFA that rejects the string w. Since B rejects w, by\nsimulating B on input w in step 1, the simulation will end in a non-accept state.\nSince the simulation ends in a non-accept state, M rejects hB, wi.\nThis shows that M is a decider for ADFA . Therefore, ADFA is decidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\nANFA\nLet ANFA = {hB, wi | B is an NFA that accepts input string w}.\nShow that ANFA is decidable.\nAgain, to show that ANFA is decidable, we need to construct\na TM N and prove the following:\n1\n2\n\nIf hB, wi ∈ ANFA , N accepts hB, wi\nIf hB, wi 6∈ ANFA , N rejects hB, wi\n\nAs discussed earlier in Chapter 1, there is an algorithm that\nallows us convert an NFA B into an equivalent DFA C\nSince there is an algorithm, a Turing machine that convert an\nNFA B into a equivalent DFA C where L(B) = L(C)\nIf NFA B accepts w, DFA C also accepts w\nIf DFA C accepts w, hC, wi ∈ ADFA\nIf hC, wi ∈ ADFA , TM M (decider for ADFA ) accept hC, wi\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Languages\nTM N can be constructed as follows:\nN =“On input hB, wi, where B is a NFA and w is a string:\n1\n\n2\n3\n\nConvert NFA B to an equivalent DFA C, using the procedure\nfor this conversion given in Theorem 1.39.\nRun TM M from Theorem 4.1 on input hC, wi.\nIf M accepts, accept; otherwise, reject.”\n\nNotes about TM N\nRecall that the language of TM M is ADFA :\nADFA = {hB, wi | B is a DFA that accepts w}\nTM N runs TM M on input hC, wi where\nC is a DFA that it just constructed from NFA B\nw is a string\nM will accept or reject hC, wi depending on whether hC, wi\nin ADFA\n\nTM N runs TM M on input hB, wi, M will always reject the\ninput since B is an NFA\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fProve that ANFA is Decidable\nProve that ANFA = {hB, wi | B is an NFA that accepts w} is decidable.\nSolution: Construct a TM N as follows:\nN =“On input hB, wi, where B is a NFA and w is a string:\n1\n\nConvert NFA B to an equivalent DFA C, using the procedure for this conversion\ngiven in Theorem 1.39.\n\n2\n\nRun TM M from Theorem 4.1 on input hC, wi.\n\n3\n\nIf M accepts hC, wi, accept; otherwise, reject.”\n\nNext, we need to prove that N decides ANFA :\n1\n\nCase 1: Assume that hB, wi ∈ ANFA . Since hB, wi ∈ ANFA , by the definition\nof the language ANFA , B is an NFA that accepts the string w. By converting\nthe NFA B into an equivalent DFA C (L(B) = L(C)), C also accepts the string\nw. Since C is a DFA that accepts the string w, hC, wi ∈ ADFA . Since M is a\ndecider for ADFA , by running M on input hC, wi, M will accept hC, wi which\nwill cause N to accept hB, wi.\n\n2\n\nCase 2: Assume that hB, wi 6∈ ANFA , Since hB, wi 6∈ ANFA , by the definition\nof the language ANFA , B is an NFA that rejects the string w. By converting\nthe NFA B into an equivalent DFA C (L(B) = L(C)), C also rejects the string\nw. Since C is a DFA that rejects the string w, hC, wi 6∈ ADFA . Since M is a\ndecider for ADFA , by running M on input hC, wi, M will reject hC, wi which\nwill cause N to reject hB, wi.\n\nThis shows that TM N is a decider for ANFA . Therefore, ANFA is decidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fDecidable Language\nAREX\nLet\nAREX = {hR, wi | R is a regular expression that generates string w}.\n\nShow that AREX is decidable.\nAgain, to show that AREX is decidable, we need to construct\na TM P and prove the following:\n1\n2\n\nIf hR, wi ∈ AREX , P accepts hR, wi\nIf hR, wi 6∈ AREX , P rejects hR, wi\n\nRecall that we have an algorithm that generates an NFA A\nfrom a regular expression R where the language of A is\nexpressed by R\nAgain, since there is an algorithm, we can have a Turing\nmachine that convert a regular expression R into an equivalent\nNFA A\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\fProve that AREX is Decidable\nProve that AREX = {hR, wi | R is a regular expression that generates string w} is\ndecidable.\nSolution: Construct a TM P as follows:\nP =“On input hR, wi, where R is a regular express and w is a string:\n1 Convert regular expression R to an equivalent NFA A by using the procedure for\nthis conversion given in Theorem 1.54.\n2 Run TM N from Theorem 4.2 on input hA, wi.\n3 If N accepts hA, wi, accept; if N rejects hA, wi, reject.”\nNext, we need to prove that P decides AREX :\n1 Case 1: Assume that hR, wi ∈ AREX . Since hR, wi ∈ AREX , from the\ndefinition of AREX , R is a regular expression that generates the string w. Since\nR is a regular expression that generates w, by converting R into an equivalent\nNFA A, A accepts w. Since NFA A accepts w, hA, wi ∈ ANFA . Since TM N is\na decider for ANFA , by running N on input hA, wi, N will accept hA, wi which\ncauses P to accept hR, wi.\n2 Case 2: Assume that hR, wi 6∈ AREX . Since hR, wi 6∈ AREX , from the\ndefinition of AREX , R is a regular expression that does not generate the string\nw. Since R is a regular expression that does not generate w, by converting R\ninto an equivalent NFA A, A rejects w. Since NFA A rejects w,\nhA, wi 6∈ ANFA . Since TM N is a decider for ANFA , by running N on input\nhA, wi, N will reject hA, wi which causes P to reject hR, wi.\nThis shows that TM P is a decider for AREX . Therefore, AREX is decidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 01\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":257,"segment": "self_training_2", "course": "cs1502", "lec": "lec05_finite_automata_04","text":"Finite Automata 04\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions\nA regular expression can be constructed using the following\nrules:\n1\n2\n3\n4\n\n5\n\n6\n\na is a regular expression for some a in the alphabet Σ,\nε is a regular expression\n∅ is a regular expression\nIf R1 and R2 are regular expressions, R1 ∪ R2 is a regular\nexpression\nIf R1 and R2 are regular expressions, R1 ◦ R2 or R1 R2 is a\nregular expression\nIf R is a regular expression, R∗ is a regular expression\n\nNote that to use rules 4, 5, or 6, you need to have regular\nexpressions R1 , R2 , or R first which can only be constructed\nfrom rules 1, 2, or 3\nThis is a recursive definition\nOnce you get regular expressions, you can apply rules 4, 5, or 6\nmultiple time to obtain more and more regular expressions\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression Examples\nSuppose Σ = {0, 1}\nFrom rule 1, we have:\n0 is a regular expression\n1 is a regular expression\n\nFrom rule 2, we have:\nε is a regular expression\n\nFrom rule 3, we have:\n∅ is a regular expression\n\nSo far, we have four regular expressions, 0, 1, ε, and ∅\nFrom rule 4, 0 ∪ 1 is a regular expression\nFrom rule 5, 11 is a regular expression\nFrom rule 6, 0∗ is a regular expression\n\nWe can keep building larger and larger regular expressions by\napplying rules 4, 5, and 6 multiple times\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression Examples\n\nSuppose Σ = {0, 1}\nFrom rule 1, 0 and 1 are regular expressions\nFrom rule 4, 0 ∪ 1 is a regular expression\nFrom rule 6, (0 ∪ 1)∗ is a regular expression\nFrom rule 5, 01 is a regular expression\nFrom rule 5, 011 is a regular expression\nFrom rule 5, (0 ∪ 1)∗ 011 is a regular expression\nFrom rule 5, (0 ∪ 1)∗ 011(0 ∪ 1)∗ is a regular expression\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions\nIn arithmetic, an arithmetic expression can be used to\nrepresent its object (number)\n5 + 12 is an arithmetic expression\n5 + 12 can be used to represent 17\nWe usually write 5 + 12 = 17\n\nIn theory of computation, a regular expression can be used to\nexpress a language\nLet Σ = {0, 1}\nThe regular expression 0 = {0}\nThe regular expression 1 = {1}\nThe regular expression ε = {ε}\nThe regular expression ∅ = { }\nSuppose regular expressions R1 = A and R2 = B for\nlanguages A and B, the regular expression R1 ∪ R2 = A ∪ B\nSuppose regular expressions R1 = A and R2 = B for\nlanguages A and B, the regular expression R1 R2 = AB\nSuppose the regular expression R = A for a language A,\nR ∗ = A∗\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions (Other Notations)\n\nR+ is a shorthand notation for RR∗ for a regular expression R\nAll strings that are 1 or more concatenations of strings from R\nExamples:\n1+ = 11∗\n(0 ∪ 1)+ = (0 ∪ 1)(0 ∪ 1)∗\n\nRk is a shorthand notation for the concatenation of k R’s\nwith each other\nExamples:\n15 = 11111\n(0 ∪ 1)3 = (0 ∪ 1)(0 ∪ 1)(0 ∪ 1)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions (By Examples)\n\nA regular expression expresses a language:\n0∪1\n0 ∪ 1 = {0} ∪ {1} = {0, 1}\nA language that consists of two strings, 0 and 1\n\n0∗\n0∗ = {0}∗\nRecall that A∗ = {x1 x2 . . . xk | k ≥ 0 and each xi ∈ A}\n{0}∗ = {ε, 0, 00, 000, 0000, 00000, . . . }\n0∗ expresses the language that consists of all string that\ncontains nothing but 0s including the empty string\n\n(0 ∪ 1)0∗\n(0 ∪ 1)0∗ = {0, 1} ◦ 0∗ = {0, 1} ◦ {ε, 0, 00, 000, . . . }\nRecall that A ◦ B = {xy | x ∈ A and y ∈ B}\n{0, 1}0∗ = {0, 1, 00, 10, 000, 100, 0000, 1000, . . . }\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions (By Examples)\nA regular expression expresses a language:\n(0 ∪ 1)∗\n(0 ∪ 1)∗ = {0, 1}∗ = {ε, 0, 1, 00, 01, 10, 11, 000, 001, . . . }\nThis is the set of all strings over 0 and 1\nWe generally use Σ∗ instead of (0 ∪ 1)∗\n\n0∗ 10∗\n0∗ 10∗ = {0}∗ ◦ {1} ◦ {0}∗\n{0}∗ is a language consisting of all strings containing zero or\nmore 0s ({ε, 0, 00, 000, . . . })\n0∗ 10∗ = {w | w contains a single 1}\n\nΣ∗ 1Σ∗\nΣ∗ 1Σ∗ = Σ∗ ◦ {1} ◦ Σ∗\nΣ∗ is the language consisting of all stings over Σ\nΣ∗ 1Σ∗ = {w | w has at least one 1}\n\nΣ∗ 001Σ∗\nΣ∗ 001Σ∗ = Σ∗ ◦ {001} ◦ Σ∗\nΣ∗ 001Σ∗ = {w | w contains the string 001 as a substring}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions (By Examples)\nA regular expression expresses a language:\n1∗ (01+ )∗\n1∗ (01+ )∗ = {1}∗ ◦ (01+ )∗\n{1}∗ is a language consisting of all strings containing zero or\nmore 1s ({ε, 1, 11, 111, . . . })\n01+ = {0} ◦ {1}+ = {0} ◦ ({1} ◦ {1}∗ ) = {01, 011, 0111, . . . }\n(01+ )∗ = {01, 011, 0111, . . . }∗ =\n{ε, 01, 0101, 01011, 01101, 011011, . . . }\n1∗ (01+ )∗ = {w | every 0 in w is followed by at least one 1}\n\n(ΣΣ)∗\nΣΣ = Σ ◦ Σ = {0, 1} ◦ {0, 1} = {00, 01, 10, 11}\n(ΣΣ)∗ = {00, 01, 10, 11}∗\n(ΣΣ)∗ = {w | w is a string of even length}\n\n(ΣΣΣ)∗\nΣΣΣ = {0, 1} ◦ {0, 1} ◦ {0, 1} =\n{000, 001, 010, 011, 100, 101, 110, 111}\n(ΣΣΣ)∗ = {000, 001, 010, 011, 100, 101, 110, 111}∗\n(ΣΣΣ)∗ = {w | the length of w is a multiple of 3}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions (By Examples)\nA regular expression expresses a language:\n01 ∪ 10\n01 ∪ 10 = ({0} ◦ {1}) ∪ ({1} ◦ {0}) = {01} ∪ {10} = {01, 10}\n\n0Σ∗ 0 ∪ 1Σ∗ 1 ∪ 0 ∪ 1\n0Σ∗ 0 = {0} ◦ Σ∗ ◦ {0} is a set of all string that start and end\nwith 0\n1Σ∗ 1 is a set of all string that start and end with 1\n0Σ∗ 0 ∪ 1Σ∗ 1 ∪ 0 ∪ 1 =\n{w | w starts and ends with the same symbol}\n\n(0 ∪ ε)1∗\n0 ∪ ε = {0} ∪ {ε} = {0, ε}\n1∗ = {ε, 1, 11, 111, . . . }\n(0 ∪ ε)1∗ = {0, ε} ◦ {ε, 1, 11, 111, . . . }\n(0 ∪ ε)1∗ = {ε, 0, 01, 011, . . . , 1, 11, 111, . . . }\n(0 ∪ ε)1∗ = 01∗ ∪ 1∗\n\n(0 ∪ ε)(1 ∪ ε) = {0, ε} ◦ {1, ε} = {ε, 0, 1, 01}\n1∗ ∅ = {1}∗ ◦ ∅ = ∅\n∅∗ = {ε}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fOperator Precedences and Identities\nOperator Precedences:\n∗\n\nhas the highest precedence.\n∪ has the lowest precedence.\n\nSome properties:\nR∪∅=R\nAdding the empty language to any other language will not\nchange it\n\nR◦ε=R\nJoining the empty string to any string will not change it\n\nR ∪ {ε} =\n6 R\nIf R = {0, 1}, R ∪ {ε} = {0, 1, ε} 6= R\n\nR ◦ ∅ 6= R\nIf R = {0, 1}, R ◦ ∅ = {0, 1} ◦ ∅\n{0, 1}◦ = {xy | x ∈ {0, 1} and y ∈ ∅} = ∅\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fExample: Regular Expression\nProblem: Write a program to evaluate a string representing a\nfloating-point number\n72\ntrue\n35.9\ntrue\n20 67\nfalse\n+7.\ntrue\n71a3\nfalse\n-.29\ntrue\n\nSolution\nCreate a DFA that recognize a set of all strings that are valid\nfloating-point numbers and turn it into a program\nIt would be great if there is an easy way to do this\n\nFor now, can we create a regular expression that express the\nset of all valid floating-point representations?\n(+ ∪ − ∪ ε)(D+ ∪ D+ .D∗ ∪ D∗ .D+ )\nwhere D = {0, 1, 2, . . . , 9}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression and Finite Automata\n\nNow we know that a regular expression can be used to express\na language\nQuestion: Is language expressed by a regular expression\nregular?\nLet’s try to prove that a regular expression expresses a regular\nlanguage\n\nRecall a regular expression:\nA regular expression is defined recursively (those 6 rules)\nSo, we need to show that a regular expressions constructed\nfrom those rules expresses a regular language\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fLemma 1.55\nFrom rule 1: a for some a in the alphabet Σ is a regular\nexpression\nWe need to show that a language expressed by a regular\nexpression generated from this rule is regular\nGiven a symbol a ∈ Σ, according to this rule, a is a regular\nexpression\na expresses the language {a}\nIs {a} regular?\nCan we construct a DFA that recognizes the language {a}?\n\nThis is an NFA that recognizes {a}\na\nq1\n\nq2\n\nFor every NFA, there is an equivalent DFA\nThus {a} is a regular language\nTherefore, a expresses a regular language\n\nA regular expression constructed by rule 1 expresses a regular\nlanguage\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fLamma 1.55\nFrom rule 2: ε is a regular expression\nAgain, we need to show that a language expressed by ε is\nregular\nRecall that ε expresses the language {ε}\nIs {ε} regular?\nCan we construct a DFA that recognizes the language {ε}?\n\nThis is an NFA that recognizes {ε}\nq1\n\nFor every NFA, there is an equivalent DFA\nThus, {ε} is a regular language\nTherefore, ε expresses a regular language\n\nA regular expression constructed by rule 2 expresses a regular\nlanguage\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fLamma 1.55\nFrom rule 3: ∅ is a regular expression\nAgain, we need to show that a language expressed by ∅ is\nregular\nRecall that ∅ expresses the language { }\nIs { } regular?\nCan we construct a DFA that recognizes the language { }?\n\nThis is an NFA that recognizes { }\nq\n\nFor every NFA, there is an equivalent DFA\nThus, ∅ is a regular language\nTherefore, ∅ expresses a regular language\n\nA regular expression constructed by rule 3 expresses a regular\nlanguage\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fLemma 1.55\nRecall that you cannot use rules 4, 5, and 6 to create a new\nregular expressions unless you already have some regular\nexpressions created by rules 1, 2, or 3\nWe already prove that each regular expression created by rules\n1, 2, or 3 expresses a regular language\nFor rule 4:\nR1 and R2 are regular expressions that express regular\nlanguages (from rules 1, 2, or 3)\nRecall that R1 = A and R2 = B for some languages A and B,\nR1 ∪ R2 = A ∪ B\nWe already prove that if A and B are regular languages, A ∪ B\nis regular (regular language is closed under union operation)\nThus, a regular expression obtain by rule 4 express a regular\nlanguage\n\nSame for rules 5 and 6\nRegular language is closed under concatenation and star\noperations\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fLemma 1.55\n\nLemma 1.55\nIf a language is described by a regular expression, then it is regular.\nWe just proved the above lemma\nBy proving the above lemma, it gives us a tool to construct\nan NFA that recognizes the language expressed by a regular\nexpression\nBased on how we prove that regular language is closed under\nunion, concatenation, and star operations\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expressions to NFA\n\nSuppose we want to construct an NFA that recognizes the\nlanguage expressed by the regular expression (ab ∪ a)∗ where\nΣ = {a, b}\nWe need to apply those 6 rules to obtain the regular expression\nThis will be a guideline step-by-step to construct an NFA\n\nHere are steps to construct the regular expression (ab ∪ a)∗\n1\n2\n3\n4\n5\n\na is a regular expression (rule 1)\nb is a regular expression (rule 1)\nab is a regular expression (rule 5 with (1) and (2))\nab ∪ a is a regular expression (rule 4 with (3) and (1))\n(ab ∪ a)∗ is a regular expression (rule 6 with (4))\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\n\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\n\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nb\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nε\n\nb\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nε\n\nb\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nab U a\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nab U a\na\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\n\n(ab U a) *\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\n\na\nε\n(ab U a) *\na\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\n\na\nε\n(ab U a) *\n\nε\na\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (ab ∪ a)∗ to an NFA.\na\na\nb\nb\na\n\nε\n\nb\n\na\n\nε\n\nb\n\nab\n\nε\nab U a\na\nε\nε\n\na\nε\n(ab U a) *\n\nε\na\nε\n\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nε\n\nb\n\n\fRegular Expressions to NFA\n\nSuppose we want to construct an NFA that recognizes the\nlanguage expressed by the regular expression (a ∪ b)∗ aba\nwhere Σ = {a, b}\nAgain, we need to apply those 6 rules to get a guideline\nHere are steps to construct the regular expression (a ∪ b)∗ aba\n1\n2\n3\n4\n5\n6\n7\n\na is a regular expression (rule 1)\nb is a regular expression (rule 1)\na ∪ b is a regular expression (rule 4 with (1) and (2))\n(a ∪ b)∗ is a regular expression (rule 6 with (3))\nab is a regular expression (rule 5 with (1) and (2))\naba is a regular expression (rule 5 with (5) and (1))\n(a ∪ b)∗ aba is a regular expression (rule 5 with (4) and (6))\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\n\nb\n\naUb\n\n(a U b) *\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\n\nb\n\naUb\n\n(a U b) *\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\n\naUb\n\n(a U b) *\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\naUb\n\n(a U b) *\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\naUb\nb\n\n(a U b) *\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\n\nε\n\na\n\naUb\nb\nε\n\n(a U b) *\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\n\nε\n\na\n\naUb\nb\nε\n\nε\n\na\n\n(a U b) *\nb\nε\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\n\nε\n\na\n\nε\n(a U b) *\nb\nε\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\nε\nε\n\na\n\nε\n(a U b) *\nb\nε\nε\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\nε\na\n\nε\nε\n(a U b) *\n\nb\nε\nε\na\n\nε\n\nb\n\nε\n\na\n\naba\n\n(a U b) *aba\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\nε\na\n\nε\nε\n(a U b) *\n\nb\nε\nε\na\n\nb\n\nε\n\nε\n\na\n\naba\n\nε\nε\n\na\n\nε\n(a U b) *aba\nb\nε\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\nε\na\n\nε\nε\n(a U b) *\n\nb\nε\nε\na\n\nε\n\nb\n\nε\n\na\n\na\n\nε\n\nb\n\naba\n\nε\nε\n\na\n\nε\n(a U b) *aba\nb\nε\nε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nε\n\na\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\nε\na\n\nε\nε\n(a U b) *\n\nb\nε\nε\na\n\nb\n\nε\n\nε\n\na\n\nε\n\nb\n\naba\n\nε\nε\n\na\n\nε\n(a U b) *aba\nb\nε\n\nε\nε\nε\n\nε\na\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nε\n\na\n\n\fRegular Expression to NFA\nConvert the regular expression (a ∪ b)∗ aba to an NFA.\na\na\nb\nb\na\n\nε\naUb\n\nb\nε\nε\na\n\nε\nε\n(a U b) *\n\nb\nε\nε\na\n\nb\n\nε\n\nε\n\na\n\nε\n\nb\n\naba\n\nε\nε\n\na\n\nε\n(a U b) *aba\nb\nε\n\nε\nε\nε\n\nε\na\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\nε\n\na\n\n\fUseful Tool\n\nRecall that the set of all valid floating-point representation is\nexpressed by a regular expression\n(+ ∪ − ∪ ε)(D+ ∪ D+ .D∗ ∪ D∗ .D+ )\nwhere\nD = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\nNow, we can easily convert the above regular expression into\nan NFA, construct an equivalent DFA, and turn the DFA into\na program\nThis process is done by computer\nOften used in compiler\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\fConclusion\n\nA regular expression expresses a regular language\nWe have a tool to convert a regular expression into an\nequivalent NFA\nWhat is next?\nWe still do not know whether every regular language can be\nexpressed by a regular expression\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nFinite Automata 04\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":258,"segment": "unlabeled", "course": "cs1502", "lec": "lec15_decidability_03","text":"Decidability 03\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fUndecidability\nConsider the following language:\nATM = {hM, wi | M is a TM and M accepts w}\nIs ATM recognizable?\nTo show that ATM is recognizable, you need to construct a\nTM that accepts all strings in ATM\nBut if a string is not in ATM , no need to explicitly reject\nLoop indefinitely is acceptable\n\nThe following is an example of a TM that recognizes ATM :\nU =“On input hM, wi, where M is a TM and w is a string:\n1\n2\n\nSimulate M on input w.\nIf M ever enters its accept state, accept; if M enters its reject\nstate, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fUndecidability\nATM = {hM, wi | M is a TM and M accepts w}\nThe following is an example of a TM that recognizes ATM :\nU =“On input hM, wi, where M is a TM and w is a string:\n1\n2\n\nSimulate M on input w.\nIf M ever enters its accept state, accept; if M enters its reject\nstate, reject.”\n\nLet’s look at the above machine:\nIf hM, wi ∈ ATM :\nM is a TM that accepts w\nTM U will accept hM, wi\n\nIf hM, wi 6∈ ATM :\n1\n\n2\n\nM is a TM that rejects w\nTM U will reject hM, wi\nM is a TM that loops indefinitely on input w\nTM U will loop indefinitely on input hM, wi\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fUndecidability\n\nTM U accepts all strings in ATM and does not accept\n(either rejects or loop indefinitely) on all strings not in ATM\nATM is recognizable\n\nIs ATM decidable?\nUnfortunately, ATM is undecidable\nBut how to prove it?\n\nWe will try to prove that it is undecidable later\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fUnrecognizable Lanugage\nCorollary 4.18\nSome languages are not Turing-recognizable.\nTo show that there exists an unrecognizable language, we only\nneed to find a language that is not recognized by any Turing\nmachines\nIt is not not easy to come up with such a language\n\nNote that a Turing machine recognizes only one language\nTo prove that an unrecognizable language exists, we can\nsimply show that the number of languages is more than\nthe number of Turing machines\nBut we have a problem\nThere are infinite number of languages\nThere are infinite number of Turing machines\n\nHow do we know which set is larger?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fCantor’s Observation\nCantor observed that two finite sets A and B have the same\nsize if we can pair one element from A with the element of B\n{a, s, d, f}\n\n{a, s, d, f}\n\n{1, 4, 7, 3}\n\n{1, 4, 7, 3}\n\nPairing #1\n\nPairing #2\n\nThe pairing does not have to be unique\nWe can consider pairing method as a function f : A → B that\nhas the following properties:\none-to-one: f (a) 6= f (b) whenever a 6= b\nonto: For every b ∈ B there is an a ∈ A such that f (a) = b\n\nFunctions that are one-to-one and onto are called bijection or\ncorrespondence\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fExample\nSame Size?\nLet N be the set of natural numbers {1, 2, 3, . . . } and let E be the\nset of even natural number {2, 4, 6, . . . }. Show that N and E have\nthe same size.\nTo show that N and E have the same size, according to the\ncanter’s observation, we need to show a correspondence\nbetween N and E\nA mapping function can be f : N → E where f (n) = 2n\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ E\n2\n4\n6\n8\n10\n..\n.\n\nBy Cantor’s definition of size, they have the same size\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fInfinite Ways of Correspondence\nRecall that a correspondence does not have to be unique\nPrevious slide show a correspondence between N and E\nHow many correspondence are there between N and E\nInfinite\nn ∈ N f (n) ∈ E\n1\n2\n2\n4\n3\n6\n4\n8\n5\n10\n..\n..\n.\n.\n\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ E\n4\n2\n6\n8\n10\n..\n.\n\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ E\n6\n2\n4\n8\n10\n..\n.\n\nThere are infinite many permutation of E\nOnce there is a correspondence between two infinite sets,\nthere are infinitely many correspondences\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fCountable\nDefinition 4.14\nA set A is countable if either it is finite or it has the same size as\nN.\nTo show that an infinite set A is countable, we need to show\nthat it has the same size as N\nTo show that an infinite set A has the same size as N, we\nneed to show a correspondence f : N → A between A and N\nCan be a definition of a function (one-to-one and onto)\nCan be a table that demonstrates one-to-one and onto\n\nAn infinite set B is uncountable if there is no correspondence\nwith N\nAn infinite set A is smaller than an infinite set B if A is\ncountable but B is not countable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Integers is Countable\nTo show that Z is countable is to show a correspondence\nbetween N and Z\nExample (Invalid Correspondence)\nn∈N\n1\n2\n3\n4\n..\n.\n\nf (n) ∈ Z\n0\n1\n2\n3\n..\n.\n\n?\n..\n.\n\n-1\n..\n.\n\nThe above table is not a valid correspondence\nIt is one-to-one\nIt is not onto\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Integers is Countable\n\nFor Z we need to pair elements from both sides of 0\nExample\nN 1 2 3 4 5 6 7 ...\nZ 0 1 -1 2 -2 3 -3 . . .\nThe above correspondence can be represented by a function\nf : N → Z where\nf (1) = 0\n(\nf (n) =\n\nn\n2\n\n− n−1\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nif n is divisible by 2\nif n is not divisible by 2\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\nLet Q be a set of rational numbers:\nQ = {m\nn | m ∈ N and n ∈ N}\nAgain, we need to find a correspondence between N and Q\nThe best starting point is to try to list them all in an order\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThen, try to pair one-by-one but make sure that it will cover\nall of them\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 1\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 2\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 3\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 4\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 5\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 6\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 7\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 8\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 9\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 10\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\n\nCorrespond with natural number 11\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\fThe Set of Positive Rational Numbers is Countable\nCorrespond with natural number greater than 12\n\n1\n1\n\n1\n2\n\n1\n3\n\n1\n4\n\n1\n5\n\n2\n1\n\n2\n2\n\n2\n3\n\n2\n4\n\n2\n5\n\n3\n1\n\n3\n2\n\n3\n3\n\n3\n4\n\n3\n5\n\n4\n1\n\n4\n2\n\n4\n3\n\n4\n4\n\n4\n5\n\n5\n1\n\n5\n2\n\nThis is an example of a correspondence between N and Q\nQ is countable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 03\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":259,"segment": "self_training_2", "course": "cs1502", "lec": "lec12_turing_machine_04","text":"Turing Machine 04\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fRecognizable and Decidable\nGiven a language R, if some Turing machines accept every\nstrings s ∈ R and does not accept (either reject or loop\nindefinitely) every string s 6∈ R, we say that “R is\nrecognizable”\nNote that these machines must accept on all input s ∈ R\nHowever, if s 6∈ R, these machines either reject or loop\ninfinitely\n\nGiven a language D, if some Turing machine accept every\nstrings s ∈ D and rejects every string s 6∈ D, we say that “D\nis decidable”\nNote that these Turing machines must be deciders\nThese machine either accept or reject on all input strings\nThese machine will not loop indefinitely on any strings\n\nIf D is decidable, D is also recognizable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fDecidable Language\n\nFollowing languages are examples of decidable languages:\nn\n\nA = {02 | n ≥ 0}\nB = {w#w | w ∈ Σ∗ }\n\nWe already demonstrated that there exists Turing machines\n(deciders) that decide above languages\nThere are some languages that are recognizable but not\ndecidable\nSuppose R is recognizable but not decidable\nThere are TMs that accept all strings in R and does not\naccept all strings not in R\nNo TM can accept all strings in R and reject all strings not\nin R\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fUndecidable Language\nConsider a polynomial:\n6x3 yz 2 + 3xy 2 − x3 − 10\nA root of a polynomial is an assignment to its variables which\nresults in that value of polynomial is 0\nA polynomial has an integral root if all variables are assigned\ninteger values\nThe above polynomial has an integral root x = 5, y = 3, and\nz=0\n\nGiven a polynomial with an integral root, can you find out\nits root?\nYes, brute force\n\nGiven a polynomial, can you find out whether it has an\nintegral root?\nNot always\nHilbert’s tenth problem stated that there is no algorithm that\ntests whether a polynomial has an integral root.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fUndecidable Language\nLet hxi be a string representation of the object x\nLet D be the set of all string representations of polynomials\nthat have integral root\nFormally\nD = {hpi | p is a polynomial with an integral root}\nGiven hpi (a string representation of a polynomial p), if a\nTuring machine can decide whether\nhpi ∈ D (polynomial p has an integral root) or\nhpi 6∈ D (polynomial p does not have an integral root)\n\nD is decidable\nHilbert’s tenth problem simply stated that D is not decidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fUndecidable Language\nConsider polynomials with one variable (e.g., 2x2 + x − 7)\nLet\nD1 = {hpi | hpi is a polynomial over x with an integral root}\nIs D1 recognizable?\nYes, if there exists a Turing machine that accepts every\nhpi ∈ D1 and does not accept every hpi 6∈ D1\n\nExample: M1 that recognizes D1 using a brute force\nalgorithm in high-level definition\nM1 =“On input hpi where p is a polynomial over the variable\nx:\n1\n\nEvaluate p with x set successively to the value 0, 1, -1, 2, -2, 3,\n-3, . . . . If at any point the polynomial evaluates to 0, accept”\n\nNote that M1 accepts all hpi ∈ D1 and loop indefinitely on all\nhpi 6∈ D1\nTherefore, D1 is recognizable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fUndecidable Language\nConsider polynomials with one variable (e.g., 2x2 + x − 7)\nLet\nD1 = {hpi | hpi is a polynomial over x with an integral root}\nIs D1 decidable?\nYes, if there exists a Turing machine that accepts every\nhpi ∈ D1 and rejects every hpi 6∈ D1\n\nLuckily there is an upper\/lower bound of the value of x that a\nmachine needs to test:\n±k cmax\nc1\nwhere k is the number of terms in the polynomial, cmax is the\ncoefficient with the largest absolute value, and c1 is the\ncoefficient of the highest order term\nChange M1 such that it rejects after testing value goes\nout-of-bound\nTherefore, D1 is decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fUndecidable Language\nLet D be the set of all polynomials that have integral root\nD = {hpi | hpi is a polynomial with an integral root}\nWe can create a machine that tries all possible assignment\nvalues starting from 0s\nFor example, in case of two variables x and y, try the following\nvalues [x, y]:\n[0, 0], [0, 1], [1, 0], [1, 1], [0, −1], [−1, 0], [−1, −1], [0, 2], . . .\nIf a polynomial p has an integral root, eventually it will be\nevaluated to 0\n\nTherefore, D is recognizable\nUnfortunately, there is no bound that we can check and\nmachine may loop infinitely\nIf the polynomial p does not have an integral root, we will keep\ntrying new values of [x, y] forever (loop indefinitely)\n\nD is not decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fDescribing Turing Machines\nA description of a Turing machine can be huge even for a very\nsimple algorithm\nExample, compare two strings {w#w | w ∈ {0, 1}∗ }\n\n1\n\nx,\nR\n\nq1\n\n0\n\nR\nx,\n\n→\n\n→\n\n#→R\n\n0, 1 → R\n\nx→R\n\nx→R\n\nq8\n\nq2\n#→R\n\nt\n\n#→R\n\nqaccept\n\nq4\n\n0, 1 → R\n\nq3\n\n→R\nq5\n\n0\n1\n\nL\nx,\n\n→\n\n→\n\nx,\n\nL\n\nx→R\n\n0, 1, x → L\n\nq6\n#→L\nx→R\n\nq7\n\n0, 1 → L\n\nThe above state diagram represents the formal description in\na form of state diagram of a Turing machine\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fDescribing Turing Machines\n\nAn implementation description of the previous Turing\nmachine that decides {w#w | w ∈ {0, 1}∗ } is shown below\nOn input string w:\n1\n\n2\n\nZig-zag across the tape to corresponding positions on either\nside of the # symbol to check whether those positions contain\nthe same symbol. If they do not, or if no # is found, reject.\nCross off symbols as they are checked to keep track of which\nsymbols correspond.\nWhen all symbols to the left of the # have been crossed off,\ncheck for any remaining symbols to the right of the #. If any\nsymbols remain, reject; otherwise, accept.\n\nNote that the above description describes the way the Turing\nmachine moves its head and store data (cross off symbols)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fDescribing Turing Machines\nAn high-level description of the previous Turing machine\nthat decides {w#w | w ∈ {0, 1}∗ } is shown below:\nM =“On input s where s = x#y for some string x and y:\n1\n2\n\nCompare whether the string x is identical to the string y.\nIf they are identical, accept; otherwise, reject.”\n\nNote that the where clause behaves like a filter\nAny string that does not satisfy the where clause will be\nrejected immediately\n\nWhat a TM can do?\nFrom the Church-Turing thesis, if there is an algorithm to do\nsomething, a TM can do the same thing\nExamples:\nCompare two strings\nCheck whether the length of a string is a power of 2\nAddition, subtraction, multiplication, division, modulo\nAny algorithms discussed in Chapter 1\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fDescribing Turing Machines\n\nHigh-level description of a Turing machine is suitable for\ndescribing universal Turing machine\nConsider the following language:\nA = {x1 #x2 # . . . #xn | xi = xj for every i and j}\nThe following machine M 0 decides A using TM M as a\nsubroutine:\nM 0 =“On input s where s = x1 #x2 # . . . #xn :\n1\n2\n3\n4\n\nFor every i where 1 ≤ i ≤ n − 1:\nRun M on input xi #xi+1 .\nIf M rejects, reject.\naccept”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\fConclusions\n\nAlgorithm and Turing Machine are consider equivalent\nAnything that an algorithm can do, there exists a TM that can\ndo the same thing\nSimply convert the algorithm to TM\n\nAnything that a Turing machine can do, there exists an\nalgorithm that can do the same thing\nSimply convert the TM to algorithm\n\nBecause of this, if there is a problem that a TM cannot solve,\nno algorithm can solve the same thing\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 04\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":260,"segment": "unlabeled", "course": "cs1502", "lec": "lec18_reducibility_01","text":"Reducibility 01\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fReducibility\nReducibility is a way to convert a problem A into another\nproblem B where\na solution to the problem B can be used to solve problem A\n\nProblems in our discussion are membership problems\nConsider two sets A ⊆ X and B ⊆ Y and a function\nf : X → Y such that\nw ∈ A ↔ f (w) ∈ B\nRecall that\nP ↔ Q ≡ (P → Q) ∧ (Q → P )\n= (¬P ∨ Q) ∧ (¬Q ∨ P )\n= (Q ∨ ¬P ) ∧ (P ∨ ¬Q)\n= (¬Q → ¬P ) ∧ (¬P → ¬Q)\n\nIf the above property holds, we have:\nIf w ∈ A, f (w) ∈ B\nIf f (w) ∈ B, w ∈ A\n\nIf f (w) 6∈ B, w 6∈ A\nIf w 6∈ A, f (w) 6∈ B\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fReducibility\nAgain, consider two sets A ⊆ X and B ⊆ Y and a function\nf : X → Y with the following property:\nw ∈ A ↔ f (w) ∈ B\nProblem #1: Is a specific element in the set A\nProblem #2: Is a specific element in the set B\nReducibility:\nGiven an element w ∈ X , suppose no idea whether w ∈ A\nHowever, given an element s ∈ Y, we know whether s ∈ B\nSuppose f () exists and the above property hold\nConvert w to f (w) and check whether f (w) ∈ B\nIf f (w) ∈ B, w ∈ A\nIf f (w) 6∈ B, w 6∈ A\n\nSolution to problem #2 can be used to solve problem #1\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fExample\n\nSuppose we have a sport car and we need to know whether it\ncan go over 200 MPH\nUnfortunately, we cannot drive our car that fast in Pittsburgh\nWay too many potholes\n\nLuckily, we have a psychic lady who has a very special ability\nShe can tell whether an object can explode into multiple pieces\n\nSo, let’s put a bomb into our sport car\nPut the trigger at the speedometer at 200 MPH mark\n\nTake the car to the psychic lady:\nIf she says that it can explode\nour sport car can go over 200\nMPH\nour sport car cannot go\nIf she says that it cannot explode\nover 200 MPH\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fExample\nLet’s compare the previous example with the concept of\nreducibility\nLet w be our sport car\nLet X be the set of cars\nThe set A ⊆ X can be\nA = {c | c is a sport car that can go over 200 MPH}\nProblem #1: Is our sport car in the set A? (Is w ∈ A?)\nWe want to know the answer to this problem\n\nLet Y be the set of objects\nThe set B ⊆ Y can be\nB = {x | x is an object that can explode}\nProblem #2: Is an object s can explode? (Is s ∈ B?)\nThe psychic lady knows how to answer this problem\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fExample\n\nThe function f : X → Y converts a car into an object by\nstrapping a bomb to the car and wire the trigger into the\nspeedometer\nThe function f () converts an instance of the problem #1 into\nan instance of the problem #2 such that\nw ∈ A ↔ f (w) ∈ B\nRecall that we put a bomb into our car (w) and wire the\ntrigger to speedometer\nWe convert w into f (w)\nWe convert an instance of the problem #1 into and instance\nof the problem #2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fExample\n\nThen we ask the psychic lady whether f (w) ∈ B\nThe answer from the psychic lady can be use to answer our\nfirst problem because of the above property\nIf the psychic lady says f (w) ∈ B\n\nw∈A\n\nOur sport car can go over 200 MPH\n\nIf the psychic lady says f (w) 6∈ B\n\nw 6∈ A\n\nOur sport car cannot go over 200 MPH\n\nReducibility\nA is reducible to B if a solution to the problem B can be used to\nsolve problem A.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fReducibility\nSuppose B is a language\nWhat does it mean by the psychic lady know whether an\nobject is in B?\nB is decidable\nB is decide by the psychic lady\n\nSuppose A is a language\nWhat happen if f () exists and the following property hold?\nx ∈ A ↔ f (x) ∈ B\nA is decidable by the following TM\nM = “On input x where x is a string:\n1\n2\n3\n\nConvert x to f (x)\nRun the psychic lady on input f (x)\nIf the psychic lady accept f (x), accept. Otherwise, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fReducibility\nIf B is decidable and f : Σ∗ → Σ∗ exists such that\nx ∈ A ↔ f (x) ∈ B\nA is decidable.\nAgain, let’s apply some boolean algebra:\nP ≡ B is decidable\nQ ≡ f : Σ∗ → Σ∗ exists such that . . .\nR ≡ A is decidable\n\nAccording to the boolean algebra, we have\n(P ∧ Q) → R ≡ (¬R ∧ Q) → ¬P\nIf A is undecidable and f : Σ∗ → Σ∗ exists such that\nx ∈ A ↔ f (x) ∈ B\nB is undecidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fUndecidability\n\nHow to prove that a language A is undecidable?\nUse the fact that ATM is undecidable\n\nBut how?\nImagine if you can prove that\nIf A is decidable, ATM is decidable\nThe above statement is in the form of P → Q\nSince P → Q ≡ ¬Q → ¬P , we have\nIf ATM is undecidable, A is undecidable\nThis allows you to conclude that A is undecidable because\nATM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\fUse ATM\nTo show that a language A is undecidable (proof by\ncontradiction)\n1\n2\n3\n4\n\nAssume that A is decidable\nShow that if A is decidable, ATM is also decidable\nSince ATM is decidable — contradiction\nTherefore, A is undecidable\n\nBut why all the sudden ATM is decidable?\nBy assuming that a language A is decidable, it means there\nexists a TM R that decides A\nR is a decider for A\nL(R) = A\nR always halts\nTM R can tell you whether a specific element is in A or not\nIf w ∈ A, R accepts w\nIf w 6∈ A, R rejects w\n\nThis TM R will be our helper to help us decide ATM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 01\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":261,"segment": "unlabeled", "course": "cs1502", "lec": "lec24_pcp","text":"Post Correspondence Problem\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fReducibility So Far\n\nTo prove that ETM is undecidable, we need to show that if\nETM is decidable, then ATM is decidable.\n1\n2\n\nAssume that ETM is decidable and obtain its decider R\nConstruct TM S with the help of R to decide ATM\nThis TM S takes hM, wi (an instance of ATM ) and construct\na new TM M 0\nL(M 0 ) 6= ∅ if M accepts w\nL(M 0 ) = ∅ if M does not accepts w (either rejects or loops\nindefinitely)\n\n3\n4\n5\n\nProve that TM S is a decider for ATM\nConclude that ATM is decidable — contradiction\nTherefore, ETM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fMapping Reducibility So Far\n\nTo prove that EQTM is undecidable, we need to show that\nATM is mapping reducible to EQTM\n1\n\nCreate a computable function f in the form of a TM F\nThis TM F takes hM, wi (an instance of ATM ) and construct\ntwo TMs M1 and M2\nL(M1 ) = L(M2 ) if M accepts w\nL(M1 ) 6= L(M2 ) if M does not accepts w (either rejects or\nloops indefinitely)\nOutput hM1 , M2 i (an instance of EQTM )\n\n2\n3\n\nProve that hM, wi ∈ ATM ↔ hM1 , M2 i ∈ EQTM\nSince ATM is undecidable, EQTM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fReduction via Computation History\n\nTo prove that ELBA is undecidable, we need to show that if\nELBA is decidable, then ATM is decidable.\n1\n2\n\nAssume that ELAB is decidable and obtain its decider R\nConstruct TM S with the help of R to decide ATM\nThis TM S take hM, wi (an instance of ATM ) an construct a\nnew LBA M 0\nThis LBA M 0 only accepts the accepting computation history\nfor M on input w\nL(M 0 ) 6= ∅ if M accepts w\nL(M 0 ) = ∅ if M does not accept w (either rejects or loops\nindefinitely)\n\n3\n4\n5\n\nProve that S is a decider for ATM\nConclude that ATM is decidable — contradiction\nTherefore, ELBA is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fCommon Theme\n\nAll three methods so far share a common process\nTake hM, wi (an instance of ATM )\nConstruct a new TM, TMs, or LBA\nBehaviors of new TM, TMs, or LBA depend on whether M\naccepts w\n\nConstruct TM or TMs from TM M and string w are quite\nstraightforward\nWe can simply run M on input w\n\nConstruct LBA from TM M and string w is not quite\nstraightforward\nWe cannot simply run M on input w\nUse computation history (sequence of configurations)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fPost Correspondence Problem (PCP)\nA Post Correspondence Problem (PCP) is a problem\nconcerning string manipulation.\nConsider a domino\n\u0014\n\n\u0015\na\nab\nThere are two non-empty strings in each domino, one on the\ntop and one on the bottom.\nGiven a set of dominoes\n(\u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015)\nb\na\nca\nabc\n,\n,\n,\nca\nab\na\nc\nIs it possible to make a horizontal line of one or more\ndominoes, with duplicates allowed, so that the string obtained\nby reading across the top halves matches the one obtained by\nreading across the bottom?\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fPost Correspondence Problem (PCP)\n\nFor this set of dominoes:\n(\u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015)\nb\na\nca\nabc\n,\n,\n,\nca\nab\na\nc\nThere is a match\u0014 as \u0015\u0014\nshown\u0015\u0014below:\n\u0015\u0014 \u0015\u0014\n\u0015\na\nb\nca\na\nabc\nab ca\na\nab\nc\nThe following set of dominoes does not have a match:\n(\u0014\n\u0015 \u0014 \u0015 \u0014\n\u0015)\nabc\nca\nacc\n,\n,\nab\na\nba\nFor obvious reason, each top string is longer than its bottom\nstring.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fPost Correspondence Problem (PCP)\nAn instance P of PCP\nof dominoes\n(\u0014is a\u0015 collection\n\u0014 \u0015\n\u0014 \u0015 )\nt1\nt2\ntk\nP =\n,\n,...,\n,\nb1\nb2\nbk\nA match is a sequence i1 , i2 , . . . , il such that\nti1 ti2 . . . til = bi1 bi2 . . . bil\nAs usual, we define PCP as a set\nP CP = {hP i | P is an instance of the Post Correspondence\nProblem with a match}.\nIs P CP decidable?\nGiven an instance P (a set of dominoes) of PCP, can we\ndecide whether P ∈ P CP or not?\nP ∈ PCP if P has a match\nP 6∈ PCP if P does not have a match\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fPost Correspondence Problem (PCP)\nTo prove that PCP is undecidable, we use the same method\nas what we did for other undecidable languages\n1\n2\n\nAssume that PCP is decidable and obtain its decider R\nConstruct TM S with the help of R to decide ATM\nTake hM, wi (an instance of ATM ) and construct P an\ninstance of PCP which is a set of dominos\nP has a match if M accepts w\nP does not have a match if M does not accept w\n\n3\n4\n5\n\nProve that S is a decider for ATM\nConclude that ATM is decidable — contradiction\nTherefore, PCP is undecidable\n\nNote that we have to construct a set of dominoes from TM\nM and string w\nThis process is long but straightforward\nWe also need to look at another closely related problem\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fModified Post Correspondence Problem (MPCP)\nA Modified Post Correspondence Problem (MPCP) looks\nexactly like PCP but the match is required to start with the\nfirst domino.\nAn instance P of MPCP\nof dominoes\n(\u0014 is\u0015 a\u0014collection\n\u0015\n\u0014 \u0015 )\nt1\nt2\ntk\nP =\n,\n,...,\n,\nb1\nb2\nbk\nA match is a sequence 1,i2 , i3 , . . . , il such that\nt1 ti2 . . . til = b1 bi2 . . . bil\nAs usual, we define MPCP as a language\nM P CP = {hP i | P is an instance of the Post Correspondence\nProblem with a match that starts with he first\ndomino}.\nWe are going to construct P 0 (an instance of MPCP) from\nTM M and string w such that P 0 has a match if an only if M\naccepts w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConstructing a Set of Dominoes\n\nFor simplicity, we are going to pick a specific TM M and a\nstring w\nConstruct a set of dominoes P 0 from M and w\nVerify that P 0 has a match if and only if M accepts w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\nConsider the following TM M :\n0→R\nq0\n1, t → R\n\nq1\n\n1→L\nt\n\nqreject\n\n0→R\n\n→R\n\nqaccept\n\nThe above machine:\naccept 00:\nq0 00 → 0q1 0 → 00qaccept\nreject 1:\nq0 1 → 1qreject\nloops indefinitely on 01:\nq0 01 → 0q1 1 → q0 01 → 0q 11 → . . .\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fComputation History\nRecall that M accepts w if and only if accepting\ncomputation history for M on input w exists\nSince M (from previous slide) accepts 00, the accepting\ncomputation history for M on input 00 exists\nq0 00 → 0q1 0 → 00qaccept\nWe can turn it into a string as\n#q0 00#0q1 0#00qaccept #\nThis will be our match\nIf the accepting computation history for M on input w exist,\nthere is a match\nAnd the match looks exactly like the computation history\n\nIf the accepting computation history for M on input w does\nnot exist, there is no match\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fFormal Definition of TM M\n\nThe formal definition of the previous TM M is as follows:\nQ = {q0 , q1 }\nΣ = {0, 1}\nΓ = {0, 1, t}\nδ are as follows:\nδ(q0 , 0) = (q1 , 0, R)\nδ(q0 , 1) = (qreject , 1, R)\nδ(q0 , t) = (qreject , t, R)\nδ(q1 , 0) = (qaccept , 0, R)\nδ(q1 , 1) = (q0 , 1, L)\nδ(q1 , t) = (qreject , t, R)\n\nq0 is the start state\nqaccept\nqreject\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\nFor the TM M (from earlier slide) and the string 00, let’s\nconstruct a set of dominoes (an instance of MPCP):\nFirst, create the domino where the starting configuration is at\nthe bottom part:\n\u0015\n\u0014\n#\n#q0 00#\nThis is the first domino, a match must start with this\ndomino.\n\nThis is what we have so far:\n(\u0014\n\u0015)\n#\n#q0 00#\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\nConsider a configuration\nucqav\nwhere u and v are strings, a and c are symbols in Γ, and q is\na state\nSuppose M has a transition as follows:\nδ(q, a) = (r, b, R)\nThe above configuration yields\nucbrv\nIf we put one of top of the other:\nucqav\nqa\nthe part that change from top to bottom is\nucbrv\nbr\nwhich will be our dominoes\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\nFor every a, b ∈ Γ and every q, r ∈ Q where q 6= qreject\n\u0014 \u0015\nqa\ninto the set\nif δ(q, a) = (r, b, R), put\nbr\nThere are five transitions that move the tape head to the right\ndirection:\n\u0014\n\u0015\nδ(q0 , 0) = (q1 , 0, R) gives us\n\nq0 0\n0q\u00141\n\n\u0015\nq0 1\n\u00141qreject \u0015\nq0 t\nδ(q0 , t) = (qreject , t, R) gives us\ntqreject\n\u0015\n\u0014\nq1 0\nδ(q1 , 0) = (qaccept , 0, R) gives us\n\u0014 0qaccept \u0015\nq1 t\nδ(q1 , t) = (qreject , t, R) gives us\ntqreject\nδ(q0 , 1) = (qreject , 1, R) gives us\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\nThis is what we have so far:\n(\u0014\n\u0015\n#\n,\n#q0 00#\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015)\n\u0014\nq0 1\nq1 0\nq0 t\nq1 t\nq0 0\n,\n,\n,\n,\n0q1\n1qreject\n0qaccept\ntqreject\ntqreject\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\nConsider a configuration\nucqav\nwhere u and v are strings, c and a are symbols in Γ, and q is\na state\nSuppose M has a transition as follows:\nδ(q, a) = (r, b, L)\nThe above configuration yields\nurcbv\nIf we put one of top of the other:\nucqav\ncqa\nthe part that change from top to bottom is\nurcbv\nrcb\nwhich will be our dominoes\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\nFor every a, b, c ∈ Γ and every q, r ∈ Q where q 6= qreject\n\u0014\n\u0015\ncqa\nif δ(q, a) = (r, b, L), put\ninto the set\nrcb\nThere is only one transition that move the tape head to the\nleft direction:\nδ(q1 , 1) = (q0 , 1, L) gives us the following dominoes:\n\u0014\n\u0015 \u0014\n\u0015\n\u0014\n\u0015\n0q1 1\n1q1 1\ntq1 1\n,\n, and\nq0 01\nq0 11\nq0 t1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\nThis is what we have so far:\n(\u0014\n\u0015\n#\n,\n#q0 00#\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n,\n,\n,\n,\n,\n0q1\n1qreject\n0qaccept\ntqreject\ntqreject\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015)\n0q1 1\n1q1 1\ntq1 1\n,\n,\nq0 01\nq0 11\nq0 t1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\nFor every a ∈ Γ, put\n\n\u0014 \u0015\na\ninto the set\na\n\nOur Γ is {0, 1, t}, this gives us the following dominoes:\n\u0014 \u0015 \u0014 \u0015\n\u0014 \u0015\n0\nt\n1\n,\n, and\n0\n1\nt\n\n\u0014 \u0015\n\u0014 \u0015\n#\n#\nNext, put\nand\ninto the set\n#\nt#\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\nThis is what we have so far:\n(\u0014\n\u0015\n#\n,\n#q0 00#\n\u0015 \u0014\n\u0015 \u0014\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 1\nq0 t\nq0 0\nq1 0\nq1 t\n,\n,\n,\n,\n,\n0q1\n1qreject\n0qaccept\ntqreject\ntqreject\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\n0q1 1\n1q1 1\ntq1 1\n,\n,\n,\nq0 01\nq0 11\nq0 t1\n\u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015)\n0\n1\nt\n#\n#\n,\n,\n,\n,\nt\n#\nt#\n0\n1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\n\n\u0014\nFor every a ∈ Γ, put\n\naqaccept\nqaccept\n\n\u0015\n\n\u0014\nand\n\n\u0015\nqaccept a\ninto the set.\nqaccept\n\nThis part gives us the following dominoes:\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n0qaccept\n1qaccept\ntqaccept\nqaccept 0\nqaccept 1\nqaccept t\n,\n,\n,\n,\n,\nqaccept\nqaccept\nqaccept\nqaccept\nqaccept\nqaccept\n\n\u0014\n\nqaccept ##\nLastly, simply add the domino\n#\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n\u0015\n\nPost Correspondence Problem\n\n\fConverting a TM and a string to a set of dominoes\nFor the TM M and the string 00, we get this set of dominoes:\n(\u0014\n\u0015\n#\n,\n#q0 00#\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq0 t\nq1 0\nq1 t\n,\n,\n,\n,\n,\ntqreject\ntqreject\n0q1\n1qreject\n0qaccept\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntq1 1\n0q1 1\n1q1 1\n,\n,\n,\nq0 01\nq0 11\nq0 t1\n\u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015\n1\n#\n#\n0\nt\n,\n,\n,\n,\n,\nt#\n0\n1\n#\nt\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n0qaccept\n1qaccept\ntqaccept\n,\n,\n,\nqaccept\nqaccept\nqaccept\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015)\n\u0014\nqaccept 0\nqaccept 1\nqaccept t\nqaccept ##\n,\n,\n,\nqaccept\nqaccept\nqaccept\n#\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qreject\n0qaccept\nq0 01\ntqreject\ntqreject\n\u0014\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n#\n1q1 1\ntq1 1\n0\n1\nt\n#\n0qaccept\n1qaccept\n,\n,\n,\n,\n,\n,\n,\n,\n,\nq0 11\nq0 t1\n0\n1\n#\nqaccept\nqaccept\nt\nt#\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015)\n\u0014\ntqaccept\nqaccept 0\nqaccept 1\nqaccept t\nqaccept ##\n,\n,\n,\n,\nqaccept\nqaccept\nqaccept\nqaccept\n#\n\nIf we consider q0 , q1 , qaccept and qreject as symbols (not\nstrings)\nWe can categorize dominoes into three groups based on the\nlength of the top string vs the bottom string\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nThe top string is shorter than the bottom string:\n\u0014\n\u0015 \u0014 \u0015\n#\n#\n,\n#q0 00#\nt#\nThe top string and the bottom string have equal length:\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\nq0 1\nq1 0\nq0 t\nq1 t\nq0 0\n,\n,\n,\n,\n,\n0q1\n1qr\n0qa\ntqr\ntqr\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015\n0q1 1\ntq1 1\nt\n#\n1q1 1\n0\n1\n,\n,\n,\n,\n,\n,\nt\nq0 01\nq0 11\nq0 t1\n0\n1\n#\nThe top string is longer than the bottom string:\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n0qa\n1qa\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\n,\n,\nqa\nqa\nqa\nqa\nqa\nqa\n#\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nThe set of dominoes\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\ntqr\ntqr\nq0 01\n\u0014\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n1q1 1\ntq1 1\n0\n1\nt\n#\n0qa\n1qa\n#\n,\n,\n,\n,\n,\n,\n,\n,\n,\nq0 11\nq0 t1\n0\n1\nt\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\nIt was purely constructed from the definition of TM M and\ninput string 00\nGiven any TM M and a string w, we can use the same\nprocess to construct a set of dominoes\nLet’s play\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\n#\n\nq0 0\n\n0\n\n#\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n#\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n#\n\n0\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n#\n\n0\n\n0\n\nqa\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0\n\n0\n\nqa #\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0\n\nqa #\n\n0\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0\n\nqa\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\n0\n\nqa #\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n0\n\nqa\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n0\n\nqa #\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\nqa\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n0\n\nqa #\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n0\n\nqa #\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\nqa #\n\n\fThe set of dominoes\nLet’s focus on the set of dominoes that we just constructed:\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\ntq1 1\n0\n1\nt\n#\n#\n0qa\n1qa\n1q1 1\n,\n,\n,\n,\n,\n,\n,\n,\n,\nt\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n0\n\nqa #\n\nqa #\n\n#\n\n#\n\nq0 0\n\n0\n\n#\n\n0\n\nq1 0\n\n#\n\n0\n\n0\n\nqa #\n\n0\n\nqa #\n\nqa #\n\n#\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nThe set of dominoes\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n#\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n,\n,\n,\n,\n,\n,\n,\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\ntqr\ntqr\n\u0015 \u0014\n\u0015\n\u0014\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n#\n#\n0qa\n1qa\n1q1 1\ntq1 1\nt\n0\n1\n,\n,\n,\n,\n,\n,\n,\n,\n,\n#\nt#\nqa\nqa\nt\nq0 11\nq0 t1\n0\n1\n)\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\ntqa\nqa 0\nqa 1\nqa t\nqa ##\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\nThe above set of dominoes was constructed from TM M and\nthe string 00\nIf M accepts 00, the accepting computation history for M on\ninput 00 exists, results in a match\nIf M does not accept 00, the accepting computation history\nfor M on input 00 does not exists, results in no match\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fThe set of dominoes\nThe set of dominoes\n(\u0014\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n#\nq0 0\nq0 1\nq1 0\nq0 t\nq1 t\n0q1 1\n,\n,\n,\n,\n,\n,\n,\ntqr\ntqr\n#q0 00#\n0q1\n1qr\n0qa\nq0 01\n\u0015 \u0014\n\u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014 \u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\n0\n1\n#\n0qa\n1qa\ntq1 1\nt\n1q1 1\n#\n,\n,\n,\n,\n,\n,\n,\n,\n,\nq0 11\nq0 t1\n0\n1\n#\nt#\nqa\nqa\nt\n\u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015)\nqa 0\nqa 1\nqa t\nqa ##\ntqa\n,\n,\n,\n,\nqa\nqa\nqa\nqa\n#\n\nUnfortunately, the above set of dominoes is an instance of\nMPCP not PCP\nIf the above set of dominoes is an instance of PCP it has a\nmatch regardless of whether M accepts 00\nCan you see one?\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting P 0 of MPCP to P of PCP\nNow we have an instance P 0 of MPCP. We need to convert it\nto an instance of PCP.\nWe need to enforce that the only match of P must start with\na specific domino only.\nLet u = u1 u2 · · · un be any string of length n. Let\n?u = ∗ u1 ∗ u2 ∗ u3 ∗ · · · ∗ un\nu? =\n\nu1 ∗ u2 ∗ u3 ∗ · · · ∗ un ∗\n\n?u? = ∗ u1 ∗ u2 ∗ u3 ∗ · · · ∗ un ∗\nExample: Suppose u = 011\n?u = ∗ 0 ∗ 1 ∗ 1\nu? =\n\n0 ∗ 1 ∗ 1∗\n\n?u? = ∗ 0 ∗ 1 ∗ 1∗\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConverting P 0 of MPCP to P of PCP\nIf P 0 were the collection\n(\u0014 \u0015 \u0014 \u0015 \u0014 \u0015\n\u0014 \u0015)\nt1\nt2\nt3\ntk\n,\n,\n,...,\nb1\nb2\nb3\nbk\nwe let(P be the collection\n\u0015 \u0014\n\u0015 \u0014\n\u0015 \u0014\n\u0015\n\u0014\n\u0015 \u0014 \u0015)\n\u0014\n?t1\n?t2\n?t3\n?tk\n?\u0005\n?t1\n,\n,\n,\n,...,\n,\n?b1 ?\nb1 ?\nb2 ?\nb3 ?\nbk ?\n\u0005\nIn doing so, the match can only start with\n\u0014\n\u0015\n?t1\n?b1 ?\nThe only domino that top and bottom start with the same\nsymbol (∗)\n\u0014 \u0015\n?\u0005\nThe domino\nallows us to add extra ∗ to the top at the\n\u0005\nend of the match.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConclusion\n\nAssume that PCP is decidable. Thus, there exists TM R\ndecides PCP.\nConstruct TM S that decides ATM\nS =“On input hM, wi where M is a TM and w is a string:\n1\n\n2\n3\n\nConstruct a set of dominoes P an instance of PCP from TM\nM and input w as we discussed\nThen run R on input hP i\nIf R accepts hP i, accept\nIf R rejects hP i, reject\n\nTo prove that TM S a decider for ATM is straightforward\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\fConclusion\nShow that TM S is a decider for ATM\n1\n\n2\n\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M accepts\nw. Since M accepts w, the set of dominoes P constructed in\nstep 1 will have a match. In other words, hP i ∈ PCP. Since R\nis a decider for PCP, by running R on input hP i, R will accept\nhP i. Since R accepts hP i, S accepts hM, wi.\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M does\nnot accept w. Since M does not accept w, the set of\ndominoes P constructed in step 1 will have not have match. In\nother words, hP i 6∈ PCP. Since R is a decider for PCP, by\nrunning R on input hP i, R will reject hP i. Since R rejects\nhP i, S rejects hM, wi.\n\nThis shows that S is a decider for ATM . Therefore, ATM is\ndecidable — contradiction\nPCP is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nPost Correspondence Problem\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":262,"segment": "self_training_2", "course": "cs1502", "lec": "lec11_turing_machine_03","text":"Turing Machine 03\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fCombining Turing Machines\n\nA Turing machine represents an algorithm\nGenerally an algorithm can be described as a number of\nsmaller algorithms working in combination\nSimilarly, we can combine several Turing machines into a\nlarger one\nExample, two Turing machines T1 and T2 sharing the same\ntape:\nWhen T1 finishes (either in the accept or reject state), T2\ntakes over\nThis new machine is represented by T1 T2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fCombining Turing Machines\nSuppose we have two Turing machines:\n1\n1\n1\n) and\nT1 = (Q1 , Σ, Γ, δ1 , qstart\n, qaccept\n, qreject\n2\n2\n2\n)\nT2 = (Q2 , Σ, Γ, δ2 , qstart , qaccept , qreject\n\nLet T = (Q, Σ, Γ, δ, qstart , qaccept , qreject ) be T1 T2 which can\nbe constructed as follows:\nQ = Q1 ∪ Q2 (states of T2 are relabeled if necessary)\n1\nInitial state of T is the initial state of T1 (qstart = qstart\n)\n1\n1\nδ = δ1 ∪ δ2 except those of T1 that go to qaccept and qreject\nq\n\nx → y, D\n\nq1\n\nq\n\nx → y, D\n\nq2\n\na\ns\nA transition\nin T1 becomes\n1\n2\nwhere qa is the accept state of T1 and qs is the start state of\nT2\n\nIf T1 enter its accept state, T2 takes over. The moves that\ncause T to accept are precisely those that case T2 to accept\nHowever, if T1 enter the reject state and halt, so does T\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fExample\n\nSuppose we want to create a machine that recognize a\npalindrome (e.g., racecar)\nSuppose we have the following Turing machines:\nCopy: From tx to txtx\nNB : Moves tape head to the next blank symbol to the right\nPB : Moves tape head to the next blank symbol to the left\nR: Reverses the content of the tape from tx to txr\nxr is the reverse of a string x\n\nEqual : Compare two strings separated by a blank symbol\n\nFor simplicity, we put the blank symbol on the first square of\nthe tape to indicate the left-end of the tape.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fExample\nLet the content of a tape be tx where x is a string\nThe following machine will accept if x is a palindrome:\nCopy → NB → R → PB → Equal\nStep by Step:\nMachine\n\nTape\n\nStart\n\nt\n\n↓\n↓\n\nx\n\nCopy\n\nt\n\nNB\n\nt\n\nR\n\nt\n\nPB\n\nt\n\nEqual\n\nt\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nxtx\n↓\n\nxtx\n↓\n\nxtxr\n\n↓\n\nxtxr\n↓\n\nxtxr t\n\nTuring Machine 03\n\n\fMultitape Turing Machines\n\nA Turing machine can have multiple tape and tape heads:\n0\n\n1\n\n0\n\na\n\na\n\na\n\nb\n\na\n\n1\n\n0\n\nM\n\nAll tape heads can read then write and move in a single\nTuring machine step\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fMultitape Turing Machines\nTransition function need to control\/make decision based on\nsymbols read from all tapes\nExample: A transition function of a three-tape TM:\nδ(q, x, y, z) → (r, a, b, c, R, L, R)\nCurrent state is q, the first tape reads x, the second tape reads\ny, and the third tape reads z\nChange the current state to r\nWrite a on to the first tape, write b onto the second tape, and\nwrite c onto the third tape\nMove the first tape head to the right direction, move the\nsecond tape head to the left direction, and move the third tape\nhead to the right direction\n\nMultitape TMs are suitable for algorithms in which several\nkinds of data are involved\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fMultitape to One-Tape\nA multitape Turing machine from previous slide, can be\nconvert into one-tape Turing machine as shown below:\nS\n#\n\n0\n\n1\n\n0\n\n1\n\n0\n\n#\n\na\n\na\n\na\n\n#\n\nb\n\na\n\n#\n\nUse # symbol to separate content between tapes\n•\nUse x to indicate the current position of each tape head\nOne move of multitape machine will be equal to several moves\nof one-tape machine\nFor example,\nδ(q, 1, a, b) → (r, 0, b, a, L, L, R)\nwill be\n1\n\n2\n\n3\n\nMove to the next • on the right, write 0, move to the left\nsquare, write • over the symbol, and move to the right square\nMove to the next • on the right, write b, move to the left\nsquare, write • over the symbol, and move to the right square\nMove to the next • on the right, write b, move to the right\nsquare, write • over the symbol, and move to the left-end\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fMultitape to One-Tape\nRecall that a tape will be filled with blank after the last\nsymbol of the string on the tape\n0\n\n1\n\n0\n\na\n\na\n\na\n\nb\n\na\n\n1\n\n0\n\n1\n\n0\n\na\n\na\n\nM\n\nS\n#\n\n0\n\n1\n\n0\n\n#\n\na\n\n#\n\nb\n\na\n\n#\n\nFrom the above multi-tape TM, if the second tape head needs\nto move tot he right direction, it should be on top of a blank\nsymbol\nBut on a single-tape TM, it will be on top of the # symbol\nSingle-tape TM must insert the blank symbol with a dot at\nthe #\n\nEvery multitape TM has an equivalent single-tape TM\n(slower)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fNondeterministic Turing Machine\nSimilar to Nondeterministic Finite Automata (NFA)\nProcessing one input symbol results in one or more machine.\nδ : Q × Γ → P(Q × Γ × {L, R})\nComputation is a tree similar to NFA\n\nFor a nondeterministic Turing machine (NTM):\nIf a branch is in the accept state, the machine accepts the\ninput string\nIf all branches are in the reject state, the machine rejects the\ninput string\nIf no branch is in the accept state and at least one branch\nenter an infinite loop, the machine loops indefinitely on the\ninput string\n\nTheorem 3.16\nEvery nondeterministic Turing machine has an equivalent\ndeterministic Turing machine.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fProof Idea\n\nTheorem 3.16 Rewording\nFor every nondeterministic TM T = (Q, Σ, Γ, δ, qstart , qaccept , qreject ), there\n0\n0\n0\n)\nis an ordinary (deterministic) TM T 0 = (Q0 , Σ, Γ0 , δ 0 , qstart\n, qaccept\n, qreject\n0\nwith L(T ) = L(T ).\n\nRecall that δ : Q × Γ → P(Q × Γ × {L, R})\nProcessing a tape alphabet at a state may result in multiple\nmachines\nThe upper bound of the number of machines is\n|Q| × |Γ| × |{L, R}|\nFor simplicity, assume that for every combination of nonhalting\nstate and tape symbol, there are exactly two moves (split to\ntwo machines)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fProof Idea\nComputational Tree of a TM on an input\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n1\n\n0\n\n1\n\nThe branch in blue behaves like a deterministic TM\nThe move follows the path in blue can be represented by 0110\nUse 0110 as a guideline to simulate a branch\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fProof Idea\nSuppose a deterministic TM picks a branch and simulate it\nIf that branch ends in the accept state, the NTM accepts the\ninput string\nIf that branch ends in the reject state, no conclusion\nIf another branch is in the accept state, NTM accepts the\ninput string\nIf all other branches are in the reject state, NTM rejects the\ninput string\nIf no branch is in the accept state and at least one branch\nenter infinite loop, NTM loops indefinitely on the input string\n\nIf that branch enter infinite loop, the simulation will not end\nWe do not always know that a TM has enter an infinite loop\nEven if we know that it enters an infinite loop, we still cannot\nconclude whether NTM accepts or rejects the input string\n\nMachine T 0 that simulate an NTM will have to test all\npossible moves (level order, breadth first search)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fProof Idea\nMachine T 0 consists of four tapes\nTape 1 will be the input string and its contents never change\nTape 2 contain the binary string that represents the sequence\nof moves we are currently testing. (e.g., 0110t)\nTape 3 is the working tape of a copy of NTM\nTape 4 keeps track of all possible reject sequences\n\nIf a sequence of moves result in the accept state, T 0 accepts\nthe input string\nIf all possible sequence of the same length end in the reject\nstate, T 0 rejects the input string\nIf NTM loops indefinitely on the input string, the simulation\nwill also loop indefinitely\nSince T 0 is a multitape Turing machine, there is an equivalent\nsingle-tape Turing machine\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fUniversal Turing Machine\nA universal TM is a TM that can run another TM on an input\nstring\nImagine a multi-tape TM:\nTape 1 contains the formal definition of a TM M , followed by\na # symbol, and an input string w\nTape 2 will be a working tape for TM M\nTape 3 will be used to keep track of the current state of TM M\n\nInitially:\nCopy input string w to tape 2\nPut the start state of TM M onto tape 3\n\nTo run a step, simply search for δ(q, a) in the formal definition\nof TM M\nq is the current state of tape 3\na is the symbol under the second tape head\n\nand update tapes 2 and 3 until tape 3 contains qaccept or\nqreject\nA universal TM will loop indefinitely if the TM that it is\nrunning loop indefinitely\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\fThe Church-Turing Thesis\n\nTo say that the Turing machine is a general model of\ncomputation means that any algorithmic procedure that can\nbe carried out at all, by a human computer or a team of\nhumans or an electronic computer, can be carried out by a\nTuring machine.\nNote that a Turing machine depends on low-level operations\nA complex algorithm is simply a series of simple instruction\n(e.g., assembly) that involve\nsophisticated logic (state machine) or\ncomplex bookkeeping (tape\/memory) strategies\n\nAn algorithm is a procedure that can be carried out by a\nTuring machine.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machine 03\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":263,"segment": "self_training_2", "course": "cs1502", "lec": "lec20_reducibility_as_methods","text":"Turing Machines as Methods\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Turing Machine\n\nThe language of a Turing machine M denoted by L(M ) is the\nset of all strings accepted by the Turing machine\nGiven a Turing machine M and a string w\nIf M accepts w, w ∈ L(M )\nIf M rejects w, w 6∈ L(M )\nIf M loops indefinitely on w, w 6∈ L(M )\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\n\nLets think about Turing machines as Java methods with the\nfollowing signature:\nboolean method_name(String x)\n\nLets\ntrue in Java means accept\nfalse in Java means reject\n\nNow, we can define the language of a method as the set of\nall strings that the method returns true\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nConsider the following method\nboolean M1(String x)\n{\nreturn true;\n}\n\nWhat is the language of the above method?\nObviously, M1() returns true no matter what argument of\ntype String you use\nThus, the language of the method M1() is Σ∗\n\nThe above method is pretty much the same as the Turing\nmachine below:\nM1 = “On input x:\n1\n\naccept”\n\nNote that L(M1 ) = Σ∗\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nConsider the following method\nboolean M2(String x)\n{\nreturn false;\n}\n\nWhat is the language of the above method?\nObviously, M2() returns false no matter what argument of\ntype String you use\nThus, the language of the method M2() is ∅\n\nThe above method is pretty much the same as the Turing\nmachine below:\nM2 = “On input x:\n1\n\nreject”\n\nNote that L(M2 ) = ∅\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\n\nConsider the following method\nboolean M3(String x)\n{\nif(x.equals(\"\"))\nreturn true;\nelse\nreturn false;\n}\n\nWhat is the language of the above method?\nThe above method only returns true if we call use the empty\nstring as the argument\nFor any other arguments that are not the empty string, it will\nalways return false\nThus, the language of the method M3() is {ε}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\n\nThe method M3() is pretty much the same as the Turing\nmachine below:\nM3 = “On input x:\n1\n2\n\nIf x = ε, accept\nIf x 6= ε, reject\n\nNote that L(M3 ) = {ε}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\n\nSuppose we have the method named is 0n1n with the\nfollowing signature:\nboolean is_0n1n(String s)\n\nThis method returns true if the string s is in the form of 0n 1n\nfor any n ≥ 0\nIt returns false for any other strings\nExamples:\nis 0n1n(\"\") is true\nis 0n1n(\"0101\") is false\nis 0n1n(\"01\") is true\nis 0n1n(\"00111\") is false\nis 0n1n(\"000111\") is true\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\n\nConsider the following method\nboolean M4(String x)\n{\nboolean p = is_0n1n(x);\nif(p)\nreturn true;\nelse\nreturn false;\n}\n\nWhat is the language of the above method?\nObviously, M4() returns true for any argument of type\nString of the form 0n 1n for any n ≥ 0\nFor any other strings, it will return false\nThus, the language of the method M4() is {0n 1n | n ≥ 0}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\n\nThe method M4() is pretty much the same as the Turing\nmachine below:\nM4 = “On input x:\n1\n2\n\nIf x is in the form of 0n 1n , accept\nIf x is not in the form of 0n 1n , reject\n\nThe language of the above machine is\nL(M4 ) = {0n 1n | n ≥ 0}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nConsider the following method\nboolean M5(String x)\n{\nwhile(true);\nreturn true;\n}\n\nWhat is the language of the above method?\nObviously, no matter what argument of type String you use\nwhen call the above method, it will not return\nIt loops indefinitely on all arguments.\nThus, the language of the method M5() is ∅\n\nThe above method M5() is pretty much the same as the\nTuring machine below:\nM5 = “On input x:\n1\n2\n\nEnter infinite loop\naccept”\n\nNote that L(M5 ) = ∅\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nNow suppose we have a method named M() and and a string\nnamed w as follows:\nboolean M(String s) {...}\nString w = \"...\";\n\nNote that we have no idea what is the method M()\nIt just a method that takes an argument of type String and\nreturns boolean\nJust imagine that it can be any method with the above\nsignature\nCan even thing of M as a reference variable that refers to a\nmethod of the above signature\n\nObviously, w is a variable of type String.\nWhat if we call M(w)?\nWe do not know whether it will return true, or false\nIt may not return at all (loops indefinitely)\nWe have no idea what are M and w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nConsider the following method\nboolean N1(String x)\n{\nboolean p = M(w);\nif(p)\nreturn true;\nelse\nreturn false;\n}\n\nIn this case, we have no idea about the language of the\nmethod N1()\nThis is because have no idea what does M(w) return when\ncalled\nIt may return true\nIt may return false\nIt may not return at all (M(w) loops indefinitely)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nSuppose M(w) returns true, we can replace M(w) by true as\nshown below:\nboolean N1(String x)\n{\nboolean p = true;\nif(p)\nreturn true;\nelse\nreturn false;\n}\n\n\/\/ Used to be boolean p = M(w);\n\nNow, what is the language of the above method N1?\nThe language of the method N1 is Σ∗\nThis is under the assumption that M(w) returns true\n\nSo, here is what we can say about the method N1() so far:\nIf M(w) returns true, the language of N1() is Σ∗\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nSuppose M(w) returns false, we can replace M(w) by false\nas shown below:\nboolean N1(String x)\n{\nboolean p = false;\nif(p)\nreturn true;\nelse\nreturn false;\n}\n\n\/\/ Used to be boolean p = M(w);\n\nNow, what is the language of the above method N1?\nThe language of the method N1() is ∅\nThis is under the assumption that M(w) returns false\n\nSo, here is what we can say about the method N1() so far:\nIf M(w) returns true, the language of N1() is Σ∗\nIf M(w) returns false, the language of N1() is ∅\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nSuppose M(w) does not return at all\nIt loops indefinitely when called\n\nSince N1() calls M(w) and M(w) loops indefinitely, N1() also\nloops indefinitely.\nIn this case, we can insert the statement while(true); as\nshown below\nboolean N1(String x)\n{\nwhile(true);\nboolean p = M(w);\nif(p)\nreturn true;\nelse\nreturn false;\n}\n\n\/\/ Inserted to make N1 loops indefinitely\n\nNow, what is the language of the above method N1?\nSince N1() does not return true on any argument of type\nString\nThe language of the method N1 is ∅\n\nThis is under the assumption that M(w) loops indefinitely\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nSo, here is what we can say about the method N1() below:\nboolean N1(String x)\n{\nboolean p = M(w);\nif(p)\nreturn true;\nelse\nreturn false;\n}\n\nIf M(w) returns true, the language of N1() is Σ∗\nIf M(w) returns false, the language of N1() is ∅\nIf M(w) loops indefinitely, the language of N1() is ∅\n\nThe behavior of the methods N1() depends on what happen\nwhen M(w) is called\nBut again, we have no idea what are M() and w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\fLanguage of a Method\nGiven a Turing machine M and a string w, the method N1()\nis pretty much the same as the Turing machine below:\nN1 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept\nIf M rejects w, reject”\n\nSimilarly, we have no idea what is the language of N1 but at\nleast we know that\nIf M accepts w, L(N1 ) = Σ∗\nIf M rejects w, L(N1 ) = ∅\nIf M loops indefinitely on w, L(N1 ) = ∅\n\nWe can also conclude that\nIf M accepts w, the language of N1 is not empty\nIf M does not accept w, the language of N1 is empty\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTuring Machines as Methods\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":264,"segment": "unlabeled", "course": "cs1502", "lec": "lec21_reducibility_04","text":"Reducibility 04\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fUndecidability Proofs\n\nSo far, we have only one undecidable language:\nATM = {hM, wi | M is a TM that accepts w}\nThis is a fact since we already prove it\nGiven a language A, how to prove that it is undecidable?\nFirst, we are going to use the proof by contradiction\nAssume that A is decidable\nShow that since A is decidable, ATM is decidable\nContradict with the fact that ATM is undecidable\nA is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHalting Problem\nFrom a programmer point of view:\nGiven a method foo and an input x, there is no algorithm that\ncan check whether foo will halt on input x\nFor some methods, you may be able to spot that it will halt or\nwill not halt but not all\nBut there are infinite number of methods and infinite number\nof inputs\n\nFrom the computation theory point of view:\nGiven a Turing machine M and input string w, there is no\nalgorithm that can check whether M will halt on input w\nFormally,\nHALT TM = {hM, wi | M is a TM that halts on input w}\n\nTo prove halting problem, we just need to show that\nHALT TM is undecidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\nFirst, assume that HALT TM is decidable\nNext, we need to show that since HALT TM is decidable ATM\nis decidable\nWe need to take the advantage of HALT TM being decidable\nto help us decide ATM\n\nWhat does it mean when HALT TM is decidable?\nThere exists a TM R that decides HALT TM\n\nWhat TM R (a decider for HALT TM ) can do?\nIf we run R on input hM, wi where M is a TM and w is a\nstring, R will either accept or reject hM, wi\nIf R accepts hM, wi\n\nhM, wi ∈ HALT TM\nM halts on input w\nhM, wi 6∈ HALT TM\nIf R rejects hM, wi\nM does not halts on input w\n\nCan R loops in definitely on input hM, wi?\nNo because R is a decider\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\nWhen HALT TM is decidable, given a TM M 0 and a string w0 ,\nwe know whether M 0 halts on input w0\nBy running R on input hM 0 , w0 i\nIf R accepts hM 0 , w0 i\nhM 0 , w0 i ∈ HALT TM\nM 0 halts on input w0\n0\n0\nIf R rejects hM , w i\nhM 0 , w0 i 6∈ HALT TM\n0\nM does not halt on input w0\n\nWe need to use TM R to help us decide ATM\nBut what does it mean when ATM is undecidable?\nGiven a TM M and a string w, we have no idea whether\nM accepts w or\nM does not accepts w\n\nIf we can distinguish between M accepts w and M does not\naccept w, ATM is decidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\nWhat if we can construct TM M 0 and w0 (from the TM M\nand the string w) such that\nIf M accepts w, M 0 halts on input w0\nIf M does not accept w, M 0 does not halt on input w0\n\nand give hM 0 , w0 i to TM R?\nBy running R on the above input hM 0 , w0 i:\nIf R accepts hM 0 , w0 i:\nR accepts hM 0 , w0 i\nhM 0 , w0 i ∈ HALT TM\n0\n0\nhM , w i ∈ HALT TM\nM 0 halts on input w0\n0\n0\nM halts on input w\nM accepts w\nM accepts w\nhM, wi ∈ ATM\n\nIf R rejects hM 0 , w0 i:\nR rejects hM 0 , w0 i\nhM 0 , w0 i 6∈ HALT TM\n0\n0\nhM , w i 6∈ HALT TM\nM 0 does not halt on input w0\nM 0 does not halt on input w0\nM does not accept w\nM does not accept w\nhM, wi 6∈ ATM\n\nThis gives us the conclusion that ATM is decidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\n\nAgain, we need to construct TM M 0 and w0 (from the TM M\nand the string w) such that\nIf M accepts w, M 0 halts on input w0\nIf M does not accept w, M 0 does not halt on input w0\n\nHere is an example of TM M 0 :\nM 0 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nNote that there are infinite number of TMs that satisfy the\nabove specification\nLet’s analyze the behavior of the above TM M 0 case-by-case\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\n\nThe TM M 0 :\nM 0 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nWhat if M accepts w (hM, wi ∈ ATM )?\nM accepts w\nM 0 accepts all input strings\n0\nM 0 halts on all input strings\nM accepts all input strings\n0\nM halts on all input strings\nM 0 halts on ε\n0\n0\nhM , εi ∈ HALT TM\nM halts on ε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\n\nThe TM M 0 :\nM 0 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nWhat if M rejects w (hM, wi 6∈ ATM )?\nM rejects w\nM 0 enters infinite loop on all input strings\n0\nM 0 does not halt\nM enters infinite loop on all input strings\non all input strings\nM 0 does not halt on ε\nM 0 does not halt on all input strings\n0\n0\nhM , εi 6∈ HALT TM\nM does not halt on ε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\n\nThe TM M 0 :\nM 0 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, enter infinite loop.”\n\nWhat if M loops indefinitely on w (hM, wi 6∈ ATM )?\nM 0 loops indefinitely (in step 1)\nM loops indefinitely on w\non all input strings\nM 0 does not halt\nM 0 loops indefinitely on all input strings\non all input strings\nM 0 does not halt on all input strings\nM 0 does not halt on ε\n0\n0\nhM , εi 6∈ HALT TM\nM does not halt on ε\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\n\nAgain, we just construct TM M 0 and w0 = ε from a TM M\nand a string w such that\nIf hM, wi ∈ ATM , hM 0 , εi ∈ HALT TM\nIf hM, wi 6∈ ATM , hM 0 , εi 6∈ HALT TM\n\nNow, we can simply run TM R (a decider for HALT TM to\ncheck whether hM 0 , εi is in HALT TM\nIf R accepts hM 0 , εi\nIf R rejects hM 0 , εi\n\nhM 0 , εi ∈ HALT TM\nhM 0 , εi 6∈ HALT TM\n\nhM, wi ∈ ATM\nhM, wi 6∈ ATM\n\nLet’s see the whole proof where we construct a TM S that\ndecides ATM where\nS constructs TM M 0 as we discussed earlier and\nuses R as a helper machine to help decide ATM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fHALT TM\nProve that HALT TM = {hM, wi | M is a TM that halts in input w} is undecidable.\nProof: Assume that HALT TM is decidable. Since HALT TM is decidable, there exists\na TM R that decides HALT TM . Let construct a TM S (to decide ATM ) as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1 Construct TM M 0 as follows:\nM 0 = “On input x:\n1 Run M on input w\n2 If M accepts w, accept. If M rejects w, enter infinite loop.”\n2 Run R on input hM 0 , εi\n3 If R accepts hM 0 , εi, accept. If R rejects hM 0 , εi, reject.”\nNext, we need to prove that TM S is a decider for ATM .\nProve that if hM, wi ∈ ATM , S accepts hM, wi: Assume that hM, wi ∈ ATM .\nSince hM, wi ∈ ATM , M is a TM that accepts w. Since M accepts w, from the\ndefinition of TM M 0 , M 0 halts on all strings including ε. Since M 0 halts on ε,\nhM 0 , εi ∈ HALT TM . Since R is a decider for HALT TM , by running R on input\nhM 0 , εi, R will accept hM 0 , εi. Since R accepts hM 0 , εi, S accepts hM, wi.\nProve that if hM, wi 6∈ ATM , S rejects hM, wi: Assume that hM, wi 6∈ ATM .\nSince hM, wi 6∈ ATM , M is a TM that does not accept w. Since M does not\naccept w, from the definition of TM M 0 , M 0 does not halt on all strings\nincluding ε. Since M 0 does not halt on ε, hM 0 , εi 6∈ HALT TM . Since R is a\ndecider for HALT TM , by running R on input hM 0 , εi, R will reject hM 0 , εi.\nSince R rejects hM 0 , εi, S rejects hM, wi.\nThis show that TM S is a decider for ATM . Therefore, ATM is decidable\ncontradiction. Therefore, HALT TM is undecidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fUndecidability\n\nLet’s recap and make some notes\nTo prove that a language A is undecidable:\nAssume that A is decidable\nSince A is decidable, there exists a TM R that decides A\nNext, we try to decide ATM using R as a helper machine\nTo decide ATM , we need to know whether hM, wi ∈ ATM\nIn other words, we need to know whether M accepts w\n\nSince R is a decider for the language A, R can tell you\nwhether a string s is in A\n\nSo, we need to construct a string s such that whether s is in\nA depending on whether M accepts w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fUndecidability\n\nTo construct a string s such that whether s is in A depending\non whether M accepts w, there are two choices:\n1\n\nChoice #1:\nIf M accepts w, s ∈ A\nIf M does not accept w, s 6∈ A\n\n2\n\nChoice #2:\nIf M accepts w, s 6∈ A\nIf M does not accept w, s ∈ A\n\nWhich one?\nIt depends on the problem\nFor some problems, either choices will work\nFor some problems, only one of those choices will work\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fUndecidability\n\nFrom the previous example, we have two choices as well:\n1\n\nChoice #1:\nIf M accepts w, hM 0 , w0 i ∈ HALT TM\nIf M does not accept w, hM 0 , w0 i 6∈ HALT TM\n\n2\n\nChoice #2:\nIf M accepts w, hM 0 , w0 i 6∈ HALT TM\nIf M does not accept w, hM 0 , w0 i ∈ HALT TM\n\nWe picked the first choice\nIf you try, you will see that you cannot construct a TM M 0\nthat satisfies the second choice\nThe hard part is when M loops indefinitely on input w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fETM\n\nProblem: Given a TM M , how to check whether L(M ) = ∅?\nUnfortunately, we cannot\nNote that a TM may have a path from the start state to its\naccept state but it accepts no string\n\nFormally:\nETM = {hM i | M is a TM and L(M ) = ∅}\nTo show that there is no algorithm that can solve the above\nproblem, we need to show that ETM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fETM\nAgain, for the sake of contradiction, assume that ETM is\ndecidable\nWe need to show that if ETM is decidable, ATM is decidable\n\nSince ETM is decidable, there exists a TM R that decides\nETM\nAgain, in plain English, given a TM M 0 , we know whether\nL(M 0 ) = ∅ by simply run R on input hM 0 i\nIf R accepts hM 0 i\nIf R rejects hM 0 i\n\nhM 0 i ∈ ETM\nhM 0 i 6∈ ETM\n\nL(M 0 ) = ∅\nL(M 0 ) 6= ∅\n\nWe are going to use the same method as in previous example:\nSince we are trying to decide ATM , we need to know whether\nM accepts w\nWe are going to construct a TM M 0 from a given TM M and\na string w such that L(M 0 ) (whether it is empty or not)\ndepends on whether M accepts w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fETM\n\nSpecification: Construct a TM M 0 from a given TM M and\na string w such that L(M 0 ) (whether it is empty or not)\ndepends on whether M accepts w\nAccording to the above specification, we have two choices:\n1\n\nChoice #1:\nIf M accepts w, L(M 0 ) = ∅\nIf M does not accept w, L(M 0 ) 6= ∅\n\n2\n\nChoice #2:\nIf M accepts w, L(M 0 ) 6= ∅\nIf M does not accept w, L(M 0 ) = ∅\n\nIn this case, we will not be able to construct TM M 0 that\nsatisfies the first choice\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fETM\nHere is an example of TM M 0 (constructed from a TM M\nand a string w) that satisfies:\nIf M accepts w, L(M 0 ) 6= ∅\nIf M does not accept w, L(M 0 ) = ∅\n\nTM M 0 :\nM 0 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nFrom the above TM M 0 :\nIf M accepts w, M 0 accepts all strings (L(M 0 ) 6= ∅)\nIf M does not accepts w:\nIf M rejects w, M 0 rejects all strings (L(M 0 ) = ∅)\nIf M loops indefinitely on input w, M 0 loops indefinitely on all\nstrings (L(M 0 ) = ∅)\n\nNow, let’s see the whole proof\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\fETM\nProve that ETM = {hM i | M is a TM and L(M ) = ∅} is undecidable.\nProof: Assume that ETM is decidable. Since ETM is decidable, there exists a TM R\nthat decides ETM . Let construct a TM S (to decide ATM ) as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1 Construct TM M 0 as follows:\nM 0 = “On input x:\n1 Run M on input w\n2 If M accepts w, accept. If M rejects w, reject.”\n2 Run R on input hM 0 i\n3 If R accepts hM 0 i, reject. If R rejects hM 0 i, accept.”\nNext, we need to prove that TM S is a decider for ATM .\nProve that if hM, wi ∈ ATM , S accepts hM, wi: Assume that hM, wi ∈ ATM .\nSince hM, wi ∈ ATM , M is a TM that accepts w. Since M accepts w, from the\ndefinition of TM M 0 , M 0 accepts all strings. In other words, L(M 0 ) 6= ∅. Since\nL(M 0 ) 6= ∅, hM 0 i 6∈ ETM . Since R is a decider for ETM , by running R on\ninput hM 0 i, R will reject hM 0 i. Since R rejects hM 0 i, S accepts hM, wi.\nProve that if hM, wi 6∈ ATM , S rejects hM, wi: Assume that hM, wi 6∈ ATM .\nSince hM, wi 6∈ ATM , M is a TM that does not accept w. Since M does not\naccept w, from the definition of TM M 0 , M 0 accepts no strings. In other words,\nL(M 0 ) = ∅. Since L(M 0 ) = ∅, hM 0 i ∈ ETM . Since R is a decider for ETM , by\nrunning R on input hM 0 i, R will accept hM 0 i. Since R accepts hM 0 i, S rejects\nhM, wi.\nThis show that TM S is a decider for ATM . Therefore, ATM is decidable\ncontradiction. Therefore, ETM is undecidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 04\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":265,"segment": "self_training_2", "course": "cs1502", "lec": "lec30_final_review","text":"Review\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fPSUBSETDFA\n\nQuestion\nProve that\nPSUBSETDFA = {hA, Bi|A and B are DFAs and L(A) ⊂ L(B)}\nis decidable.\nRecall that you need to perform two tasks:\n1\n2\n\nConstruct a TM that you think it decides PSUBSETDFA\nProve that your TM is actually a decider for PSUBSETDFA\n\nRecall L(A) ⊂ L(B) (proper subset)\nL(A) 6= L(B) and L(A) is a subset of L(B)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fPSUBSETDFA is Decidable\n\nSolution: Construct a TM M 0 to decide PSUBSETDFA as follows:\nM 0 =“On input hA, Bi where A and B are DFAs:\n1\n\nRun TM F on input hA, Bi\n\n2\n\nIf F accepts hA, Bi, reject.\n\n3\n\nConstruct a DFA C such that L(C) = L(A) ∩ L(B)\n\n4\n\nRun TM T on input hCi\n\n5\n\nIf T accepts hCi, accept. If T rejects hCi, reject.”\n\nNext, we need to prove that the above TM M 0 is a decider for PSUBSETDFA .\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fPSUBSETTM is Decidable\nTo show that TM M 0 is a decider for PSUBSETDFA , we need to perform the\nfollowing:\nShow that if hA, Bi ∈ PSUBSETDFA , M 0 will accept hA, Bi.\nAssume that hA, Bi ∈ PSUBSETDFA . Since hA, Bi ∈ PSUBSETDFA , A and B\nare DFAs and L(A) is a proper subset of L(B). In other words, L(A) is not\nequal to L(B) and L(A) is a subset of L(B). Since L(A) is not equal to L(B),\nhA, Bi 6∈ EQDFA . Since F is a decider for EQDFA , by running F on input hA, Bi,\nF will reject hA, Bi. Since L(A) is a subset of L(B), L(C) will be empty. Since\nL(C) is empty, hCi ∈ EDFA . Since T is a decider for EDFA , by running T on\ninput hCi, T will accept hCi. Since T accepts hCi, M 0 accepts hA, Bi.\nShow that if hA, Bi 6∈ PSUBSETTM , M 0 will reject hA, Bi.\nAssume that hA, Bi 6∈ PSUBSETDFA . Since hA, Bi 6∈ PSUBSETDFA , A and B\nare DFAs and L(A) is not a proper subset of L(B). In other words,\nL(A) = L(B) or L(A) is not a subset of L(B).\nSuppose L(A) = L(B). Since L(A) = L(B), hA, Bi ∈ EQDFA . Since F is a\ndecider for EQDFA , by running F on input hA, Bi, F will accept hA, Bi. Since\nF accepts hA, Bi, M 0 rejects hA, Bi.\nSuppose L(A) is not a subset of L(B). Since L(A) is not a subset of L(B),\nL(C) will not be empty. Since L(C) is not empty, hCi 6∈ EDFA . Since T is a\ndecider for EDFA , by running T on input hCi, T will reject hCi. Since T rejects\nhCi, M 0 rejects hA, Bi.\nThis shows that M 0 is a decider for PSUBSETDFA DFA. Therefore, PSUBSETDFA is\ndecidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fALLBUTONETM\n\nQuestion\nProve that\nALLBUTONETM = {hM i|M is a TM that accepts all but one string}\n\nis undecidable. Note that if TM M accepts all but one string,\nL(M ) = Σ∗ − {w} for a string w.\nRecall that there are two methods:\n1\n2\n\nShow that if ALLBUTONETM is decidable, ATM is decidable\nUse mapping reducibility to show that\nATM ≤m ALLBUTONETM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fALLBUTONETM is Undecidable (Method 1)\n\nAssume that ALLBUTONETM is decidable. Since ALLBUTONETM is decidable, there\nexists a TM R that decides ALLBUTONETM . Construct a TM S to decide ATM as\nfollows:\nS =“On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct a TM M 0 as follows:\nM 0 =“On input x:\n1\n2\n3\n\nIf x == ε, reject.\nRun M on input w.\nIf M accepts w, accept. If M rejects w, reject.”\n\n2\n\nRun TM R on input hM 0 i.\n\n3\n\nIf R accepts hM 0 i, accept. If R rejects hM 0 i, reject.”\n\nNext, we need to show that the above TM S is a decider for ATM .\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fALLBUTONETM is Undecidable (Method 1)\nTo show that TM M 0 is a decider for ALLBUTONETM , we need to perform the\nfollowing:\nIf hM, wi ∈ ATM , S will accept hM, wi.\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M is a TM that accepts w.\nSince M accepts w, from the description of TM M 0 , L(M 0 ) = Σ∗ − {ε}. Since\nL(M 0 ) = Σ∗ − {ε}, hM 0 i ∈ ALLBUTONETM . Since R is a decider for\nALLBUTONETM , by running R on input ALLBUTONETM , R will accept hM 0 i.\nSince R accepts hM 0 i, S accepts hM, wi.\nIf hM, wi 6∈ ATM , S will reject hM, wi.\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M is a TM that does not\naccept w. Since M does not accept w, from the description of TM M 0 ,\nL(M 0 ) = ∅. Since L(M 0 ) = ∅, L(M 0 ) 6= Σ∗ − {w} for a string w. In other\nwords, hM 0 i 6∈ ALLBUTONETM . Since R is a decider for ALLBUTONETM , by\nrunning R on input ALLBUTONETM , R will reject hM 0 i. Since R rejects hM 0 i,\nS rejects hM, wi.\nThis shows that S is a decider for ATM . In other words ATM is decidable. This\ncontradicts with the fact that ATM is undecidable. Therefore, ALLBUTONETM is\nundecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fALLBUTONETM is Undecidable (Method 2)\nNeed to show that ATM ≤m ALLBUTONETM . Construct a TM F as follows:\nF =“On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct a TM M 0 as follows:\nM 0 =“On input x:\n1\n2\n3\n\n2\n\nIf x == ε, reject.\nRun M on input w.\nIf M accepts w, accept. If M rejects w, reject.”\n\nOutput hM 0 i.\n\nNext, we need to show that the above TM F is a reduction from ATM to\nALLBUTONETM . In other word, we have to show that it satisfies:\nhM, wi ∈ ATM ↔ F (hM, wi) ∈ ALLBUTONETM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fALLBUTONETM is Undecidable (Method 2)\n\nTo show that TM F is a reduction from ATM to ALLBUTONETM , we need to perform\nthe following:\nIf hM, wi ∈ ATM , F (hM, wi) ∈ ALLBUTONETM .\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M is a TM that accepts w.\nSince M accepts w, from the description of TM M 0 , L(M 0 ) = Σ∗ − {ε}. Since\nL(M 0 ) = Σ∗ − {ε}, hM 0 i ∈ ALLBUTONETM . Since F (hM, wi) = hM 0 i,\nF (hM, wi) ∈ ALLBUTONETM .\nIf hM, wi 6∈ ATM , F (hM, wi) 6∈ ALLBUTONETM .\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M is a TM that does not\naccept w. Since M does not accept w, from the description of TM M 0 ,\nL(M 0 ) = ∅. Since L(M 0 ) = ∅, L(M 0 ) 6= Σ∗ − {w} for a string w. In other\nwords, hM 0 i 6∈ ALLBUTONETM . Since F (hM, wi) = hM 0 i,\nF (hM, wi) 6∈ ALLBUTONETM .\nThis shows that F is a reduction from ATM to ALLBUTONETM . Since ATM is\nundecidable, ALLBUTONETM is undecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fNOTPRIME\n\nQuestion\nConsider the following language\nNOTPRIME = {hni|n is a natural number and n is not a prime number}\n\nPerform the following:\n1\n\nConstruct a verifier for the language NOTPRIME\n\n2\n\nConstruct an NTM that decides NOTPRIME\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fA Verifier of NOTPRIME\nGiven a language A, a verifier V of the language A is a TM\nsatisfying\nA = {w | V accepts hw, ci for some c}\nFrom the above description\nGiven a string w ∈ A, V must accept hw, ci for some c\nGiven a string w 6∈ A, V must reject hw, ci for all c\n\nA verifier for NOTPRIME must satisfies\nNOTPRIME = {hni | V accepts hn, ci for some c}\nAn example of a verifier for the language NOTPRIME:\nV =“On input hn, ci where n and c are natural numbers:\n1\n2\n\nIf c == 1 or c ≥ n, reject.\nIf n mod c == 0, accept.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\fAn NTM that decides NOTPRIME\n\nThink about how brute force algorithm should work\nGenerate all possible strings between 2 and n − 1 (inclusive)\nIf n is divisible by one of them, n is not a prime number\n\nAn example of an NTM that decides the language\nNOTPRIME:\nN =“On input hni where n is a natural numbers:\n1\n\n2\n\nSelect a number c where c is nondeterministically selected\nbetween 2 and n − 1 (inclusive)\nIf n mod c == 0, accept.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReview\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":266,"segment": "self_training_2", "course": "cs1502", "lec": "lec16_decidability_04","text":"Decidability 04\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fThe Set of Real Numbers is Uncountable\nA real number R is a number with decimal representation\nπ = 3.14159265 . . .\n345 = 345.000 . . .\n1.23 = 1.23000 . . .\n\nTo show that a set R is not countable, we generally use proof\nby contradiction\nAssume that R is countable\nR is countable, there is a correspondence between N and R\nSince there is a correspondence between N and R, there are\ninfinitely many correspondence between N and R\nThis allows us to fill the right column of the correspondence\ntable any way we want (no duplicate):\nn∈N\n1\n2\n3\n..\n.\n\nf (n) ∈ R\n?\n?\n?\n..\n.\n\nn∈N\n1\n2\n3\n..\n.\n\nf (n) ∈ R\n1.000 . . .\n2.000 . . .\n3.000 . . .\n..\n.\n\nn∈N\n1\n2\n3\n..\n.\n\nAlways results in a valid correspondence\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\nf (n) ∈ R\n1.414 . . .\n3.141 . . .\n2.222 . . .\n..\n.\n\n\fDiagonalization\nConsider an assignment of the function f : N → R as shown\nbelow:\nn∈N\nf (n) ∈ R\n1\n3.141592 . . .\n2\n55.555555 . . .\n3\n0.123456 . . .\n4\n0.500000 . . .\n5\n1.414213 . . .\n..\n..\n.\n.\nThis is one of infinitely many valid correspondences\nWe need to find an element x ∈ R where f (n) 6= x for any\nn∈N\nIf we can find such x, the above table is not a valid\ncorrespondence (not onto)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fDiagonalization\nConsider an assignment of the function f : N → R as shown\nbelow:\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ R\n3. 1 41592 . . .\n55.5 5 5555 . . .\n0.12 3 456 . . .\n0.500 0 00 . . .\n1.4142 1 3 . . .\n..\n.\n\nFrom the above table:\nWe choose the 1st digit after the decimal point of the real\nnumber that corresponds with 1\nWe choose the 2nd digit after the decimal point of the real\nnumber that corresponds with 2\nWe choose the 3rd digit after the decimal point of the real\nnumber that corresponds with 3\nAnd so on\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fDiagonalization\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ R\n3. 1 41592 . . .\n55.5 5 5555 . . .\n0.12 3 456 . . .\n0.500 0 00 . . .\n1.4142 1 3 . . .\n..\n.\n\nx = 0.26412 . . . ∈ R\n\nConstructing x ∈ R starting from the first digit after the\ndecimal point:\nThe 1st digit after the decimal point of f (1) is 1\nThe 2nd digit after the decimal point of f (2) is 5\nThe 3rd digit after the decimal point of f (3) is 3\nThe 4th digit after the decimal point of f (4) is 0\nThe 5th digit after the decimal point of f (4) is 0\nAnd so on\n\nObviously, x is a real number (x ∈ R)\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\npick 2\npick 6\npick 4\npick 1\npick 2\n\n\fDiagonalization\n\nLet’s recap how we constructed x:\nIf the nth digit after the decimal point of f (n) is y, pick a\nnumber that is not equal to y\nWe use that number (not equal to y) as the nth digit after the\ndecimal point of x\n\nClaim: No n ∈ N such that f (n) = x\nBecause the nth digit after the decimal point of x will be\ndifferent that the nth digit after the decimal point of f (n)\n\nThe correspondence is not onto\nNot a valid correspondence\nR is uncountable\n\nThis method of constructing x is called diagonalization\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fΣ∗ is Countable\nLet Σ = {0, 1}\nHow to show that Σ∗ is countable\nWe need to show a correspondence between N and Σ∗\nLet’s list element of Σ∗ from the shortest one\n\nn∈N\n1\n2\n3\n4\n5\n6\n7\n8\n..\n.\n\nf (n) ∈ Σ∗\nε\n0\n1\n00\n01\n10\n11\n000\n..\n.\n\nBy listing from the shortest, all\nstrings will be on the right column\nThus, one-to-one and onto (a valid\ncorrespondence)\nΣ∗ is countable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fΣ∗ is Countable\nAnother way is to explicitly define the function f : N → Σ∗\nNot necessary\nThe table from the previous slide is good enough\n\nExample, let Σ = {0, 1}\nf (1) = ε\nf (n) = binary representation of n − 2blog2 (n)c with length\nblog2 (n)c\nN log2 n blog2 nc 2blog2 (n)c n − 2blog2 (n)c Σ∗\n1\nε\n2\n1\n1\n2\n0\n0\n3\n1.585\n1\n2\n1\n1\n4\n2\n2\n4\n0\n00\n5\n2.322\n2\n4\n1\n01\n6\n2.585\n2\n4\n2\n10\n7\n2.807\n2\n4\n3\n11\n8\n3\n3\n8\n0\n000\n9\n3.169\n3\n8\n1\n001\n\nΣ∗ is countable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fInfinite Binary Sequence is Uncountable\nLet B be a set of infinite binary sequence:\nB = {x1 x2 . . . x∞ | xi = 0 or xi = 1}.\nShow that B is uncountable\nAgain, we need to use proof by contradiction and\ndiagonalization:\nAssume that B is countable\nThus, there exists a correspondence f : N → B\nSince there is one, there are infinitely many correspondences\nThis is one of infinite many valid correspondences (based on\nthe assumption that B is countable):\nn∈N\nf (n) ∈ B\n1\n0100101 . . .\n2\n1101011 . . .\n3\n1010101 . . .\n4\n0010011 . . .\n5\n0001000 . . .\n..\n..\n.\n.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fDiagonalization Example for Infinite Binary Sequence\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ B\n0 100101 . . .\n1 1 01011 . . .\n10 1 0101 . . .\n001 0 011 . . .\n0001 0 00 . . .\n..\n.\n\nFrom the above correspondence:\nPick the 1st bit (from left) of f (1)\nPick the 2nd bit (from left) of f (2)\nPick the 3rd bit (from left) of f (3)\nPick the 4th bit (from left) of f (4)\n..\n.\nPick the nth bit (from left) of f (n)\n..\n.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fDiagonalization Example for Infinite Binary Sequence\nn∈N\n1\n2\n3\n4\n5\n..\n.\n\nf (n) ∈ B\n0 100101 . . .\n1 1 01011 . . .\n10 1 0101 . . .\n001 0 011 . . .\n0001 0 00 . . .\n..\n.\n\nx = 10011 . . . ∈ B\n\nConstruct x ∈ B starting from the first bit (from left)\nThe 1st bit (from left) of f (1) is 0\nThe 2nd bit (from left) of f (2) is 1\nThe 3rd bit (from left) of f (3) is 1\nThe 4th bit (from left) of f (4) is 0\nThe 5th bit (from left) of f (4) is 0\n..\n.\n\npick 1\npick 0\npick 0\npick 1\npick 1\n\nThe nth bit (from left) of f (n) is y\n..\n.\n\npick ¬y\n\nObviously, x is an infinite binary sequence\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fDiagonalization Example for Infinite Binary Sequence\n\nWhen we construct x:\nIf the nth bit (from left) of f (n) is y, pick a number that is\nnot equal to y\nIf it is 0, pick 1\nIf it is 1, pick 0\n\nWe use that number as the nth bit (from left) of x\n\nClaim: No n ∈ N such that f (n) = x\nBecause the nth digit bit (from left) of x will be different that\nthe nth bit (from left) of f (n)\n\nThe correspondence is not onto\nNot a valid correspondence\nThe set of all infinite binary sequences (B) is uncountable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fThe Set of Turing Machines in Countable\n\nEach Turing machine has its own encoding hM i\nA string representation of TM M\nA string is a finite sequence of symbols over a Σ\n\nhM i for every machine has a finite length\nSimilar to Σ∗ , we can list all Turing machines in their\nencoding form starting from the shortest length\nTherefore, the set of Turing Machines is countable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fSet of All Languages is Uncountable\n\nDiagonalization works well when all elements are infinite\nWe need to pick the nth of something\nThe nth digit after the decimal point\nThe nth bit (from left)\n\nIf all elements are infinite long, it is guaranteed to have the nth\nof something\n\nBut some languages are finite and some are infinite\nDiagonalization does not work well for proving that the set of\nall languages is uncountable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fSame Size, Countable, and Uncountable\n\nThe following are what we have so far:\nThe set of all Turing machine is countable\nThe set of all infinite binary sequences is uncountable\n\nWe know that an uncountable set is larger than a countable\nset\nWe need to show that the set of all languages has the same\nsize as the set of all infinite binary sequences\nTo show that two sets have the same size, we need to show a\ncorrespondence\nWe need a correspondence between the set of all languages\nand the set of all infinite binary sequences\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fThe Set of All Languages is Uncountable\nWe can list the set of all strings over Σ = {0, 1} starting from\nthe shortest string as follows:\n{ε, 0, 1, 00, 01, 10, 11, 000, 001, . . . }\nGiven a language (either finite or infinite), we can also list the\nlanguage starting from the shortest string\nThe set of all strings that start with a 1\n{1, 10, 11, 100, 101, 110, 111, 1000, . . . }\nThe set of all strings of length 2\n{00, 01, 10, 11}\n\nNote that we use dictionary order for those with the same\nlength\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\fThe Set of All Languages is Uncountable\n\nGiven a language, we can always list their strings to align with\nstrings in Σ∗\nThe set of all strings of length 2\nΣ∗ = { ε, 0, 1, 00, 01, 10,\nA ={\n00, 01, 10,\nThe set of all strings that start with a 1\nΣ∗ = { ε, 0, 1, 00, 01, 10,\nA ={\n1,\n10,\nThe set of all strings that start with a 0\nΣ∗ = { ε, 0, 1, 00, 01, 10,\nA ={\n0,\n00, 01,\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n11,\n11\n\n000,\n\n001,\n\n...}\n}\n\n11,\n11,\n\n000,\n\n001,\n\n...}\n...}\n\n11,\n\n000,\n000,\n\n001,\n001,\n\n...}\n...}\n\n\fThe Set of All Languages is Uncountable\nWhen we align them as in previous slide, we can convert the\nalignment into an infinite binary sequence XA :\nΣ∗ = { ε, 0, 1, 00, 01, 10, 11, 000, 001,\nA ={\n0,\n00, 01,\n000, 001,\nXA =\n0 1 0\n1\n1\n0\n0\n1\n1\nXA is the infinite binary sequence that corresponds with the\nlanguage A\nXA is a member of the set of all infinite binary sequences B\n\nThe above shows that for every language A, it corresponds to\nXA ∈ B\nThe set of all languages has the same size as the set of all\ninfinite binary sequences\nSince the set of all infinite binary sequences is uncountable,\nthe set of all languages is uncountable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n...}\n...}\n...\n\n\fSome Languages are not Turing-recognzable\n\nRecall that\nthe set of all Turing machines is countable\nthe set of all languages is uncountable\n\nThe set of all languages is larger than the set of all Turing\nmachine\nNote that one Turing machine only recognizes one language\nCorollary 4.18\nSome languages are not Turing-recognizable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nDecidability 04\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":267,"segment": "unlabeled", "course": "cs1502", "lec": "lec23_reducibility_06","text":"Reducibility 06\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fReductions\nRecall the concept of reducibility:\nA problem A is reducible to a problem B if the solution to the\nproblem B can be used to solve the problem A\n\nTo show that a language A is undecidable, we show that ATM\n(or other undecidable language) is reducible to A\nATM is reducible to HALT TM\nWe want to know whether hM, wi ∈ ATM\nTurn hM, wi to hM 0 , w0 i (in our case w0 = ε)\nCheck whether hM 0 , w0 i ∈ HALT TM\nhM, wi ∈ ATM ↔ hM 0 , w0 i ∈ HALT TM\n\nATM is reducible to ETM\nWe want to know whether hM, wi ∈ ATM\nTurn hM, wi to hM 0 i\nCheck whether hM 0 i ∈ ETM\nhM, wi ∈ ATM ↔ hM 0 i 6∈ ETM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fReductions\nATM is reducible to REGULAR TM\nWe want to know whether hM, wi ∈ ATM\nTurn hM, wi to hM 0 i\nCheck whether hM 0 i ∈ REGULAR TM\nhM, wi ∈ ATM ↔ hM 0 i ∈ REGULAR TM\nhM, wi ∈ ATM ↔ hM 0 i 6∈ REGULAR TM\n\nATM is reducible to EQ TM\nWe want to know whether hM, wi ∈ ATM\nTurn hM, wi to hM1 , M2 i\nCheck whether hM1 , M2 i ∈ EQ TM\nhM, wi ∈ ATM ↔ hM1 , M2 i ∈ EQ TM\n\nETM is reducible to EQ TM\nWe want to know whether hM i ∈ ETM\nTurn hM i to hM, ME i\nCheck whether hM, ME i ∈ EQ TM\nhM i ∈ ETM ↔ hM, ME i ∈ EQ TM\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fReductions via Computation Histories\n\nSo for we construct TMs to show that ATM is reducible to\ncertain languages\nAnother method is Reduction via Computation Histories\nGenerally use when the problem to be shown undecidable\ninvolves testing for the existence of something\nGiven a polynomial p, does it has an integral root?\nGiven a set of dominoes, does it has a match?\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fDefinitions\n\nRecall definitions about configuration:\nSuppose the start state of TM M is q0 , the start\nconfiguration of M on input w is q0 w\nAn accepting configuration is a configuration uqaccept v for\nstrings u and v\nA rejecting configuration is a configuration uqreject v for\nstrings u and v\nA configuration Ci legally go to Cj according to the\ntransitions of the TM M\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fReductions via Computation Histories\nDefinition 5.5\nLet M be a Turing machine and w an input string. An accepting\ncomputation history for M on w is a sequence of configurations,\nC1 , C2 , . . . Cl where C1 is the start configuration of M on w, Cl is\nan accepting configuration of M , and each Ci legally follows from\nCi−1 according to the rules of M . A rejecting computation\nhistory for M on w is defined similarly, except that Cl is a\nrejecting configuration.\nNote that M must halt on input w. Otherwise, accepting or\nrejection computation history does not exists\nFor simplicity, we only consider deterministic Turing machine\nIf M halts on w, exactly one computation history exists\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fReductions via Computation Histories\nAgain, to show that a language A is undecidable, we show\nthat ATM is reducible to A\nGiven a TM M and a string w, there is no algorithm to check\nwhether hM, wi ∈ ATM because ATM is undecidable\nBut we know that\nIf M accepts w, hM, wi ∈ ATM\nIf M does not accept w, hM, wi 6∈ ATM\n\nUsing the concept of computation histories:\nIf the accepting computation history of M on input w exists\nhM, wi ∈ ATM\nM accepts w\nIf the accepting computation history of M on input w does\nM does not accept w\nhM, wi 6∈ ATM\nnot exists\n\nLet’s try to use reduction via computation histories with a\nvariation of a TM called Linear Bounded Automaton (LBA)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\nDefinition 5.6\nA linear bounded automaton is a restricted type of Turing\nmachine wherein the tape head isn’t permitted to move off the\nportion of the tape containing the input. If the machine tries to\nmove its head off either end of the input, the head stays where it is.\ncontrol\n\na\n\nb\n\na\n\nb\n\na\n\nn\n\nWith an input of length n, the amount of memory available is\nlinear in n.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\n\nProblem: Given an LBA M and a string w, is there an\nalgorithm to determine whether M accepts w?\nFormally,\nALBA = {hM, wi | M is an LBA that accepts w}\nIs ALBA decidable?\nThis sounds like ATM\nATM = {hM, wi | M is a TM that accepts w}\nEven though ATM is undecidable, ALBA is actually decidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\n\nRecall a configuration uqv\nuv is the content of the tape\n\nRecall that very TM has an infinite long tape\nTherefore, there are infinite number of distinct strings uv\nBut the tape of an LBA is finite\nThe length of its tape on input string w is |w|\nThe number of states and |Γ| of an LBA are also finite\nBecause of these, the set of all distinct configurations that an\nLBA can have on an input string w is finite\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\nConsider an LBA with the following specification:\nThe number of states is q\n|Γ| = g (number of symbols in its tape alphabets)\nThe length of the tape is n\n\nFrom the above LBA:\nFor each square on the tape, there can be one of g symbols\nThere are the total of g n unique tape contents\n\nThere are n possible positions of the tape head\nFor each tape head position, it can be in one of q states\n\nThus, there are exactly qng n distinct configurations\nBecause of this, we can detect when an LBA is entering a\nloop by checking whether sequence of configurations when\nrunning an LBA and a string w contains a duplicate\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\nTo detect an infinite loop in an LBA, consider an LBA M\nwhere its series of configuration while processing w is as\nfollows:\nC1 , C2 , C3 , C4 , C5 , C6 , C7 , C8 , C9 , C10 , C11 , . . .\nWhat if the configuration C3 is the same as C9 ?\nThe content of the tape are exactly the same\nThe current state of M are the same\nThe tape head are at the same location\n\nThus\nC10 will be the same as C4\nC11 will be the same as C5\nand so on\n\nHowever, this method requires us to keep track of sequence of\nconfigurations\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\n\nRecall that if we run an LBA on input w for 1 step, we\nalready sees two configurations\nThe start configuration\nThe next configuration that legally goes from the start\nconfiguration\n\nThus, if we run an LBA on input w for k steps, we already see\nk + 1 configurations\nRecall that the total number of distinct configurations is qng n ,\nwe can simply run the LBA for qng n steps or until it halts\nIf it does not halt after qng n steps, we already see qng n + 1\nconfigurations\nBut there are only qng n distinct configurations\nThus, at least two of them must be the same (loop indefinitely)\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fLinear Bounded Automaton\n\nTheorem 5.9\nALBA = {hM, wi | M is an LBA that accepts string w} is\ndecidable.\nConstruct a TM L that decides ALBA as follows:\nL = “On input hM, wi, where M is an LBA and w is a string:\n1\n2\n\nSimulate M on input w for qng n steps or until it halts.\nIf M has halted, accept if it has accepted and reject if it has\nrejected. if it has not halted, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\n\nNow, we just verify that ALBA is decidable\nBut what about the problem of determining whether an LBA\nM accepts no strings (L(M ) = ∅)\nFormally,\nELBA = {hM i | M is an LBA and L(M ) = ∅}\nIt turns out that this problem is unsolvable\nIn other words, ELBA is undecidable\nLet’s try to prove it using the same method that we used to\nprove that ETM is undecidable to try to prove that ELBA is\nundecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\nAssume that ELBA . Since ELBA is decidable, there exists a TM R that decides\nELBA . Construct a TM S (to decide ATM ) as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1 Construct an LBA M 0 as follows:\nM 0 = “On input x:\nRun M on input w.\nIf M accepts w, accept; otherwise, reject.”\n2 Run R (a decider for ELBA ) on input hM 0 i.\n3 If R accepts hM 0 i, reject. If R rejects hM 0 i, accept.”\nNext, we need to prove that the TM S above, decides ATM . . .\n1\n\n2\n\nDid you see a problem in the above prove?\nM 0 (constructed by S) must be an LBA\nAn LBA has a finite long tape\nTo run TM M on input w, the LBA M 0 may need infinite\nlong tape\nYou cannot construct an LBA that can always run a TM M\non an input w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\nIn this case, we need to show that ATM is reducible to ELBA\nusing the computation history method\nThis method will also give you the same conclusion:\nIf ELBA is decidable, ATM is decidable\n\nThe idea is to construct an LBA B that accepts exactly one\nstring\nThe string will be a string representation of the accepting\ncomputation history for M on w\nWhy this work?\nIf hM, wi ∈ ATM\nM accepts w\nthe accepting\nLBA B accepts\ncomputation history for M on w exists\nexactly one string\nL(B) 6= ∅\nhBi 6∈ ELBA\nM does not accept w\nthe accepting\nIf hM, wi 6∈ ATM\nLBA B\ncomputation history for M on w does not exists\nL(B) = ∅\nhBi ∈ ELBA\naccepts no strings\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\n\nBut can we construct an LBA that accepts exactly one string\nwhich is the accepting computation history for M on w\nwithout requiring an infinite amount of tape?\nRecall that an LBA cannot always runs M on input w because\nof its finite long tape\nSo, can an LBA checks whether a given string is the accepting\ncomputation history for M on w without actually run M on\ninput w?\nIt turns out, we can\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\nAn accepting computation history for M on w is a series of\nconfigurations C1 , C2 , . . . Cl where\nC1 is the start configuration of M on input w\nCl is an accepting configuration\nCi yields Ci+1 according to M\n\nWe can represents an accepting computation history by a\nsingle string with the configurations separated from each\nother by the # symbol\n#\n\n#\nC1\n\n#\nC2\n\n#\nC3\n\n#\n\n#\nCl\n\nFor example an accepting computation history of TM M2 in\nExample 3.7 on input 0 can be represented as a single string\nas follows:\n#q1 0#tq2 #ttqaccept #\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\nWith an input string x to the LBA B, the machine B must\ncheck the following conditions before accepting x\n1\n2\n3\n4\n\nx is in the form of #C1 #C2 # . . . #Cl # for a number l\nC1 is the start configuration for M on w\nCi+1 legally follows from Ci\nCl is an accepting configuration for M\n\nGiven the definition of a TM M and a input string w:\nWe know the start state of M\nThus, we know the start configuration\n\nWe know the transition function of M\nThus, we can check whether Ci+1 legally follows from Ci\n\nAccepting configuration is just uqaccept v for strings u and v\n\nThis show that there is an algorithm to check whether an\ninput string x is an accepting computation history for M on w\nwithout actually running M on input w\nTherefore, the LBA B can be constructed\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\n\nTheorem 5.10\nELBA = {hM i | M is an LBA and L(M ) = ∅} is undecidable.\nFor the sake of contradiction, assume that ELBA is decidable\nThere exists a TM R decides ELBA\nConstruct a TM S decides ATM as follows:\nS = “On input hM, wi, where M is a TM and w is a string:\n1\n2\n3\n\nConstruct LBA B from M and w as described in the proof idea\nRun R on input hBi\nIf R rejects hBi, accept; if R accepts hBi, reject.”\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\fELBA is Undecidable\nSketch of the proofs:\nIf hM, wi ∈ ATM :\nM accepts w\nThe accepting computation history for M on w exists\nB accepts exactly one string\nL(B) 6= ∅\nR rejects hBi\nS accepts hM, wi\n\nIf hM, wi 6∈ ATM :\nM does not accept w\nThe accepting computation history for M on w does not exists\nB accepts no strings\nL(B) = ∅\nR accepts hBi\nS rejects hM, wi\n\nThis show that S is a decider for ATM\nIn other words, ATM is decidable — contradiction\nTherefore, ELBA is undecidable\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 06\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":268,"segment": "self_training_2", "course": "cs1502", "lec": "lec29_time_complexity_04","text":"Time Complexity 04\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fNP-COMPLETENESS\n\nNP-Complete is a class of problems discovered by Cook and\nLevin\nComplexity of these problems are closely related\nIf a polynomial time algorithm exists for a problem in this\nclass, problems of entire class can be solved in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fPolynomial Time Reducibility\nDefinition 7.28\nA function f : Σ∗ → Σ∗ is a polynomial time computable\nfunction if some polynomial time Turing machine M exists that\nhalts with just f (w) on its tape, when started on any input w.\nDefinition 7.29\nLanguage A is polynomial time mapping reducible, or simply\npolynomial time reducible, to language B, written A ≤p B, if a\npolynomial time computable function f : Σ∗ → Σ∗ exists, where\nfor every w,\nw ∈ A ↔ f (w) ∈ B.\nThe function f is called the polynomial time reduction of A to\nB.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fPolynomial Time Reducibility\n\nRecall the property from previous slide:\nw ∈ A ↔ f (w) ∈ B\nThis is exactly the same as mapping reducibility property\nRecall that if A ≤m B and B is decidable, A is decidable\nA ≤p B gives you exactly the same as A ≤m B with one\nextra property that the reduction f runs in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fPolynomial Time Reducibility\n\nTheorem 7.31\nIf A ≤p B and B ∈ P , then A ∈ P .\nIf you can convert an instance of a problem A into an\ninstance of a problem B in polynomial time\nIf the problem B can be solved in polynomial time\nThe problem A can be solved in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fPolynomial Time Reducibility\nTheorem 7.31\nIf A ≤p B and B ∈ P , then A ∈ P .\nProof:\nAssume that A ≤p B\nThere exists a polynomial time reduction f satisfying:\nw ∈ A ↔ f (w) ∈ B\n\nAssume that B ∈ P\nThere exists a TM MB that decides B\nMB runs in polynomial time\n\nWe need to show that we can decide A in polynomial time\nNeed a TM MA that decides A in polynomial time\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fPolynomial Time Reducibility\n\nThis TM MA decides A:\nMA = “On input w:\n1\n\n2\n3\n\nCompute f (w) where f is a polynomial time reduction from A\nto B.\nRun MB (a decider of B) on f (w).\nIf MB accepts f (w), accept. If MB rejects f (w), reject.”\n\nProve that MA is a decider for A:\nw∈A\nw 6∈ A\n\nf (w) ∈ B\nf (w) 6∈ B\n\nMB accepts f (w)\nMB rejects f (w)\n\nMA accepts w\nMA rejects w\n\nSince f runs in polynomial time and MB can decide B in\npolynomial time, MA runs in polynomial time\nSince MA decides A in polynomial time, A ∈ P\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fSatisfiability Problem\n\nA formula φ can be construed from boolean variables and\nboolean operators (∧, ∨, and ¬)\nSometimes we use x instead of ¬x for a boolean variable x\nFor simplicity, this textbook uses 1 for true and 0 for false\n\nFor example, φ = (x ∧ y) ∨ (x ∧ z)\nA formula is satisfiable if there exists an assignment of 0s\nand 1s to the variables which makes the formula evaluate to 1\nIn the above example, φ is 1 when x = 0, y = 1, and z = 0.\n\nThe satisfiability problem is defined formally as follows:\nSAT = {hφi | φ is a satisfiable Boolean formula}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fConjunctive Normal Form (CNF)\nWe will use a special form of boolean formula called\nConjunctive Normal Form (cnf)\nA literal is a Boolean variable or a negation of a boolean\nvariable, as in x or x\nA clause is several literals connected with ∨s\n(x1 ∨ x2 ∨ x3 ∨ x4 )\nA cnf-formula comprises several clauses connected with ∧s\n(x1 ∨ x2 ∨ x3 ∨ x4 ) ∧ (x3 ∨ x5 ∨ x6 ) ∧ (x3 ∨ x6 )\nA 3cnf-formula is a cnf-formula where all the clauses have\nexactly three literals, as in\n(x1 ∨ x2 ∨ x3 ) ∧ (x3 ∨ x5 ∨ x6 ) ∧ (x3 ∨ x6 ∨ x4 ) ∧ (x4 ∨ x5 ∨ x6 )\nLet 3SAT = {hφi | φ is a satisfiable 3cnf-formula}\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f3SAT ≤p CLIQUE\n\nIs 3SAT ≤p CLIQUE ?\nWe need to create a polynomial time reduction\nf : Σ∗ → Σ∗ such that\nw ∈ 3SAT ↔ f (w) ∈ CLIQUE\nRecall 3SAT and CLIQUE :\n3SAT = {hφi | φ is a satisfiable 3cnf-formula}\nCLIQUE = {hG, ki | G is an undirected graph with a k-clique}\n\nThese are two totally different problems\nWe need to convert hφi to hG, ki such that φ is satisfiable if\nand only if graph G contains k-clique for some k\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f3SAT ≤p CLIQUE\nA 3cnf-formula is of the form φ = c1 ∧ c2 ∧ . . . cn for some\nn≥1\nφ is evaluated to 1 if ci = 1 for every 1 ≤ i ≤ n\nEvery clause ci in φ depends on other clauses cj in φ to be 1\nto make φ evaluated to 1\nEach clause c in a 3cnf-formula is of the form\nc = (xp ∨ xq ∨ xr )\nfor literals xp , xq , and xr\nThe clause c is evaluated to 1 if one of its literal is 1\nIf a literal in a clause is 1, for φ to be evaluated to 1, it\ndepends on at least one literal in other clauses to be 1\nNote that it cannot depend on its negation literals\nIt does not depend on the other two literals in its clause since\nits clause is already evaluated to 1\n\nThis create relations among n literals (one from each clause)\nwhich can be represented using edges in an indirected graph\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f3SAT ≤p CLIQUE\nLet φ be a formula with k clauses as\nφ = (a1 ∨ b1 ∨ c1 ) ∧ (a2 ∨ b2 ∨ c2 ) ∧ · · · ∧ (ak ∨ bk ∨ ck )\nNodes in G are organized in k groups of tree nodes each call\ntriples\nEach triple corresponds to one of the clauses in φ\nEach node in a triple corresponds to a literal in its associated\nclause\nEdges connect every two nodes in G except the following:\nNo edge among nodes in the same triple\nNo edge is present between two nodes with contradictory\nlabels, as in x2 and x2\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f3SAT ≤p CLIQUE\nx1\n\nx2\n\nx2\n\nx1\n\nx1\n\nx1\n\nx2\n\nx2\n\nx2\n\nφ = (x1 ∨ x1 ∨ x2 ) ∧ (x1 ∨ x2 ∨ x2 ) ∧ (x1 ∨ x2 ∨ x2 )\n3SAT ≤p CLIQUE\nIf a boolean 3cnf formular φ with k clauses is satisfiable, the\ngraph converted from φ will contain k clique\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f3SAT ≤p CLIQUE\nIf φ is satisfiable, there exists an assignment that makes φ = 1\nFor each clause, select exactly one true literal based on the\nassignment\nIf there are k clauses, selected literals form k-clique\n\nIf graph G has a k-clique\nEach node in k-clique corresponds to a node (literal) in a triple\n(clause)\nAssign true to those literals\n\nRecall the theorem 7.31:\nIf A ≤p B and B ∈ P , then A ∈ P\nIf we can solve CLIQUE in polynomial time, we can solve\n3SAT in polynomial time by simply convert hφi to hG, ki (in\npolynomial time) and then solve it\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fNP-Completeness\n\nRecall that NP-Complete is a class of problems that are\nclosely related\nIf you can solve one problem in this class in polynomial time,\nall problems in this class can be solved in polynomial time\nDefinition 7.34\nA language B is NP-Complete if it satisfies two conditions:\n1\n\nB is in NP , and\n\n2\n\nEvery language A in NP is polynomial time reducible to B.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fNP-Completeness\n\nIn other words, to show that a language B is NP-Complete\nwe need to show the following:\n1\n\nShow that B is in NP which can be done by one of the\nfollowing:\n1\n2\n\n2\n\nShow a polynomial time verifier V for B, or\nShow a NTM N that decide B in polynomial time\n\nShow that every language A in NP is polynomial time\nreducible to B\nShow an polynomial time reduction f : Σ∗ → Σ∗ satisfying\nw ∈ A ↔ f (w) ∈ B\nfor every language A ∈ NP\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fSAT is NP-Complete\nSo, to show that SAT is NP-Complete, we need to show the\nfollowing:\n1\n\nSAT ∈ NP\nSimply construct a NTM that generates all possible\nassignments\nIf one of them evaluated to 1, accept\n\n2\n\nShow that a every language A in NP is polynomial time\nreducible to SAT\nUnfortunately, there are a lot of languages in NP\nBut if we have an NP-Complete language B\nB is in NP\nEvery language A ∈ NP , A ≤p B\nIf we can show that B ≤p SAT , every language A ∈ NP ,\nA ≤p SAT\nBut we do not have an NP-Complete problem yet\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fSAT is NP-Complete\n\nSuppose a language A is in NP\nThere exists a NTM M that decides A in polynomial time\n\nNote that we do not specify what is the language A and what\nis the description of NTM M\nThe language A can be any language that is in NP and M is\nan NTM that decides A in polynomial time\n\nWhat we need is to show that A ≤p SAT?\nSince we do not know what is the language A, we cannot\nsimply use the description of A\nEven though we do not have the description of NTM M but\nwe know the following:\nM is a nondeterministic Turing machine\nNTM M is a decider for the language A\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fSAT is NP-Complete\nAbout NTM M that decides A\nw∈A\nthe accepting computation\nIf M accepts w\nhistory for M on input w exists\nIf M rejects w\nw 6∈ A\nthe accepting computation history\nfor M on input w does not exists\n\nThis is an NTM, there can be multiple computation histories\nWe need to use reduction via computation histories\nConvert computation histories of M on input w to a formula φ\nsuch that\nIf the accepting computation history for M on input w exists,\nφ is satisfiable (φ ∈ SAT)\nIf the accepting computation history for M on input w does\nnot exists, φ is not satisfiable (φ 6∈ SAT)\n\nCook-Levin Theorem shows the method describe above and\nconclude that SAT is NP-Complete.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f3SAT is NP-Complete\n\nSince we already know that SAT is NP-Complete, to show\nthat 3SAT is NP-Complete, we need to show the following:\n1\n\n3SAT ∈ NP :\nConstruct an NTM that generates all possible assignments\nIf one of them evaluate to 1, accept\n\n2\n\nSAT ≤p 3SAT :\nConvert the formula φ into a 3cnf f (φ) satisfying\nφ ∈ SAT ↔ f (φ) ∈ 3SAT\nwhere f is a polynomial time reduction from SAT to 3SAT\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fFrom SAT to 3SAT\n\nIf a clause has less than three literal, add one or two literals\nfrom the same clause\nx1\n(x1 ∨ x1 ∨ x1 )\n(x1 ∨ x2 )\n(x1 ∨ x1 ∨ x2 )\n(x1 ∨ x2 )\n(x1 ∨ x2 ∨ x2 )\nThese results are logically equivalent with their original clauses\n\nIf a clause has more than tree literals, split it into a number of\nclauses of three literals each.\n(x1 ∨ x2 ∨ x3 ∨ x4 )\n\n(x1 ∨ x2 ∨ z) ∧ (z ∨ x3 ∨ x4 )\n\nThis is not logically equivalent\nIt is equivalent in terms of satisfiability\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\fAdditional NP-Complete Problems\n\nCLIQUE is NP-Complete\nWe already show that 3SAT ≤p CLIQUE\n\nVERTEX −COVER is NP-Complete where\nVERTEX −COVER = {hG, ki | G is an undirected graph that\nhas a k-node vertex cover}\nTextbook shows that 3SAT ≤p VERTEX −COVER\n\nHAMPATH is NP-Complete\nTextbook shows that 3SAT ≤p HAMPATH\n\nSUBSET −SUM is NP-Complete\nTextbook shows that 3SAT ≤p SUBSET −SUM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nTime Complexity 04\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":269,"segment": "self_training_2", "course": "cs1502", "lec": "lec19_reducibility_02","text":"Reducibility 02\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\nConsider the following Turing machine:\nM1 = “On input x:\n1\n\naccept”\n\nWhat is the language of TM M1 ?\nL(M1 ) = Σ∗\n\nWhy?\nAction\nRun M1 on input ε\nRun M1 on input 0\nRun M1 on input 1\nRun M1 on input 00\nRun M1 on input 01\n..\n.\nBecause it accepts all strings\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nResult\naccept\naccept\naccept\naccept\naccept\n..\n.\n\n\fTuring Machine\nConsider the following Turing machine:\nM2 = “On input x:\n1\n\nreject”\n\nWhat is the language of TM M2 ?\nL(M2 ) = ∅\n\nWhy?\nAction\nRun M2 on input ε\nRun M2 on input 0\nRun M2 on input 1\nRun M2 on input 00\nRun M2 on input 01\n..\n.\nBecause it rejects all strings\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nResult\nreject\nreject\nreject\nreject\nreject\n..\n.\n\n\fTuring Machine\nConsider the following Turing machine:\nM3 = “On input x:\n1\n\nEnter infinite loop”\n\nWhat is the language of TM M3 ?\nL(M3 ) = ∅\n\nWhy?\nAction\nRun M3 on input ε\nRun M3 on input 0\nRun M3 on input 1\nRun M3 on input 00\nRun M3 on input 01\n..\n.\n\nResult\nLoops indefinitely\nLoops indefinitely\nLoops indefinitely\nLoops indefinitely\nLoops indefinitely\n..\n.\n\nBecause it loops indefinitely on all strings\n\nIt does not accept or reject on all strings\nThe language of a TM is the set of all strings accepted by it\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\nConsider the following Turing machine:\nM4 = “On input x:\n1\n2\n\nIf x is in the form of 0n 1n , accept.\nIf x is not in the form of 0n 1n , reject.”\n\nWhat is the language of TM M4 ?\nL(M4 ) = {0n 1n | n ≥ 0}\n\nWhy?\nAction\nRun M4 on input ε\nRun M4 on input 0\nRun M4 on input 1\nRun M4 on input 00\nRun M4 on input 01\nRun M4 on input 10\n..\n.\n\nResult\naccept\nreject\nreject\nreject\naccept\nreject\n..\n.\n\nTM M4 only accepts ε, 01, 0011, 000111, 00001111, . . .\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\nConsider the following Turing machine:\nM5 = “On input x:\n1\n2\n\nIf x starts with a 0, accept.\nEnter infinite loop.”\n\nWhat is the language of TM M5 ?\nL(M5 ) = 0Σ∗\n\nWhy?\nAction\nRun M5 on input ε\nRun M5 on input 0\nRun M5 on input 1\nRun M5 on input 00\nRun M5 on input 01\nRun M5 on input 10\n..\n.\n\nResult\nLoops indefinitely\naccept\nLoops indefinitely\naccept\naccept\nLoops indefinitely\n..\n.\n\nTM M5 only accepts 0, 00, 01, 000, 001, 010, 011, 0000, . . .\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\nConsider the following Turing machine:\nM6 = “On input x:\n1\n2\n\nIf x = ε , accept.\nreject.”\n\nWhat is the language of TM M6 ?\nL(M6 ) = {ε}\n\nWhy?\nAction\nRun M6 on input ε\nRun M6 on input 0\nRun M6 on input 1\nRun M6 on input 00\nRun M6 on input 01\nRun M6 on input 10\n..\n.\nTM M6 only accepts ε\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nResult\naccept\nreject\nreject\nreject\nreject\nreject\n..\n.\n\n\fTuring Machine\nConsider the following Turing machine:\nM7 = “On input x:\n1\n2\n\nIf x 6= ε , accept.\nreject.”\n\nWhat is the language of TM M7 ?\nL(M7 ) = Σ∗ − {ε}\n\nWhy?\nAction\nRun M7 on input ε\nRun M7 on input 0\nRun M7 on input 1\nRun M7 on input 00\nRun M7 on input 01\nRun M7 on input 10\n..\n.\n\nResult\nreject\naccept\naccept\naccept\naccept\naccept\n..\n.\n\nTM M7 accepts any string that is not the empty string\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\nGiven a TM M and a string w, answer the following questions:\n1\n\nWhat is L(M )?\n\n2\n\nIs M a decider?\n\n3\n\nWhat is the result of running M on input w?\n\nWe have no idea\nWe have no idea\nWe have no idea\n4\n\nIs hM, wi ∈ ATM ?\nWe have no idea\n\nWhat do we know about the given TM M and a string w?\nTechnically, we know almost nothing about M and w\nBut we know that if we run TM M on input w, there are three\npossible outcomes:\n1\n2\n3\n\nM accepts w\nM rejects w\nM loops indefinitely on w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\n\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN1 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N1 ?\nWe cannot answer this question\nThe answer to the above question depends on whether\nM accepts w,\nM rejects w, or\nM loops indefinitely on w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN1 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N1 if M accepts w?\nL(N1 ) = Σ∗ if M accepts w\n\nWhy?\nAction\nRun N1 on input ε\nRun N1 on input 0\nRun N1 on input 1\nRun N1 on input 00\nRun N1 on input 01\n..\n.\n\nStep 1 of TM N1\nRun M on input w\nRun M on input w\nRun M on input w\nRun M on input w\nRun M on input w\n..\n.\n\nStep 1 Result\naccept\naccept\naccept\naccept\naccept\n..\n.\n\nTM N1 accepts all strings if M accepts w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nResult\naccept\naccept\naccept\naccept\naccept\n..\n.\n\n\fTuring Machine\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN1 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N1 if M rejects w?\nL(N1 ) = ∅ if M rejects w\n\nWhy?\nAction\nRun N1 on input ε\nRun N1 on input 0\nRun N1 on input 1\nRun N1 on input 00\nRun N1 on input 01\n..\n.\n\nStep 1 of TM N1\nRun M on input w\nRun M on input w\nRun M on input w\nRun M on input w\nRun M on input w\n..\n.\n\nTM N1 rejects all strings if M rejects w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nStep 1 Result\nreject\nreject\nreject\nreject\nreject\n..\n.\n\nResult\nreject\nreject\nreject\nreject\nreject\n..\n.\n\n\fTuring Machine\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN1 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N1 if M loops indefinitely on w?\nL(N1 ) = ∅ if M loops indefinitely on input w\n\nWhy?\nAction\nRun N1 on input ε\nRun N1 on input 0\nRun N1 on input 1\nRun N1 on input 00\nRun N1 on input 01\n.\n.\n.\n\nStep 1 of TM N1\nRun M on input w\nRun M on input w\nRun M on input w\nRun M on input w\nRun M on input w\n.\n.\n.\n\nStep 1 Result\nM loops indefinitely\nM loops indefinitely\nM loops indefinitely\nM loops indefinitely\nM loops indefinitely\n.\n.\n.\n\nResult\nN1 loops indefinitely\nN1 loops indefinitely\nN1 loops indefinitely\nN1 loops indefinitely\nN1 loops indefinitely\n.\n.\n.\n\nTM N1 loops indefinitely on all strings if M loops indefinitely\non input w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\n\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN1 = “On input x:\n1\n2\n\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N1 under the following\nassumptions:\nL(N1 ) = Σ∗\nIf M accepts w\nL(N1 ) = ∅\nIf M rejects w\nIf M loops indefinitely on input w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nL(N1 ) = ∅\n\n\fTuring Machine\n\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN2 = “On input x:\n1\n2\n3\n\nIf x = ε, accept\nRun M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N2 under the following\nassumptions:\nIf M accepts w\nL(N2 ) = Σ∗\nL(N2 ) = {ε}\nIf M rejects w\nIf M loops indefinitely on input w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nL(N2 ) = {ε}\n\n\fTuring Machine\n\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN3 = “On input x:\n1\n2\n\nIf x = ε, Run M on input w and accept if M does.\nreject.”\n\nWhat is the language of TM N3 under the following\nassumptions:\nL(N3 ) = {ε}\nIf M accepts w\nL(N3 ) = ∅\nIf M rejects w\nIf M loops indefinitely on input w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\nL(N3 ) = ∅\n\n\fTuring Machine\n\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN4 = “On input x:\n1\n\n2\n\nIf x = ε, Run M on input w. if M accepts w, accept.\nOtherwise, reject.\naccept.”\n\nWhat is the language of TM N4 under the following\nassumptions:\nIf M accepts w\nL(N4 ) = Σ∗\nL(N4 ) = Σ∗ − {ε}\nIf M rejects w\nIf M loops indefinitely on input w\nL(N4 ) = Σ∗ − {ε}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fTuring Machine\n\nSuppose we have a TM M and a string w\nConsider the following Turing machine:\nN5 = “On input x:\n1\n2\n3\n\nIf x is in the form of 0n 1n , accept\nIf x is not in the form of 0n 1n , run M on input w\nIf M accepts w, accept. If M rejects w, reject.”\n\nWhat is the language of TM N5 under the following\nassumptions:\nL(N5 ) = Σ∗\nIf M accepts w\nIf M rejects w\nL(N5 ) = {0n 1n | n ≥ 0}\nL(N5 ) = {0n 1n | n ≥ 0}\nIf M loops indefinitely on input w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\fConclusion\n\nAgain, given a TM M and a string w, we know almost\nnothing about them\nIf we construct a TM N that run M on input w, the behavior\nor language of TM N depends on whether\n1\n2\n3\n\nM accepts w\nM rejects w\nM loops indefinitely on input w\n\nWhat you have seen are some example of turning a TM M\nand a string w into a new TM\nReducibility\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 02\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
{"id":270,"segment": "unlabeled", "course": "cs1502", "lec": "lec22_reducibility_05","text":"Reducibility 05\nThumrongsak Kosiyatrakul\ntkosiyat@cs.pitt.edu\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fREGULAR TM is Undecidable\nProblem: Is there an algorithm to determine whether the\nlanguage of a TM is regular?\nFormally,\nREGULAR TM = {hM i | M is a TM and L(M ) is a regular language}\n\nUnfortunately, no algorithm can solve this problem\nTo verify this, we need to prove that REGULAR TM is\nundecidable\nAs usual, we are going to use the proof by contradiction\nAssume that REGULAR TM is decidable\nThus, there exists a TM R that decides REGULAR TM\nThis gives us the ability to check whether the language of a\nTM M 0 is regular or not regular\n\nAs usual, we want to show that if REGULAR TM is decidable,\nATM is decidable\nSo, we need to construct a TM M 0 such that whether the\nlanguage of TM M 0 is regular or not depending on whether M\naccepts w\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fREGULAR TM is Undecidable\n\nOnce again, we have two choices:\n1\n\nChoice #1:\nIf M accepts w, L(M 0 ) is regular\nIf M does not accept w, L(M 0 ) is not regular\n\n2\n\nChoice #2:\nIf M accepts w, L(M 0 ) is not regular\nIf M does not accept w, L(M 0 ) is regular\n\nWhich one?\nThis is an example of the problem where you can pick any\nchoices\nLet’s look at how to construct a TM M 0 that satisfies each\nchoice\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fREGULAR TM is Undecidable\nChoice #1\nIf M accepts w, L(M 0 ) is regular\nIf M does not accept w, L(M 0 ) is not regular\nHere is an example of a TM M 0 that satisfies choice #1:\nM 0 = “On input x:\n1\n2\n3\n\nIf x is in the form of 0n 1n , accept.\nIf x is not in the form of 0n 1n , run M on input w.\nIf M accepts w, accept. If M rejects w, reject.”\n\nFrom the definition of the above TM M 0 :\nIf M accepts w, L(M 0 ) = Σ∗\nIf M does not accept w:\nIf M rejects w, L(M 0 ) = {0n 1n | n ≥ 0}\nIf M loops indefinitely on w, L(M 0 ) = {0n 1n | n ≥ 0}\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fREGULAR TM is Undecidable\nChoice #2\nIf M accepts w, L(M 0 ) is not regular\nIf M does not accept w, L(M 0 ) is regular\nHere is an example of a TM M 0 that satisfies choice #2:\nM 0 = “On input x:\n1\n2\n\n3\n\nRun M on input w.\nIf M accepts w and x is in the form of 0n 1n , accept;\notherwise, reject.\nIf M rejects w, reject.”\n\nFrom the definition of the above TM M 0 :\nIf M accepts w, L(M 0 ) = {0n 1n | n ≥ 0}\nIf M does not accept w:\nIf M rejects w, L(M 0 ) = ∅\nIf M loops indefinitely on w, L(M 0 ) = ∅\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fREGULAR TM is undecidable (Choice #1)\nProof: Assume that REGULAR TM is decidable. Since REGULAR TM is decidable,\nthere exists a TM R that decides REGULAR TM . Let construct a TM S as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1 Construct TM M 0 as follows:\nM 0 = “On input x:\n1 If x is in the form of 0n 1n , accept.\n2 If x is not in the form of 0n 1n , run M on input w.\n3 If M accepts w, accept. If M rejects w, reject.”\n2 Run R on input hM 0 i.\n3 If R accepts hM 0 i, accept. If R rejects hM 0 i, reject.”\nNext, we need to prove that TM S is a decider for ATM .\nProve that if hM, wi ∈ ATM , S accepts hM, wi: Assume that hM, wi ∈ ATM .\nSince hM, wi ∈ ATM , M is a TM that accepts w. Since M accepts w, from the\ndefinition of TM M 0 , L(M 0 ) is Σ∗ which is a regular language. Since L(M 0 ) is\nregular, hM 0 i ∈ REGULAR TM . Since R is a decider for REGULAR TM , by\nrunning R on input hM 0 i, R will accept hM 0 i. Since R accepts hM 0 i, S accepts\nhM, wi.\nProve that if hM, wi 6∈ ATM , S rejects hM, wi: Assume that hM, wi 6∈ ATM .\nSince hM, wi 6∈ ATM , M is a TM that does not accept w. Since M does not\naccept w, from the definition of TM M 0 , L(M 0 ) is {0n 1n | n ≥ 0} which is not\na regular language. Since L(M 0 ) is not regular, hM 0 i 6∈ REGULAR TM . Since\nR is a decider for REGULAR TM , by running R on input hM 0 i, R will reject\nhM 0 i. Since R rejects hM 0 i, S rejects hM, wi.\nThis show that TM S is a decider for ATM . Therefore, ATM is decidable\ncontradiction. Therefore, REGULAR TM is undecidable.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fMore Fact for Contradiction\n\nSo far, we have been using ATM as the mean of contradiction\nBecause we know from the fact that ATM is undecidable\n\nBut now, we have the following undecidable languages:\nATM = {hM, wi | M is a TM that accepts w}\nHALT TM = {hM, wi | M is a TM that halts on input w}\nETM = {hM i | M is a TM and L(M ) = ∅}\nREGULAR TM = {hM i | M is a TM and L(M ) is regular}\n\nWe can now use those undecidable languages\nFor example, to show that a language A is undecidable, we can\nshow that if A is decidable, ETM is decidable\n\nBut we can always use ATM\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fEQ TM is Undecidable\nProblem: Is there an algorithm to determine whether\nlanguages of two TMs M1 and M2 are identical?\nFormally,\nEQ TM = {hM1 , M2 i | M1 and M2 are TMs and L(M1 ) = L(M2 )}\n\nUnfortunately, EQ TM is undecidable\nAs usual, we are going to assume that EQ TM is decidable\nIn other words, given two TMs M1 and M2 , we know whether\nhM1 , M2 i ∈ EQ TM or not\n\nFor this language, we generally need to construct two TMs\nM1 and M2 from a TM M and a string w\nFor this problem, we are going to look at two proofs:\nContradict with the fact that ATM is undecidable\nContradict with the fact that ETM is undecidable\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fEQ TM is Undecidable\nProve that EQ TM = {hM1 , M2 i | M1 and M2 are TMs and L(M1 ) = L(M2 )} is\nundecidable.\nProof: Assume that EQ TM is decidable. Since EQ TM is decidable, there exists a TM\nR that decides EQ TM . Let construct a TM S (to decide ATM ) as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1 Construct TM M1 as follows:\nM1 = “On input x:\n1 accept.\n2 Construct TM M2 as follows:\nM2 = “On input x:\n1 Run M on input w.\n2 If M accepts w, accept. If M rejects w, reject.”\n3 Run R on input hM1 , M2 i.\n4 If R accepts hM1 , M2 i, accept. If R rejects hM1 , M2 i, reject.”\nNext, we need to prove that TM S is a decider for ATM .\nProve that if hM, wi ∈ ATM , S accepts hM, wi:\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M is a TM that accepts w.\nSince M accepts w, from the definition of TM M1 , L(M1 ) = Σ∗ and from the\ndefinition of TM M2 , L(M2 ) = Σ∗ . In other words, L(M1 ) = L(M2 ). Since\nL(M1 ) = L(M2 ), hM1 , M2 i ∈ EQ TM . Since R is a decider for EQ TM , by\nrunning R on input hM1 , M2 i, R will accept hM1 , M2 i. Since R accepts\nhM1 , M2 i, S accepts hM, wi.\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fEQ TM is Undecidable\n\nProof: (continue)\nProve that if hM, wi 6∈ ATM , S rejects hM, wi:\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M is a TM that does not\naccept w. Since M does not accept w, from the definition of TM M1 ,\nL(M1 ) = Σ∗ and from the definition of TM M2 , L(M2 ) = ∅. In other words,\nL(M1 ) 6= L(M2 ). Since L(M1 ) 6= L(M2 ), hM1 , M2 i 6∈ EQ TM . Since R is a\ndecider for EQ TM , by running R on input hM1 , M2 i, R will reject hM1 , M2 i.\nSince R rejects hM1 , M2 i, S rejects hM, wi.\nThis show that TM S is a decider for ATM . Therefore, ATM is decidable\ncontradiction. Therefore, EQ TM is undecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fEQ TM is Undecidable\nProve that EQ TM = {hM1 , M2 i | M1 and M2 are TMs and L(M1 ) = L(M2 )} is\nundecidable.\nProof: Assume that EQ TM is decidable. Since EQ TM is decidable, there exists a TM\nR that decides EQ TM . Let construct a TM S (to decide ETM ) as follows:\nS = “On input hM i where M is a TM:\n1\n\nConstruct TM ME as follows:\nME = “On input x:\n1 reject.\n\n2\n\nRun R on input hM, ME i.\n\n3\n\nIf R accepts hM, ME i, accept. If R rejects hM, ME i, reject.”\n\nNext, we need to prove that TM S is a decider for ETM .\nProve that if hM i ∈ ETM , S accepts hM i:\nAssume that hM i ∈ ETM . Since hM i ∈ ETM , M is a TM and L(M ) = ∅.\nFrom the definition of TM ME , L(ME ) = ∅. Since L(M ) = ∅ and L(ME ) = ∅,\nL(M ) = L(ME ). In other words, hM, ME i ∈ EQ TM . Since R is a decider for\nEQ TM , by running R on input hM, ME i, R will accept hM, ME i. Since R\naccepts hM, ME i, S accepts hM i.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fEQ TM is Undecidable\n\nProof: (continue)\nProve that if hM i 6∈ ETM , S rejects hM i:\nAssume that hM i 6∈ ETM . Since hM i 6∈ ETM , M is a TM and L(M ) 6= ∅.\nFrom the definition of TM ME , L(ME ) = ∅. Since L(M ) 6= ∅ and L(ME ) = ∅,\nL(M ) 6= L(ME ). In other words, hM, ME i 6∈ EQ TM . Since R is a decider for\nEQ TM , by running R on input hM, ME i, R will reject hM, ME i. Since R\nrejects hM, ME i, S rejects hM i.\nThis show that TM S is a decider for ETM . Therefore, ETM is decidable\ncontradiction. Therefore, EQ TM is undecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fONE TM is Undecidable\n\nProblem: Given a TM M , is there an algorithm to check\nwhether M only accept one string (|L(M )| = 1)\nUnfortunately, this is another unsolvable problem\nFormally,\nONE TM = {hM i | M is a TM and |L(M )| = 1}\nTo show that the above problem is unsolvable, we just need to\nshow that ONE TM is undecidable\nUse the same method:\nShow that if ONE TM is decidable, ATM is decidable\nWe need to construct a TM M 0 from a TM M and a string w\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fONE TM is Undecidable\n\nTwo choices as usual:\n1\n\nChoice #1:\nIf hM, wi ∈ ATM , hM 0 i ∈ ONE TM\nIf M accepts w, |L(M 0 )| = 1\nIf hM, wi 6∈ ATM , hM 0 i 6∈ ONE TM\nIf M does not accept w, |L(M 0 )| =\n6 1\n\n2\n\nChoice #2:\nIf hM, wi ∈ ATM , hM 0 i 6∈ ONE TM\nIf M accepts w, |L(M 0 )| 6= 1\nIf hM, wi 6∈ ATM , hM 0 i ∈ ONE TM\nIf M does not accept w, |L(M 0 )| = 1\n\nThis is another example where you can pick either choices\nWe are going to use the choice #1\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fONE TM is Undecidable\nProve that ONE TM = {hM i | M is a TM and |L(M )| = 1} is undecidable.\nProof: Assume that ONE TM is decidable. Since ONE TM is decidable, there exists a\nTM R that decides ONE TM . Let construct a TM S (to decide ATM ) as follows:\nS = “On input hM, wi where M is a TM and w is a string:\n1\n\nConstruct TM M 0 as follows:\nM 0 = “On input x:\n1 If x = ε, run M on input w. If M accepts w, accept; otherwise, reject.\n2 If x 6= ε, reject.\n\n2\n\nRun R on input hM 0 i.\n\n3\n\nIf R accepts hM 0 i, accept. If R rejects hM 0 i, reject.”\n\nNext, we need to prove that TM S is a decider for ATM .\nProve that if hM, wi ∈ ATM , S accepts hM, wi:\nAssume that hM, wi ∈ ATM . Since hM, wi ∈ ATM , M is a TM and that\naccepts w. Since M accepts w, from the definition of TM M 0 , L(M 0 ) = {ε}.\nSince L(M 0 ) = {ε}, |L(M 0 )| = 1. In other words, hM 0 i ∈ ONE TM . Since R is\na decider for ONE TM , by running R on input hM 0 i, R will accept hM 0 i. Since\nR accepts hM 0 i, S accepts hM, wi.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fONE TM is Undecidable\n\nProof: (continue)\nProve that if hM, wi 6∈ ATM , S rejects hM, wi:\nAssume that hM, wi 6∈ ATM . Since hM, wi 6∈ ATM , M is a TM and that does\nnot accept w. Since M does not accept w, from the definition of TM M 0 ,\nL(M 0 ) = ∅. Since L(M 0 ) = ∅, |L(M 0 )| 6= 1. In other words, hM 0 i 6∈ ONE TM .\nSince R is a decider for ONE TM , by running R on input hM 0 i, R will reject\nhM 0 i. Since R rejects hM 0 i, S rejects hM, wi.\nThis show that TM S is a decider for ATM . Therefore, ATM is decidable\ncontradiction. Therefore, ONE TM is undecidable.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\fConclusion\nSo far, to prove that a language A is undecidable, we use\nproof by contradiction\nAssume that A is decidable\nSince A is decidable, there exists a TM R that decides A\nThen we show that if A is decidable, ATM is decidable\nAnother known undecidable language can be used instead of\nATM\n\nSo, we need to construct a TM S that decides ATM and prove\nthat S is a decider for ATM\nGiven a TM M and a string w, TM S need to construct a\nstring s such that whether s ∈ A depends on whether M\naccepts w\nThen use TM R (a decider for A) as a helper machine to\ndecide ATM\n\nTry to prove that ONE TM is undecidable using the choice #2.\n\nThumrongsak Kosiyatrakul tkosiyat@cs.pitt.edu\n\nReducibility 05\n\n\f","label":[[-2, -1, "Concept"]],"Comments":[]}
