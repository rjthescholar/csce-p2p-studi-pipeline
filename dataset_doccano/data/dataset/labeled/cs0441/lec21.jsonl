{"id": 23, "segment": ["train_set", "labeled"], "course": "cs0441", "lec": "lec21", "text": "Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #21: Probability Theory\n\nBased on materials developed by Dr. Adam Lee\n\n\fNot all events are equally likely to occur\u2026\n\nSporting events\n\nGames of strategy\n\nNature\n\nInvestments\n\n\fWe can model these types of real-life situations\nby relaxing our model of probability\nAs before, let S be our sample space. Unlike before,\nwe will allow S to be either finite or countable.\nWe will require that the following conditions hold:\n1. 0 \u2264 p(s) \u2264 1 for each s \u2208 S\n2.\n\nNo event can have a negative likelihood of\noccurrence, or more than a 100% chance\nof occurence\n\nIn any given experiment, some event will\noccur\n\nThe function p : S \u2192 [0,1] is called a probability\ndistribution\n\n\fRecap our formulas for the probability of\ncombinations of events\nProperty 1: p(E) = 1 \u2013 p(E)\n\nS=\n\nE\n\nl Recall that S = E \u222a E for any event E\nl Further, \u2211!\u2208# \ud835\udc5d \ud835\udc60 = 1\nl So, p(S) = p(E) + p(E) = 1\nl Thus, p(E) = 1 \u2013 p(E)\n\nE\n\nProperty 2: p(E1 \u222a E2) = p(E1) + p(E2) \u2013 p(E1 \u2229 E2)\nl Recall that \ud835\udc5d \ud835\udc38 = \u2211!\u2208$ \ud835\udc5d \ud835\udc60\nS=\nl Let x be some outcome in E1 \u222a E2\nl If x is in one of E1 or E2, then p(x) is counted\nonce on the RHS of the equation\nl If x is in both E1 or E2, then p(x) is counted\n1 + 1 \u2013 1 = 1 times on the RHS of the equation\n\nE1\n\nE2\n\n\fA formula for the probability of pairwise disjoint events\nTheorem: If E1, E2, \u2026, En is a sequence of pairwise disjoint events in\na sample space S, then we have:\n$\n\n$\n\n\ud835\udc5d # \ud835\udc38! = & \ud835\udc5d \ud835\udc38!\n!\"#\n\n!\"#\n\nRecall: E1, E2, \u2026, En are pairwise disjoint iff Ei \u2229 Ej = \u2205 for 1 \u2264 i,j \u2264 n\nS=\nE1\n\nE2\n\n\u2026\n\nEn\n\nWe can prove this theorem using mathematical induction!\n\n\fHow can we incorporate prior knowledge?\nSometimes we want to know the probability of some event given\nthat another event has occurred.\n\nExample: A fair coin is flipped three times. The first flip turns\nup tails. Given this information, what is the probability that an\nodd number of tails appear in the three flips?\n\nSolution:\nl Let F = \u201cthe first flip of three comes up tails\u201d\nl Let E = \u201ctails comes up an odd number of times in three flips\u201d\nl Since F has happened, S is reduced to {THH, THT, TTH, TTT}\nl We know:\nl p(E) = |E|/|S|\nl\n= |{THH, TTT}| / |{THH, THT, TTH, TTT}|\nl\n= 2/4\nl\n= 1/2\n\n\fConditional Probability\nDefinition: Let E and F be events with p(F) > 0. The conditional\nprobability of E given F, denoted p(E | F), is defined as:\n\n\ud835\udc5d \ud835\udc38\u2229\ud835\udc39\n\ud835\udc5d \ud835\udc38 \ud835\udc39 =\n\ud835\udc5d \ud835\udc39\n\nIntuition:\nl Think of the event F as reducing the sample space that can be considered\nl The numerator looks at the likelihood of the outcomes in E that overlap\nthose in F\nl The denominator accounts for the reduction in sample size indicated by our\nprior knowledge that F has occurred\n\n\fBit strings\nExample: Suppose that a bit string of length 4 is generated at\nrandom so that each of the 16 possible 4-bit strings is equally\nlikely to occur. What is the probability that it contains at least\ntwo consecutive 0s, given that the first bit in the string is a 0?\n\nSolution:\nl Let E = \u201cA 4-bit string has at least two consecutive zeros\u201d\nl Let F = \u201cThe first bit of a 4-bit string is a zero\u201d\nl Want to calculate p(E | F) = p(E \u2229 F)/p(F)\nl E \u2229 F = {0000, 0001, 0010, 0011, 0100}\nl So, p(E \u2229 F) = 5/16\nl Since each bit string is equally likely to occur,\np(F) = 8/16 = 1/2\nl So p(E | F) = (5/16)/(1/2) = 10/16 = 5/8\n\n\fKids\nExample: What is the conditional probability that a family with\ntwo kids has two boys, given that they have at least one boy?\nAssume that each of the possibilities BB, BG, GB, GG is equally\nlikely to occur.\nBoy is older\n\nSolution:\nl Let E = \u201cA family with 2 kids has 2 boys\u201d\nl E = {BB}\nl Let F = \u201cA family with 2 kids has at least 1 boy\u201d\nl F = {BB, BG, GB}\nl E \u2229 F = {BB}\nl So p(E | F) = p(E \u2229 F)/p(F)\nl\nl\n\n= (1/4) / (3/4)\n= 1/3\n\nGirl is older\n\n\fDoes prior knowledge always help us?\nExample: Suppose a fair coin is flipped twice. Does knowing that\nthe coin comes up tails on the first flip help you predict whether\nthe coin will be tails on the second flip?\n\nSolution:\nl S = {HH, HT, TH, TT}\nl F = \u201cCoin was tails on the first flip\u201d = {TH, TT}\nl E = \u201cCoin is tails on the second flip\u201d = {TT, HT}\nl p(E) = 2/4 = 1/2\nl p(E | F) = p(E \u2229 F)/p(F)\nl\n= (1/4) / (2/4)\nl\n= 1/2\nl Knowing the first flip does not help you guess the second flip!\n\n\fIndependent Events\nDefinition: We say that events E and F are independent if and\nonly if p(E \u2229 F) = p(E)p(F).\n\nRecall: In our last example\u2026\nl S = {HH, HT, TH, TT}\nl F = {TH, TT}\nl E = {HT, TT}\nl E \u2229 F = {TT}\n\nSo:\nl p(E \u2229 F) = |E \u2229 F|/|S|\nl\n= 1/4\nl p(E)p(F) = 1/2 \u00d7 1/2\nl\n= 1/4\n\nThis checks out!\n\n\fExample: Bit Strings\nExample: Suppose that E is the event that a randomly generated\nbit string of length four begins with a 1, and F is the event that\nthis bit string contains an even number of 1s. Are E and F\nindependent if all 4-bit strings are equally likely to occur?\n\nSolution:\nl By the product rule, |S| = 24 = 16\nl E = {1111, 1110, 1101, 1011, 1100, 1010, 1001, 1000}\nl F = {0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111}\nl So p(E) = p(F) = 8/16 = 1/2\nl p(E)p(F) = 1/4\nl E \u2229 F = {1111, 1100, 1010, 1001}\nl p(E \u2229 F) = 4/16 = 1/4\nl Since p(E \u2229 F) = p(E)p(F), E and F are independent events\n\n\fExample: Distribution of kids\nExample: Assume that each of the four ways that a family can\nhave two children are equally likely. Are the events E that a\nfamily with two children has two boys, and F that a family with\ntwo children has at least one boy independent?\n\nSolution:\nl E = {BB}\nl F = {BB, BG, GB}\nl p(E) = 1/4\nl p(F) = 3/4\nl p(E)p(F) = 3/16\nl E \u2229 F = {BB}\nl p(E \u2229 F) = 1/4\nl Since 1/4 \u2260 3/16, E and F are not independent\n\n\fIf probabilities are independent, we can use the product rule to\ndetermine the probabilities of combinations of events\nExample: What is the probability of flipping heads 4 times in a\nrow using a fair coin?\n\nAnswer: p(H) = 1/2, so p(HHHH) = (1/2)4 = 1/16\n\nExample: What is the probability of rolling the same number 3\ntimes in a row using an unbiased 6-sided die?\n\nAnswer:\nl\nl\nl\nl\n\nFirst roll agrees with itself with probability 1\n2nd roll agrees with first with probability 1/6\n3rd roll agrees with first two with probability 1/6\nSo probability of rolling the same number 3 times is 1 \u00d7 1/6 \u00d7 1/6 = 1/36\n\n\fIn-class exercises\nTop Hat\n\n\fMany experiments only have two outcomes\n\nP(x)\nCoin flips: Heads or tails?\n\nBit strings: 0 or 1?\n\nPredicates: T or F?\n\nThese types of experiments are called Bernoulli trials\nTwo outcomes:\nl Success\nl Failure\n\nProbability p\nProbability q = 1 \u2013 p\n\nMany problems can be solved by examining the probability of k\nsuccesses in an experiment consisting of mutuallyindependent Bernoulli trials\n\n\fExample: Coin flips\nExample: A coin is biased so that the probability of heads is 2/3.\nWhat is the probability that exactly four heads come up when the\ncoin is flipped seven times, assuming that each flip is independent?\n\nSolution:\nl 27 = 128 possible outcomes for seven flips\nl There are C(7,4) ways that heads can be flipped four times\nl Since each flip is independent, the probability of each of these outcomes is\n(2/3)4(1/3)3\nl So, the probability of exactly 4 heads occurring in 7 flips of this biased coin\nis C(7,4)(2/3)4(1/3)3 = 560/2187\n\n7 Choose 4 outcomes\nto make heads\n\nProbability of each tails\ncombined using product rule\nProbability of each heads\ncombined using product rule\n\n\fThis general reasoning provides us with a nice formula\u2026\nTheorem: The probability of exactly k successes in n independent\nBernoulli trials, with probability of success p and probability of\nfailure q = 1-p, is C(n,k)pkqn-k.\n\nProof:\nl The outcome of n Bernoulli trials is an n-tuple (t1, t2, \u2026, tn)\nl Each ti is either S (for success) or F (for failure)\nl C(n,k) ways to choose k tis to label S\nl Since each trial is independent, the probability of each outcome with k\nsuccesses and n-k failures is pkqn-k\nl So, the probability of exactly k successes is C(n,k)pkqn-k. \u274f\n\nNotation: We denote the probability of k successes in n independent\nBernoulli trials with probability of success p as b(k; n, p).\n\n\fBits (Again)\nExample: Suppose that the probability that a 0 bit is generated is\n0.9, that the probability that a 1 bit is generated is 0.1, and that\nbits are generated independently. What is the probability that\nexactly eight 0 bits are generated when ten random bits are\ngenerated?\n\nSolution:\nl Number of trials\nl Number of successes\nl Probability of success\nl Probability of failure\nl Want to compute b(k; 10, 0.9)\nl\n= C(10, 8)0.980.12\nl\n= 0.1937102445\n\nn = 10\nk=8\np = 0.9\nq = 1 \u2013 0.9 = 0.1\n\n\fMany probability questions are concerned with some\nnumerical value associated with an experiment\n\nNumber of boys in a family\nNumber of 1 bits generated\n\nBeats per minute of a heart\n\nLongevity of a chicken\n\nNumber of \u201cheads\u201d flips\n\n\fWhat is a random variable?\nDefinition: A random variable is a function X from the sample\nspace of an experiment to the set of real numbers R. That is, a\nrandom variable assigns a real number to each possible outcome.\n\nNote: Despite the name, X is not a variable,\nand is not random. X is a function!\n\nExample: Suppose that a coin is flipped three times. Let X(s) be\nthe random variable that equals the numbers of heads that\nappear when s is the outcome. Then X(s) takes the following\nvalues:\nl X(HHH) = 3\nl X(HHT) = X(HTH) = X(THH) = 2\nl X(TTH) = X(THT) = X(HTT) = 1\nl X(TTT) = 0\n\n\fRandom variables and distributions\nDefinition: The distribution of a random variable X on a sample\nspace S is the set of pairs (r, p(X=r)) for all r \u2208 X(S), where p(X=r)\nis the probability that X takes the value r.\nNote: A distribution is usually described by specifying p(X=r) for\neach r \u2208 X(S)\n\nExample: Assume that our coin flips from the previous slide were\nall equally likely to occur. We then get the following distribution\nfor the random variable X:\nl p(X=0) = 1/8\nl p(X=1) = 3/8\nl p(X=2) = 3/8\nl p(X=3) = 1/8\n\n\fExample: Rolling dice\nLet X be the sum of the numbers that appear when a pair of fair dice\nis rolled. What are the values of this random variable for the 36\npossible outcomes (i, j) where i and j are the numbers that appear on\nthe first and second die, respectively?\n\nAnswer:\nl X(1,1) = 2\nl X(1,2) = X(2, 1) = 3\nl X(1,3) = X(2,2) = X(3,1) = 4\nl X(1,4) = X(2,3) = X(3,2) = X(4,1) = 5\nl X(1,5) = X(2,4) = X(3,3) = X(4,2) = X(5,1) = 6\nl X(1,6) = X(2,5) = X(3,4) = X(4,3) = X(5,2) = X(6,1) = 7\nl X(2,6) = X(3,5) = X(4,4) = X(5,3) = X(6,2) = 8\nl X(3,6) = X(4,5) = X(5,4) = X(6,3) = 9\nl X(4,6) = X(5,5) = X(6,4) = 10\nl X(5,6) = X(6,5) = 11\nl X(6,6) = 12\n\np(X=2) = 1/36\np(X=3) = 2/36\np(X=4) = 3/36\np(X=5) = 4/36\np(X=6) = 5/36\np(X=7) = 6/36\np(X=8) = 5/36\np(X=9) = 4/36\np(X=10) = 3/36\np(X=11) = 2/36\np(X=12) = 1/36\n\n\fSometimes probabilistic reasoning can lead us to some\ninteresting and unexpected conclusions\u2026\nQuestion: How many people need to be in the same room so that\nthe probability of two people sharing the same birthday is greater\nthan 1/2?\n\nAssumptions:\n1. There are 366 possible birthdays\n2. All birthdays are equally likely to occur\n3. Birthdays are independent\n\nSolution tactic:\nl Find the probability pn that the n people in a room all have\ndifferent birthdays\nl Then compute 1-pn, which is the probability that at least two\npeople share the same birthday\n\n\fLet\u2019s figure this out\u2026\nLet\u2019s assess probabilities as people enter the room\nl Person 1 clearly doesn\u2019t have the same birthday as anyone else in the\nroom\nl P2 has a different birthday than P1 with probability 365/366\nl P3 has a different birthday than P1 and P2 with probability 364/366\nl \u2026\n\nIn general, Pj has a different birthday than P1, P2, \u2026, Pj-1 with\nprobability [366-(j-1)]/366 = (367-j)/366\nRecall that pn is the probability that n people in the room all have\ndifferent birthdays. Using our above observations, this means:\n\n\fBut we\u2019re interested in 1-pn \u2026\n\nTo check the minimum number of people need in the room to\nensure that pn > 1/2, we\u2019ll use trial and error:\nl If n = 22, then 1 \u2013 pn \u2248 0.475\nl If n = 23, then 1 \u2013 pn \u2248 0.506\n\nSo, you need only 23 people in a room to have a better than 50%\nchance that two people share the same birthday!\n\n\fIn-class exercises\nProblem 3: What is the probability that exactly 2 heads occur\nwhen a fair coin is flipped 7 times?\nProblem 4: Consider a game between Alice and Bob. Over time,\nAlice has been shown to win this game (against Bob) 75% of the\ntime. If Alice and Bob play 6 games in a row, what is the\nprobability that Alice wins every game?\nProblem 5: Consider generating a uniformly-random 4-character\nbit string. Also consider R, a random variable that measures the\nlongest run of 1 bits in the generated string. Determine the\ndistribution of R.\n\n\fFinal Thoughts\nn Today we covered\nl Conditional probability\nl Independence\nl Bernoulli trials\nl Random variables\nl Probabilistic analysis\n\nn Next time:\nl Bayes\u2019 Theorem (Section 7.3)\n\n\fThe proof\u2026\nP(n) \u2261 \ud835\udc5d \u22c3$!\"# \ud835\udc38! = \u2211$!\"# \ud835\udc5d \ud835\udc38!\nBase case: P(2): Let E1, E2 be disjoint events.\nBy definition, p(E1 \u222a E2) = p(E1) + p(E2) \u2013 p(E1 \u2229 E2).\nSince E1 \u2229 E2 = \u2205, p(E1 \u222a E2) = p(E1) + p(E2)\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) \u2192 P(k+1)\nn Consider E = E1 \u222a E2 \u222a \u2026 \u222a Ek \u222a Ek+1\nn Let J = E1 \u222a E2 \u222a \u2026 \u222a Ek, so E = J \u222a Ek+1\nn p(E) = p(J \u222a Ek+1)\nn\n= p(J) \u222a p(Ek+1)\nn\n= p(E1 \u222a E2 \u222a \u2026 \u222a Ek) \u222a p(Ek+1)\nn\n= p(E1) \u222a p(E2) \u222a \u2026 \u222a p(Ek) \u222a p(Ek+1)\n\nby definition of E\nby I.H.\nby definition of J\nby I.H.\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction \u274f\n\n\f", "label": [[738, 762, "Concept"], [1328, 1352, "Concept"], [1396, 1420, "Concept"], [1514, 1531, "Concept"], [2231, 2254, "Concept"], [2308, 2331, "Concept"], [4249, 4267, "Concept"], [4311, 4322, "Concept"], [5596, 5607, "Concept"], [6366, 6382, "Concept"], [7397, 7408, "Concept"], [7409, 7425, "Concept"], [8725, 8740, "Concept"], [8756, 8771, "Concept"], [9296, 9312, "Concept"], [9317, 9330, "Concept"], [9347, 9359, "Concept"], [9365, 9380, "Concept"], [9519, 9531, "Concept"], [12615, 12638, "Concept"], [12641, 12653, "Concept"], [12656, 12672, "Concept"], [12675, 12691, "Concept"], [12694, 12716, "Concept"]], "Comments": []}