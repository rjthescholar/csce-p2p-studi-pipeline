{"id": 22, "segment": ["train_set", "labeled"], "course": "cs0441", "lec": "lec23", "text": "Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #23: Expected Value\n\nBased on materials developed by Dr. Adam Lee\n\n\fWhat is a random variable?\nDefinition: A random variable is a function X from the sample\nspace of an experiment to the set of real numbers R. That is, a\nrandom variable assigns a real number to each possible outcome.\n\nNote: Despite the name, X is not a variable,\nand is not random. X is a function!\n\nExample: Suppose that a coin is flipped three times. Let X(s) be\nthe random variable that equals the numbers of heads that\nappear when s is the outcome. Then X(s) takes the following\nvalues:\nl X(HHH) = 3\nl X(HHT) = X(HTH) = X(THH) = 2\nl X(TTH) = X(THT) = X(HTT) = 1\nl X(TTT) = 0\n\n\fRandom variables and distributions\nDefinition: The distribution of a random variable X on a sample\nspace S is the set of pairs (r, p(X=r)) for all r \u2208 X(S), where p(X=r)\nis the probability that X takes the value r.\nNote: A distribution is usually described by specifying p(X=r) for\neach r \u2208 X(S)\n\nExample: Assume that our coin flips from the previous slide were\nall equally likely to occur. We then get the following distribution\nfor the random variable X:\nl p(X=0) = 1/8\nl p(X=1) = 3/8\nl p(X=2) = 3/8\nl p(X=3) = 1/8\n\n\fMany times, we want to study the expected\nvalue of a random variable\nDefinition: The expected value (or expectation) of a random\nvariable X(s) on the sample space S is equal to:\n\ud835\udc38 \ud835\udc4b = %\ud835\udc5d \ud835\udc60 \ud835\udc4b \ud835\udc60\n!\u2208#\n\nFor every outcome\u2026\n\n\u2026 use the probability of\nthat outcome occuring\u2026\n\n\u2026 to weight the value of the\nrandom variable for that\noutcome.\n\nNote: The expected value of a random variable defined on an\ninfinite sample space is defined iff the infinite series in the\ndefinition is absolutely convergent.\n\n\fA roll of the dice\u2026\nExample: Let X be the number that comes up when a die is\nrolled. What is the expected value of X?\n\nSolution:\nl 6 possible outcomes: 1, 2, 3, 4, 5, 6\nl Each outcomes occurs with the probability 1/6\nl E(X) = 1/6 + 2/6 + 3/6 + 4/6 + 5/6 + 6/6\nl\n= 21/6\nl\n= 7/2\n\n\fA flip of the coin\u2026\nExample: A fair coin is flipped three times. Let S be the sample\nspace of the eight possible outcomes, and X be the random\nvariable that assigns to an outcome the number of heads in that\noutcome. What is the expected value of X?\n\nSolution:\nl Since coin flips are independent, each outcome is equally likely\nl E(X) = 1/8[X(HHH) + X(HHT) + X(HTH) + X(THH) + X(TTH)\n+ X(THT) + X(HTT) + X(TTT)]\nl\n= 1/8[3 + 2 + 2 + 2 + 1 + 1 + 1 + 0]\nl\n= 12/8\nl\n= 3/2\n\n\fIf S is large, the definition of expected value can be\ndifficult to use directly\nDefinition: If X is a random variable and p(X=r) is the probability\nthat X = r (i.e., p(X=r) = \u2211s\u2208S,X(s)=r p(s)), then\n\ud835\udc38 \ud835\udc4b = % \ud835\udc5d \ud835\udc4b=\ud835\udc5f \ud835\udc5f\n$\u2208% #\n\nEach value of X\u2026\n\n\u2026 is weighted by its probability of\noccurrence.\n\nProof:\nl Suppose that X is a random variable ranging over S\nl Note that p(X=r) is the probability that X takes the value r\nl This means that p(X=r) is the sum of the probabilities of the outcomes s\u2208S\nsuch that X(s) = r\nl It thus follows that \ud835\udc38 \ud835\udc4b = \u2211!\u2208# $ \ud835\udc5d \ud835\udc4b = \ud835\udc5f \ud835\udc5f \u274f\n\n\fRolling two dice\nExample: Let X be the sum of the numbers that appear when a pair of fair\ndice is rolled. What is the expected value of X?\n\nRecall from last week:\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n\nX(1,1) = 2\nX(1,2) = X(2, 1) = 3\nX(1,3) = X(2,2) = X(3,1) = 4\nX(1,4) = X(2,3) = X(3,2) = X(4,1) = 5\nX(1,5) = X(2,4) = X(3,3) = X(4,2) = X(5,1) = 6\nX(1,6) = X(2,5) = X(3,4) = X(4,3) = X(5,2) = X(6,1) = 7\nX(2,6) = X(3,5) = X(4,4) = X(5,3) = X(6,2) = 8\nX(3,6) = X(4,5) = X(5,4) = X(6,3) = 9\nX(4,6) = X(5,5) = X(6,4) = 10\nX(5,6) = X(6,5) = 11\nX(6,6) = 12\n\np(X=2) = 1/36\np(X=3) = 2/36 = 1/18\np(X=4) = 3/36 = 1/12\np(X=5) = 4/36 = 1/9\np(X=6) = 5/36\np(X=7) = 6/36 = 1/6\np(X=8) = 5/36\np(X=9) = 4/36 = 1/9\np(X=10) = 3/36 = 1/12\np(X=11) = 2/36 = 1/18\np(X=12) = 1/36\n\nSo we have that:\nl E(X) = 2(1/36) + 3(1/18) + 4(1/12) + 5(1/9) + 6(5/36) + 7(1/6) + 8(5/36)\n+ 9(1/9) + 10(1/12) + 11(1/18) + 12(1/36)\nl\n=7\n\n\fWe can apply this formula to reason about\nBernoulli trials!\nTheorem: The expected number of successes when n independent\nBernoulli trials are performed, in which p is the probability of\nsuccess, is np.\nThe proof of this theorem is straightforward (cf. Sec 7.4 of the text)\nBut let\u2019s think about it intuitively\u2026\nl 6 coin flips, how many will be heads?\nl Bernoulli trials: n = 6, p = 0.5, q = 0.5\nl Intuitively, you\u2019d expect half of your flips to be heads\nl Mathematically, 6 * 0.5 = 3\n\n\fExpected values are linear!\nTheorem: If X1, X2, \u2026, Xn are random variables on S and if a and b\nare real numbers, then\n1.\n2.\n\nE(X1 + X2 + \u2026 + Xn) = E(X1) + E(X2) + \u2026 + E(Xn)\nE(aX + b) = aE(X) + b\n\nProof:\nl\nl\nl\nl\nl\n\nTo prove the first result for n=2, note that\nE(X1 + X2) = \u2211s\u2208S p(s)(X1(s) + X2(s))\nDef\u2019n of E(X)\n= \u2211s\u2208S p(s)X1(s) + \u2211s\u2208S p(s)X2(s)\nProperty of summations\n= E(X1) + E(X2)\nDef\u2019n of E(X)\nThe case with n variables is an easy proof by induction\n\nl\nl\nl\nl\nl\n\nTo prove the second property, note that\nE(aX + b) = \u2211s\u2208S p(s)(aX(s) + b)\n= \u2211s\u2208S p(s)aX(s) + \u2211s\u2208S p(s)b\n= a\u2211s\u2208S p(s)X(s) + b\u2211s\u2208S p(s)\n= aE(X) + b \u274f\n\nDef\u2019n of E(X)\nProperty of summations\nProperty of summations\nDef\u2019n of E(X), \u2211s\u2208S p(s) = 1\n\n\fDice, revisited\nExample: What is the expected value of the sum of the numbers\nthat appear when two fair dice are rolled?\n\nSolution:\nl Let X1 and X2 be random variables indicating the value on the first and\nsecond die, respectively\nl Want to calculate E(X1+X2)\nl By the previous theorem, we have that E(X1+X2) = E(X1)+E(X2)\nl From earlier in lecture, we know that E(X1) = E(X2) = 7/2\nl So, E(X1+X2) = 7/2 + 7/2 = 7\n\nNote: This agrees with the (more complicated)\ncalculation that we made earlier in lecture.\n\n\fIn-class exercises\nTop Hat\n\n\fSometimes we need more information than the\nexpected value can give us\nThe expected value of a random variable doesn\u2019t tell\nus the whole story\u2026\n\np(X(s)=r)\n\np(X(s)=r)\nX(s)\n\nX(s)\n\np(X(s)=r)\nX(s)\n\n\fThe variance of a random variable gives us information\nabout how wide it is spread\nDefinition: The variance of a random variable \ud835\udc4b on a\nsample space \ud835\udc46 is defined as:\n\ud835\udc49 \ud835\udc4b = % \ud835\udc4b \ud835\udc60 \u2212\ud835\udc38 \ud835\udc4b\n\n$\n\n\ud835\udc5d \ud835\udc60\n\n!\u2208#\nSquared difference from\nexpected value\n\nWeighted by probability of\noccurrence\n\nDefinition: The standard deviation of a random\nvariable \ud835\udc4b on a sample space \ud835\udc46 is defined as\n\n\ud835\udc49 \ud835\udc4b .\n\n\fVariance of a die\nExample: A fair die is rolled. What is the variance of the random\nvariable X representing the face that appears?\n\nSolution:\nl Recall that E(X) = 3.5\nl X(1) = 1, p(1)=1/6\nl X(2) = 2, p(2)=1/6\nl X(3) = 3, p(3)=1/6\nl X(4) = 4, p(4)=1/6\nl X(5) = 5, p(5)=1/6\nl X(6) = 6, p(6)=1/6\nl Thus, V(X) = (1/6)(1\u22123.5)2 + (1/6)(2\u22123.5)2 + (1/6)(3\u22123.5)2 +\n(1/6)(4\u22123.5)2 + (1/6)(5\u22123.5)2 + (1/6)(6\u22123.5)2\nl V(X) = 6.25/6 + 2.25/6 + 0.25/6 + 0.25/6 + 2.25/6 + 6.25/6\nl V(X) = 17.5/6 \u2248 2.92\n\n\fVariance: The short form\nTheorem: If X is a random variable on a sample space S,\nthen \ud835\udc49 \ud835\udc4b = \ud835\udc38 \ud835\udc4b $ \u2212 \ud835\udc38 \ud835\udc4b $.\n\nProof:\nl V(X) = \u2211s\u2208S (X(s) \u2013 E(X))2p(s)\nl\n= \u2211s\u2208S X(s)2p(s) \u2013 2E(X)\u2211s\u2208S X(s)p(s) + E(X)2\u2211s\u2208S p(s)\nl\n= E(X2) \u2013 2E(X)E(X) + E(X)2\nl\n= E(X2) \u2013 E(X)2\n\u274f\n\n\fVariance of a die, revisited\nExample: A fair die is rolled. What is the variance of the random\nvariable X representing the face that appears?\n\nSolution:\nl Recall that E(X) = 3.5\nl X2(1) = 1, p(1)=1/6\nl X2(2) = 4, p(2)=1/6\nl X2(3) = 9, p(3)=1/6\nl X2(4) = 16, p(4)=1/6\nl X2(5) = 25, p(5)=1/6\nl X2(6) = 36, p(6)=1/6\nl Thus, E(X2) = (1/6)(1) + (1/6)(4) + (1/6)(9) + (1/6)(16) + (1/6)(25) +\n(1/6)(36)\nl E(X2) = 1/6 + 4/6 + 9/6 + 16/6 + 25/6 + 36/6 = 91/6\nl V(X) = E(X2) \u2212 E(X)2 = 91/6 \u2212 3.52 \u2248 2.92\n\n\fMultiple Dice\nExample: Two dice are rolled. What is the variance of the\nrandom variable X((j,k)) = 2j, where j is the number\nappearing on the first die and k is the number appearing on\nthe second die.\n\nSolution:\nl V(X) = E(X2) \u2013 E(X)2\nl Note that p(X=m) = 1/6 for m = 2,4,6,8,10,12 and is 0 otherwise\nl E(X) = (2+4+6+8+10+12)/6 = 7\nl E(X2) = (22+42+62+82+102+122)/6 = 182/3\nl So V(X) = 182/3 \u2013 49 = 35/3\n\n\fVariance of a Bernoulli Distribution\nExample: What is the variance of random variable X\nwith X(t)=1 if a Bernoulli trial is a success and X(t)=0\notherwise? Assume that the probability of success is p.\n\u00a77.4 also proves that the variance\nof n Bernoulli trials is npq\nSolution:\nl Note that X takes only the values 0 and 1\nl Hence, X(t) = X2(t)\nl V(X) = E(X2) \u2013 E(X)2\nl\n= p \u2013 p2\nl\n= p(1-p)\nl\n= pq\nThis tells us that the variance of\nANY Bernoulli distribution is pq!\n\n\fVariance of n Bernoulli trials\nExample: A fair die is rolled 5 times. Let X be the\nrandom variable that assigns to an outcome the\nnumber of throws less than 3. What is the variance of\nX?\n\nSolution:\nl n = 5, p = 1/3, q = 2/3\nl V(X) = npq = 5 * 1/3 * 2/3 \u2248 1.11\n\n\fIn-class exercises\nTop Hat\n\n\fFinal Thoughts\nn Analyzing the expected value of a random variable\nallows us to answer a range of interesting questions\nn The variance of a random variable tells us about the\nspread of values that the random variable can take\n\n\f", "label": [[109, 123, "Concept"], [182, 197, "Concept"], [213, 228, "Concept"], [325, 340, "Concept"], [753, 769, "Concept"], [774, 787, "Concept"], [804, 816, "Concept"], [822, 837, "Concept"], [976, 988, "Concept"], [1305, 1319, "Concept"], [1325, 1340, "Concept"], [1357, 1371, "Concept"], [1376, 1387, "Concept"], [3997, 4013, "Concept"], [4028, 4043, "Concept"], [4064, 4075, "Concept"], [4076, 4092, "Concept"], [4441, 4456, "Concept"], [5882, 5890, "Concept"], [5977, 5985, "Concept"], [6169, 6187, "Concept"], [6254, 6262, "Concept"], [8687, 8701, "Concept"], [8707, 8722, "Concept"], [8782, 8790, "Concept"]], "Comments": []}