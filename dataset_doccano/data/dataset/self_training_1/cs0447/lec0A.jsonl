{"id": 180, "segment": "self_training_1", "course": "cs0447", "lec": "lec0A", "text": "#A\nCS 0447\nIntroduction to\nComputer Programming\n\nFractions and\nFloating Point\nOriginal slides by: Jarrett Billingsley\nModified with bits from: Bruce Childers,\nDavid Wilkinson\n\nLu\u00eds Oliveira\n\nFall 2020\n\n\fAnnouncements\n\u25cf Don\u2019t forget the points for discussing your project solution with your TA are expiring\n\n2\n\n\fFractional Binary\n\n3\n\n\fFractional numbers\n\u25cf Up to this point we have been working with integer numbers.\no Unsigned and signed!\n\n2019\n2 0 1 9.320\n\n\u25cf However, Real world numbers are\u2026 Real numbers. Like so:\n\n\u25cf That create new challenges!\no Let\u2019s start by taking a look at them.\n\n4\n\n\fJust a fraction of a number\n\u25cf The numbers we use are written positionally: the position of a digit within the\nnumber has a meaning.\n\u25cf What about when the numbers go over the decimal point?\n\n?\n2 0 1 9. 3 2 0\n\n1000s\n\n100s\n\n10s\n\n1s\n\n10ths 100ths 1000ths\n\n103\n\n102\n\n101\n\n100\n\n10-1\n\n10-2\n\n10-3\n\n5\n\n\fA fraction of a bit?\n\u25cf Binary is the same!\n\u25cf Just replace 10s with 2s.\n\n0 1 1 0 .1 1 0 1\n23\n8s\n\n22\n4s\n\n21\n2s\n\n20\n1s\n\n2-1\n2ths\n\n?\n\n2-2\n4ths\n\n2-3\n8ths\n\n2-4\n16ths\n\n6\n\n\fTo convert into decimal, just add stuff\n\n0 1 1 0 .1 1 0 1=\n23\n\n22\n\n21\n\n20\n\n0\u00d78+\n1\u00d74+\n1\u00d72+\n0\u00d71+\n1 \u00d7 .5 +\n1 \u00d7 .25 +\n0 \u00d7 .125 +\n1 \u00d7 .0625\n\n2-1\n\n2-2\n\n2-3\n\n2-4\n\n= 6.812510\n\n7\n\n\fFrom decimal to binary? Tricky?\n\n6\u00f7210 = 3R0\n\n6.8125 10\n\n3\u00f7210 = 1R1\n\n1 1 0. 1101\n\n0.812510\nx\n2\n1.6250\n\nMSB\n\n0.625010\nx\n2\n1.2500\n0.250010\nx\n2\n0.5000\n0.500010\nx\n2\n1.0000\n\nLSB\n8\n\n\fSo, it\u2019s easy right? Well\u2026\n\nWhat about: 0.1 10\n\n0.110\nx 2\n0.2\n0.210\nx2\n0.4\n\n0. 0001\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n9\n\n\fSo, it\u2019s easy right? Well\u2026\u2026\n\nWhat about: 0.1 10\n\n0. 0001\n\n1001\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n10\n\n\fSo, it\u2019s easy right? Well\u2026\u2026\u2026\n\nWhat about: 0.1 10\n\n0. 0001\n1001\n10\n0\n1\n...\n\n0.610\nx 2\n1.2\n\n0.610\nx 2\n1.2\n\n0.110\nx 2\n0.2\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.210\nx2\n0.4\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.410\nx 2\n0.8\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n\n0.810\nx 2\n1.6\n11\n\n\fHow much is it worth?\n\n\u2022Well, it depends on where you stop!\n\n0.0001 2\n\n= 0.0625\n\n0.00011001 2\n\n= 0.0976\u2026\n\n0.000110011001 2 = 0.0998\u2026\n12\n\n\fLimited space!\n\u25cf How much should we store?\no We have 32-bit registers, so 32-bits?\n\u25aa Let\u2019s say we do!\n\u25cf How many bits are used to store the integer part?\n\u25cf How many bits are used to store the fractional part?\n\n\u25cf What are the tradeoffs?\n\n13\n\n\fA rising tide\n\u25cf Maybe half-and-half? 16.16 number looks like this:\n\n0011 0000 0101 1010.1000 0000 1111 1111\nbinary point\nthe largest (signed) value we\nthe smallest fraction we can\ncan represent is +32767.999\nrepresent is 1/65536\nWhat if we place the binary point to the left\u2026\n\n0011.0000 0101 1010 1000 0000 1111 1111\n\u2026we can get much higher accuracy near 0\u2026\n\n\u2026but if we place the binary point to the right\u2026\n\n0011 0000 0101 1010 1000 0000.1111 1111\n\u2026then we trade off accuracy for range further away from 0.\n\n14\n\n\fMind the point\n\u25cf In this representation we assume that the lowest n digits are the decimal places.\n\n$12.34\n+$10.81\n$23.15\n\n1234\n+1081\n2315\n\nthis is called fixed-point\nrepresentation\nAnd it\u2019s a bitfield :D\n\n15\n\n\fMove the point\n\u25cf What if we could float the point around?\no Enter scientific notation: The number -0.0039 can be represented:\n\n-0.39\n-3.9\n\n\u00d7 10-2\n\u00d7 10-3\n\n\u25cf These are both representing the same number, but we need to move the decimal\npoint according to the power of ten represented.\n\n\u25cf The bottom example is in normalized scientific notation.\no There is only one non-zero digit to the left of the point\n\u25cf Because the decimal point can be moved, we call this representation\n\nFloating point\n\n16\n\n\fFloating-point number\nrepresentation\n\n17\n\n\fThis could be a whole unit itself...\n\u25cf floating-point arithmetic is COMPLEX STUFF\n\n\u25cf However...\no it's good to have an understanding of why limitations exist\no it's good to have an appreciation of how complex this is... and how much better\nthings are now than they were in the 1970s and 1980s\no It\u2019s good to know things do not behave as expected when using float and double!\n\n18\n\n\fBinary numbers using IEEE 754\n\u25cf est'd 1985, updated as recently as 2008\n\u25cf standard for floating-point representation and arithmetic that virtually every CPU\nnow uses\n\u25cf floating-point representation is based around scientific notation\n\n1348 = +1.348 \u00d7 10+3\n-0.0039 = -3.9\n\u00d7 10-3\n-1440000 = -1.44 \u00d7 10+6\nsign significand\n\nexponent\n\n19\n\n\fBinary Scientific Notation\n\u25cf scientific notation works equally well in any other base!\no (below uses base-10 exponents for clarity)\n\n+1001 0101 = +1.001 0101 \u00d7 2+7\n-0.001 010 = -1.010\n\u00d7 2-3\n-1001 0000 0000 0000 = -1.001\n\u00d7 2+15\nwhat do you notice about the digit\nbefore the binary point using\nnormalized representation?\n\n(-1)s x 1.f \u00d7 2exp s \u2013 sign\nf \u2013 fraction\n1.f \u2013 significand\nexp \u2013 exponent\n\n20\n\n\fIEEE 754 Single-precision\n\u25cf Known as float in C/C++/Java etc., 32-bit float format\n\u25cf 1 bit for sign, 8 bits for the exponent, 23 bits for the fraction\n\n\u25cf Tradeoff:\no More accuracy \u2794 More fraction bits\no More range \u2794 More exponent bits\n\u25cf Every design has tradeoffs \u00af\\_(\u30c4)_/\u00af\n\nillustration from user Stannered on Wikimedia Commons\n\n21\n\n\fIEEE 754 Single-precision\n\u25cf Known as float in C/C++/Java etc., 32-bit float format\n\u25cf 1 bit for sign, 8 bits for the exponent, 23 bits for the fraction\n\n\u25cf The fraction field only stores the digits after the binary point\n\u25cf The 1 before the binary point is implicit!\no This is called normalized representation\no In effect this gives us a 24-bit significand\n\u25cf The significand of floating-point numbers is in sign-magnitude!\no Do you remember the downside(s)?\n\nillustration from user Stannered on Wikimedia Commons\n\n22\n\n\fThe exponent field\n\u25cf the exponent field is 8 bits, and can hold positive or negative exponents, but... it\ndoesn't use S-M, 1's, or 2's complement.\n\u25cf it uses something called biased notation.\no biased representation = exponent + bias constant\no single-precision floats use a bias constant of 127\n\nexp + 127 => Biased\n\n-127 + 127 => 0\n-10 + 127 => 117\n34 + 127 => 161\n\n\u25cf the exponent can range from -126 to +127 (1 to 254 biased)\no 0 and 255 are reserved!\n\u25cf why'd they do this?\no You can sort floats with integer comparisons!\n23\n\n\fBinary Scientific Notation (revisited)\n\u25cf Our previous numbers are actually\n\nbias = 127\n\n+1.001 0101 \u00d7 2+7\n\nsign = 0 (positive number!)\nBiased exponent = exp + 127 = 7 + 127 = 134\n= 10000110\nfraction = 0010101 (ignore the \u201c1.\u201d)\ns\n\nE\n\nf\n\n0 10000110 00101010000000000\u2026000\n(-1)0 x 1.001 0101 \u00d7 2134-127\n\n24\n\n\fBinary Scientific Notation (revisited)\n\u25cf Our previous numbers are actually\n\nbias = 127\n\n-1.010 \u00d7 2-3 =\n\nsign = 1 (negative number!)\nBiased exponent = exp + 127 = -3 + 127 = 124\n= 01111100\nfraction = 010 (ignore the \u201c1.\u201d)\ns\n\nE\n\nf\n\n1 01111100 01000000000000000\u2026000\n(-1)1 x 1.010\n\n\u00d7 2124-127\n\n25\n\n\fBinary Scientific Notation (revisited)\n\u25cf Our previous numbers are actually\n\nbias = 127\n\n-1.001 \u00d7 2+15=\nsign = ?\nBiased exponent = ?\nfraction = ?\ns\n\nE\n\nf\n\n?\n\n?\n\n?\n26\n\n\fCheck it on C++ (there are online tools for this!)\n#include <iostream>\n#include<bitset>\nint main() {\n// This is the number from the previous slide\nfloat x {-0b1001000000000000};\n// C++ does not shift floats :(\n// This is C++-whispering: it allows me to shift the bits :)\nint num {*(int*)&x};\n// Extract the fields!\nstd::bitset <1> sign = (num >> 31) & 0x1 ;\nstd::bitset <8> biased_exp = (num >> 23) & 0xFF;\nstd::bitset <23> frac = (num >> 0) & 0x7FFFFF;\n\n}\n\n// Now let\u2019s print\nstd::cout << \"sign: \" << sign << std::endl;\nstd::cout << \"biased exponent: \" << biased_exp << std::endl;\nstd::cout << \"frac: \" << frac << std::endl;\nreturn 0;\n\nTry it in: https://repl.it/languages/cpp\n\n27\n\n\fEncoding a number as a float\nYou have an number, like -12.5937510\n1. Convert it to binary.\nInteger part: 11002\nFractional part: 0.100112\n2. Write it in scientific notation:\n1100.100112 x 20\n3. Normalize it:\n1.100100112 x 23\n\n0.5937510\nx\n2\n1.18750\n\nMSB\n\n0.1875010\nx\n2\n0.37500\n\n0.3750010\nx\n2\n0.75000\n0.7500010\nx\n2\n1.50000\n0.5000010\nx\n2\n1.00000\n\nLSB\n\n28\n\n\fEncoding a number as a float\nYou have an number, like -12.5937510\n1. Convert it to binary.\nInteger part: 11002\nFractional part: 0.100112\n2. Write it in scientific notation:\n1100.100112 x 20\n3. Normalize it:\n1.100100112 x 23\n4. Calculate biased exponent\n+3 + 127 = 13010 = 100000102\n\n0xC1498000\n\ns\n\nexponent\n\nfraction\n\n1 10000010 10010011000000000\u2026000\n29\n\n\fAdding floating point numbers\n\n1.11 \u00d7 20 + 1.00 \u00d7 2-2\n\u25cf Step 1 \u2013 Make both exponents the same\n\n1.11 \u00d7 20 + 0.01 \u00d7 20\n\n\u25cf Step 2 \u2013 Add the significands\n\n1.11 \u00d7 20 + 0.01 \u00d7 20 = 10.00 \u00d7 20\n\n\u25cf Step 3 \u2013 Normalize the result\n\n10.00 \u00d7 20 = 1.000 \u00d7 21\n30\n\n\fMultiply floating point numbers\n\n1.11 \u00d7 20 x 1.01 \u00d7 2-2\n\u25cf Step 1 \u2013 Add the exponents\n\n0 + (-2) = [0+127]+[-2+127] =\n[127] + [125] \u2013 127 = [125] = -2\n\u25cf Step 2 \u2013 Multiply the significands\n\n1.11 x 1.01 = 10.0011\n\u25cf Step 3 \u2013 Normalize the result\n\n10.0011 \u00d7 2-2 = 1.00011 \u00d7 2-1\n31\n\n\fDivide floating point numbers\n\n1.001 \u00d7 20 / 1.1 \u00d7 2-2\n\u25cf Step 1 \u2013 Subtract the exponents\n\n0 - (-2) = [0+127]-[-2+127] =\n[127] - [125] + 127 = [129] = 2\n\n\u25cf Step 2 \u2013 Divide the significands\n\n1.001 / 1.1 = 0.11\n\u25cf Step 3 \u2013 Normalize the result\n\n0.11 \u00d7 22 = 1.1 \u00d7 21\n32\n\n\fOther formats\n\u25cf the most common other format is double-precision (C/C++/Java double), which uses\nan 11-bit exponent and 52-bit fraction\n\n\u25cf GPUs have driven the creation of a half-precision 16-bit floatingpoint format. it's adorable\n\n1023 bias\n15 bias\n\nboth illustrations from user Codekaizen on Wikimedia Commons\n\n33\n\n\fSpecial cases\n\u25cf IEEE 754 can represent data outside of the norm.\no Zero! How do you do that with normalized numbers?\no +/- Infinity\no NaN (Not a number). E.g. when you divide zero by zero.\no Other denormalized number: Squeeze the most out of our bits!\n\u25aa E.g.: 0.00000000000000000000001 x 2-127\nSingle precision\n\nDouble precision\n\nMeaning\n\nExponent\n\nFraction\n\nExponent\n\nFraction\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n!=0\n\n0\n\n!=0\n\nNumber is denormalized\n\n255\n\n0\n\n2047\n\n0\n\nInfinity (sign-bit defines + or -)\n\n255\n\n!=0\n\n2047\n\n!=0\n\nNaN (Not a Number)\n34\n\n\fCheck out this cool thing in MARS\n\u25cf go to Tools > Floating Point Representation\n\u25cf Try it out!\n\n35\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}