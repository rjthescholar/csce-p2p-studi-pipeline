{"id": 169, "segment": "unlabeled", "course": "cs0441", "lec": "lec22", "text": "Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #22: Bayes\u2019 Theorem\n\nBased on materials developed by Dr. Adam Lee\n\n\fToday\u2019s Topics\nn Bayes\u2019 Theorem\nl What do we compute conditional probabilities with\nincomplete information?\n\n\fConditional Probability\nDefinition: Let E and F be events with p(F) > 0. The conditional\nprobability of E given F, denoted p(E | F), is defined as:\n\n\ud835\udc5d \ud835\udc38\u2229\ud835\udc39\n\ud835\udc5d \ud835\udc38 \ud835\udc39 =\n\ud835\udc5d \ud835\udc39\n\nIntuition:\nl Think of the event F as reducing the sample space that can be considered\nl The numerator looks at the likelihood of the outcomes in E that overlap\nthose in F\nl The denominator accounts for the reduction in sample size indicated by our\nprior knowledge that F has occurred\n\n\fBayes\u2019 Theorem\nBayes\u2019 Theorem allows us to relate the conditional and marginal\nprobabilities of two random events.\n\n?\nIn English: Bayes\u2019 Theorem will help us assess the probability\nthat an event occurred given only partial evidence.\nDoesn\u2019t our formula for conditional probability do this already?\n\nWe can\u2019t always use this\nformula directly\u2026\n\n\fA Motivating Example\nSuppose that a certain drug test correctly identifies a person who\nuse the drug as testing positive 99% of the time, and will correctly\nidentify a non-user as testing negative 99% of the time. If a\ncompany suspects that 0.5% of its employees are users of the\ndrug, what is the probability that an employee that tests positive\nfor this drug is actually a user?\n\nQuestion: Can we use our simple\nconditional probability formula?\n\ud835\udc5d \ud835\udc38\ud835\udc39 =\n\nX is a user\n\n!(#\u2229%)\n!(%)\n\nX tested positive\n\n\fThe 1,000 foot view\u2026\nIn situations like those on the last slide, Bayes\u2019 theorem can help!\nEssentially, Bayes\u2019 theorem will allow us to calculate P(E|F)\nassuming that we know (or can derive):\nl \ud835\udc43 \ud835\udc38\nl \ud835\udc43 \ud835\udc39\ud835\udc38\n&\nl \ud835\udc43(\ud835\udc39|\ud835\udc38)\n\nProbability that X is a user\nTest success rate\nTest false positive rate\n\nProbability that X is a user of the\ndrug, given a positive test\n\nReturning to our earlier example:\nl Let E = \u201cPerson X is a user of the drug\u201d\nl Let F = \u201cPerson X tested positive for the drug\u201d\n\nIt sounds like Bayes\u2019 Theorem could help in this case\u2026\n\n\fNew Notation\nTo simplify expressions, we will use the notation EC to\ndenote the complementary event of E\nThat is:\n\nC\nE=E\n\n\fA Simple Example\nWe have two boxes. The first contains two green balls and seven\nred balls. The second contains four green balls and three red\nballs. Bob selects a ball by first choosing a box at random. He\nthen selects one of the balls from that box at random. If Bob has\nselected a red ball, what is the probability that he took it from\nthe first box?\n\n1\n\n2\n\n\fPicking the problem apart\u2026\nFirst, let\u2019s define a few events relevant to this problem:\nl Let E = Bob has chosen a red ball\nl By definition EC = Bob has chosen a green ball\nl Let F = Bob chose his ball from this first box\nl Therefore, FC = Bob chose his ball from the second box\n\nWe want to find the probability that Bob chose from the first box,\ngiven that he picked a red ball. That is, we want p(F|E).\nGoal: Given that p(F|E) = p(F \u2229 E)/p(E), use what we know to\nderive p(F \u2229 E) and p(E).\n\n\fWhat do we know?\nWe have two boxes. The first contains two green balls and seven red balls.\nThe second contains four green balls and three red balls. Bob selects a ball\nby first choosing a box at random. He then selects one of the balls from that\nbox at random. If Bob has selected a red ball, what is the probability that he\ntook it from the first box?\nStatement: Bob selects a ball by first choosing a box at random\nl Bob is equally likely to choose the first box, or the second box\nl p(F) = p(FC) = 1/2\n\nStatement: The first contains two green balls and seven red balls\nl The first box has nine balls, seven of which are red\nl p(E|F) = 7/9\n\nStatement: The second contains four green balls and three red balls\nl The second box contains seven balls, three of which are red\nl p(E|FC) = 3/7\n\n\fNow, for a little algebra\u2026\nThe end goal: Compute p(F|E) = p(F \u2229 E)/p(E)\nNote that p(E|F) = p(E \u2229 F)/p(F)\nl If we multiply by p(F), we get p(E \u2229 F) = p(E|F) p(F)\nl Further, we know that p(E|F) = 7/9 and p(F) = 1/2\nl So p(E \u2229 F) = 7/9 \u00d7 1/2 = 7/18\n\nRecall:\n\u2022p(F) = p(FC) = 1/2\n\u2022p(E|F) = 7/9\n\u2022p(E|FC) = 3/7\n\nSimilarly, p(E \u2229 FC) = p(E|FC) p(FC) = 3/7 \u00d7 1/2 = 3/14\nObservation: E = (E \u2229 F) \u222a (E \u2229 FC)\nl This means that p(E) = p(E \u2229 F) + p(E \u2229 FC)\nl\nl\nl\nl\n\n= 7/18 + 3/14\n= 49/126 + 27/126\n= 76/126\n= 38/63\n\n\fDenouement\nThe end goal: Compute p(F|E) = p(F \u2229 E)/p(E)\nSo, p(F|E) = (7/18) / (38/63) \u2248 0.645\n\nRecall:\n\u2022p(F) = p(FC) = 1/2\n\u2022p(E|F) = 7/9\n\u2022p(E|FC) = 3/7\n\u2022p(E \u2229 F) = 7/18\n\u2022p(E) = 38/63\n\nHow did we get here?\n1.\n2.\n\nExtract what we could from the problem definition itself\nRearrange terms to derive p(F \u2229 E) and p(E)\n\n3.\n\nUse our trusty definition of conditional probability to do the rest!\n\n\fThe reasoning that we used in the last problem\nessentially derives Bayes\u2019 Theorem for us!\nBayes\u2019 Theorem: Suppose that E and F are events from some\nsample space S such that p(E) \u2260 0 and p(F) \u2260 0. Then:\n\ud835\udc5d \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39\n\ud835\udc5d \ud835\udc39 \ud835\udc38 =\n\ud835\udc43 \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39 + \ud835\udc5d \ud835\udc38 \ud835\udc39' \ud835\udc5d \ud835\udc39'\n\nProof:\nl The definition of conditional probability says that\n\u27a3 p(F|E) = p(F \u2229 E)/p(E)\n\u27a3 p(E|F) = p(E \u2229 F)/p(F)\n\nl This means that\n\u27a3 p(E \u2229 F) = p(F|E)p(E)\n\u27a3 p(E \u2229 F) = p(E|F)p(F)\n\nl So p(F|E)p(E) = p(E|F)p(F)\nl Therefore, p(F|E) = p(E|F)p(F)/p(E)\n\n\fProof (continued)\nNote: To finish, we must prove p(E) = p(E | F)p(F) + p(E | FC)p(FC)\nl Observe that E = E \u2229 S\nl\n= E \u2229 (F \u222a FC)\nl\n= (E \u2229 F) \u222a (E \u2229 FC)\nl Note also that (E \u2229 F) and (E \u2229 FC) are disjoint (i.e., no x can be in both F and FC)\nl This means that p(E) = p(E \u2229 F) + p(E \u2229 FC)\nl We already have shown that p(E \u2229 F) = p(E|F)p(F)\nl Further, since p(E | FC) = p(E \u2229 FC)/p(FC), we have that p(E \u2229 FC) = p(E|FC)p(FC)\nl So p(E) = p(E \u2229 F) + p(E \u2229 FC) = p(E|F)p(F) + p(E|FC)p(FC)\n\nPutting everything together, we get:\n\ud835\udc5d \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39\n\ud835\udc5d \ud835\udc39 \ud835\udc38 =\n\ud835\udc43 \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39 + \ud835\udc5d \ud835\udc38 \ud835\udc39' \ud835\udc5d \ud835\udc39'\n\u274f\n\n\fAnd why is this useful?\nIn a nutshell, Bayes\u2019 Theorem is useful if you want to find p(F|E),\nbut you don\u2019t know p(E \u2229 F) or p(E).\n\n\fHere\u2019s a general solution tactic\nStep 1: Identify the independent events that are being\ninvestigated. For example:\nl F = Bob chooses the first box, FC = Bob chooses the second box\nl E = Bob chooses a red ball, EC = Bob chooses a green ball\n\nStep 2: Record the probabilities identified in the problem\nstatement. For example:\nl p(F) = p(FC) = 1/2\nl p(E|F) = 7/9\nl p(E|FC) = 3/7\n\nStep 3: Plug into Bayes\u2019 formula and solve\n\n\fExample: Pants and Skirts\nSuppose there is a co-ed school having 60% boys and 40% girls as\nstudents. The girl students wear pants or skirts in equal numbers;\nthe boys all wear pants. An observer sees a (random) student\nfrom a distance; all they can see is that this student is wearing\npants. What is the probability this student is a girl?\nStep 1: Set up events\nl\nl\nl\nl\n\nE = X is wearing pants\nEC = X is wearing a skirt\nF = X is a girl\nFC = X is a boy\n\nStep 2: Extract probabilities from problem definition\nl\nl\nl\nl\n\np(F) = 0.4\np(FC) = 0.6\np(E|F) = p(EC|F) = 0.5\np(E|FC) = 1\n\n\fPants and Skirts (continued)\n\ud835\udc5d \ud835\udc39 \ud835\udc38 =\n\n\ud835\udc5d \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39\n\ud835\udc43 \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39 + \ud835\udc5d \ud835\udc38 \ud835\udc39! \ud835\udc5d \ud835\udc39!\n\nStep 3: Plug in to Bayes\u2019 Theorem\n\nRecall:\n\u2022 p(F) = 0.4\n\u2022 p(FC) = 0.6\n\u2022 p(E|F) = p(EC|F) = 0.5\n\u2022 p(E|FC) = 1\n\nl p(F|E) = (0.5 \u00d7 0.4)/(0.5 \u00d7 0.4 + 1 \u00d7 0.6)\nl\n= 0.2/0.8\nl\n= 1/4\n\nConclusion: There is a 25% chance that the person seen was a\ngirl, given that they were wearing pants.\n\n\fDrug screening, revisited\nSuppose that a certain drug test correctly identifies a person who\nuses the drug as testing positive 99% of the time, and will\ncorrectly identify a non-user as testing negative 99% of the time.\nIf a company suspects that 0.5% of its employees are users of the\ndrug, what is the probability that an employee that tests positive\nfor this drug is actually a user?\nStep 1: Set up events\nl\nl\nl\nl\n\nF = X is a user\nFC = X is not a user\nE = X tests positive for the drug\nEC = X tests negative for the drug\n\nStep 2: Extract probabilities from problem definition\nl p(F) = 0.005\nl p(FC) = 0.995\nl p(E|F) = 0.99\nl p(E|FC) = 0.01\n\n\fDrug screening (continued)\n\ud835\udc5d \ud835\udc39 \ud835\udc38 =\n\n\ud835\udc5d \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39\n\ud835\udc43 \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39 + \ud835\udc5d \ud835\udc38 \ud835\udc39! \ud835\udc5d \ud835\udc39!\n\nStep 3: Plug in to Bayes\u2019 Theorem\n\nRecall:\n\u2022 p(F) = 0.005\n\u2022 p(FC) = 0.995\n\u2022 p(E|F) = 0.99\n\u2022 p(E|FC) = 0.01\n\nl p(F|E) = (0.99 \u00d7 0.005)/(0.99 \u00d7 0.005 + 0.01 \u00d7 0.995)\nl\n= 0.3322\n\nConclusion: If an employee tests positive for the drug, there is\nonly a 33% chance that they are actually a user!\n\n\fIn-class exercises\nSuppose that 1 person in 100,000 has a particular rare disease. A\ndiagnostic test is correct 99% of the time when given to someone\nwith the disease, and is correct 99.5% of the time when given to\nsomeone without the disease.\nProblem 1: Calculate the probability that someone who tests\npositive for the disease actually has it.\nProblem 2: Calculate the probability that someone who tests\nnegative for the disease does not have the disease.\n\n\fApplication: Spam filtering\nDefinition: Spam is unsolicited bulk email\nI didn\u2019t ask for it, I probably\ndon\u2019t want it\n\nSent to lots of people\u2026\n\nIn recent years, spam has become increasingly\nproblematic. For example, in 2015, spam accounted\nfor ~50% of all email messages sent.\nTo combat this problem, people have developed spam\nfilters based on Bayes\u2019 theorem!\n\n\fHow does a Bayesian spam filter work?\nEssentially, these filters determine the probability that a message\nis spam, given that it contains certain keywords.\n\ud835\udc5d \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39\n\ud835\udc5d \ud835\udc39 \ud835\udc38 =\n\ud835\udc43 \ud835\udc38 \ud835\udc39 \ud835\udc5d \ud835\udc39 + \ud835\udc5d \ud835\udc38 \ud835\udc39' \ud835\udc5d \ud835\udc39'\nMessage is spam\n\nMessage contains\nquestionable keyword\n\nIn the above equation:\nl p(E|F) = Probability that our keyword occurs in spam messages\nl p(E|FC) = Probability that our keyword occurs in legitimate messages\nl p(F) = Probability that an arbitrary message is spam\nl p(FC) = Probability that an arbitrary message is legitimate\n\nQuestion: How do we derive these parameters?\n\n\fWe can learn these parameters by examining\nhistorical email traces\nImagine that we have a corpus of email messages\u2026\nWe can ask a few intelligent questions to learn the parameters of our\nBayesian filter:\nl How many of these messages do we consider spam?\nl In the spam messages, how often does our keyword appear?\nl In the good messages, how often does our keyword appear?\n\np(F)\np(E|F)\np(E|FC)\n\nAside: This is what happens when you click the \u201cmark as spam\u201d button\nin your email client!\n\nGiven this information, we can apply Bayes\u2019 theorem!\n\n\fFiltering spam using a single keyword\nSuppose that the keyword \u201cRolex\u201d occurs in 250 of 2000 known spam\nmessages, and in 5 of 1000 known good messages. Estimate the\nprobability that an incoming message containing the word \u201cRolex\u201d is\nspam, assuming that it is equally likely that an incoming message is\nspam or not spam. If our threshold for classifying a message as spam\nis 0.9, will we reject this message?\nStep 1: Define events\nl F = message is spam\nl FC = message is good\nl E = message contains the keyword \u201cRolex\u201d\nl EC = message does not contain the keyword \u201cRolex\u201d\n\nStep 2: Gather probabilities from the problem statement\nl p(F) = p(FC)= 0.5\nl p(E|F) = 250/2000 = 0.125\nl p(E|FC) = 5/1000 = 0.005\n\n\fSpam Rolexes (continued)\nRecall:\n\u2022 p(F) = p(FC) = 0.5\n\u2022 p(E|F) = 0.125\n\u2022 p(E|FC) = 0.005\n\nStep 3: Plug in to Bayes\u2019 Theorem\nl p(F|E) = (0.125 \u00d7 0.5)/(0.125 \u00d7 0.5 + 0.005 \u00d7 0.5)\nl\n= 0.125/(0.125 + 0.005)\nl\n\u2248 0.962\n\nConclusion: Since the probability that our message is spam given\nthat it contains the string \u201cRolex\u201d is approximately 0.962 >\n0.9, we will discard the message.\n\n\fProblems with this simple filter\nHow would you choose a single keyword/phrase to use?\nl \u201cAll natural\u201d\nl \u201cNigeria\u201d\nl \u201cClick here\u201d\nl \u2026\n\nUsers get upset if false positives occur, i.e., if legitimate\nmessages are incorrectly classified as spam\nl When was the last time you checked your spam folder?\n\nHow can we fix this?\nl Choose keywords so p(spam | keyword) is very high or very low\nl Filter based on multiple keywords\n\n\fSpecifically, we want to develop a Bayesian filter that tells us\np(F | E1 \u2229 E2)\nFirst, some assumptions\n1. Events E1 and E2 are independent\n2. The events E1|F and E2|F are independent\n3. p(F) = p(FC) = 0.5\n\nBy Bayes\u2019 theorem\n\nNow, let\u2019s derive formula for this p(F | E1 \u2229 E2)\n\ud835\udc5d \ud835\udc39 \ud835\udc38( \u2229 \ud835\udc38) =\n=\n=\n\n\ud835\udc5d \ud835\udc38(\n\n\ud835\udc5d \ud835\udc38( \u2229 \ud835\udc38)\n\n\ud835\udc5d \ud835\udc38( \u2229 \ud835\udc38) \u2223 \ud835\udc39 \ud835\udc5d \ud835\udc39\n\ud835\udc39 \ud835\udc5d \ud835\udc39 + \ud835\udc5d \ud835\udc38( \u2229 \ud835\udc38) \ud835\udc39 ' \ud835\udc5d \ud835\udc39 '\n\n\ud835\udc5d(\ud835\udc38( \u2229 \ud835\udc38) \u2223 \ud835\udc39)\n\ud835\udc5d \ud835\udc38( \u2229 \ud835\udc38) \ud835\udc39 + \ud835\udc5d \ud835\udc38( \u2229 \ud835\udc38) \ud835\udc39 '\n\nAssumption 3\nAssumptions 1 and 2\n\n\ud835\udc5d \ud835\udc38( \ud835\udc39 \ud835\udc5d(\ud835\udc38) \u2223 \ud835\udc39)\n\ud835\udc39 \ud835\udc5d \ud835\udc38) \ud835\udc39 + \ud835\udc5d \ud835\udc38( \ud835\udc39 ' \ud835\udc5d \ud835\udc38) \ud835\udc39 '\n\n\fSpam filtering on two keywords\nSuppose that we train a Bayesian spam filter on a set of 2000 spam\nmessages and 1000 messages that are not spam. The word \u201cstock\u201d\nappears in 400 spam messages and 60 good messages, and the word\n\u201cundervalued\u201d appears in 200 spam messages and 25 good messages.\nEstimate the probability that a message containing the words \u201cstock\u201d\nand \u201cundervalued\u201d is spam. Will we reject this message if our spam\nthreshold is set at 0.9?\nStep 1: Set up events\nl F = message is spam, FC = message is good\nl E1 = message contains the word \u201cstock\u201d\nl E2 = message contains the word \u201cundervalued\u201d\n\nStep 2: Identify probabilities\nl P(E1|F) = 400/2000 = 0.2\nl p(E1|FC) = 60/1000 = 0.06\nl p(E2|F) = 200/2000 = 0.1\nl p(E2|FC) = 25/1000 = 0.025\n\n\fTwo keywords (continued)\n\ud835\udc5d \ud835\udc39 \ud835\udc38! \u2229 \ud835\udc38\" =\n\n\ud835\udc5d \ud835\udc38!\n\n\ud835\udc5d \ud835\udc38! \ud835\udc39 \ud835\udc5d(\ud835\udc38\" \u2223 \ud835\udc39)\n\ud835\udc39 \ud835\udc5d \ud835\udc38\" \ud835\udc39 + \ud835\udc5d \ud835\udc38! \ud835\udc39 # \ud835\udc5d \ud835\udc38\" \ud835\udc39 #\n\nStep 3: Plug in to Bayes\u2019 Theorem\n\nRecall:\n\u2022 p(E1|F) = 0.2\n\u2022 p(E1|FC) = 0.06\n\u2022 p(E2|F) = 0.1\n\u2022 p(E2|FC) = 0.025\n\nl p(F|E1 \u2229 E2) = (0.2 \u00d7 0.1)/(0.2 \u00d7 0.1 + 0.06 \u00d7 0.025)\nl\n= 0.02/(0.02 + 0.0015)\nl\n\u2248 0.9302\n\nConclusion: Since the probability that our message is spam\ngiven that it contains the strings \u201cstock\u201d and\n\u201cundervalued\u201d is \u2248 0.9302 > 0.9, we will reject this\nmessage.\n\n\fIn-class exercises\nProblem 3: A business records incoming emails for 1 week and\ncollects 1,000 spam messages and 400 non-spam messages. The\nword \u201copportunity\u201d appears in 175 spam messages and 20 nonspam messages. Assuming this week's emails were typical\n(including the proportion of spam), should an incoming message\nbe labeled as spam if it contains the word \u201copportunity\u201d and the\nthreshold for rejecting is 0.9?\nProblem 4: Suppose that a Bayesian spam filter is trained on a\nset of 10,000 spam messages and 5,000 messages that are not\nspam. The word \u201cenhancement\u201d appears in 1,500 spam messages\nand 20 non-spam messages, while the word \u201cherbal\u201d appears in\n800 spam messages and 200 non-spam messages. Estimate the\nprobability that a received message containing both the words\n\u201cenhancement\u201d and \u201cherbal\u201d is spam. Here, you may assume that\n50% of emails are spam.\n\n\fFinal Thoughts\nn Conditional probability is very useful\nn Bayes\u2019 theorem\nl Helps us assess conditional probabilities\nl Has a range of important applications\n\nn Next time:\nl Expected values and variance (Section 7.4)\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}