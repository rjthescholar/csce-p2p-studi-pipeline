{"id": 194, "segment": "unlabeled", "course": "cs0449", "lec": "lec14", "text": "14\n\nThe\nMemory\n\nHierarchy\n\nCS/COE 0449\nIntroduction to\nSystems Software\n\nLuis Oliveira\n(with content borrowed from wilkie and Vinicius Petrucci)\n\n\fThis is a Pyramid Scheme\nBut this knowledge is a safe investment.\n\n2\n\n\fWanting Moore and Moore\nProcessors and memory work together but improve at different rates.\nMemory was initially faster than CPU, but its innovation was slower.\n\nThroughput\n\nInnovation starts to slow\n\nThe CPU overtakes\nmemory performance\n\n3\n\n\fCost of DRAM/Disk in 2020\n\u2022 8GiB\n\n$35 - 70\n\n\u2022 1TiB\n\n$50 \u2013 80\n\n\u2022 16GiB\n\n$70 - 100\n\n\u2022 8TiB\n\n$200 - $300\n\n\u2022 32GiB\n\n$140 - 300\n\n\u2022 16TiB\n\n$400 - 500\n\n4\n\n\fThe memory hierarchy\nRegisters\nFaster,\nDenser\nExpensive\n\nL1 Cache\n\nCheaper,\nSlower,\nLarger\n\n(DRAM) Main Memory\n\nL2 Cache\n\nLocal Disk\nDistributed Storage\n(Don\u2019t forget it!) Tape\n\n5\n\n\fThe hierarchy of speed\n\nFaster,\nDenser\nExpensive\n\nCheaper,\nSlower,\nLarger\n\nA \u201ccache\u201d is used to hold\nuseful data closer than\nmain memory to\nimprove speed.\n\nRegisters\nL1 Cache\n\nL2 Cache\nDRAM is simply\ntoo slow\n(DRAM) Main Memory\nLocal Disk\nDistributed Storage\n(Don\u2019t forget it!) Tape\n\n6\n\n\fMemory Hierarchy: Core 2 Duo\nNot drawn to scale\n\nSRAM\n\nDRAM\n\nStatic Random Access Memory\n\nDynamic Random Access Memory\n\n~4 MB\nL2\nunified\ncache\n\nL1\nI-cache\n\n~8 GB\n\n~500 GB\n\nMain\nMemory\n\nDisk\n\n32 KB\nCPU\n\nReg\n\nThroughput: 16 B/cycle\nLatency: 3 cycles\n\nL1\nD-cache\n8 B/cycle\n\n2 B/cycle\n\n1 B/30 cycles\n\n14 cycles\n\n100 cycles\n\nmillions\n\nMiss Penalty\n(latency)\n33x\n\nMiss Penalty\n(latency)\n10,000x\n\n\fMemory Caching\nCache: Another thing us teachers could really use more of.\n\n8\n\n\fExperiment: Scientific Maths\n\nSpring 2019/2020\n\n9\n\n\fPractical Performance\n\u2022 Caching is necessary for the\nutility of computers.\n\u25aa The CPU/Memory gap increases\n(The Memory Wall)\n\n\u2022 In order to actually use these fast\nCPUs, we need to improve the\napparent speed of RAM.\n\n\u201cThat\u2019s a nice CPU you have there\u2026 it\u2019d be\nterrible if something were to happen to it.\u201d\n\n\u25aa Programs use memory a whole lot.\n\u25aa The bottleneck would grind\nperformance to the point where\nCPUs cannot improve.\n10\n\n\fThe problem: data is faaaaar away\n\u2022 Let\u2019s say you want to read a book.\n\u2022 You check it out of the library.\n\u25aa You have to go there.\n\u25aa Find the book.\n\u25aa Maybe take the bus back.\n\u2022 Wait in traffic.\n\n\u2022 Now it sits on your desk.\n\u25aa As long as it is near you, it\u2019s easy to access the\ninformation.\n\u25aa Yet, if you need another book\u2026\n\u2022 You would take the book ALL THE WAY back!\n(bare with me)\n\n11\n\n\fCaching: Keeping things close\n\u2022 Let\u2019s say you want to read a book.\n\u25aa It\u2019s not on your bookshelf.\n\n\u2022 So, still have to check it out of the library.\n\u25aa You gotta go there. Find the book. Etc.\n\u25aa Take the bus back.\n\n\u2022 Now it sits on your desk.\n\u25aa As long as it is near you, it is easy to access the\ninformation.\n\u25aa When we need another book\u2026 we put it aside.\n\u2022 Maybe a bookshelf.\n\n\u25aa The next time we need it, it will be nearby.\n12\n\n\fThe metaphorical cache\n\u2022 The bookshelf is a cache.\n\u25aa It holds information that you might want later.\n\n\u2022 It is [much] smaller than a library, but faster\nto retrieve things.\n\u2022 However, it is small. Placing a new book on\nthe shelf may require taking an old book off.\n\n13\n\n\fMemory cache (CPU)\n\u2022 RAM is the library. It is far away and getting\nstuff from there is slow.\n\u2022 To better handle the performance gap\nbetween the CPU and memory we add a\nsmaller, fast memory near the CPU.\n\u2022 This is the CPU cache.\n\n14\n\n\fData, the journey\n\u2022 When data is requested, the goal is to read\na word into a CPU register.\n\u2022 The CPU first contacts the cache and asks\nif it has a copy.\n\u25aa If it does\u2026 that is a cache hit, and, well, that was\neasy. Just copy that value into the register.\n\n\u2022 If it does not, this is a cache miss.\n\u25aa It will then contact the next component in the\nmemory hierarchy. (RAM)\n\n\u2022 Ram copies the value to cache, and the\ncache copies the value to the register.\n\n15\n\n\fMissing the mark\n\u2022 When the CPU requests memory in an\nempty cache, the data obviously won\u2019t be\navailable locally.\n\u2022 This is a compulsory miss, a \u201cmiss\u201d due to\nthe first access of a block of data.\n\u25aa Also known as a \u201ccold miss.\u201d\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n8\n\n\u2022 These are, as they suggest, completely\nunavoidable.\n\u25aa They always incur the high penalty associated\nwith a memory read.\n\n8\n\n16\n\n\fHitting the target\n\u2022 When the CPU requests memory that\nhappens to already be in the cache, the\ndata is read locally (quickly).\n\u2022 This is a cache hit.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n\u25aa Your best-case scenario.\n\n\u2022 These avoid having to communicate at all\nwith memory.\n\u25aa No penalty taken for reading/writing to\nmemory.\n\u25aa Very cheap in terms of time.\n\n8\n\n8\n\n17\n\n\fA cache half full\u2026\n\u2022 As the CPU requests memory, the cache\nwill fill to satisfy each compulsory miss.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n\u2022 When it fills up completely, it will have no\nfurther room for the next miss.\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n\u2022 On a miss, it requests the data from\nmemory.\n\n4\n\n6\n\nC\n\n1\n\n8\nE\n\n\u25aa Yet, where does it go?? We must remove one.\n\n\u2022 This is a capacity miss. The memory\nrequirements of the program are larger\nthan the cache.\n\n8\n\n18\n\n\fLooking closer\u2026\n\u2022 It is difficult to know what block of data\nto omit from this cache on such a miss.\n\u25aa However we can exploit the common locality\npatterns of programs to improve our cache.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n\u2022 There is temporal locality: accessed data\nis likely to be used again in near future.\n\nA\n\nB\n\nC\n\nD\n\nE\n\n4\n\n6\n\nC\n\n1\n\n8\n\n\u25aa This is what caches generally capture.\n\n\u2022 However, spatial locality is also likely:\ndata is often grouped together.\n\u25aa When we access a struct field, we will often\naccess another which is nearby in memory.\n\n8\n\n19\n\n\fExploring space\u2026\n\u2022 We would like to keep data that is\nadjacent in memory in the cache, together,\nat the same time.\n\u2022 To do this, we \u201chash\u201d the address. This is\nused to determine the cache slot.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n\u25aa Just a fancy way to say: we divide the address\nby the cache size and use the remainder.\n\n\u2022 Every 0th block, 1st block, 2nd block, etc.\n\u2022 The 5th block (in this example) goes to the\nslot, the 6th goes to the slot, and so on.\n8\n\n20\n\n\fDirect and to the point\u2026\n\u2022 Let\u2019s read addresses 3, 4, 5, 6, and 7 (in\nthat order) from memory.\n\u2022 Reading address 8 next incurs a capacity\nmiss, but it evicts the address that is\nfurthest away from the others.\n\u25aa This type of cache is good for programs that\nread through data sequentially.\n\u25aa That is because such programs will always\nremove the least recently used block on a miss,\nas shown here.\n\n\u2022 Because every address has a specific cache\nslot, this is called a direct-mapped cache.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n5\n\n6\n\n7\n\n3\n8\n\n4\n\n8\n\n21\n\n\fMissing your connection\u2026\n\u2022 Let\u2019s consider an antagonist pattern.\n\u25aa What is the worst case for this cache?\n\n\u2022 If we read every 5th address in our memory\nin order, we would overwhelm our directmapped cache.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\n\u25aa Let\u2019s access 0, 5, A in that order.\n\n\u2022 Accessing address 0 is a compulsory miss.\n\nA\n5\n0\n\n\u25aa Address 5, however, is a miss.\n\u25aa But our cache isn\u2019t full!!\n\n\u2022 A miss that occurs even though your cache\ncould fit the block is called a conflict miss.\n\nA\n\n22\n\n\fHow big is that block?\n\u2022 Spatial locality is SO prevalent that it\nmakes a whole lot of sense to pull more\ndata than is requested.\n\u25aa If we request a word (8 bytes) from memory,\nand we have a cache miss, let\u2019s pull 8 words at\na time (64 bytes).\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n64 bytes\n\n\u2022 Therefore, the blocks visualized to the\nright can have a size, called the block size.\n\u25aa The bigger the block, the better spatial locality\nwill become.\n\u25aa However, the more time it takes to copy from\nmemory and the higher penalty if you throw it\naway on a miss!\n\n8 bytes\n\n23\n\n\fBlock size helps locality\n\u2022 When we request an address from our\ncache, we are requesting the block that\ncontains that address.\n\u25aa Here, Block 0 contains byte addresses 0x00\nthrough 0x39. Block 1 is 0x40 to 0x79, etc.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n64 bytes\n\n\u2022 Let\u2019s request 64-bit words in order starting\nat address 0x40 (Block 1)\n\u25aa There are 8 words in each cache block.\n\u25aa Therefore, we have only one compulsory miss.\n\u25aa And then we have 7 cache hits!!\n\n\u2022 If we request the ninth word, we will be at\naddress 0x80 (and a compulsory miss.)\n\n1\n\n2\n\n24\n\n\fOnce again\u2026 A Tale of Two C\u2026 um\u2026 programs\n\nAllocates matrices.\n(Array of arrays)\nCopies one matrix to another.\n(data itself is uninitialized.)\n\nSpring 2019/2020\n\nIterates through column.\n(Other code goes through row)\n\n25\n\n\fOnce again\u2026 A Tale of Two C\u2026 um\u2026 programs\n\u2022 We will simplify by looking at a 4x4 matrix.\n\u25aa We want to get the addresses being used to\nsee the access pattern. (Goes across row)\n\n26\n\n\fOnce again\u2026 A Tale of Two C\u2026 um\u2026 programs\n\u2022 We will simplify by looking at a 4x4 matrix.\n\u25aa Notice the different type of access pattern.\n\u25aa (Goes down the column)\n\n27\n\n\fThe Antagonist\n\u2022 One program reads words sequentially in\nmemory (good spatial locality)\n\u25aa The other reads each word as far apart as\npossible! (worst spatial locality)\n\n\u2022 Let\u2019s look at making the matrices much\nlarger! Let\u2019s make each row span 256 Bytes.\n(4 blocks, which is the size of our cache!)\n\nThis cache\nitem holds\n,\n,\n, etc\n\n0\n\n1\n\n2\n\n3\n\nCache size: 256B\n64 bytes per block.\n\n28\n\n\fCache Performance\n\u2022 Recall that caches make computers practical.\n\n\u25aa Why? Well\u2026\n\u25aa Our \u201cslow\u201d program effectively did not use cache, and it was 10 times slower.\n\n\u2022 Simply: Caches offer much faster accesses than DRAM.\n\u25aa Perhaps 100s of times faster.\n\n\u2022 Consider the math:\n\u25aa miss rate (MR): Fraction of memory accesses not in cache.\n\u25aa hit rate (HR): Fraction of memory accesses found in cache. ( H\ud835\udc45 = 1 \u2212 \ud835\udc40\ud835\udc45 )\n\u25aa hit time (HT): Time it takes to read a block from cache to CPU. (Best case)\n\u25aa miss penalty (MP): Time it takes to read from main memory to cache.\n29\n\n\fCache Performance\n\u2022 Recall that caches make computers practical.\n\u2022 Consider the math:\n\u25aa miss rate (MR): Fraction of memory accesses not in cache.\n\u25aa hit rate (HR): Fraction of memory accesses found in cache. ( H\ud835\udc45 = 1 \u2212 \ud835\udc40\ud835\udc45 )\n\u25aa hit time (HT): Time it takes to read a block from cache to CPU. (Best case)\n\u25aa miss penalty (MP): Time it takes to read from main memory to cache.\n\n\u2022 Average Memory Access Time (AMAT): The time it takes, on average,\nto perform a memory request, considering the cache performance.\n\u25aa \ud835\udc34\ud835\udc40\ud835\udc34\ud835\udc47 = \ud835\udc3b\ud835\udc47 + \ud835\udc40\ud835\udc45 \u00d7 \ud835\udc40\ud835\udc43\n\n\u2022 Assuming a HT of 1 clock cycle and a MP of 100 clock cycles\u2026\n\u25aa A HR of 97%: \ud835\udc34\ud835\udc40\ud835\udc34\ud835\udc47 = 1 + 0.03 \u00d7 100 = 4 \ud835\udc50\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52\ud835\udc60\n\u25aa A HR of 99%: \ud835\udc34\ud835\udc40\ud835\udc34\ud835\udc47 = 1 + 0.01 \u00d7 100 = 2 \ud835\udc50\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52\ud835\udc60\n\u25aa A hit-rate jump from just 97% to 99% doubles memory performance. Wow.\n\n30\n\n\fCache Layout Summary\n\u2022 We have seen two types of cache layouts.\n\u25aa A freeform cache: blocks go wherever. \u00af\\_(\u30c4)_/\u00af\n\u2022 Also called a fully associative cache.\n\n\u25aa A direct-mapped cache: blocks go into slots.\n\n\u2022 They have their own trade-offs, and as\nusual\u2026\n\u25aa We can have a hybrid approach!\n\n\u2022 Here, each cache slot has multiple bins.\n\u25aa You only need to evict when you fill up the bins.\nBest of both worlds!\n\u25aa Which do you evict? (Hmm\u2026 difficult choice.)\n31\n\n\fAssociativity\n\u2022 With an associative cache, the address\ndetermines the slot.\n\u25aa Much like a direct-mapped cache.\n\u25aa However, the slot has a number of bins.\n\u25aa Any bin in the slot is viable for a block.\n\u25aa The number of bins is the number of \u201cways\u201d\n\u2022 A direct-mapped cache is a 1-way cache.\n\n\u2022 When the cache determines if the block is in\nthe cache already\u2026\n\u25aa It determines the slot.\n\u25aa It scans every bin for a block tagged with that\nexact address.\n\u25aa Therefore, the cache performance degrades as you\nincrease the number of ways.\n\n32\n\n\fAnother way of viewing it\nAll have the same size, so\u2026\n\nOn a fully associative cache all\nblocks belong to the same set\n\nOn a direct-mapped cache all\nblocks belong to a different set\n\nOn a n-way associative cache n\nblocks belong to the same set\n\n33\n\n\fMapping\nWT F\u2026Cache!?\n\n\fSooo\u2026 exactly what can go where?\n\u2022 Given a memory address, in which set does it go?\n\u2022 How many sets are there?\n\u2022 Let\u2019s start by defining a cache size\n\u2022 32KiB\n\n64B\n\n32\ud835\udc3e\ud835\udc56\ud835\udc35\n215\nI need 64\ud835\udc35 = 26 =\n29 = 512 lines\n\n\u2022 How many cache lines do you need?\n\u2022 Well it depends on the size of each cache line\n\u2022 # Cache line? # Cache row? # Block? # bins?\n\n\u2022 Let\u2019s use: 64B!\n\n35\n\n\fSooo\u2026 exactly what can where?\n64B\n\n\u2022 Let\u2019s start by defining a cache size\n\u2022 32KiB\n\n\u2022 How many cache rows do you need?\n\u2022 Well it depends on the size of each row\n\u2022 Let\u2019s use: 64B!\n\n\u2022 Decrease the size\n\n32\ud835\udc3e\ud835\udc56\ud835\udc35\n215\nI need 64\ud835\udc35 = 26 =\n29 = 512 rows\n\n\u2022 +rows, + (but faster) memory access, -locality\n\n\u2022 Increase the size\n\u2022 -rows, -(but slower) memory access, +locality\n\n36\n\n\fHow many sets\n\u2022 The number of sets depends on the associativity\n\u2022 Remember we have a fixed amount of rows!\n\n\u2022 For fully associative (easy) we have 1 set \u263a\n\u2022 For n-way associative cache we need some maths:\n\u2022 n-way associative means we divide the lines in groups of n-elements\n0\n\n\u2026\n\ns-1\nSo how many sets?\nEach column (set)\nhas n rows\n\nStill 512 rows!\n\ns=\n\n# rows 512\n=\n\ud835\udc5b\n\ud835\udc5b\n\n37\n\n\fHow many sets\n\u2022 If we apply this to a 4-way associative cache\n\n0\n\n\u2026\n\n127\nSo how many sets?\nEach column (set)\nhas 4 rows\n\nStill 512 rows!\n\n# rows 512\ns=\n=\n\ud835\udc5b\n4\n= 128 \ud835\udc60\ud835\udc52\ud835\udc61\ud835\udc60\n\n38\n\n\fAddress Manipulation\n\nRequest from CPU:\nAccess PT:\n\n\ud835\udc5b-bit virtual address\nVirtual Page Number\n\nPage offset\n\nTRANSLATION\n\ud835\udc5a-bit physical\naddress:\nSplit to access\ncache:\n\nPhysical Page Number\nCache Tag\n\nPage offset\n\nSet Index\n\nOffset\n\n\fUsing our example\n\u2022 On our example:\n\u2022 Cache size: 32KiB\n\u2022 Block size: 64B = 26\n\u2022 Associativity: 4-way\n\u2022 Number of sets: 128 = 27\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nPhysical Page Number\n\nPage offset\n\nCache Tag\n\nSet Index\n\nOffset\n\n12 bits\n\n7 bits\n\n6 bits\n\nTAG\n\nSet\n\nvalid\n\nData\n\n0xFFF\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\nOnly showing 2 blocks per set (slide space)\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nHit!\n\n111111111111000000\n\n0000000\n\n0xFFF\n\n0\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nMiss!\n\n000000000000000000\n\n0000000\n\n0x000\n\n0\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nHit!\n\n000100100011000000\n\n1000000\n\n0x123\n\n1\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fUsing our example\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\nMiss!\n\n101010101010010101\n\n0000000\n\n0xAAA\n\n42\n\n0\n\n12 bits\n\n7 bits\n\n6 bits\n\n\fOffset\nTAG\n\nSet\n\nvalid\n\nData\n\n0x000\n\n0\n\n0\n\n(64B of data)\n\n0xFFF\n\n0\n\n1\n\n(64B of data)\n\n0xFFF\n\n1\n\n1\n\n(64B of data)\n\n0x123\n\n1\n\n1\n\n(64B of data)\n\n0x456\n\n42\n\n1\n\n(64B of data)\n\n0xFFF\n\n42\n\n1\n\n(64B of data)\n\nHit!\n\n63\n\n62\n\n61\n\n\u2026\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\n0xF5\n\n0x32\n\n0x45\n\n\u2026\n\n0xFF\n\n0xFE\n\n0x00\n\n0x68\n\n0x67\n\n0x65\n\n25-bit physical\naddress:\nSplit to access\ncache:\n\n111111111111000000\n\n0000010\n\n0xFFF\n\n0\n\n2\n\n12 bits\n\n7 bits\n\n6 bits\n\n45\n\n\fSummary\n\u2022 The notion of storing data is a complicated one.\n\u25aa Different technologies have different strengths (and costs)\n\u25aa Often trade-off between:\n\u2022 fast / small, expensive\n\u2022 slow / big, cheap\n\n\u25aa Hardware designs attempt to accommodate a variety of technologies.\n\u2022 Often using fast/small memories to act as a \u201ccache\u201d for slower ones.\n\n\u2022 Caches can be arranged in several ways:\n\u25aa Blocks go anywhere (fully-associative)\n\u25aa Blocks go in particular slots (direct-mapped / 1-way associative)\n\u25aa Hybrid: Blocks go to particular slots\u2026 but then can go in any bin in that slot.\n\n\u2022 Caches attempt to exploit temporal and spatial locality of programs.\n\u25aa And even a slight improvement to hit rate can dramatically improve overall\nperformance of a program!\n\n46\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}