{"id": 223, "segment": "unlabeled", "course": "cs1541", "lec": "lec2.5_vliw_processors", "text": "VLIW Processors\nCS 1541\nWonsun Ahn\n\n\fLimits on Deep Pipelining\n\u25cf Ideally, CycleTimePipelined = CycleTimeSingleCycle / Number of Stages\no In theory, can indefinitely improve performance with more stages\n\u25cf Limitation 1: Cycle time does not improve indefinitely\no With shorter stages, delay due to latches become significant\no With many stages, hard to keep stage lengths perfectly balanced\no Manufacturing variability exacerbates the stage length unbalance\n\u25cf Limitation 2: CPI tends to increase with deep pipelines\no Penalty due to branch misprediction increases\no Stalls due to data hazards cause more bubbles\n\u25cf Limitation 3: Power consumption increases with deep pipelines\no Wires for data forwarding increase quadratically with depth\n\u25cf Is there another way to improve performance?\n2\n\n\fWhat if we improve CPI?\n\u25cf Remember the three components of performance?\ninstructions\nX\nprogram\n\ncycles\ninstruction\n\nX\n\nseconds\ncycle\n\n\u25cf Pipelining focused on seconds / cycle, or cycle time\n\u25cf Can we improve cycles / instruction, or CPI?\no But the best we can get is CPI = 1, right?\no How can an instruction be executed in less than a cycle?\n\n3\n\n\fWide Issue Processors\n\n4\n\n\fFrom CPI to IPC\n\u25cf How about if we fetch two instructions each cycle?\no Maybe, fetch one ALU instruction and one load/store instruction\n\n\u25cf Then, IPC (Instructions per Cycle) = 2\no And by extension, CPI = 1 / IPC = 0.5 !\n\u25cf Wide-issue processors can execute multiple instructions per cycle\n5\n\n\fPipeline design for previous example\n\u25cf One pipeline for ALU/Branches and one for loads and stores\n\nI-Mem\n\nIns. Decoder\n\nALU\nPipeline\nRegister\nFile\n\nMemory\nPipeline\n\nALU\n\n+\n\nD-Mem\n\n\u25cf This introduces new structural hazards that we didn\u2019t have before!\n6\n\n\fStructural Hazard in Storage Locations (Solved)\n\nI-Mem\n\nIns. Decoder\n\n\u25cf Two instructions must be fetched from instruction memory\n\u2192 Add extra read ports to the instruction memory\n\u25cf Two ALUs must read from the register file at the same time\n\u2192 Add extra read ports to the register file\n\u25cf Two instructions must write to register file at WB stage (not shown)\n\u2192 Add extra write ports to the register file\n\nRegister\nFile\n\nALU\n\n+\n\nD-Mem\n7\n\n\fStructural Hazard in Functional Units (Still Remaining)\n\nI-Mem\n\nIns. Decoder\n\n\u25cf Structural hazard on EX units\no Top ALU can handle all arithmetic ( +, -, *, /)\no Bottom ALU can only handle +, needed for address calculation\n\u25cf Structural hazard on MEM unit\no ALU pipeline does not have a MEM unit to access memory\n\nRegister\nFile\n\nALU\n\n+\n\nD-Mem\n8\n\n\fStructural Hazard in Functional Units: Example\n\u25cf Code on the left will result in a timeline on the right\no If it were not for the bubbles, we could have finished in 4 cycles!\n\nlw\n$t0, 0($s1)\nlw\n$t1, -4($s1)\naddi $t2, $t2, -8\nadd $t3, $t0, $s1\nadd $t4, $s1, $s1\nsw\n$t5, 8($t3)\nsw\n$t6, 4($s1)\n\nCC\n\nALU Pipeline\n\n1\n\nlw t0\n\n2\n\naddi t2\n\n3\n\nadd t3\n\n4\n\nadd t4\n\n5\n\nMem Pipeline\n\nlw t1\nsw t5\nsw t6\n\n9\n\n\fStructural Hazard Solution: Reordering\n\u25cf Of course we can come up with a better schedule\no While still adhering to the data dependencies\n\nlw\n$t0, 0($s1)\nlw\n$t1, -4($s1)\naddi $t2, $t2, -8\nadd $t3, $t0, $s1\nadd $t4, $s1, $s1\nsw\n$t5, 8($t3)\nsw\n$t6, 4($s1)\n\nCC\n\nALU Pipeline\n\n1\n\nlw t0\n\n2\n\naddi t2\n\n3\n\nadd t3\n\n4\n\nadd t4\n\n5\n\nMem Pipeline\n\nlw t1\nsw t5\nsw t6\n\n10\n\n\fWhy not just duplicate all resources?\n\u25cf Why not have two full ALUs, have MEM units at both pipelines?\no That way, we can avoid those structural hazards in the first place\no But that leads to low utilization\n\u00a7 ALU/Branch type instructions will not use the MEM unit\n\u00a7 Load/Store instructions will not need the full ALU\n\u25cf Most processors have specialized pipelines for different instructions\no Integer ALU pipeline, FP ALU pipeline, Load/Store pipeline, \u2026\no With scheduling, can achieve high utilization and performance\n\u25cf Who does the scheduling? Well, we talked about this already:\no Static scheduling \u2192 Compiler\no Dynamic scheduling \u2192 Processor\n11\n\n\fVLIW vs. Superscalar\n\u25cf There are two types of wide-issue processors\n\u25cf If the compiler does static scheduling, the processor is called:\no VLIW (Very Long Instruction Word) processor\no This is what we will learn this chapter\n\u25cf If the processor does dynamic scheduling, the processor is called:\no Superscalar processor\no This is what we will learn next chapter\n\n12\n\n\fVLIW Processors\n\n13\n\n\fVLIW Processor Overview\n\u25cf What does Very Long Instruction Word mean anyway?\no It means one instruction is very long!\no Why? Because it contains multiple operations in one instruction\n\u25cf A (64 bits long) VLIW instruction for our example architecture:\nALU/Branch Operation (32 bits)\n\nLoad/Store Operation (32 bits)\n\n\u25cf An example instruction could be:\naddi $t2, $t0, -8\n\nlw $t1, -4($s1)\n\n\u25cf Or another example could be:\nnop\n\nlw $t1, -4($s1)\n14\n\n\fA VLIW instruction is one instruction\n\nIns. Decoder\n\nI-Mem\n\nLoad/Store Op\n\nALU Op\n\n\u25cf For all purposes, a VLIW instruction acts like one instruction\no It moves as a unit through the pipeline\n\nRegister\nFile\n\nALU\n\n+\n\nD-Mem\n\n15\n\n\fVLIW instruction encoding for example\nnop\nlw\n$t0, 0($s1)\naddi $t2, $t2, -8\nlw\n$t1, -4($s1)\nadd $t3, $t0, $s1\nnop\nadd $t4, $s1, $s1\nsw\n$t5, 8($t3)\nnop\nsw\n$t6, 4($s1)\n\nInst\n\nALU Op\n\nLoad/Store Op\n\n1\n\nnop\n\nlw t0\n\n2\n\naddi t2\n\nlw t1\n\n3\n\nadd t3\n\nnop\n\n4\n\nadd t4\n\nsw t5\n\n5\n\nnop\n\nsw t6\n\n\u2022\n\u2022\n\nEach square is an instruction.\n(There are 5 instructions.)\nNops are inserted by the compiler.\n16\n\n\fVLIW instruction encoding (after reordering)\nadd $t4, $s1, $s1\nlw\n$t0, 0($s1)\naddi $t2, $t2, -8\nlw\n$t1, -4($s1)\nadd $t3, $t0, $s1\nsw\n$t6, 4($s1)\nnop\nsw\n$t5, 8($t3)\n\nInst\n\nALU Op\n\nLoad/Store Op\n\n1\n\nadd t4\n\nlw t0\n\n2\n\naddi t2\n\nlw t1\n\n3\n\nadd t3\n\nsw t6\n\n4\n\nnop\n\nsw t5\n\n\u2022\n\nSame program with 4 instructions!\n\n17\n\n\fVLIW Architectures are (Very) Power Efficient\n\u25cf All scheduling is done by the compiler offline\n\u25cf No need for the Hazard Detection Unit\no Nops are inserted by the compiler when necessary\n\u25cf No need for a dynamic scheduler\no Which can be even more power hungry than the HDU\n\u25cf Even no need for the Forwarding Unit\no If compiler is good enough and fill all bubbles with instructions\no Or, may have cheap compiler-controlled forwarding in ISA\n\n18\n\n\fChallenges of VLIW\n\u25cf All the challenges of static scheduling apply here X 2\n\u25cf Review: what were the limitations?\no Compiler must make assumptions about the pipeline\n\u2192 ISA now becomes much more than instruction set + registers\n\u2192 ISA restricts modification of pipeline in future generations\no Compiler must do scheduling without runtime information\n\u2192 Length of MEM stage is hard to predict (due to Memory Wall)\n\u2192 Data dependencies are hard (must do pointer analysis)\n\u25cf These limitations are exacerbated with VLIW\n19\n\n\fNot Portable due to Assumptions About Pipeline\n\u25cf VLIW ties ISA to a particular processor design\no One that is 2-wide and has an ALU op and a Load/Store op\no What if future processors are wider or contain different ops?\n\u25cf Code must be recompiled repeatedly for future processors\no Not suitable for releasing general purpose software\no Reason VLIW is most often used for embedded software\n(Because embedded software is not expected to be portable)\n\u25cf Is there any way to get around this problem?\n\n20\n\n\fMaking VLIW Software Portable\n\u25cf There are mainly two ways VLIW software can become portable\n1. Allow CPU to exploit parallelism according to capability\no Analogy: multithreaded software does not specify number of cores\n\u00a7 SW: Makes parallelism explicit by coding using threads\n\u00a7 CPU: Exploits parallelism to the extent it has number of cores\no Portable VLIW: ISA does not specify number of ops in instruction\n\u00a7 SW: Makes parallelism explicit by using bundles\n\u2013 Bundle: a group of ops that can execute together\n\u2013 Wider processors fetch several bundles to form one instruction\n\u2013 A \u201cstop bit\u201d tells processor to stop fetching the next bundle\n\u00a7 Intel Itanium EPIC(Explicitly Parallel Instruction Computing)\n\u2013 A general-purpose ISA that uses bundles\n21\n\n\fMaking VLIW Software Portable\n\u25cf There are mainly two ways VLIW software can become portable\n2. Binary translation\no Have firmware translate binary to new VLIW ISA on the fly\no Very similar to how Apple Rosetta converts x86 to ARM ISA\no Doesn\u2019t this go against the power efficiency of VLIWs?\n\u00a7 Yes, but if SW runs for long time, one-time translation is nothing\n\u00a7 Translation can be cached in file system for next run\no Transmeta Crusoe converted x86 to an ultra low-power VLIW\n\n22\n\n\fScheduling without Runtime Information\n\u25cf Up to the compiler to create schedule with minimal nops\no Use reordering to fill nops with useful operations\n\u25cf All the challenges of static scheduling remain\no Length of MEM stage is hard to predict (due to Memory Wall)\no Data dependencies are hard to figure out (due to pointer analysis)\n\u25cf But these challenges become especially acute for VLIW\no For 4-wide VLIW, need to find 4 operations to fill \u201cone\u201d bubble!\no Operations in one instruction must be data independent\n\u00a7 Data forwarding will not work within one instruction\n(Obviously because they are executing on the same cycle)\n\n23\n\n\fPredicates Help in Compiler Scheduling\n\u25cf Use of predicates can be a big help in finding useful operations\no Reordering cannot happen across control dependencies\nlw $t1, 0($s0)\naddi $t1, $t1, 1\nbne $s1, $s2, else\nthen:\nli $t0, 1\n\nCan only reorder within this block\n\nelse:\nli $t0, 0\n\no Predicates can convert if-then-else code into one big block\npne $p1, $s1, $s2\nlw $t1, 0($s0)\nli.p $t0, 1, !$p1\nli.p $t0, 0, $p1\naddi $t1, $t1, 1\n\nCan reorder within a larger window!\n\n24\n\n\fBut Predicates Cannot Remove Loopback Branches\n\u25cf Loops are particularly challenging to the compiler. Why?\no Scheduling is limited to within the loop\no For tight loops, not much compiler can do with a handful of insts\nloop:\nlw $s0, 0($t1)\naddi $s0, $s0, 10\naddi $t1, $t1, 1\nbne $s0, $s1, loop\n\nCan only reorder within this block\n\n\u2026\n\nThis block is off limits!\n\n25\n\n\fCompiler Scheduling of a Loop\n\u25cf Suppose we had this example loop (in MIPS):\n\nLoop:\nlw\n$t0, 0($s1)\nadd $t0, $t0, $s2\nsw\n$t0, 0($s1)\naddi $s1, $s1, -4\nbne $s1, $zero, Loop\n\n// $t0 = array[$s1]\n// $t0 = $t0 + $s2\n// array[$s1] = $t0\n// $s1++\n// loopback if $s1 != 0\n\n\u25cf Loop iterates over an array adding $s2 to each element\n\n26\n\n\fCompiler Scheduling of a Loop\n\u25cf Let\u2019s first reschedule to hide the use-after-load hazard\n\nLoop:\nlw\n$t0, 0($s1)\nadd $t0, $t0, $s2\nsw\n$t0, 0($s1)\naddi $s1, $s1, -4\nbne $s1, $zero, Loop\n\nLoop:\nlw\n$t0, 0($s1)\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nbne $s1, $zero, Loop\n\n\u25cf Now dependence on $t0 is further away\n\u25cf Note we broke a WAR (Write-After-Read) dependence on $s1\no Now $s1 in sw $t0, 0($s1) is result of addi $s1, $s1, -4\n\n\u25cf We must compensate by changing the sw offset by +4:\no sw $t0, 0($s1) \u2192 sw $t0, 4($s1)\n\n27\n\n\fCompiler Scheduling of a Loop\n\u25cf Below is the VLIW representation of the rescheduled MIPS code:\nLoop:\nALU/Branch Op\nLoad/Store Op\nlw\n$t0, 0($s1)\nLoop: addi $s1, $s1, -4 lw $t0, 0($s1)\naddi $s1, $s1, -4\nnop\nnop\nadd $t0, $t0, $s2\nadd $t0, $t0, $s2\nnop\nsw\n$t0, 4($s1)\nbne $s1, $0, Loop sw $t0, 4($s1)\nbne $s1, $zero, Loop\n\n\u25cf We can\u2019t fill any further nops due to data hazards\n\u25cf In terms of MIPS instructions, IPC = 5 / 4 = 1.25\no Ideally, IPC can reach 2 so we are not doing very well here\n\u25cf Is there a way compiler can expand the \u201cwindow\u201d for scheduling?\no Idea: use multiple iterations of the loop for scheduling!\n\n28\n\n\fLoop unrolling\n\n29\n\n\fWhat is Loop Unrolling?\n\u25cf Loop unrolling : a compiler technique to enlarge loop body\no By duplicating loop body for an X number of iterations\n\nfor(i = 0; i < 100; i++)\nUnrolled loop (2X)\na[i] = b[i] + c[i];\nfor(i = 0; i < 100; i += 2){\nOriginal loop\na[i] = b[i] + c[i];\na[i+1] = b[i+1] + c[i+1];\n}\n\u25cf What does this buy us?\no More instructions inside loop to reorder and hide bubbles\no And less instructions to execute as a whole\n\u00a7 Less frequent loop branches\n\u00a7 Two i++ are merged into one i+= 2\n30\n\n\fLet\u2019s try unrolling our example code\nLoop:\nlw\n$t0, 0($s1)\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nbne $s1, $zero, Loop\n\nLoop:\nlw\n$t0, 0($s1)\nUnroll 2X\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nlw\n$t1, -4($s1)\naddi $s1, $s1, -4\nadd $t1, $t1, $s2\nsw\n$t1, 4($s1)\nbne\n\n$s1, $zero, Loop\n\n\u25cf Instructions are duplicated but using $t1 instead of $t0\n\u25cf This is intentional to minimize false dependencies during reordering\n31\n\n\fNow time to reorder the code!\nLoop:\nlw\nlw\n\n$t0, 0($s1)\n$t1, -4($s1)\n\naddi $s1, $s1, -4\naddi $s1, $s1, -4\nadd\nadd\n\n$t0, $t0, $s2\n$t1, $t1, $s2\n\nsw\nsw\n\n$t0, 8($s1)\n$t1, 4($s1)\n\nbne\n\n$s1, $zero, Loop\n\nReorder!\n\nLoop:\nlw\n$t0, 0($s1)\naddi $s1, $s1, -4\nadd $t0, $t0, $s2\nsw\n$t0, 4($s1)\nlw\n$t1, -4($s1)\naddi $s1, $s1, -4\nadd $t1, $t1, $s2\nsw\n$t1, 4($s1)\nbne\n\n$s1, $zero, Loop\n\n\u25cf Interleaving iterations spaces out dependencies (2X = unroll factor)\n32\n\n\fMerge induction variable increment\nLoop:\nlw\nlw\n\n$t0, 0($s1)\n$t1, -4($s1)\n\naddi $s1, $s1, -4\naddi $s1, $s1, -4\nadd\nadd\nsw\nsw\nbne\n\n$t0, $t0, $s2\n$t1, $t1, $s2\n$t0, 8($s1)\n$t1, 4($s1)\n\nMerge\n\nLoop:\nlw\nlw\n\n$t0, 0($s1)\n$t1, -4($s1)\n\naddi $s1, $s1, -8\nadd\nadd\n\n$t0, $t0, $s2\n$t1, $t1, $s2\n\nsw\nsw\n\n$t0, 8($s1)\n$t1, 4($s1)\n\nbne\n\n$s1, $zero, Loop\n\n$s1, $zero, Loop\n\n\u25cf Two addi $s1, $s1, -4 are merged into addi $s1, $s1, -8\n33\n\n\fScheduling unrolled loop onto VLIW\nLoop:\nlw\n$t0, 0($s1)\nALU/Branch Op\nLoad/Store Op\nlw\n$t1, -4($s1)\nLoop:\nnop\nlw $t0, 0($s1)\naddi $s1, $s1, -8\nadd $t0, $t0, $s2\naddi $s1, $s1, -8 lw $t1, -4($s1)\nadd $t1, $t1, $s2\nadd $t0, $t0, $s2 nop\nsw\n$t0, 8($s1)\nadd $t1, $t1, $s2 sw $t0, 8($s1)\nsw\n$t1, 4($s1)\nbne $s1, $0, Loop sw $t1, 4($s1)\nbne $s1, $zero, Loop\n\n\u25cf Now we spend 5 cycles for 2 iterations of the loop\no So, 5 / 2 = 2.5 cycles per iteration\no Much better than the previous 4 cycles for 1 iteration!\n\n34\n\n\fLet\u2019s try unrolling our example code 4X\n\u25cf 4X Unrolled loop converted to VLIW:\nALU/Branch Op\nLoop: addi $s1, $s1, -16\nnop\nadd $t0, $t0, $s2\nadd $t1, $t1, $s2\nadd $t2, $t1, $s2\nadd $t3, $t1, $s2\nnop\nbne $s1, $0, Loop\n\nLoad/Store Op\nlw $t0, 0($s1)\nlw $t1, 12($s1)\nlw $t2, 8($s1)\nlw $t3, 4($s1)\nsw $t0, 16($s1)\nsw $t1, 12($s1)\nsw $t2, 8($s1)\nsw $t3, 4($s1)\n\nInst\n1\n2\n3\n4\n5\n6\n7\n8\n\n\u25cf Now we spend 8 cycles for 4 iterations of the loop\no So, 8 / 4 = 2 cycles per iteration\no Even better 2.5 cycles per iteration for 2X unrolling\n35\n\n\fWhat happens when you unroll 8X?\n\u25cf 8X Unrolled loop converted to VLIW:\nALU/Branch Op\nLoop: addi $s1, $s1, -32\nnop\n\u2026\nadd $t1, $t1, $s2\nadd $t2, $t1, $s2\nadd $t3, $t1, $s2\n\u2026\nbne $s1, $0, Loop\n\nLoad/Store Op\nlw $t0, 0($s1)\nlw $t1, 28($s1)\n\u2026\nlw $t7, 4($s1)\nsw $t0, 32($s1)\nsw $t1, 28($s1)\n\u2026\nsw $t7, 4($s1)\n\nInst\n1\n2\n\u2026\n8\n9\n10\n\u2026\n16\n\n\u25cf Now we spend 16 cycles for 8 iterations of the loop\no So, 16 / 8 = 2 cycles per iteration (no improvement over 4X)\no 2 is minimum because you need one lw and one sw per iteration\n36\n\n\fWhen should the compiler stop unrolling?\n\u25cf Obviously when there is no longer a benefit as we saw just now\n\u25cf But there are other constraints that can prevent unrolling\n1. Limitation in number of registers\no More unrolling uses more registers $t0, $t1, $t2, \u2026\no For this reason, VLIW ISAs have many more registers than MIPS\n\u00a7 Intel Itanium has 256 registers!\n2. Limitation in code space\no More unrolling means more code bloat\no Embedded processors don\u2019t have lots of code memory\no Matters even for general purpose processors because of caching\n(Code that overflows i-cache can lead to lots of cache misses)\n37\n\n\fList Scheduling\n\n38\n\n\fHow does the compiler schedule instructions?\n\u25cf Compiler will first expand the instruction window that it looks at\no Instruction window: block of code without branches\no Compiler uses predication and loop unrolling\n\u25cf Once compiler has a sizable window, it will construct the schedule\n\u25cf A popular scheduling algorithm is list scheduling\no Idea: list instructions in some order of priority and schedule\no Instructions on the critical path should be prioritized\n\u25cf List scheduling can be used with any statically scheduled processor\no Simple single-issue statically scheduled processor (not just VLIW)\no GPUs are also statically scheduled using list scheduling\n39\n\n\fCritical Path in Code\n\u25cf At below is a data dependence graph for a code with 7 instructions\no Nodes are instructions\no Arrows are data dependencies annotated with required delay\n\u25cf Q: How long is the critical path in this code?\n3\n2\n\n1\n\n1\n2\n\n1\n1\n\n1\n\n2\n\n2\n\nThat means, at minimum,\nthis code will take 7\ncycles, period. Regardless\nof how wide your\nprocessor is or how well\nyou do your scheduling.\n\ntotal 7 cycles\n\n40\n\n\fInstruction Level Parallelism (ILP)\n\u25cf The 7 cycles is achievable only through instruction level parallelism\no That is, parallel execution of instructions\no The nodes marked in red can execute in parallel with blue nodes\n\u25cf This tell us that this code is where a VLIW processor can shine\n3\n2\n\n1\n\n1\n2\n\n1\n1\n\n1\n\n2\n\n2\n\ntotal 7 cycles\n\n41\n\n\fMaximizing Instruction Level Parallelism (ILP)\n\u25cf The more ILP code has, the more VLIW will shine\n\u25cf So before list scheduling, compiler maximizes ILP in code\no What constrains ILP? Data dependencies!\no Some data dependencies can be removed by the compiler\n\u25cf There are 3 types of data dependencies actually:\no RAW (Read-After-Write): cannot be removed\no WAR (Write-After-Read): can be removed\no WAW (Write-After-Write): can also be removed\n\u25cf How about Read-After-Read? Not a data dependency.\n\n42\n\n\fRead-After-Write (RAW) Dependency\n\u25cf RAW dependencies are also called true dependencies\no In the sense that other dependencies are not \u201creal\u201d dependencies\n\u25cf Suppose we reorder this snippet of code:\nRAW!\n\nlw t0, 0(s0)\naddi t1, t0, 4\n\nReorder\n\naddi t1, t0, 4\nlw t0, 0(s0)\n\n\u25cf The code is incorrect because now t1 has a wrong value\no No amount of compiler tinkering will allow this reordering\no Value must be loaded into t0 before being used to compute t1\n\n43\n\n\fWrite-After-Read (WAR) Dependency\n\u25cf WAR dependencies are also called anti-dependencies\no In the sense that they are the opposite of true dependencies\n\u25cf Suppose we reorder this snippet of code:\nWAR!\n\nlw t0, 0(s0)\naddi t1, t0, 4\nlw t0, 0(s1)\n\nReorder\n\nlw t0, 0(s0)\nlw t0, 0(s1)\naddi t1, t0, 4\n\n\u25cf The code is again incorrect because t1 has the wrong value\no addi should not read t0 produced by lw t0, 0(s1)\n\u25cf Q: Is there a way for addi to not use that value?\no t0 and t0 contain different values. Why use the same register?\no Just rename register t0 to some other register!\n44\n\n\fRemoving WAR with SSA\n\u25cf Static Single Assignment: Renaming registers with different values\no A register is assigned a value only a single time (never reused)\n\u25cf Reordering after converting to SSA form:\nNO WAR!\n\nlw t0, 0(s0)\naddi t1, t0, 4\nlw t2, 0(s1)\n\nReorder\n\nlw t0, 0(s0)\nlw t2, 0(s1)\naddi t1, t0, 4\n\n\u25cf Note how destination registers always use a new register\no Yes, if you do this, you will need lots of registers\no But, no more WAR dependencies!\n\n45\n\n\fWrite-After-Write (WAW) Dependency\n\u25cf WAW dependencies are also called false dependencies\no In the sense that they are not real dependencies\n\u25cf Suppose we reorder this snippet of code:\nWAW!\n\nlw t0, 0(s0)\nlw t0, 0(s1)\naddi t1, t0, 4\n\nReorder\n\nlw t0, 0(s1)\nlw t0, 0(s0)\naddi t1, t0, 4\n\n\u25cf The code is again incorrect because t1 has the wrong value\no addi should not read t0 produced by lw t0, 0(s0)\n\u25cf Q: Is there a way for addi to not use that value?\no Again, rename register t0 to some other register!\n\n46\n\n\fRemoving WAW with SSA\n\u25cf Again, Static Single Assignment (SSA) to the rescue!\no SSA removes both WAR and WAW dependencies\n\u25cf Reordering after converting to SSA form:\nNO WAW!\n\nlw t0, 0(s0)\nlw t1, 0(s1)\naddi t2, t1, 4\n\nReorder\n\nlw t1, 0(s1)\nlw t0, 0(s0)\naddi t2, t1, 4\n\n\u25cf SSA form is now the norm in all mature compilers\no Clang / LLVM (\u201cApple\u201d Compiler)\no GCC (GNU C Compiler)\no Java Hotspot / OpenJDK Compiler\no Chrome JavaScript Compiler\n47\n\n\fList scheduling guarantees < 2X of optimal cycles\n\u25cf Back to our original example.\n\u25cf The critical path length is 7 cycles but that is not always achievable\no If processor is not wide enough for the available parallelism\no If compiler does a bad job at scheduling instructions\n\u2192 List scheduling guarantees compiler is within 2X of optimal\n3\n2\n\n1\n\n1\n2\n\n1\n1\n\n1\n\n2\n\nQ: Can the graph be cyclic?\n2\n\ntotal 7 cycles\n\n48\n\n\fList Scheduling is a Greedy Algorithm\n\u25cf Idea: Greedily prioritize instructions on the critical path\n\u25cf Steps:\n1. Create a data dependence graph\n2. Assign a priority to each node (instruction)\n\u00a7 Priority = critical path length starting from that node\n3. Schedule nodes one by one starting from ready instructions\n\u00a7 Ready = all dependencies have been fulfilled\n(Initially, only roots of dependency chains are ready)\n\u00a7 When there are multiple nodes that are ready\n\u2192 Choose the node with the highest priority\n\u25cf 2 \u2013 2/(n+1) X of optimal schedule, where n = processor width\n\no Graham, Ronald L.. \u201cBounds on Multiprocessing Timing Anomalies.\u201d SIAM\nJournal of Applied Mathematics 17 (1969): 416-429.\n49\n\n\fList Scheduling Example\n\u25cf Assume all edges have a delay of 1\no Red dashed lines indicate priority levels\nn insertions +\nn removals.\nIf using priority heap\n= O(nlogn)\n\n6\nReady instructions List\n\n5\n4\n\nOperation 1\n\n3\n\nOperation 2\n\n2\n1\n50\n\n\fList Scheduling Example\n\u25cf This will result in the following schedule:\nOperation 1\n1\n2\n3\n4\n6\n8\n10\n12\n13\n\nOperation 2\n6\n\n5\n7\n9\n11\n\n\u25cf 9 cycles. We couldn\u2019t achieve 7 cycles!\no But could\u2019ve if we had a wider processor\n\n5\n4\n3\n2\n1\n51\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}