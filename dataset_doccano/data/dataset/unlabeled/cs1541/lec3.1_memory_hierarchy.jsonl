{"id": 246, "segment": "unlabeled", "course": "cs1541", "lec": "lec3.1_memory_hierarchy", "text": "Memory Hierarchy\nCS 1541\nWonsun Ahn\n\n\fUsing the PMU to\nUnderstand Performance\n\n2\n\n\fExperiment on kernighan.cs.pitt.edu\n\u25cf The source code for the experiments are available at:\nhttps://github.com/wonsunahn/CS1541_Spring2022/tree/main/res\nources/cache_experiments\n\u25cf Or on the following directory at linux.cs.pitt.edu:\n/afs/cs.pitt.edu/courses/1541/cache_experiments/\n\u25cf You can run the experiments by doing \u2018make\u2019 at the root\no It will take a few minutes to run all the experiments\no In the end, you get two plots: IPC.pdf and MemStalls.pdf\n\n3\n\n\fFour benchmarks\n\u25cf linked-list.c\no Traverses a linked list from beginning to end over and over again\no Each node has 120 bytes of data\n\u25cf array.c\no Traverses an array from beginning to end over and over again\no Each element has 120 bytes of data\n\n\u25cf linked-list_nodata.c\no Same as linked-list but nodes have no data inside them\n\u25cf array_nodata.c\no Same as array but elements have no data inside them\n4\n\n\fCode for linked-list.c\n// Define a linked list node type with data\ntypedef struct node {\nstruct node* next; // 8 bytes\nint data[30];\n// 120 bytes\n} node_t;\n\u2026\n// Create a linked list of length items\nvoid *create(void *unused) {\nfor(int i=0; i<items; i++) {\nnode_t* n = (node_t*)malloc(sizeof(node_t));\nif(last == NULL) { // Is the list empty? If so, the new node is the head and tail\nhead = n;\nlast = n;\n} else {\nlast->next = n;\nlast = n;\n}\n}\n}\n5\n\n\fCode for linked-list.c\n#define ACCESSES 1000000000\n\n// MEASUREMENT BEGIN\n// Traverse list over and over until we\u2019ve visited `ACCESSES` nodes\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\n// reached the end\nelse current = current->next;\n// next node\n}\n// MEASUREMENT END\n\u25cf Note: executed instructions are equivalent regardless of list length\n\u25cf So we expect performance to be same regardless of length. Is it?\n6\n\n\fCode for array.c\n// Define a linked list node type with data\ntypedef struct node {\nstruct node* next; // 8 bytes\nint data[30];\n// 120 bytes\n} node_t;\n\u2026\n// Create a linked list but allocate nodes in an array\nvoid *create(void *unused) {\nhead = (node_t *) malloc(sizeof(node_t) * items);\nlast = head + items - 1;\nfor(int i=0; i<items; i++) {\nnode_t* n = &head[items];\nn->next = &head[items+1]; // Next node is next element in array\n}\nlast->next = NULL;\n}\n7\n\n\fCode for array.c\n#define ACCESSES 1000000000\n\n// MEASUREMENT BEGIN\n// Traverse list over and over until we\u2019ve visited `ACCESSES` nodes\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\n// reached the end\nelse current = current->next;\n// next node\n}\n// MEASUREMENT END\n\u25cf Note: same exact loop as the linked-list.c loop.\n\u25cf So we expect performance to be exactly the same. Is it?\n8\n\n\fkernighan.cs.pitt.edu specs\n\u25cf Two CPU sockets. Each CPU:\no Intel(R) Xeon(R) CPU E5-2640 v4\no 10 cores, with 2 threads per each core (SMT)\no L1 i-cache: 32 KB 8-way set associative (per core)\no L1 d-cache: 32 KB 8-way set associative (per core)\no L2 cache: 256 KB 8-way set associative (per core)\no L3 cache: 25 MB 20-way set associative (shared)\n\u25cf Memory\no 128 GB DRAM\n\u25cf Information obtained from\no \u201ccat /proc/cpuinfo\u201d on Linux server\no \u201ccat /proc/meminfo\u201d on Linux server\no https://en.wikichip.org/wiki/intel/xeon_e5/e5-2640_v4\n9\n\n\fExperimental data collection\n\u25cf Collected using CPU Performance Monitoring Unit (PMU)\no PMU provides performance counters for a lot of things\no Cycles, instructions, various types of stalls, branch mispredictions,\ncache misses, bandwidth usage, \u2026\n\n\u25cf Linux perf utility summarizes this info in easy to read format\no https://perf.wiki.kernel.org/index.php/Tutorial\n\n10\n\n\fCPI (Cycles Per Instruction) Results\nWhy the three \u201cplateaus\u201d?\n\nWhy increase in CPI with larger size?\n\nWhy is array\nfaster than\nlinked list?\n\nWhy constant IPC, regardless of size?\n\n11\n\n\fMemory Stall Cycle Percentage\nThe three \u201cplateaus\u201d are in memory stalls too!\n\nIPC decrease due to memory stall increases!\n\nArray has less\nmemory stalls\nthan linked list\n\nNo IPC decrease because no increase in memory stalls!\n\n12\n\n\fData Structure Performance \u221d Memory Stalls\n\n\u25cf Data structure performance is proportional to memory stalls\no Applies to other data structures such as trees, graphs, \u2026\n\u25cf In general, more data leads to worse performance\no But why? Does more data make MEM stalls longer? (Hint: yes)\no And why is an array not affected by data size? (I wonder \u2026)\n\u25cf You will be able to answer all these questions when we are done.\n13\n\n\fMemory Technologies\n\n14\n\n\fStatic RAM (SRAM)\n\u25cf SRAM uses a loop of NOT gates to\nstore a single bit\n\u25cf This is usually called a 6T SRAM cell\nsince it uses... 6 Transistors!\n\u25cf Pros:\no Very fast to read/write\n\u25cf Cons:\no Volatile (loses data without power)\no Relatively many transistors needed\n-> expensive\n\n15\n\n\fDynamic RAM (DRAM)\n\u25cf DRAM uses one transistor and one capacitor\no The bit is stored as a charge in the capacitor\no Capacitor leaks charge over time\n-> Must be periodically recharged (called refresh)\n-> During refresh, DRAM can\u2019t be accessed\no Accesses are slower\n-> Small charge must be amplified to be read\n-> Also after read, capacitor needs recharging again\no Reading a DRAM cell is slower than reading SRAM\n\u25cf Pros:\no Higher density -> less silicon -> much cheaper than SRAM\n\u25cf Cons:\no Still volatile (even more volatile than SRAM)\no Slower access time\n16\n\n\fSpinning magnetic disks (HDD)\n\u25cf Spinning platter coated with a ferromagnetic\nsubstance magnetized to represent bits\no Has a mechanical arm with a head\no Reads by placing arm in correct cylinder,\nand waiting for platter to rotate\n\u25cf Pros:\no Nonvolatile (magnetization persists without power)\no Extremely cheap (1TB for $50)\n\u25cf Cons:\no Extremely slow (it has a mechanical arm, enough said)\n\n17\n\n\fOther technology\n\u25cf Flash Memory\no Works using a special MOSFET with \u201cfloating gate\u201d\no Pros: nonvolatile, much faster than HDD\no Cons:\n\u25aa Slower than DRAM\n\u25aa More expensive than HDDs (1TB for $250)\n\u25aa Writing is destructive and shortens lifespan\n\n\u25cf Experimental technology\no Ferroelectric RAM (FeRAM), Magnetoresistive RAM (MRAM),\nPhase-change memory (PRAM), carbon nanotubes ...\no In varying states of development and maturity\no Nonvolatile and close to DRAM speeds\n18\n\n\fMemory/storage technologies\nVolatile\n\nNonvolatile\n\nSRAM\n\nDRAM\n\nHDDs\n\nFlash\n\nSpeed\n\nFAST\n\nOK\n\nSLOW\n\nPretty good!\n\nPrice\n\nExpensive\n\nOK\n\nCheap!\n\nMeh\n\nPower\n\nGood!\n\nMeh\n\nBad\n\nOK\n\nDurability\n\nGood!\n\nGood!\n\nGood!\n\nOK\n\nReliability\n\nGood!\n\nPretty good!\n\nMeh\n\nPretty good!\n\nI\u2019m using Durability to mean \u201chow well it holds data after repeated use.\u201d\nI\u2019m using Reliability to mean \u201chow resistant is it to external shock.\u201d\n\n19\n\n\fDo you notice a trend?\n\u25cf The faster the memory the more expensive and lower density.\n\n\u25cf The slower the memory the less expensive and higher density.\n\u25cf There exists a hierarchy in program data:\no Small set of data that is accessed very frequently\no Large set of data that is accessed very infrequently\n\n\u25cf Thus, memory should also be constructed as a hierarchy:\no Fast and small memory at the upper levels\no Slow and big memory at the lower levels\n\n20\n\n\fLarger capacity memories are slower\n\nThoziyoor, Shyamkumar & Muralimanohar, Naveen & Ahn, Jung Ho & Jouppi,\nNorman. (2008). CACTI 5.1.\n\n21\n\n\fDRAM faster than SRAM at high capacity due to density\n\n\u25cf Access Time = Memory Cell Delay + Address Decode Delay + Wire Delay\n\u25cf With high capacity, Wire Delay (word lines + bit lines) starts to dominate\n\u25cf Wire Delay \u221d Memory Structure Area\n\u25cf DRAM density > SRAM density \u2192 DRAM Wire Delay < SRAM Wire Delay\n22\n\n\fThe Memory Hierarchy\n\n23\n\n\fSystem Memory Hierarchy\n\u25cf Use fast memory (SRAM) to store frequently used data inside the CPU\n\u25cf Use slow memory (e.g. DRAM) to store rest of the data outside the CPU\n\nCPU\n\nPipeline\n\nMemory bus\ndelay\n\nSRAM\n(regs)\n\nPCIe bus\ndelay\n\nDRAM\n(memory)\n\nHDD/SDD\n(files, swapped\nout memory)\n\n\u25cf Registers are used frequently for computation so are stored in SRAM\n\u25cf Memory pages used frequently are stored in DRAM\n\u25cf Memory pages used infrequently are stored in HDD/SDD (in swap space)\n\u25cf Note: Memories outside CPU suffers from bus delay as well\n24\n\n\fSystem Memory Hierarchy\n\u25cf Use fast memory (SRAM) to store frequently used data inside the CPU\n\u25cf Use slow memory (e.g. DRAM) to store rest of the data outside the CPU\n\nCPU\n\nPipeline\n\nMemory bus\ndelay\n\nSRAM\n(regs)\n\nPCIe bus\ndelay\n\nDRAM\n(memory)\n\nHDD/SDD\n(files, swapped\nout memory)\n\n\u25cf Drawback: Memory access is much slower compared to registers\n\u25cf Q: Can we make memory access speed comparable to register access?\n\n25\n\n\fSystem Memory Hierarchy\n\u25cf Use fast memory (SRAM) to store frequently used data inside the CPU\n\u25cf Use slow memory (e.g. DRAM) to store rest of the data outside the CPU\n\nCPU\nSRAM\n(registers)\n\nDRAM\n(memory)\n\nPipeline\nSRAM\n(cache)\n\nHDD/SDD\n(files, swapped\nout memory)\n\n\u25cf Drawback: Memory access is much slower compared to registers\n\u25cf Q: Can we make memory access speed comparable to register access?\no How about storing frequently used memory data in SRAM too?\no This is called caching. The hardware structure is called a cache.\n26\n\n\fCaching\n\u25cf Caching: keeping a temporary copy of data for faster access\n\u25cf DRAM is in a sense also caching frequently used pages from swap space\no We are just extending that idea to bring cache data inside the CPU!\n\u25cf Now instructions like lw or sw never directly access DRAM\no They first search the cache to see if there is a hit in the cache\no Only if they miss will they access DRAM to bring data into the cache\n\nCPU\nSRAM\n(registers)\n\nDRAM\n(memory)\n\nPipeline\nSRAM\n(cache)\n\nHDD/SDD\n(files, swapped\nout memory)\n\n27\n\n\fCache Flow Chart\n\u25cf Cache block: unit of data used to cache data\no What page is to memory paging\no Cache block size is typically multiple words\n(e.g. 32 bytes or 64 bytes. You\u2019ll see why.)\n\n\u25cf Good: Memory Wall can be surmounted\no On cache hit, no need to go to DRAM!\n\u25cf Bad: MEM stage has variable latency\no Typically, only a few cycles if cache hit\no More than a 100 cycles if cache miss!\n(Processor must go all the way to DRAM!)\no Makes performance very unpredictable\n28\n\n\fCache Locality: Temporal and Spatial\n\n\u2022 Temporal Locality\n\n\u2022 Spatial Locality\nCache Block\nData\nItem 0\n\nData\nItem\n\nMiss!\n\nHit!\nTime\n\nHit!\n\nMiss!\n\nData\nItem 1\n\nHit!\n\nData\nItem 2\n\nHit!\nTime\n\n29\n\n\fCache Locality: Temporal and Spatial\n\u25cf Caching works because there is locality in program data accesses\no Temporal locality\n\u25aa Same data item is accessed many times in succession\n\u25aa 1st access will miss but following accesses will hit in the cache\no Spatial locality\n\u25aa Different data items spatially close are accessed in succession\n\u25aa 1st access will miss but bring in an entire cache block\n\u25aa Accesses to other items within same cache block will hit\n\u25aa E.g., fields in an object, elements in an array, \u2026\n\u25cf Locality, like ILP, is a property of the program\n\n30\n\n\fCold Misses and Capacity Misses\n\u25cf Cold miss (a.k.a. compulsory miss)\no Miss suffered when data is accessed for the first time by program\no Cold miss since cache hasn\u2019t been warmed up with accesses\no Compulsory miss since there is no way you can hit on the first access\no Subsequent accesses will be hits since now data is fetched into cache\n\u25aa Unless it is replaced to make space for more frequently used data\n\u25cf Capacity miss\no Miss suffered when data is accessed for the second or third times\no This miss occurred because data was replaced to make space\no If there had been more capacity, miss wouldn\u2019t have happened\no Capacity decides how much temporal locality you can leverage\n\n31\n\n\fReducing Cold Misses and Capacity Misses\n\u25cf Reducing capacity misses is straightforward\no Increase capacity, that is cache size!\n\u25cf But how do you reduce cold misses? Is it even possible?\no Yes! By taking advantage of spatial locality.\no Have a large cache block so you bring in other items on a miss\no Those other items may be accessed for the first time but will hit!\n\u25cf Large cache blocks can \u2026\no Potentially reduce cold misses (and/or reduce capacity misses)\n(given some spatial locality, can bring in more data on a miss)\no Potentially increase capacity misses\n(with no spatial locality, can store less data items in same capacity)\n\u2192 Each program has a sweet spot. Architects choose a compromise.\n32\n\n\fSo how big do we want the cache to be?\n\u25cf Uber big!\n\nCaches\n\n\u25cf On the right is a diagram of\nthe Xeon Broadwell CPU used\nin kernighan.cs.pitt.edu.\no Caches take up almost as\nmuch real estate as cores!\no A cache miss is that painful.\n\u25cf But having a big cache comes\nwith its own set of problems\no Cache itself gets slower\n\n33\n\n\fBigger caches are slower\n\u25cf Below is a diagram of a Nehalem CPU (an older Intel CPU)\n\u25cf How long do you\nthink it takes for\ndata to make it from\nhere...\n\u25cf ...to here?\n\u25cf It must be routed\nthrough all this.\n\u25cf Can we cache the\ndata in the far away\n\u201cL3 Cache\u201d to a\nnearby \u201cL2 Cache\u201d?\n34\n\n\fMulti-level Caching\n\u25cf This is the structure of the kernighan.cs.pitt.edu Xeon CPU:\nL1 I-Cache\n\nL1 D-Cache\n\nL2 Cache\nL3 Cache\n\n\u25cf L1 cache: Small but fast. Interfaces with CPU pipeline MEM stage.\no Split to i-cache and d-cache to avoid structural hazard\n\u25cf L2 cache: Middle-sized and middle-fast. Intermediate level.\n\u25cf L3 cache: Big but slow. Last line of defense against memory access.\n\u25cf Allows performance to degrade gracefully\n35\n\n\fRevisiting Our Experiments\n\n36\n\n\fRevisiting our CPI Results with the new perspective\nCorrespond to multiple levels of memory\n\nBut this is just conjecture.\nIs it actually true?\n\nCPI increases with bigger size: more of\ndata structure is stored in lower memory\n\n37\n\n\fkernighan.cs.pitt.edu cache specs\n\u25cf On a Xeon E5-2640 v4 CPU (10 cores):\no L1 i-cache: 32 KB 8-way set associative (per core)\no L1 d-cache: 32 KB 8-way set associative (per core)\no L2 cache: 256 KB 8-way set associative (per core)\no L3 cache: 25 MB 20-way set associative (shared)\nRef: https://en.wikichip.org/wiki/intel/xeon_e5/e5-2640_v4\n\n\u25cf Access latencies (each level includes latency of previous levels):\no L1: ~3 cycles\no L2: ~8 cycles\no L3: ~16 cycles\no DRAM Memory: ~67 cycles\n\nRef: https://www.nas.nasa.gov/assets/pdf/papers/NAS_Technical_Report_NAS-2015-05.pdf\n\n38\n\n\fCache Specs Reverse Engineering\n\u25cf Why do I have to refer to a NASA technical report for latencies?\n\no Ref: https://www.nas.nasa.gov/assets/pdf/papers/NAS_Technical_Report_NAS-2015-05.pdf\n\no Because Intel doesn\u2019t publish detailed cache specs on data sheet\n\u25cf In the technical report (does the step function look familiar?):\n\nWhy just Loads and not Stores?\n\n39\n\n\fLoads have more impact on performance\n\u25cf Suppose we added a store to our original loop:\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\n// reached the end\nelse {\ncurrent->data[0] = 100;\n// store to node data\ncurrent = current->next;\n// load next node address\n}\n\n\u25cf Which would have more impact on performance? The load or the store?\no A: The load because it is on the critical path.\n\u2026\n\ncurrent = current->next\n\ncurrent = current->next\n\ncurrent = current->next\n\n\u2026\n\ncurrent->data[0] = 100\n\ncurrent->data[0] = 100\n\ncurrent->data[0] = 100\n\n\u2026\n40\n\n\fLoads have more impact on performance\n\u25cf Loads produce values needed for computation to proceed\no Stalled loads delays computation and possibly the critical path\n\u25cf Stores write computation results back to memory\no As long as the results are written back eventually, no big hurry\no If store results in a cache miss,\nstore is marked as \u201cpending\u201d and CPU moves on to next computation\no Pending stores are maintained in a write buffer hardware structure\n\n\u25cf What if the next computation reads from a pending store?\no First check the write buffer and read in the value if it\u2019s there\no Again, performing the store is not on the critical path\n\n41\n\n\fHow Write Buffer Maintains Pending Stores\n\u25cf sw p0,4(p1) is about to commit. 4(p1) == 0xdeadbeef, p0 == 10\n\u25cf Unfortunately, address 0xdeadbeef is not in the L1 d-cache and it misses\nL1 d-cache\n\u2026\n\u2026 Miss!\n\u2026\n\u2026\n\nStore Queue\nAddress Value\n0xdeadbeef 10\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\ncommit in order\n\nInstruction Decoder\n\nWrite Buffer\nAddress Value\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n\u2026\n\u2026\n\u2026\nInstruction Queue\ninstruction dest done?\nsw p0,4(p1)\nY\n\u2026\n\u2026\n\u2026\n\nLoad/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n42\n\n\fHow Write Buffer Maintains Pending Stores\n\u25cf sw p0,4(p1) commits successfully anyway\n\u25cf The store is moved to the Write Buffer and stays there until store completes\nL1 d-cache\n\u2026\n\u2026\n\u2026\n\u2026\n\nStore Queue\nAddress Value\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n\u2026\n\u2026\n\u2026\n\ncommit in order\n\nInstruction Decoder\n\nWrite Buffer\nAddress Value\n0xdeadbeef 10\n\nInstruction Queue\ninstruction dest done?\n\u2026\n\u2026\n\u2026\n\u2026\n\nLoad/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n43\n\n\fHow Write Buffer Maintains Pending Stores\n\u25cf Later, when lw p3,0(p2) comes along, it checks Write Buffer first\n\u25cf If 0(p2) == 0xdeadbeef, Write Buffer provides value instead of memory\nL1 d-cache\n\u2026\n\u2026\n\u2026\n\u2026\n\nStore Queue\nAddress Value\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\nRetirement Register File\nt0\ns0\na0\nt1\ns1\na1\nt2\ns2\na2\n\u2026\n\u2026\n\u2026\n\ncommit in order\n\nInstruction Decoder\n\nWrite Buffer\nAddress Value\n0xdeadbeef 10\n\nInstruction Queue\ninstruction dest done?\nlw p3,0(p2) t0\nN\n\u2026\n\u2026\n\u2026\n\nLoad/\nStore\nInt ALU 1\n\nInt ALU 2\nFloat\nALU\n44\n\n\fSo are stores never on the critical path?\n\u25cf If we had an infinitely sized write buffer, no, never.\n\n\u25cf In real life, write buffer is limited and can suffer structural hazards\no If write buffer is full of pending stores, you can\u2019t insert more.\n\u2192 That will prevent a missing store from committing from i-queue\n\u2192 That will prevent all subsequent instructions from committing\n\u2192 That will eventually stall the entire pipeline\n\u25cf But with ample write buffer size, happens rarely\no And if it does happen can be detected using PMUs\n\u25cf Hence, we will focus on loads to analyze performance\n\n45\n\n\fLinked List Cache Load Miss Rates\n\nBut what do these lines\nactually mean?\n\n46\n\n\fLinked List Cache Load Miss Rates\n\nA few L2 cold misses result in high miss rate.\nBut very few L2 accesses to begin with.\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nDo the miss rates correspond to CPI results?\nLet\u2019s compare with our own eyes!\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\nMiss! Miss! Miss! Hit!\n\nL1 Cache\n\nMiss! Miss!\n\nL2 Cache\n\nMiss!\nHit!\n\nHit!\n\nHit!\n\nL3 Cache\nDRAM Memory\n47\n\n\fLinked List Cache Load Miss Rates vs CPI\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\n48\n\n\fLinked List Cache Load Miss Rates \u2013 Other Questions\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\n\u25cf Why the step up in L1 cache misses between 500 \u2013 1000 nodes?\n\u25cf Why the step up in L2 cache misses between 1000 \u2013 5000 nodes?\n\u25cf Why the step up in L3 cache misses between 100 k \u2013 500 k nodes?\n\u25cf Also, why do cache miss increases look like step functions in general?\n49\n\n\fWorking Set Overflow can cause Step Function\n\u25cf The size of a node is 128 bytes:\ntypedef struct node {\nstruct node* next;\n// 8 bytes\nint data[30];\n// 120 bytes\n} node_t;\n\u25cf Working set: amount of memory program accesses during a phase\no For linked-list.c, working set is the entire linked list\n\u25aa Program accesses entire linked list in a loop over and over again\no If there are 8 nodes in linked list, working set size = 128 * 8 = 1 KB\n\u25cf When working set overflows cache capacity, start to see cache misses\no Miss increase can be drastic, almost like a step function\no Suppose cache size is 1 KB and nodes increase from 8 \u2192 9\n\u25aa When 8 nodes: always hit (since entire list in contained in cache)\n\u25aa When 9 nodes: always miss (if least recent node is replaced first)\n50\n\n\fLinked List Cache Load Miss Rates \u2013 Other Questions\n\u25cf Why the step up in L2 cache misses between 1000 \u2013 5000 nodes?\no L2 cache size is 256 KB\no Number of nodes that can fit = 256 KB / 128 = 2048\n\u25cf Why the step up in L3 cache misses between 100 k \u2013 500 k nodes?\no L3 cache size is 25 MB\no Number of nodes that can fit = 25 MB / 128 \u2248 200 k\n\u25cf Why the step up in L1 cache misses between 500 \u2013 1000 nodes?\no L1 d-cache size is 32 KB\no Number of nodes that can fit = 32 KB / 128 = 256\no So, in theory you should already see a step up at 500 nodes\no Apparently, CPU doesn\u2019t use least-recently-used (LRU) replacement\no According to another reverse engineering paper, Intel uses PLRU\n\nRef: \u201cCacheQuery: Learning Replacement Policies from Hardware Caches\u201d by Vila et al.\nhttps://arxiv.org/pdf/1912.09770.pdf\n\n51\n\n\fLinked List Cache Load Miss Rates \u2013 Other Questions\n\nAlmost only L1 L1\n+\nL2\n\nL1 + L2 + L3\n\nL1 + L2 + L3\n+ Memory\n\nL1 + Memory\n(mostly)\n\n\u25cf Why did L1 cache miss rate saturate at around 20%?\no Shouldn\u2019t it keep increasing with more nodes like L2 and L3?\n\n52\n\n\fLinked List Cache Load Miss Rates \u2013 Other Questions\n[linked-list.c]\nvoid *run(void *unused) {\nnode_t* current = head;\nfor(int i=0; i<ACCESSES; i++) {\nif(current == NULL) current = head;\nelse current = current->next;\n}\n}\n\n[objdump -S linked-list]\n0000000000400739 <run>:\n...\n400741: mov 0x200920(%rip),%rax # %rax = head\n400748: mov %rax,-0x8(%rbp) # current = %rax\n40074c: movl $0x0,-0xc(%rbp) # i = 0\n400753: jmp 400778 # jump to i < ACCESSES comparison\n400755: cmpq $0x0,-0x8(%rbp) # current == NULL?\n40075a: jne 400769 # jump to else branch if not equal\nWithin a typical iteration in for loop:\n40075c: mov 0x200905(%rip),%rax # %rax = head\n4 blue loads that hit in L1:\n400763: mov %rax,-0x8(%rbp) # current = %rax\n\u2022 2 loads each of local vars current, i 400767: jmp 400774 # jump to end of if-then-else\n\u2022 Read frequently so never replaced\n400769: mov -0x8(%rbp),%rax # %rax = current\n1 red load that misses in L1:\n40076d: mov (%rax),%rax # %rax = current->next\n\u2022 current->next (next field of node)\n400770: mov %rax,-0x8(%rbp) # current = %rax\n\u2022 Node may not be in cache and miss 400774: addl $0x1,-0xc(%rbp) # i++\n(e.g. due to a capacity miss)\n400778: cmpl $0x3b9ac9ff,-0xc(%rbp) # i < ACCESSES ?\n\uf045 1 miss / 5 loads = 20% miss rate\n40077f: jle 400755 # jump to head of loop if less than\n53\n\n\fHow about Linked List with No Data?\n\n\u25cf Linked list, no data suffered almost no CPI degradation. Why?\n\n54\n\n\fLinked List w/ Data vs. w/o Data\n\u25cf Linked list with data\n\n\u25cf Linked list with no data\n\n55\n\n\fLinked List w/ Data vs. w/o Data. Why?\n\u25cf The size of a node with no data is only 8 bytes:\ntypedef struct node {\nstruct node* next;\n// 8 bytes\n// no data\n} node_t;\n\u25cf Compared to 128 bytes with data, can fit in 16X more nodes in cache\no Temporal locality: More likely that a node will be present in cache\n\n\u25cf How about L1 cache miss rate that hovers around 10% instead of 20%?\no By 107 nodes, there is no temporal locality with respect to the L1 cache\no Spatial locality must be responsible for the reduction in miss rate\n\n56\n\n\fLinked List w/ Data vs. w/o Data. Why?\n\u25cf Nodes of the linked list are malloced one by one in a loop:\nfor(int i=0; i<items; i++) {\nnode_t* n = (node_t*)malloc(sizeof(node_t));\n\u2026\n}\no I have no idea where glibc malloc decides to allocate each node\n\u25cf But knowing each cache block is 64 bytes long in the Xeon E5 processor\no Let\u2019s say multiple nodes are allocated on same cache block:\nnode 1\n\nmeta-data meta-data\n\nnode 37 meta-data meta-data node 23 meta-data\n\no Then even if access to node 1 misses, due to a capacity miss,\naccesses to nodes 37 and 23 that soon follow will hit!\no This is assuming there is some spatial locality in how malloc allocates\n57\n\n\fData structure with most spatial locality: Array\n\u25cf Elements of an array are guaranteed to be in contiguous memory:\nvoid *create(void *unused) {\nhead = (node_t *) malloc(sizeof(node_t) * items);\n\u2026\n}\n\u25cf Each cache block is 64 bytes long in the Xeon E5 processor\no Now 8 elements are guaranteed to be on same cache line, in order:\nnode 0 node 1 node 2 node 3 node 4 node 5 node 6 node 7\n\no Even with cold cache, only 1 out of 8 nodes miss (1/8 = 12.5% miss rate)\no Assuming that nodes are accessed sequentially\n\u25aa If accessed in random order, no spatial locality, even with array\no True regardless of capacity (even if cache contained only a single block)\n58\n\n\fLet\u2019s look at the CPI of arrays finally\n\n\u25cf Array, no data did very well as expected\no The most spatial locality of the four benchmarks (contiguous nodes)\no Smallest memory footprint so can also leverage temporal locality the best\n\u25cf Array performed the same as array, no data. How come?\no No spatial locality since each node takes up two 64-byte cache blocks\no Has much larger memory footprint compared to array, no data\no This is the real mystery. We will learn more about it as we go on. \u263a\n59\n\n\fImpact of Memory\nOn Performance\n\n60\n\n\fImpact of Memory on Performance\n\u25cf CPU Cycles = CPU Compute Cycles + Memory Stall Cycles\no CPU Compute Cycles = cycles where CPU is not stalled on memory\no Memory Stall Cycles = cycles where CPU is waiting for memory\n\u25cf Why do we need to differentiate between the two?\no Because each are improved by different design features!\n\u25cf HW/SW design features that impact CPU Compute Cycles:\no HW: Pipelining, branch prediction, wide execution, out-of-order\no SW: Optimizing the computation in the program\n\u25cf HW/SW design features that impact Memory Stall Cycles:\no HW: Caches, write buffer, prefetcher (we haven\u2019t learned this yet)\no SW: Optimizing memory access pattern of program\n\n(Taking into consideration temporal and spatial locality)\n\n61\n\n\fHow about overclocking using DVFS?\n\u25cf CPU Time = CPU Cycles * Cycle Time\n= (CPU Compute Cycles + Memory Stall Cycles) * Cycle Time\n\u25cf What if we halved the Cycle Time using DVFS?\no Memory Stall Cycles could increase by up to 2X!\no Why? DRAM speed remains the same, so you need twice the cycles.\n\u25aa The bus (wire) that connects CPU to DRAM is not getting any faster\n\u25aa The DRAM chip itself is not getting clocked any faster\no How about caches? Same number of cycles regardless of DVFS.\n\u25cf If most of your time is spent accessing DRAM, then overclocking is useless\no Memory Stall Time (Memory Stall Cycles * Cycle Time) is mostly constant\n\n62\n\n\fMemory Latency vs. Memory Bandwidth\n\u25cf Memory stall cycles comes from two sources:\no Memory latency: seconds to handle a single memory request\n\nvs.\no Memory bandwidth: maximum requests handled per second\n\nvs.\n\n63\n\n\fMemory bandwidth puts a ceiling on performance\n\u25cf Memory latency can be surmounted by smart scheduling\no Either by compiler or by instruction queue scheduling by CPU\n\u25cf No real way to surmount memory bandwidth limit\no No matter how much scheduling you do, same bandwidth\n\u25cf When you get a traffic jam, performance is dictated by bandwidth\no How quickly you pull data in, rather than how fast you process it\no Operational intensity = work (FLOP) per memory access (byte)\no Performance = work / second\n= work / byte * byte / second\n= operational intensity * memory bandwidth\n\u2192 Linear relationship between performance and operational intensity\n64\n\n\fThe Roofline Model: A bird\u2019s eye view of performance\n\u25cf Roofline: theoretical performance achievable given an intensity\no Formed by memory bandwidth bounds + peak CPU performance\n\nWhere do these gaps come from?\n\n65\n\n\fMemory bound vs. compute bound apps\n\u25cf I = intensity, \u03b2 = peak bandwidth, \u03c0 = peak performance\n\n66\n\n\fEffect of Cache on Roofline Model\n\u25cf With caching, apps shift upwards and rightwards\n\nReduction in memory latency makes CPU efficient\nReduction in memory accesses increases operational intensity\n\n67\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}