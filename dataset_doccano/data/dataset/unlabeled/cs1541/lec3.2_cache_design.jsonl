{"id": 204, "segment": "unlabeled", "course": "cs1541", "lec": "lec3.2_cache_design", "text": "Cache Design\nCS 1541\nWonsun Ahn\n\n\fOracle Cache\n\u25cf CPU Cycles = CPU Compute Cycles + Memory Stall Cycles\n\n\u25cf Oracle cache: a cache that never misses\no In effect, Memory Stall Cycles == 0\no Impossible, since even with infinite capacity, there are still cold misses\no But useful to set bounds on performance\n\u25cf Real caches may approach performance of oracle caches but can\u2019t exceed\n\n\u25cf What metric can we use to compare and evaluate real cache designs?\no AMAT (Average Memory Access Time)\n\n2\n\n\fAMAT (Average Memory Access Time)\n\u25cf AMAT (Average Memory Access Time) is defined as follows:\no AMAT = hit time + (miss rate \u00d7 miss penalty)\no Hit time: time to get the data from cache when we hit\no Miss rate: what percentage of cache accesses we miss\no Miss penalty: time to get the data from lower memory when we miss\no Shouldn\u2019t it be hit rate \u00d7 hit time?\n\u25aa Hit time is incurred regardless of hit or miss\n\u25aa It is more aptly called access time (the time to search for the data)\n\n\u25cf Hit time, miss rate, miss penalty are the 3 components of a cache design\no When evaluating a cache design, we need to consider all 3\no Cache designs trade-off one for the other\n\u25aa E.g. a large cache trade-offs longer hit time for smaller miss rate\n\u25aa Whether trade-off is beneficial depends on the resulting AMAT\n3\n\n\fCache Design Parameter 1:\nNumber of Levels\n\n4\n\n\fAMAT for Multi-level Caches\n\u25cf For a single-level cache (L1 cache):\no AMAT(L1) = L1 hit time + (L1 miss rate \u00d7 DRAM access time)\nMiss! Hit!\n\nL1 Cache\n\nHit!\n\nDRAM Memory\n\nL1 hit time\nL1 miss rate \u00d7 DRAM access time\n\n\u25cf For a multi-level cache (L1, L2 caches):\no AMAT(L2) = L1 hit time + (L1 miss rate \u00d7 L1 miss penalty)\no L1 miss penalty = L2 hit time + (L2 miss rate \u00d7 DRAM access time)\no AMAT(L2) = L1 hit time + L1 miss rate \u00d7 L2 hit time\n+ L1 miss rate \u00d7 L2 miss rate \u00d7 DRAM access time\nMiss! Miss! Hit!\n\nL1 Cache\n\nL1 hit time\n\nMiss!\n\nL2 Cache\n\nL1 miss rate \u00d7 L2 hit time\n\nHit!\n\nHit!\n\nDRAM Memory\n\nL1 miss rate \u00d7 L2 miss rate \u00d7 DRAM access time\n5\n\n\fAMAT for Multi-level Caches\n\u25cf For L2 Cache to be worth it, AMAT(L1) > AMAT(L2) needs to be true.\nL1 Cache\n\nDRAM Memory\n\nL1 hit time\nL1 miss rate \u00d7 DRAM access time\n\n>?\nL1 Cache\n\nL1 hit time\n\nL2 Cache\n\nL1 miss rate \u00d7 L2 hit time\n\nDRAM Memory\n\nL1 miss rate \u00d7 L2 miss rate \u00d7 DRAM access time\n\n\u25cf AMAT(L1) \u2013 AMAT(L2)\n= (L1 miss rate \u2013 L1 miss rate \u00d7 L2 miss rate) \u00d7 DRAM access time\n\u2013 L1 miss rate \u00d7 L2 hit time\n= L1 miss rate \u00d7 ((1 \u2013 L2 miss rate) \u00d7 DRAM access time \u2013 L2 hit time) > 0\n\u2192 (1 \u2013 L2 miss rate) \u00d7 DRAM access time > L2 hit time\n\u2192 Benefit from reduced DRAM accesses > Penalty from L2 accesses\n6\n\n\fAMAT for Multi-level Caches\n\u25cf (1 \u2013 L2 miss rate) \u00d7 DRAM access time > L2 hit time\no Let\u2019s assume L2 miss rate = 0.9 and DRAM access time = 100 cycles:\n(1 \u2212 0.9) \u00d7 100 > L2 hit time\nL2 hit time < 10\no If L2 hit time can be kept below 10 cycles, worth it to install L2 cache\n\u25cf So, should we install the L2 cache, or not? That depends on the program!\no Locality in program determines cache capacity required for 0.9 miss rate\no If we can design a cache with hit time < 10 for that capacity, go for it\n\u25cf Again, shows design decisions are heavily impacted by needs of software\n\n7\n\n\fCache Design Parameter 2:\nCache Size\n\n8\n\n\fImpact of Cache Size (a.k.a. Capacity) on AMAT\n\u25cf AMAT = hit time + (miss rate \u00d7 miss penalty)\n\n\u25cf Larger caches are good for miss rates\no More capacity means you can keep around cache blocks for longer\no Means you can leverage more of the pre-existing temporal locality\no If entire working set can fit into the cache, no capacity misses!\n\u25cf But larger caches are bad for hit times\no Longer wires and larger decoders mean longer access time\n\u25cf Exactly why there are multiple levels of caches\no Frequently accessed data where hit time is important stays in L1 cache\no Rarely accessed data which is part of a larger working set stays in L3\n\n9\n\n\fWhat cache size(s) should I choose?\n\u25cf How should each cache level be sized?\n\n\u25cf That depends on the application\no Working set sizes of the application at various levels. E.g.:\n\u25aa Small set of data accessed very frequently (typically stack variables)\n\u25aa Medium set of data accessed often (currently accessed data structure)\n\u25aa Large set of data accessed rarely (rest of program data)\no Ideally, cache levels and sizes would reflect working set sizes.\n\u25cf Simulate multiple cache levels and sizes and choose one with lowest AMAT\no Simulate on the applications that you care about\no In the end, it must be a compromise (giving best average AMAT)\n\n10\n\n\fCache Design Parameter 3:\nCache Block Size\n\n11\n\n\fImpact of Cache Block Size on AMAT\n\u25cf AMAT = hit time + (miss rate \u00d7 miss penalty)\n\n\u25cf Cache block (a.k.a. cache line)\no Unit of transfer for cache data (typically 32 or 64 bytes)\no If program accesses any byte in cache block, entire block is brought in\no Each level of a multi-level cache can have a different cache block size\n\u25cf Impact of larger cache block size on miss rate\no Maybe smaller miss rate due to better leveraging of spatial locality\no Maybe bigger miss rate due to worse leveraging of temporal locality\n(Bringing in more data at a time may push out other useful data)\n\u25cf Impact of larger cache block size on miss penalty\no With a limited bus width, may take multiple transfers for a large block\no E.g. DDR 4 DRAM bus width is 8 bytes, so 8 transfers for 64-byte block\no Could lead to increase in miss penalty\n12\n\n\fCache Block Size and Miss Penalty\n\u25cf On a miss, the data must come from lower memory\n\u25cf Besides memory access time, there\u2019s transfer time\n\u25cf What things impact how long that takes?\no The size of the cache block (words/block)\no The width of the memory bus (words/cycle)\no The speed of the memory bus (cycles/second)\n\u25cf So the transfer time will be:\n\nseconds\n\ud835\udfcf\nwords\n=\n\u00d7\ncycles words block\nblock\n\u00d7\nsecond cycle block size\nbus speed\n\nCache\n\nMemory\n\nbus width\n13\n\n\fWhat cache block size should I choose?\n\u25cf Again, that depends on the application\no How much spatial and temporal locality the application has\n\u25cf Simulate multiple cache block sizes and choose one with lowest AMAT\no Simulate on benchmarks that you care about and choose best average\no You may have to simulate different combinations for multi-level caches\n\n14\n\n\fCache Design Parameter 4:\nCache Associativity\n\n15\n\n\fMapping blocks from memory to caches\n\u25cf Cache size is much smaller compared to the entire memory space\no Must map all the blocks in memory to limited CPU cache\n\u25cf Does this sound familiar? Remember branch prediction?\no Had similar problem of mapping PCs to a limited BHT\no What did we do then?\n\u25aa We hashed PC to an entry in the BHT\n\u25aa On a hash conflict, we replaced old entry with more recent one\n\n\u25cf We will use a similar idea with caches\no Hash memory addresses to entries in cache\no On a conflict:\n\u25aa Replace old cache block with more recent one\n\u25aa Or, chain multiple cache blocks on to same hash entry\n16\n\n\fImpact of Cache Associativity on AMAT\n\u25cf Depending on hash function and chaining, a cache is either:\no Direct-mapped (no chaining allowed)\no Set-associative (some chaining allowed)\no Fully-associative (limitless chaining allowed)\n\n\u25cf Impact of more associativity on miss rate\no Smaller miss rate due to less misses due to hash conflicts\no Misses due to hash conflicts are called conflict misses\n\u25aa A third category of misses besides cold and capacity misses\n\u25cf Impact of more associativity on hit time\no Longer hit time due to need to search through long chain\n\n17\n\n\fDirect-mapped Caches\n\n18\n\n\fAssumptions\n\u25cf Let\u2019s assume for the sake of concise explanations\no 8-bit memory addresses\no 4-byte (one word) cache block sizes\n\u25cf Of course these are not typical values. Typical values are:\no 32-bit or 64-bit memory addresses (32-bit or 64-bit CPU)\no 32-byte or 64-byte cache blocks sizes (for spatial locality)\no But too many bits in addresses are going to give you a headache\n\u25cf According to our assumption, here\u2019s a breakdown of address bits\nUpper 6 bits: Offset of cache\nblock within main memory\n\nLower 2 bits: Byte offset\nwithin 4-byte cache block\n\no When I refer to addresses, I will sometimes omit the lower 2 bits\n(When we talk about cache block transfer, that part is irrelevant)\n19\n\n\fDirect-mapped Cache Hash Function\n\u25cf Each memory address maps to one cache block\n\u25cf No chaining allowed so no need to search\n\u25cf Implementing this is relatively simple\nHash function:\nFor this 8-entry cache, to\nfind cache block index,\ntake the lowest 3 cache\nblock offset bits in address.\n\nBut if our program\naccesses 001000, then\n000000, how do we tell\nthem apart?\nTags!\n\nCache\n000\n001\n010\n011\n\n100\n101\n110\n111\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n20\n\n\fTags help differentiate between conflicting blocks\nTag: part of address excluding cache block index\n\u25cf On allocation of 001000: tag = 001\nTag\n000\n001\n010\n011\n\n100\n101\n110\n111\n\n001\n\nCache\n\nData\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n21\n\n\fTags help differentiate between conflicting blocks\nTag: part of address excluding cache block index\n\u25cf On allocation of 001000: tag = 001\n\u25cf On allocation of 000000: tag = 000\nTag\n000\n001\n010\n011\n\n100\n101\n110\n111\n\n000\n\nCache\n\nData\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n22\n\n\fValid bit indicates that block contains valid data\nValid bit: indicates that the block is valid\n\u25cf Set to 0 initially when cache block is empty\n\u25cf Set to 1 when a cache block is allocated\nV\n\nTag\n\n000 1\n\n000\n\nCache\n\nData\n\n001 0\n010 0\n011 0\n\n100 0\n101 0\n110 0\n111 0\n\n\u25cf Cache hit: V == 1 &&\nCacheBlock.Tag == MemoryBlock.Tag\n\nMemory\n000000\n000001\n000010\n000011\n000100\n\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n\n001100\n001101\n001110\n001111\n23\n\n\fQuiz: Address Bits Breakdown\n\u25cf Now with the following parameters:\no 8-bit memory addresses\no 4-byte cache block sizes\no 8-block cache\n\u25cf How would we breakdown the memory address bits?\nTag\n\nBlock index\n\nOffset within\ncache block\n\no First, the correct cache block is accessed using the block index\no Then, the tag is compared to the cache block tag\no If matched, offset is used to access specific byte within block\n\n24\n\n\fExample: A Direct-mapped Cache\n\u25cf When the program first starts, we set all the valid bits to 0.\no Signals all cache lines are empty\nV\nTag\nData\n\u25cf Now let's try a sequence of reads...\n000\n0\n1\n000 something\n010\ndo these hit or miss? How do the\n001\n0\ncache contents change?\n000000 miss\n100101 miss\n100100 miss\n100101 hit\n010000 miss\n000000 miss\n\nCold misses\n\nCold miss\nCapacity miss?\n\n010\n\n0\n\n011\n\n0\n\n100\n\n0\n1\n\n100\n\nsomething\n\n101\n\n1\n0\n\n100\n\nsomething\n\n110\n\n0\n\n111\n\n0\n\n25\n\n\fConflict Misses\n\u25cf What should we call 2nd miss on 000000?\no Awkward to call it a capacity miss\n(It\u2019s not like capacity was lacking)\n000\no Let\u2019s call it a conflict miss\n\nV\n\nTag\n\nData\n\n0\n1\n\n000\n010\n\nsomething\n\n001\n\n0\n\n010\n\n0\n\n011\n\n0\n\n100\n\n0\n1\n\n100\n\nsomething\n\n101\n\n1\n0\n\n100\n\nsomething\n\n110\n\n0\n\n111\n\n0\n\n000000 miss\n100101 miss\n100100 miss\n100101 hit\n010000 miss\n000000 miss\n\nCold misses\n\nCold miss\nCapacitymiss!\nConflict\nmiss?\n\n26\n\n\fTypes of Cache Misses (Revised)\n\u25cf Besides cold misses and capacity misses, there are conflict misses\n\n\u25cf Cold miss (a.k.a. compulsory miss)\no Miss suffered when data is accessed for the first time by program\n\u25cf Capacity miss\no Miss on a repeat access suffered due to a lack of capacity\no When the program's working set is larger than can fit in the cache\n\u25cf Conflict miss\no Miss on a repeat access suffered due to a lack of associativity\no Associativity: degree of freedom in associating cache block with an index\no Direct mapped caches have no associativity\n\u25aa Since cache blocks are directly mapped to a particular block index\n27\n\n\fAssociative caches\n\n28\n\n\fFlexible block placement\n\u25cf Direct-mapped caches can have lots of conflicts\no Multiple memory locations \"fight\" for the same cache line\n\u25cf Suppose we had a 4-block direct-mapped cache\nV Tag Data\no As before, 4-byte per cache block\n00\n0 0000\n1\n0001\n0011\n0010\no Memory addresses are 8 bits.\n01\n0\n\u25cf The following locations are accessed in a loop:\n10\n0\no 0, 16, 32, 48, 0, 16, 32, 48...\n11\n0\no or 000000, 000100, 001000, 001100, \u2026\n\u25cf What would happen?\no They will all land on the same block index, and all conflict miss!\no Those other 3 blocks are not even getting used!\no What if we used the space to chain conflicting blocks?\n\n29\n\n\fFull associativity\n\u25cf Let's make our 4-block cache 4-way set-associative.\nV\n\nTag\n\n1\n0 000000\n\nD\n\nV\n\nTag\n\n*0\n\n1 001100\n0\n\nD\n\nV\n\nTag\n\n*48\n\n1\n0 000100\n\nD\n\nV\n\nTag\n\n*16\n\n0\n1 001000\n\nD\n*32\n\n\u25cf What's the difference?\no Now a hashed location can be associated with any of the 4 blocks\no Analogous to having a hash conflict chain 4-entries long\no The 4 cache blocks are said to be part of a cache set\no When set size == cache size, it is said to be fully associative\n\u25cf Let's do that sequence of reads again: 0, 16, 32, 48, 0, 16, 32, 48...\n\u25cf Notice tag is now bigger, since there are no block index bits\no Or set index bits in this context (just one set, so none needed)\n\u25cf Now cache holds the entire working set: no more misses!\n30\n\n\fExample: A 2-way Set-Associative Cache\n\u25cf 16-block 2-way set-associative cache\n\u25cf Let\u2019s try the same stream of accesses as direct-mapped cache\n\u25cf Yay! 2nd access to 000000 is no longer a conflict miss!\n\n000000 miss\n100101 miss\n100100 miss\n100101 hit\n010000 miss\n000000 hit\n\nSet\n\nV\n\nTag\n\nData\n\nV\n\nTag\n\nData\n\n000\n\n0\n1\n\n000\n\nsomething\n\n0\n1\n\n010\n\nsomething\n\n001\n\n0\n\n0\n\n010\n\n0\n\n0\n\n011\n\n0\n\n0\n\n100\n\n0\n1\n\n100\n\nsomething\n\n0\n\n101\n\n1\n0\n\n100\n\nsomething\n\n0\n\n110\n\n0\n\n0\n\n111\n\n0\n\n0\n31\n\n\fAddress Bits Breakdown\n\u25cf A fully associative cache (doesn\u2019t matter how many blocks):\nTag\n\n\u25cf With 16-block 4-way set-associative cache:\nTag\n\nSet index\n\nOffset within\ncache block\n\nOffset\n\no 16 / 4 = 4 sets in cache. So, 2 bits required for set index.\n\u25cf With 64-block 8-way set-associative cache:\nTag\n\nSet index\n\nOffset\n\no 64 / 8 = 8 sets in cache. So, 3 bits required for set index.\n32\n\n\fWant More Examples?\n\u25cf Try out the Cache Visualizer on the course github:\no https://github.com/wonsunahn/CS1541_Spring2022/tree/main/re\nsources/cache_demo\no Courtesy of Jarrett Billingsley\n\n\u25cf Visualizes cache organization for various parameters\no Cache block size\no Number of blocks in cache (capacity)\no Cache associativity\n\n33\n\n\fAssociativity is Costly\n\u25cf Associativity requires complex circuitry and may increase hit time\n\u25cf Full associativity is only used for very small caches\no And where a cache miss is extremely costly\n\u25cf Usually caches are 2-, 4-, or maybe 8- way set-associative\nBottom bit selects set (row)\n\nV\n\nTag\n\nD\n\nV\n\n1\n\n10010\n\n-2\n\n0\n\nV\n\nTag\n\nD\n\n1\n\n01010\n\n64\n\n000110?\n\n00011\n\nAddress\n\nRemaining tag bits\nShare comparators across all rows\n\nTag\n\nD\n\nV\n\nTag\n\nD\n\n1\n\n11111\n\n9999\n\n=\n\n=\nOR\n34\n\n\fAccess/cycle time as a function of associativity\n\nThoziyoor, Shyamkumar & Muralimanohar,\nNaveen & Ahn, Jung Ho & Jouppi, Norman.\n(2008). CACTI 5.1.\n\n35\n\n\fCache Design Parameter 5:\nCache Replacement Policy\n\n36\n\n\fCache Replacement\n\u25cf If we have a cache miss and no empty blocks, what then?\nV\n\nTag\n\n1\n0 000000\n\nD\n\nV\n\nTag\n\n*0\n\n1 001100\n0\n\nD\n\nV\n\nTag\n\n*48\n\n000001\n1\n0 000100\n\nD\n\nV\n\nTag\n\n*4\n*16\n\n0\n1 001000\n\nD\n*32\n\n\u25cf Let's read memory address 4 (00000100).\no Uh oh. That's a miss. Where do we put it?\n\u25cf With associative caches, you must have a replacement scheme.\no Which block to evict (kick out) when you're out of empty slots?\n\u25cf The simplest replacement scheme is random.\no Just pick one. Doesn't matter which.\n\u25cf What would make more sense?\no How about taking temporal locality into account?\n37\n\n\fLRU (Least-Recently-Used) Replacement\n\u25cf When you need to evict a block, kick out the oldest one.\nV\n\nTag\n\n000001\n1\n0 000000\n\nD\n\nV\n\n*4\n*0\n\n1 001100\n0\n\n4 reads old\n\nTag\n\nD\n\nV\n\n*48\n\n1\n0 000100\n\n1 read old\n\nTag\n\nD\n\nV\n\n*16\n\n0\n1 001000\n\n3 reads old\n\nTag\n\nD\n*32\n\n2 reads old\n\n\u25cf Our read history looked like 0, 16, 32, 48. How old are the blocks?\n\u25cf Now we want to read address 4. Which block should we replace?\n\u25cf But now we must maintain the age of the blocks\no Easy to say. How do we keep track of this in hardware?\n\u25cf Have a saturating counter for each cache block indicating age\no When accessing a set, increment counter for each block in set\no On a cache hit, reset counter to 0 (most recently used)\n\n38\n\n\fImpact of LRU on AMAT\n\u25cf AMAT = hit time + (miss rate \u00d7 miss penalty)\n\n\u25cf Impact of LRU on miss rate\no Smaller miss rate due to better leveraging of temporal locality\n(Recently used cache lines more likely to be used again)\n\u25cf Saturating counter for LRU uses bits and adds to amount of metadata\no Cache tag, the valid bit, the saturating counter are all metadata\no Every bit you spend on metadata is a bit you don\u2019t spend on real data\no Spending many bits on counter may reduce capacity for real data\no This may lead to a larger miss rate, if LRU is not very effective\n\n39\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}