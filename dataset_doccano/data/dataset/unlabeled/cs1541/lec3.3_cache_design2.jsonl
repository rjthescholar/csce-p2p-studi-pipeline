{"id": 211, "segment": "unlabeled", "course": "cs1541", "lec": "lec3.3_cache_design2", "text": "Cache Design 2\nCS 1541\nWonsun Ahn\n\n\fCache Design Parameter 6:\nWrite-Through vs. Write-Back\n\n2\n\n\fWrites and Cache Consistency\n\u25cf Assume &x is 1110102, and x == 24 initially\n\nlw\nt0, &x\naddi t0, t0, 1\nsw\nt0, &x\n\n# x++\n\n\u25cf How will the lw change the cache?\n\u25cf How will the sw change the cache?\no Uh oh, now the cache is inconsistent.\n(Memory still has the old value 24.)\n\nV\n000\n\n0\n\n001\n\n0\n\n010\n\n1\n0\n\n011\n\n0\n\n100\n\n0\n\n101\n\n0\n\n110\n\n0\n\n111\n\n0\n\nTag\n\nData\n\n111\n\n25\n24\n\n\u25cf How can we solve this? Two policies:\no Write-through: Propagate write all the way through memory\no Write-back: Write back cache block when it is evicted from cache\n3\n\n\fWrite-Through Policy\n\n4\n\n\fPolicy 1: Write-through\n\u25cf Write-through:\no On hit, write to cache block and propagate write to lower memory\no On miss, keep on propagating the write to lower memory\n\u25cf What happens if we write 25 to address 1110102?\n\u25cf What happens if we write 94 to address 0000102?\n\u2192 Caches are kept consistent at all points in time!\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV Tag Data\n\nV Tag Data\n\n...\n\n...\n\n000 0\n\n000 0\n\n000010\n\n94\n93\n\n001 0\n\n001 0\n\n...\n\n...\n\n010 1 111\n\n24\n25\n\n010 1 000\n\n93\n94\n\n111010\n\n25\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n5\n\n\fWrite-through: Reads\n\u25cf What happens if we read from address 0000102?\no We can just discard the conflicting cache block 1110102\no It\u2019s just an extra copy of the same data\n\u25cf Note how we allocate blocks only on read misses\no Write misses don\u2019t allocate blocks because it doesn\u2019t help anyway\n--- writes are propagated to lower memory even on write hits\no This policy is called no write allocate\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV Tag Data\n\nV Tag Data\n\n...\n\n...\n\n000 0\n\n000 0\n\n000010\n\n94\n93\n\n001 0\n\n001 0\n\n...\n\n...\n\n010 1 000\n111\n\n24\n94\n25\n\n010 1 000\n\n93\n94\n\n111010\n\n25\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n6\n\n\fWrite-through: Drawbacks\n\u25cf Drawback: Long write delays regardless of hit or miss\no Must always propagate writes all the way to DRAM\n\u25cf Solution: Write buffer maintaining pending writes\no CPU gets on with work after moving pending write to write buffer\no But does the write buffer solve all problems?\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV Tag Data\n\nV Tag Data\n\n...\n\n...\n\nWrite Buffer\n\n000 0\n\n000 0\n\n000010\n\n94\n93\n\nV Tag Data\n\n001 0\n\n001 0\n\n...\n\n...\n\n010 1 111\n\n24\n25\n\n010 1 000\n\n93\n94\n\n111010\n\n25\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n7\n\n\fWrite-through: Drawbacks\n\u25cf The write buffer does not solve all problems.\n\n1. Write buffer must be very big to store all pending writes\no May take more than 100 cycles for write to propagate to memory\no Write buffer is always checked before L1$ \u2192 adds to hit time\n2. Write buffer does not solve bandwidth problems\no If memory bandwidth < rate of writes in program,\nwrite buffer will fill up quickly, no matter how big it is\n\u25cf Impractical to write-through all the way to memory\no Typically only L1 caches are write-through, if any\n\u25cf We need another strategy that is not so bandwidth-intensive\n8\n\n\fWrite-Back Policy\n\n9\n\n\fPolicy 2: Write-back\n\u25cf Dirty block: a block that is temporarily inconsistent with memory\no On a hit, write to cache block, marking it dirty. No propagation.\no Write back dirty block to lower memory only when it is evicted\n\u2192 Saves bandwidth since write hits no longer access memory\n\u25cf A dirty bit is added to the cache block metadata (marked \u201cD\u201d)\no Block 0000012 is clean \u2192 can be discarded on eviction\no Block 1110102 is dirty \u2192 needs to be written back on eviction\nMemory\n\nCache\nV D Tag Data\n000 0 0\n001 0 0 000\n\n93\n\n010 1 1 111\n\n25\n\n...\n\n...\n\n...\n\nAddress\n\nData\n\n...\n\n...\n\n000001\n\n93\n\n...\n\n...\n\n111010\n\n24\n\n...\n\n...\n10\n\n\fWrite-back: Write allocate\n\u25cf What happens on a write miss?\no If no write allocate like write-through, will miss again on next write\no And on the next write, and on the next write, \u2026\no No bandwidth savings from hitting in cache\n\n\u25cf Unlike write-through, write-back has a write allocate policy\no On write miss, block is allocated in cache to stop further misses\no On allocation, the block is read in from lower memory\n\u25cf Q: Why the wasted effort?\no Aren\u2019t we going to overwrite the block anyway with new data?\no Why read in data that is going be overwritten?\n\n11\n\n\fWrite-back: Write allocate\n\u25cf Because a block is multiple bytes, and you are updating just a few\no Suppose a cache block is 8 bytes (2 words)\no Suppose you are writing to only the first word\nV D\n1\n\n1\n\nTag\n\nData\nfirst word (written)\n\nsecond word (not written)\n\no After allocate, the entire cache block is marked valid\n\u25aa That means second word as well as first word must be valid\n\u25aa That means second word must be fetched from lower memory\n\u25aa Otherwise if later second word is read, it will contain junk data\n\u25aa Unavoidable, unless you have a valid bit for each byte\n\u2013 That means spending 1 bit for every 8 bits of data\n\u2013 That\u2019s just too much metadata overhead\n12\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 25 to address 1110102?\n\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n010 1 0 111\n\n24\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n13\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 25 to address 1110102?\no L1 Cache hit! Update cache block and mark it dirty.\no That\u2019s it! How quick is that compared to write-through?\n\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n010 1 0\n1 111\n\n24\n25\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n14\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 94 to address 0000102?\no L1 Cache miss! First thing we will do is add store to Write Buffer.\n(So that the CPU can continue executing past the store)\n\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n1\n\n94\n\n010 1 1 111\n\n25\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n15\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 94 to address 0000102? (cont\u2019d)\no Next the L2 Cache is searched and it\u2019s a hit!\no To bring in block to L1 Cache, we first need to evict block 25.\no It\u2019s a dirty block, so we can\u2019t just discard it. Need to write it back!\no Since block 25 misses in L2, it will take the long trip to Memory\no Is there a way to put it aside and get to it later?\nMemory\n\nL1 Cache\n\nL2 Cache\n\nAddress\n\nData\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\nWrite Buffer 000 0 0\n\n000 0 0\n\n000010\n\n93\n\nV Tag Data 001 0 0\n\n001 0 0\n\n...\n\n...\n\n1\n\n94\n\n010 1 1 111\n\n25\n\n010 1 0 000\n\n93\n\n111010\n\n24\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n16\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 94 to address 0000102? (cont\u2019d)\no Yes! Add Write Buffers to caches, just like we did for the pipeline!\no Move block to L1 Write Buffer so L1 Cache can continue working\no Pending block will get written back to Memory eventually\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n010 1\n0 1\n0 111\n\n001 0 0\n25\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n1\n\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n0 0\n\n...\n\n...\n\n94\n\n17\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 94 to address 0000102? (cont\u2019d)\no Now we can finally read in block 93 to the L1 Cache\n\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 0 0\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n1\n\n0 1\n0 111\n1\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n...\n\n...\n\n94\n\n25\n\n18\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 94 to address 0000102? (cont\u2019d)\no Now we can finally read in block 93 to the L1 Cache\no And write 94 into the cache block, also marking it dirty\no Store is finished, so now remove it from pipeline Write Buffer!\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n1\n\n0 1\n0 111\n1\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n...\n\n...\n\n94\n\n25\n\n19\n\n\fPolicy 2: Write-back\n\u25cf What happens if we write 94 to address 0000102? (cont\u2019d)\no Eventually, the pending block in L1 Write Buffer will write back\no But this didn\u2019t affect the original store latency\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 1\n0 111\n1\n0 0\n\n0 0\n\n111010\n\n24\n\n0 0\n\n...\n\n...\n\n25\n\n20\n\n\fWrite-back: Reads\n\u25cf What happens if we read 25 from address 1110102?\no Misses in L1 and L2 caches and must go all the way to Memory\n\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 000\n\nMemory\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n0 0\n\n...\n\n...\n21\n\n\fWrite-back: Reads\n\u25cf What happens if we read 25 from address 1110102?\no Misses in L1 and L2 caches and must go all the way to Memory\no Fills the L2 Cache with 25 on the way back after evicting block 93\n(Note that block 93 can simply be discarded since it\u2019s clean)\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 111\n000\n\nMemory\n25\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n0 0\n\n...\n\n...\n22\n\n\fWrite-back: Reads\n\u25cf What happens if we read 25 from address 1110102? (cont\u2019d)\no Now it needs to evict block 94 in L1 Cache before filling with 25\no But block 94 needs to be written back since it\u2019s dirty!\no So move to Write Buffer temporarily to make space.\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n\n010 1\n0 1\n0 000\n\n94\n\n010 1 0 111\n000\n\nMemory\n25\n93\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n0 0\n\n...\n\n...\n23\n\n\fWrite-back: Reads\n\u25cf What happens if we read 25 from address 1110102? (cont\u2019d)\no Now L1 Cache can be filled with block 25\n\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n\n001 0 0\n010 1 0\n1 111\n\n010 0 0\n\nMemory\n25\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 1\n0 000\n1\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n...\n\n...\n\n94\n\n24\n\n\fWrite-back: Reads\n\u25cf What happens if we read 25 from address 1110102? (cont\u2019d)\no Now L1 Cache can be filled with block 25\no Block 94 will eventually be written back to Memory\no Write buffers in this context are also called victim caches\nL1 Cache\n\nL2 Cache\n\nV D Tag Data\n\nV D Tag Data\n\n000 0 0\n\n000 0 0\n\n001 0 0\n010 1\n0 0 111\n\n001 0 0\n25\n\n010 1 1 111\n\nMemory\n25\n\nAddress\n\nData\n\n...\n\n...\n\nWrite Buffer\n\nWrite Buffer\n\nWrite Buffer\n\n000010\n\n93\n\nV Tag Data\n\nV D Tag Data\n\nV D Tag Data\n\n...\n\n...\n\n0 1\n0 000\n1\n0 0\n\n0 0\n\n111010\n\n25\n\n0 0\n\n...\n\n...\n\n94\n\n25\n\n\fImpact of Write Policy on AMAT\n\u25cf AMAT = hit time + (miss rate \u00d7 miss penalty)\n\n\u25cf Write-through caches can have a larger write hit time\no With write-back, a read hit and write hit take the same amount of time\no With write-through, a write hit takes the same time as a write miss\n\u25cf Write-back caches can have a larger miss penalty\no Due to write allocate policy on write misses\no Due to write-backs of dirty blocks when making space for new block\n\u25cf Both issues can be mitigated using write buffers to varying degrees\n\u25cf All in all, write-back caches usually outperform write-through caches\no Because write hits are much more frequent compared to misses\n\u25cf But write-through sometimes used in L1 cache due to simplicity\no Plenty of L1 \u2192 L2 (intra-chip) bandwidth to handle write propagation\no For L3, L3 \u2192 DRAM bandwidth cannot support write propagation\n26\n\n\fCache Design Parameter 7:\nBlocking vs. Non-blocking\n\n27\n\n\fBlocking Cache FSM for Write Back Caches\n\u25cf FSM must be in Idle state for\ncache to receive new requests\n\u25cf While \u201cMemory not Ready\u201d,\nblocks subsequent requests\n\u2192 Called Blocking Cache\n\u25cf Write buffer allows cache to\ndefer write-back until later\no Allows quickly return to Idle\n\n\u25cf But how about \u201cMemory not\nReady\u201d on Allocate?\n28\n\n\fNon-blocking caches service requests concurrently\nCache\nMiss\n\n\u25cf Blocking caches:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\n\nCache Cache Stall on\nMiss Hit\nUse\n\n\u25cf Hit under miss:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\n\nCache Cache Stall on\nMiss Miss\nUse\n\n\u25cf Miss under miss:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\nMemory Stall\n\n29\n\n\fNon-blocking caches service requests concurrently\nCache Cache Stall on\nMiss Hit\nUse\n\n\u25cf Hit under miss:\n\nCPU Compute\n\nCPU Compute\nMemory Stall\n\nCache Cache Stall on\nMiss Miss\nUse\n\n\u25cf Miss under miss:\n\nCPU Compute\n\nCPU Compute\n\nMemory Stall\nMemory Stall\n\n\u25cf Non-blocking cache allows both to happen\no Allows Memory Level Parallelism (MLP)\no As important to performance as Instruction Level Parallelism (ILP)\n\u25cf Miss Status Handling Register (MSHR) table tracks pending requests\n30\n\n\fImpact of non-blocking caches\n\u25cf Non-blocking caches do not impact our three cache metrics\no Hit time, miss rate, and miss penalty remain mostly the same\n\u25cf Impact is that miss penalty can be overlapped with:\no Computation of instructions not dependent on the miss\no Miss penalties of other memory requests\n\u25cf Out-of-order processors are always coupled with a non-blocking cache\no Otherwise, the ability to do out-of-order execution is severely stymied\n\n31\n\n\fCache Design Parameter 8:\nUnified vs. Split\n\n32\n\n\fProblem with Split Caches\n\u25cf If cache is split into two (i-cache and d-cache)\no Space cannot be flexibly allocated between data and code\nCode\nI-Cache\n\nData\n\nD-Cache\n\nIf our working\nset looks like\nthis \u2013 say, in a\nsmall loop\nthat's accessing\na large array \u2013\nthen we run\nout of data\nspace.\n\nCode\n\nIf our working\nset looks like\nthis \u2013 say, in a\nlarge function\nthat's only\nusing stack\nvariables \u2013 then\nwe run out of\ncode space.\n\nData\n\n33\n\n\fImpact of Unifying Cache\n\u25cf The answer to the problem is to simply unify the cache into one\n\n\u25cf AMAT = hit time + (miss rate \u00d7 miss penalty)\n\u25cf Impact of unifying cache on miss rate:\no Smaller miss rate due to more flexible use of space\n\u25cf Impact of unifying cache on hit time:\no Potentially longer hit time due to structural hazard\no With split caches, i-cache and d-cache can be accessed simultaneously\no With unified cache, access request must wait until port is available\n\u25cf L1 cache is almost always split\no Frequent accesses directly from pipeline trigger structural hazard often\n\u25cf Lower level caches are almost always unified\no Accesses are infrequent (filtered by L1), so structural hazards are rare\n34\n\n\fCache Design Parameter 9:\nPrivate vs. Shared\n\n35\n\n\fPrivate vs. Shared Cache\n\u25cf On a multi-core system, there are two ways to organize the cache\n\n\u25cf Private caches: each core (processor) uses its own cache\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\n\u25cf Shared cache: all the cores share one big cache\n\nShared L1$\n\n36\n\n\fShared Cache can Use Space More Flexibly\n\u25cf Suppose only 1st core is active and other cores are idle\no How much cache space is available to 1st core? (Shown in red)\n\u25cf Private caches: 1st core can only use its own private cache\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\n\u25cf Shared cache: 1st core can use entire shared cache!\n\nShared L1$\n\n37\n\n\fBanking: Solution to Structural Hazards\n\u25cf Now what if all the cores are active at the same time?\no Won\u2019t that cause structural hazards due to simultaneous access?\n\nShared L1$\n\no Could add more ports, but adding banks is more cost effective\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L1$\n\n\u25aa Each bank has its own read / write port\n\u25aa As long as two cores do not access same bank, no hazard!\n38\n\n\fBanking: Solution to Structural Hazards\n\u25cf Cache blocks are interleaved between banks\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L1$\n\no Blocks 0, 4, 8 \u2026 \u2192 Bank 0\no Blocks 1, 5, 9 \u2026 \u2192 Bank 1\no Blocks 2, 6, 10 \u2026 \u2192 Bank 2\no Blocks 3, 7, 11 \u2026 \u2192 Bank 3\no That way, blocks are evenly distributed across banks\n\u25aa Causes cache accesses to also be distributed \u2192 less hazards\n\n39\n\n\fShared Cache have Longer Access Times\n\u25cf Again, suppose only 1st core is active and other cores are idle\no The working set data is shown in red\n\u25cf Private caches: entire working set data in nearby private cache\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\n\u25cf Shared cache: data sometimes distributed to remote banks\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L1$\n\n40\n\n\fShared Cache have Longer Access Times\n\u25cf Remember this picture?\n\n2/20/2017\n\nCS/COE 1541 term 2174\n\n41\n\n\fImpact of Shared Cache\n\u25cf AMAT = hit time + (miss rate \u00d7 miss penalty)\n\n\u25cf Impact of shared cache on miss rate:\no Smaller miss rate due to more flexible use of space\n\u25cf Impact of shared cache on hit time:\no Longer hit time due to sometimes having to access remote banks\n\u25cf L1 caches are almost always private\no Hit time is important for L1. Cannot afford access to remote banks.\n\u25cf L3 (last level) caches are almost always shared\no Reducing miss rate is top priority to avoid DRAM access.\n\n42\n\n\fCache Organization of Broadwell CPU\n\u25cf This is the cache organization of Broadwell used in our Linux server\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n\u25cf Intel rebrands the shared cache as the \u201cSmart Cache\u201d\n\n43\n\n\fCache Design Parameter 10:\nPrefetching\n\n44\n\n\fPrefetching\n\u25cf Prefetching: fetching data that is expected to be needed soon\no Allows you to hide the latency of fetching that data\no E.g. Web browsers prefetch resources from not-yet-clicked links\n\u2192 when user later clicks on link, response is almost instantaneous\no Caches also prefetch data that is expected to be used soon\n\u25aa Can be used to avoid even cold misses\n\u25cf Two ways prefetching can happen:\no Compiler-driven: compiler emits prefetch instructions\n\u25aa Can manually insert one in C program: __builtin_prefetch(addr)\n\u25aa Or rely on compiler to insert them using heuristics\no Hardware-driven: CPU prefetcher emits prefetches dynamically\n\u25aa Relies on prefetcher to detect a pattern in memory accesses\n45\n\n\fHardware Prefetching\n\u25cf What do you notice about both these snippets of code?\n\u25cf They both access memory sequentially. for(i = 0 .. 100000)\no The first one data, the next instructions.\nA[i]++;\n00 lw\n\u25cf These kinds of access patterns are very common.\n00 04 08 0C 10 14 18 1C\n\nSequential\n\n00 04 08 0C 10 14 18 1C\n\nReverse sequential\n\n00 04 08 0C 10 14 18 1C\n\nStrided sequential\n(think \"accessing one field\nfrom each item in an array\nof structs\")\n\n04 lw\n08 lw\n0C addi\n10 sub\n14 mul\n18 sw\n1C sw\n20 sw\n46\n\n\fHardware Prefetching Stride Detection\n\u25cf What kinds of things would you need?\n\u25cf A table of the last n memory accesses would be a good start.\nn-7\n\nn-6\n\nn-5\n\nn-4\n\nn-3\n\nn-2\n\nn-1\n\nn\n\n40C0\n\n40C4\n\n40C8\n\n40CC\n\n40D0\n\n40D4\n\n40D8\n\n40DC\n\n\u25cf Some subtractors\nto calculate the stride\n\u25cf Some comparators to\nsee if strides are the same\n\u25cf Some detection logic\n\n-\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\n-\n\n=\n\nStride Detector\n\n47\n\n\fWhere Hardware Prefetching Doesn\u2019t Work\n\u25cf Sequential accesses are where prefetcher works best\no E.g. Iterating over elements of an array\n\u25cf Some accesses don\u2019t have a pattern or is too complex to detect\no At below is how a typical linked-list traversal looks like\n\n00 04 08 0C 10 14 18 1C 20 24 28 2C 30 34 38 3C\n(colors are different cache blocks)\n\no Other pointer-chasing data structures (graphs, trees) look similar\no Can only rely on naturally occurring locality to avoid misses\no Or, have compiler insert prefetch instructions in middle of traversal\n48\n\n\fMystery Solved\n\n\u25cf How come Array performed well for even an array 1.28 GB large?\no No spatial locality since each node takes up two 64-byte cache blocks\no No temporal locality since working set of 1.28 GB exceeds any cache\n\n\u25cf The answer is: Array had the benefit of a strided prefetcher!\no Access pattern of Linked List was too complex for prefetcher to detect\n49\n\n\fImpact of Prefetching\n\u25cf Prefetcher runs in parallel with the rest of the cache hardware\no Does not slow down any on-demand reads or writes\n\u25cf What if prefetcher is wrong? It can be wrong in two ways:\no It fetched a block that was never going to be used\no It fetched a useful block but fetched it too soon or too late\n\u25aa Too soon: the block gets evicted before it can be used\n\u25aa Too late: the prefetch doesn\u2019t happen in time for the access\n\u25cf A bad prefetch results in cache pollution\no Unused data is fetched, potentially pushing out other useful data\n\u25cf On the other hand, good prefetches can reduce misses drastically!\n\n50\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}