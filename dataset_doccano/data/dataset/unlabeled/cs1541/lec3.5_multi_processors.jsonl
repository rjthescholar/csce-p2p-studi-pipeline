{"id": 227, "segment": "unlabeled", "course": "cs1541", "lec": "lec3.5_multi_processors", "text": "Multiprocessors\nand Caching\nCS 1541\nWonsun Ahn\n\n\fTwo ways to use multiple processors\n\u25cf Distributed (Memory) System\no Processors do not share memory (and by extension data)\no Processors exchange data through network messages\no Programming standards:\n\u25aa Message Passing Interface (MPI) \u2013 C/C++ API for exchanging messages\n\u25aa Ajax (Asynchronous JavaScript and XML) \u2013 API for web apps\no Data exchange protocols: TCP/IP, UDP/IP, JSON, XML\u2026\n\u25cf Shared Memory System (a.k.a. Multiprocessor System)\no Processors share memory (and by extension data)\no Programming standards:\n\u25aa Pthreads (POSIX threads), Java threads \u2013 APIs for threading\n\u25aa OpenMP \u2013 Compiler #pragma directives for parallelization\no Cache coherence protocol: protocol for exchanging data among caches\n\u2192 Just like Ethernet, caches are part of a larger network of caches\n2\n\n\fShared Data Review\n\u25cf What bad thing can happen when you have shared data?\n\n\u25cf Dataraces!\no You should have learned it in CS 449.\no But if you didn\u2019t, don\u2019t worry I\u2019ll go over it.\n\n3\n\n\fReview: Datarace Example\nint shared = 0;\nvoid *add(void *unused) {\nfor(int i=0; i < 1000000; i++) { shared++; }\nreturn NULL;\n}\nbash-4.2$ ./datarace\nint main() {\npthread_t t;\nshared=1085894\n// Child thread starts running add\nbash-4.2$ ./datarace\npthread_create(&t, NULL, add, NULL);\nshared=1101173\n// Main thread starts running add\nbash-4.2$ ./datarace\nadd(NULL);\n// Wait until child thread completes\nshared=1065494\npthread_join(t, NULL);\nprintf(\"shared=%d\\n\", shared);\nreturn 0;\n}\nQ) What do you expect from running this? Maybe shared=2000000 ?\nA) Nondeterministic result! Due to datarace on shared.\n4\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nshared\n1\n\nshared++\nThread 1\n\n\u2022 You may think shared becomes 3.\n(shared++ on each thread)\n\u2022 But that\u2019s not the only possibility!\n\u2022 I\u2019ll show you shared becoming 2.\n\nshared++\nThread 2\n\n5\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n0\n\n1\n\n0\n\nR1 = shared\nR1 = R1 + 1\nshared = R1\nThread 1\n\nR1 = shared\nR1 = R1 + 1\nshared = R1\nThread 2\n\n6\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n1\n\n1\n\n0\n\n\u2460 R1 = shared\nR1 = R1 + 1\nshared = R1\nThread 1\n\nR1 = shared\nR1 = R1 + 1\nshared = R1\nThread 2\n\n7\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n1\n\n1\n\n1\n\nR1 = shared\u2461\nR1 = R1 + 1\nshared = R1\n\n\u2460 R1 = shared\nR1 = R1 + 1\nshared = R1\nThread 1\n\nThread 2\n\n8\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n1\n\n1\n\nR1 = shared\u2461\nR1 = R1 + 1\nshared = R1\n\n\u2460 R1 = shared\n\u2462 R1 = R1 + 1\nshared = R1\nThread 1\n\nThread 2\n\n9\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n1\n\n2\n\nR1 = shared\u2461\nR1 = R1 + 1\u2463\nshared = R1\n\n\u2460 R1 = shared\n\u2462 R1 = R1 + 1\nshared = R1\nThread 1\n\nThread 2\n\n10\n\n\fReview: Datarace Example\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n2\n\n2\n\nR1 = shared\u2461\nR1 = R1 + 1\u2463\nshared = R1\n\n\u2460 R1 = shared\n\u2462 R1 = R1 + 1\n\u2464 shared = R1\nThread 1\n\nThread 2\n\n11\n\n\fReview: Datarace Example\n\u2022 Why did this occur in the first place?\n\u2022 Because data was replicated to CPU registers and each worked on its own copy!\n\n\u25cf When two threads do shared++; initially shared = 1\nR1\n\nshared\n\nR1\n\n2\n\n2\n\n2\n\n\u2460 R1 = shared\n\u2462 R1 = R1 + 1\n\u2464 shared = R1\nThread 1\n\nR1 = shared\u2461\nR1 = R1 + 1\u2463\nshared = R1\u2465\n\n\u2022 End result is 2 instead of 3!\n\u2022 Only on simultaneous access\n(with this type of interleaving)\n\nThread 2\n\n12\n\n\fReview: Datarace Example\npthread_mutex_t lock;\nint shared = 0;\nvoid *add(void *unused) {\nfor(int i=0; i < 1000000; i++) {\npthread_mutex_lock(&lock);\nshared++;\npthread_mutex_unlock(&lock);\n}\nreturn NULL;\n}\nint main() {\n\u2026\n}\n\nbash-4.2$ ./datarace\nshared=2000000\nbash-4.2$ ./datarace\n\nshared=2000000\nbash-4.2$ ./datarace\nshared=2000000\n\n\u2022 Data race is fixed! Now shared is always 2000000.\n\u2022 Problem solved? No! CPU registers is not the only place replication happens!\n\n13\n\n\fCaching also does replication!\n\u25cf What happens if caches sit in between processors and memory?\n\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nBank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n14\n\n\fCaching also does replication!\n\u25cf Let\u2019s say CPU 0 first fetches shared for incrementing\n\nshared\n\n0\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n15\n\n\fCaching also does replication!\n\u25cf Then CPU 0 increments shared 100 times to 100\n\nshared\n\n100\nL1$\n\nL1$\n\nL1$\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nL2$\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n16\n\n\fCaching also does replication!\n\u25cf Then CPU 2 gets hold of the mutex and fetches shared from L3\n\nshared\n\n100\nL1$\n\nL1$\n\nshared\n\nL1$\n0\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nshared\n\nL2$\n0\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n17\n\n\fCaching also does replication!\n\u25cf Then CPU 2 increments shared 10 times to 10\n\nshared\n\n100\nL1$\n\nL1$\n\nshared\n\nL1$\n10\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nshared\n\nL2$\n0\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n18\n\n\fCaching also does replication!\n\u25cf Clearly this is wrong. L1 caches of CPU 0 and CPU 2 are incoherent.\n\nshared\n\n100\nL1$\n\nL1$\n\nshared\n\nL1$\n10\n\nL1$\n\nPrivate L1$\n\nshared\n\n0\nL2$\n\nL2$\n\nshared\n\nL2$\n0\n\nL2$\n\nPrivate L2$\n\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\n* All caches are write-back\n\nshared\n\n0\n\n19\n\n\fCache Incoherence: Problem with Private Caches\n\u25cf This problem does not occur with a shared cache.\no All processors share and work on a single copy of data.\nshared\n\n0 Bank 0\n\nBank 1\n\nBank 2\n\nBank 3\n\nShared L3$\n\no The problem exists only with private caches.\n\u25cf The problem exists for private caches.\no Private copy is at times inconsistent with lower memory.\no Incoherence occurs when private copies differ from each other.\n\u2192 Means processors return different values for same location!\n\n20\n\n\fCache Coherence\n\n21\n\n\fCache Coherence\n\u25cf Cache coherence (loosely defined):\no All processors of system should see the same view of memory\no Copies of values cached by processors should adhere to this rule\n\u25cf Each ISA has a different definition of what that \u201cview\u201d means\no Memory consistency model: definition of what that \u201cview\u201d is\n\u25cf All models agree on one thing:\no That a change in value should reflect on all copies (eventually)\n\n22\n\n\fHow Memory Consistent Model affects correctness\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nQ) What do you expect the value of data will be when it gets printed?\nA) Most people will say 42 because that is the logical ordering.\nBut is it? Not always. There are situations where data is still 0!\n\n23\n\n\fScenario 1: Stores arrive out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n0\n\nflag\n\ndata\n\nfalse\n\n0\n\nflag\nfalse\n\nLet\u2019s assume initially both data and flag are cached in each CPU\u2019s L1 caches.\n\n24\n\n\fScenario 1: Stores arrive out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n42\n\nflag\n\ndata\n\ntrue\n\n0\n\nflag\nfalse\n\nCPU 0 updates both data and flag to 42 and true.\n\n25\n\n\fScenario 1: Stores arrive out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n42\n\nflag\n\ndata\n\ntrue\n\n0\n\nflag\nfalse\n\nNow the cached values in CPU 1 are stale and need to be invalidated.\nInvalidation: act of marking a cache block with stale data invalid.\n26\n\n\fScenario 1: Stores arrive out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\ndata\n42\n\nL1$\n\nCPU 1\n\nInvalidate for flag\nflag\ntrue\n\nInvalidate for data\n\ndata\n0\n\nL1$\n\nflag\nfalse\n\nThe invalidate messages travel through a network and may arrive out-of-order.\nLet\u2019s say invalidate for flag arrives first to CPU 1 and marks flag invalid.\n27\n\n\fScenario 1: Stores arrive out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\ndata\n42\n\nL1$\n\nCPU 1\n\nFetch for flag\nflag\ntrue\n\nInvalidate for data\n\ndata\n0\n\nL1$\n\nflag\ntrue\n\nCPU 1 fetches updated flag from CPU 0 when comparing flag == false.\nInvalidate for data is still traveling through the network.\n28\n\n\fScenario 1: Stores arrive out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\ndata\n42\n\nL1$\n\nCPU 1\n\nFetch for flag\nflag\n\nInvalidate for data\n\ntrue\n\ndata\n0\n\nL1$\n\nflag\ntrue\n\nSince flag is true, CPU 1 breaks out of while loop and prints data.\ndata=0 gets printed!\n29\n\n\fScenario 2: Loads perform out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nL1$\n\nL1$\n\ndata\n0\n\nflag\n\ndata\n\nfalse\n\n0\n\nLet\u2019s assume now flag is not cached in CPU 1.\nCPU 1 suffers a cache miss on flag when it compares flag == false.\n30\n\n\fScenario 2: Loads perform out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nInstruction Queue\nlw r1, flag (miss)\n\ndata\n0\n\nL1$\n\nflag\n\ndata\n\nfalse\n\n0\n\nL1$\n\nbeq r1, $zero, _loop\nlw r2, data (hit)\ncall println on r2\n\nInstead of stalling, CPU 1 predicts the branch not taken and issues lw r2, data.\nNow, r2 == 0. (Unless pipeline flushes due to branch misprediction.)\n31\n\n\fScenario 2: Loads perform out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nInstruction Queue\nlw r1, flag (miss)\n\ndata\n42\n\nL1$\n\nflag\ntrue\n\nFetch for flag\n\ndata\n0\n\nL1$\n\nbeq r1, $zero, _loop\n\nflag\n\nlw r2, data (hit)\n\ntrue\n\ncall println on r2\n\nNow let\u2019s say CPU 0 updates data and flag before the fetch for flag arrives.\nNow, lw r1, flag completes, allowing beq r1, $zero, _loop to issue (with r1 == true)\n32\n\n\fScenario 2: Loads perform out-of-order\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\nCPU 0\n\nCPU 1\n\nInstruction Queue\nlw r1, flag (miss)\n\ndata\n42\n\nL1$\n\nflag\n\nFetch for flag\n\ntrue\n\ndata\n0\n\nL1$\n\nbeq r1, $zero, _loop\n\nflag\n\nlw r2, data (hit)\n\ntrue\n\ncall println on r2\n\nSince r1 == true, that validates the not-taken prediction for the branch.\nSince r1 == 0, the println outputs data=0!\n33\n\n\fMemory Consistency Models are often very lax\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\n\u2022 A memory consistency model where above ordering is guaranteed is called,\nSequential Consistency (SC): Instructions appear to execute sequentially.\n\u2022 Real models allow many other orderings to allow optimizations:\n\u2022 Write buffers that allow multiple stores to be pending and perform out-of-order\n\u2022 Instruction queues that allow loads and other instructions to perform out-of-order\n\u2022 Compiler optimizations to reschedule stores and loads out-of-order\n\n\u2022 Intel, ARM, Java Virtual Machine all have relaxed memory consistency models\n\u2022 Moral: never do custom synchronization unless you know what you are doing!\n34\n\n\fMemory Consistency Models are often very lax\n\u25cf Initially, data == 0, flag == false.\nProducer Thread\n\nConsumer Thread\n\ndata == 42;\n\nwhile(flag == false) { /* wait */ }\n\nflag = true;\n\nSystem.out.println(\u201cdata=\u201c + data);\n\n\u2022 Regardless of memory consistency model, they all agree on one thing:\nthat values of data and flag must be made coherent eventually.\n\u2022 They only disagree on when that eventually is.\n\n\u2022 This property is called cache coherence.\n\n35\n\n\fImplementing Cache Coherence\n\u25cf How to guarantee changes in value are propagated to all caches?\n\n\u25cf Cache coherence protocol: A protocol, or set of rules, that all\ncaches must follow to ensure coherence between caches\no MSI (Modified-Shared-Invalid)\no MESI (Modified-Exclusive-Shared-Invalid)\no \u2026 often named after the states in cache controller FSM\n\u25cf Three states of MSI protocol (maintained for each block):\no Modified: Dirty. Only this cache has copy.\no Shared: Clean. Other caches may have copy.\no Invalid: Block contains no data.\n\n36\n\n\fMSI Snoopy Cache Coherence Protocol\nProcessor\n\nSnoop\ntag\n\nCache tag\nand data\n\nProcessor\n\nSnoop\ntag\n\nCache tag\nand data\n\nProcessor\n\nSnoop\ntag\n\nCache tag\nand data\n\nSingle bus\n\nMemory\n\nI/O\n\n\u25cfEach processor monitors (snoops) the activity on the bus\noIn much the same way as how nodes snoop the Ethernet\n\u25cfCache state changes in response to both:\noRead / writes from the local processor\noRead misses / write misses from remote processors it snoops\n37\n\n\fMSI: Example\n\u2022 All bus activity is show in blue. Cache changes block state in response.\n\u2022 Bus activity is generated only for cache misses, or for invalidates\n\u2022 Other caches must maintain coherence by monitoring that bus activity\n\nEvent\n\nIn P1\u2019s cache\nL = invalid\n\nP1 writes 10 to A\n(write miss)\nP1 reads A\n(read hit)\nP2 reads A\n(read miss)\nP2 writes 20 to A\n(write hit)\nP2 writes 40 to A\n(write hit)\nP1 write 50 to A\n(write miss)\n\nIn P2\u2019s cache\nL = invalid\nRead Exclusive A (from write in P1)\n\nL \uf0df A = 10 (modified)\n\nL = invalid\n\nL \uf0df A = 10 (modified)\n\nL = invalid\n\nRead A (from read in P2)\n\nL \uf0df A = 10 (shared)\n\nL \uf0df A = 10 (shared)\n\nInvalidate A (from write in P2)\n\nL = invalid\n\nL \uf0df A = 20 (modified)\n\nL = invalid\n\nL \uf0df A = 40 (modified)\nRead Exclusive A (from write in P1)\n\nL \uf0df A = 50 (modified)\n\nL = invalid\n38\n\n\fCache Controller FSM for MSI Protocol\n\u25cf Processor activity in red, Bus activity in blue\nRead\nWrite\n\nRead\nBusRead\n\nBusRead\n\nModified\n\nShared\n\nWrite\nBusReadX\nBusReadX BusInvalidate\nWrite\n\n\u2022\n\u2022\n\nRead\n\u2022\n\nInvalid\n\nBusRead: Read request is snooped\nBusReadX: Read exclusive request is snooped\n\u2022 Sent by a processor on a write miss\n\u2022 Since line will be modified, need to invalidate\nBusInvalidate: Invalidate request is snooped\n\u2022 Sent on a write hit on shared cache line\n\u2022 To invalidate all other shared lines in system\n\nBusRead\nBusReadX\nBusInvalidate\n39\n\n\fTLB Coherence\n\n40\n\n\fHow about TLBs?\n\u25cf We said TLBs are also a type of cache that caches PTEs.\no So what happens if a processor changes a PTE?\no How does that change get propagated to other processor TLBs?\n\u25cf Unfortunately, there is no hardware coherence for TLBs. \uf04c\n\u25cf That means software (the OS) must handle the coherence\no Which is of course much much slower\n\n41\n\n\fTLB shootdown\n\u25cf In order to update a PTE (page table entry)\no Initiator OS must first flush its own TLB\no Send IPIs (Inter-processor interrupts) to other processors\n\u25aa To flush the TLBs for all other processors too\no Source of significant performance overhead\n\n* Courtesy of Nadav Amit et al. at VMWare\n42\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}