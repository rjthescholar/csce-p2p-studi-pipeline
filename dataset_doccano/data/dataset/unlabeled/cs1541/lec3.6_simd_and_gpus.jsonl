{"id": 237, "segment": "unlabeled", "course": "cs1541", "lec": "lec3.6_simd_and_gpus", "text": "SIMD and GPUs\nCS 1541\nWonsun Ahn\n\n\fSIMD Architectures\n\n2\n\n\fISA not optimized for data parallel workloads\n\u25cf This loop does multiply accumulate (MAC):\nfor (int i = 0; i < 64; i++) {\ny[i] = a * x[i] + y[i]\n}\no A common operation in digital signal processing (DAXPY)\n\u25cf Note how we apply the same MAC operation on each data item\no This is how many data parallel workloads look like\n\u25cf A conventional ISA (likes MIPS) is not optimal for encoding this\no Results in wasted work and suboptimal performance\no Let\u2019s look at the actual MIPS translation\n\n3\n\n\fMIPS code for \ud835\udc9a \ud835\udc8a = \ud835\udc82 \u2217 \ud835\udc99 \ud835\udc8a + \ud835\udc9a \ud835\udc8a\nl.d\n$f0,0($sp)\n;$f0 = a\naddi $s2,$s0,512\n;64 elements (64*8=512 bytes)\nloop: l.d\n$f2,0($s0)\n;$f2 = x(i)\nmul.d $f2,$f2,$f0\n;$f2 = a * x(i)\nl.d\n$f4,0($s1)\n;$f4 = y(i)\nadd.d $f4,$f4,$f2\n;$f4 = a * x(i) + y(i)\ns.d\n$f4,0($s1)\n;y(i) = $f4\naddi $s0,$s0,8\n;increment index to x\naddi $s1,$s1,8\n;increment index to y\nsubu $t0,$s2,$s0\n;evaluate i < 64 loop condition\nbne\n$t0,$zero,loop ;loop if not done\n\u25cf Blue instructions don\u2019t do actual computation. There for indexing and loop control.\no Is there a way to avoid? Loop unrolling yes. But that causes code bloat!\n\u25cf Red instructions do computation. But why decode them over and over again?\no Is there a way to fetch and decode once and apply to all data items?\n4\n\n\fSIMD (Single Instruction Multiple Data)\n\u25cf SIMD (Single Instruction Multiple Data)\no An architecture for applying one instruction on multiple data items\no ISA includes vector instructions for doing just that\n\u25aa Along with vector registers to hold multiple data items\n\n\u25cf Using MIPS vector instruction extensions:\nl.d\n$f0,0($sp)\nlv\n$v1,0($s0)\nmulvs.d $v2,$v1,$f0\nlv\n$v3,0($s1)\naddv.d $v4,$v2,$v3\nsv\n$v4,0($s1)\n\n;$f0 = scalar a\n;$v1 = vector x (64 values)\n;$v2 = a * vector x\n;$v3 = vector y (64 values)\n;$v4 = a * vector x + vector y\n;vector y = $v4\n\no Note: no indexing and loop control overhead\no Note: each instruction is fetched and decoded only once\n\n5\n\n\fSIMD Processor Design\n\u25cf How would you design a processor for the vector instructions?\ndata\n\n1. One processing element (PE)\no Fetch and decode instruction once\no PE applies op on each data item\n\u25aa Item may be in vector register\nprogram\n\u25aa Item may be in data memory\n2. Multiple PEs in parallel\nprogram\no Fetch and decode instruction once\no PEs apply op in parallel\n\u25aa In synchronous lockstep\nPE\n\u2192 The more PEs, the faster!\ndata\n\ncontrol\n\nPE\n\ncontrol\n\nPE\n\nPE\n\nPE\n\ndata\n\ndata\n\ndata\n6\n\n\fExample: Adding Two Vectors\n\u25cf Instead of having a single FP adder work on each item (a)\n\u25cf Have four FP adders work on items in parallel (b)\n\u25cf Each pipelined FP unit is in charge of pre-designated items in vector\no For full parallelization, put as many FP units as there are items\n\n7\n\n\fVector Load-Store Unit\n\u25cf Striding lets you load/store non-contiguous data from memory at\nregular offsets. (e.g. the first member of each struct in an array)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0\n\n4\n\n8\n\nC\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\n\u25cf Gather-scatter lets you put pointers in a vector, then load/store\nfrom arbitrary memory addresses. (gather = load, scatter = store)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n2\n\nE\n\n7\n\n4\n\n6\n\n7\n\n8\n\n9\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n8\n\n\fHow does Gather-Scatter work?\n\n9\n\n\fWhen does Gather-Scatter make sense?\n\u25cf Often data for scientific or AI applications are sparse.\no Time-sampled points where a sensor measurement changes\no In social networking, connections in a N x N friendship matrix\no In neural networks, weights in a filter layer that are non-zero\n\u25cf To save memory, sparse data is stored in sparse format:\n\nHow a sparse filter layer is stored in a\nConvolutional Neural Network (CNN)\n\n10\n\n\fWhen does Gather-Scatter make sense?\n\u25cf Convolution works by applying filter like a shifting window:\nrow column weight\n\nImage\n\nConvolved Feature\n\n0\n\n0\n\n1\n\n0 1 2 3\n\n0\n\n2\n\n1\n\n1 1 0 1\n\n1\n\n1\n\n1\n\n2 0 1 0\n\n2\n\n0\n\n1\n\n3 1 0 1\n\n2\n\n2\n\n1\n\nFilter (sparse matrix format)\n\n\u25cf When applying filter on image, a gather needs to take place\n1. Gather values in image in corresponding rows and columns\n2. Create an image vector out of those gathered values\n3. Do a dot (\u25cf) product between image vector and filter vector\n11\n\n\fSIMD instructions in real processors\n\u25cf x86 vector extensions\no Historically: MMX, SSE, AVX, AVX-2\no Current: AVX-512 (512-bit vector instructions)\n\u25cf ARM vector extensions\no Historically: VFP (Vector Floating Point)\no Current: Neon (128-bit vector instructions)\n\u25cf Vector instructions have progressively become wider historically\no Due to increase of data parallel applications\no Due to their power efficiency\n\u25aa Compared to fetching, decoding, scheduling multiple instructions\n\u25aa Good way to increase FLOPS while staying within TDP limit\n\u25cf Enter GPUs for general computing (circa 2001)\n13\n\n\fGPUs:\nGraphical Processing Units\n\n14\n\n\fHistory of GPUs\n\u2022 VGA (Video graphic array) has been around since the early 90\u2019s\n\u2022 A display generator connected to some (video) RAM\n\u2022 By 2000, VGA controllers were handling almost all graphics computation\n\u2022 Programmable through OpenGL, Direct 3D API\n\u2022 APIs allowed accelerated vertex/pixel processing:\n\u2022 Shading\n\u2022 Texture mapping\n\u2022 Rasterization\n\u2022 Gained moniker Graphical Processing Unit\n\u2022 2007: First general purpose use of GPUs\n\u2022 2007: Release of CUDA language\n\u2022 2011: Release of OpenCL language\n15\n\n\fGPU is Really a SIMD Processor\nGPU\n\nStreaming Multi-processor (SM)\n\nStreaming\nProcessor (SP)\n\nIF/ID\n\nIF/ID\n\nIF/ID\n\nL1 cache\n\nL1 cache\n\nL1 cache\n\nShared\nmemory\n\nShared\nmemory\n\nShared\nmemory\n\nCPU\nL2 cache\n\nCPU (Host)\nmemory\n\nL2 cache\nPCIe bus\n\nGlobal (GPU) memory\n\n\u25cf Logically, a GPU is composed of SMs (Streaming Multi-processors)\no An SM is a vector unit that can process multiple pixels (or data items)\n\u25cf Each SM is composed of SPs which work on each pixel or data item\n16\n\n\fCPU-GPU architecture\nGPU\n\nStreaming Multi-processor (SM)\n\nStreaming\nProcessor (SP)\n\nIF/ID\n\nIF/ID\n\nIF/ID\n\nL1 cache\n\nL1 cache\n\nL1 cache\n\nShared\nmemory\n\nShared\nmemory\n\nShared\nmemory\n\nCPU\nL2 cache\n\nCPU (Host)\nmemory\n\nL2 cache\nPCIe bus\n\nGlobal (GPU) memory\n\n\u25cf Dedicated GPU memory separate from system memory\n\u25cf Code and data must be transferred to GPU memory for it to work on it\no Through PCI-Express bus connecting GPU to CPU\n17\n\n\fModern GPU architecture\nSM=streaming\nmultiprocessor\n\nTPC = Texture\nProcessing\nTPC = texture Cluster\nprocessing cluster\n\nROP = raster operations pipeline\n\nSFU = special\nfunction unit\n\n\fGPU Programming Model\nCopy data from CPU\nmemory to GPU memory\n\nCPU\n\nIF/ID\n\nCPU\nmemory\n\nLaunch the kernel\n\nCPU\n\nCPU\n\nCPU\nmemory\n\nIF/ID\n\nGlobal (GPU) memory\n\nIF/ID\n\nCPU\nmemory\n\nCopy data from GPU\nmemory to CPU memory\n\nIF/ID\n\nIF/ID\n\nIF/ID\n\nGlobal (GPU) memory\n\nIF/ID\n\nIF/ID\n\nIF/ID\n\nGlobal (GPU) memory\n19\n\n\fGPU Programming Model\nCPU program\n(serial code)\n\ncudaMemcpy ( \u2026 )\n\nCopy data from CPU\nmemory to GPU memory\n\nFunction <<<nb,nt >>>Launch kernel on GPU\n\ncudaMemcpy ( \u2026 )\n_global_ Function ( \u2026 )\n\nCopy results from GPU\nmemory to CPU memory\nImplementation of GPU kernel\nkernel: Function executed on the GPU\n\n20\n\n\fGPU Programming Model: Copying Data\n/* malloc in GPU global memory */\ncudaMalloc (void **pointer, size_t nbytes);\n/* free malloced GPU global memory */\ncudaFree(void **pointer) ;\n/* initialize GPU global memory with value */\ncudaMemset (void *pointer, int value, size_t count);\n/* copy to and from between CPU and GPU memory */\ncudaMemcpy(void *dest, void *src, size_t nbytes, enum cudaMemcopyKind dir);\nenum cudaMemcpyKind\n\u2022 cudaMemcpyHostToDevice\n\u2022 cudaMemcpyDeviceToHost\n\u2022 cudaMemcpyDeviceToDevice\n\nCPU\nmemory\n\nPCIe bus\n\nGPU Global\nmemory\n\n21\n\n\fExample: Copying array a to array b using the GPU\n\n22\n\n\fGPU Programming Model: Launching the Kernel\nCPU program\n(serial code)\n\ncudaMemcpy ( \u2026 )\n\nCopy data from CPU\nmemory to GPU memory\n\nFunction <<<nb,nt >>>Launch a kernel with nb\nblocks, each with nt threads\n\ncudaMemcpy ( \u2026 )\n_global_ Function ( \u2026 )\n\nCopy results from GPU\nmemory to CPU memory\nImplementation of kernel\n(the function run by each GPU thread)\n\n23\n\n\fThe Execution Model\nIF/ID\n\nL1 cache\n\nShared\nmemory\n\n\u2022 The thread blocks are dispatched to SMs\n\u2022 The number of blocks dispatched to an\nSM depends on the SM\u2019s resources\n(registers, shared memory, \u2026).\nBlocks not dispatched initially are dispatched\nwhen an SM frees up after finishing a block\n\u2022 When a block is dispatched to an\nSM, each of its threads executes on\nan SP in the SM.\n\nIF/ID\n\nL1 cache\nShared\nmemory\n24\n\n\fA thread block is executed in warps\n\u25cf Each block (up to 1K threads) is divided into groups of 32\nthreads (called warps) \u2013 empty threads are used as fillers.\n\u25cf A warp executes as a SIMD vector instruction on the SM.\n\u25cf Depending on the number of SPs per SM:\n0 1\n\n30 31\n\no If 32 SP per SM \u2192 1 thread of a warp executes on 1 SP\n(32 lanes of execution, one thread per lane)\n0 1\n\n30 31\n\n01\n\n30 31\n\no If 16 SP per SM \u2192 2 threads are time multiplexed on 1 SP (16\nlanes of execution, 2 threads per lane)\n0\n\n15\n\n0123\n\n31\n\n0\n\no If 8 SP per SM \u2192 4 threads are time multiplexed on 1 SP\n(8 lanes of execution, 4 threads per lane)\n\n7\n25\n\n\fA SM is composed of one or more warp schedulers\n\u25cf In this SM, there are 4 warp schedulers.\n\n\u25cf Warps in thread block are divided\nstatically among warp schedulers.\n\u25cf E.g. for 4 schedulers:\no Scheduler 1: Warp 0, Warp 4, \u2026\no Scheduler 2: Warp 1, Warp 5, \u2026\no Scheduler 3: Warp 2, Warp 6, \u2026\no Scheduler 4: Warp 3, Warp 7, \u2026\n\u25cf Round robin assignment to ensure\nequal distribution of warps\n26\n\n\fWarp scheduling enables latency hiding\n\u25cf Warp scheduler can hide bubbles (just like a superscalar scheduler)\no But without an instruction queue and out-of-order execution\n\u25cf How?\no In-order execution.\no Switch to different warp\nwhen a bubble is\nabout to form.\n\u25cf Warp can come from any\nthread block in SM\no More thread blocks can\nlead to higher utilization.\n27\n\n\fAll threads execute the same code\n\u25cf Launched using Kernel <<<1, 64>>> : 1 block with 64 threads\nthreadIdx.x 0\n\n1\n\n2\n\n3\n\n60\n\n61\n\n62\n\n63\n\nint i = threadIdx.x;\nB[i] = A[63-i];\nC[i] = B[i] + A[i]\n\nA[0,\u2026,63]\nB[0,\u2026,63]\nC[0,\u2026,63]\nGPU memory\n\n\u25cf Each thread in a thread block has a unique \u201cthread index\u201d \u2192 threadIdx.x\n\u25cf The same sequence of instructions can apply to different data items.\n28\n\n\fBlocks of Threads\n\u25cf Launched using Kernel <<<2, 32>>> : 2 blocks of 32 threads\nblockIdx.x = 1\n\nblockIdx.x = 0\n\nthreadIdx.x 0\n\n1\n\n30\n\n31\n\n0\n\n1\n\n30\n\n31\n\nint i = 32 * blockIdx.x + threadIdx.x;\nB[i] = A[63-i];\nC[i] = B[i] + A[i]\n\nA[0,\u2026,63]\nB[0,\u2026,63]\nC[0,\u2026,63]\nGPU memory\n\n\u25cf Each thread block has a unique \u201cblock index\u201d \u2192 blockIdx.x\n\u25cf Each thread has a unique threadIdx.x within its own block\n\u25cf Can compute a global index from the blockIdx.x and threadIdx.x\n29\n\n\fTwo-dimensions grids and blocks\n\u25cf Launched using Kernel <<<(2, 2), (4, 8)>>> : 2X2 blocks of 4X8 threads\nblockIdx.x = 0 blockIdx.x = 1\nblockIdx.y = 0 blockIdx.y = 0\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\nx\n\n0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7\n1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7\n2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7\n3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7\n\ny\n\nblockIdx.x = 0 blockIdx.x = 1\nblockIdx.y = 1 blockIdx.y = 1\n\n\u25cf Each block has two indices (blockIdx.x, blockIdx.y)\n\u25cf Each thread in a thread block has two indices (threadIdx.x, threadIdx.y)\n30\n\n\fExample: Computing the global index\nthreadIdx.x\nvoid main ()\n{ cudaMalloc (int* &a, 20*sizeof(int));\nblockIdx.x\nblockIdx.x\nblockIdx.x\nblockIdx.x\ncudaMalloc (int* &b, 20*sizeof(int));\n=0\n=1\n=2\n=3\ncudaMalloc (int* &c, 20*sizeof(int));\n0 1 2 3 4\n0 1 2 3 4\n0 1 2 3 4\n0 1 2 3 4\n\u2026\nkernel<<<4,5>>(a, b, c) ;\n\u2026\n}\nNOTE: Each block will consist of one warp \u2013\n_global_ void kernel(int *a, *b, *c)\n{ int i = blockIdx.x * blockDim.x + threadIdx.x ; only 5 threads in warp will do useful work.\n(Other 27 threads will execute no-ops.)\na[i] = i ;\nb[i] = blockIdx.x;\nc[i] = threadIdx.x;\n}\n\na[]\nGlobal\nMemory b[]\nc[]\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9 10 11 12 13 14 15 16 17 18 19\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n1\n\n1\n\n2\n\n2\n\n2\n\n2\n\n2\n\n3\n\n3\n\n3\n\n3\n\n3\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n31\n\n\fExample: Computing \ud835\udc9a \ud835\udc8a = \ud835\udc82 \u2217 \ud835\udc99 \ud835\udc8a + \ud835\udc9a \ud835\udc8a\nC program (on CPU)\n\nCUDA program (on CPU+GPU)\n\nvoid saxpy_serial(int n, float a, float\n*x, float *y)\n{\nfor(int i = 0; i<n; i++)\ny[i] = a * x[i] + y[i];\n}\n\n_global_ void saxpy_gpu(int n, float a, float *x,\nfloat *y)\n{\nint i = blockIdx.x*blockDim.x +\nthreadIdx.x;\nif (i < n ) y[i] = a * x[i] + y[i];\n}\n\nvoid main ()\n{\n\u2026\nsaxpy_serial(n, 2.0, x, y);\n\u2026\n}\n\nvoid main ()\n{\u2026\n// cudaMalloc arrays X and Y\n// cudaMemcpy data to X and Y\nint NB = (n + 255) / 256;\nsaxpy_gpu<<<NB, 256>>>(n, 2.0, X, Y);\n// cudaMemcpy data from Y\n}\n32\n\n\fExample: Computing \ud835\udc9a \ud835\udc8a = \ud835\udc82 \u2217 \ud835\udc99 \ud835\udc8a + \ud835\udc9a \ud835\udc8a\n\u25cf What happens when n = 1?\n_global_void saxpy_gpu(int n, float a, float *X, float *Y)\n{\nint i = blockIdx.x*blockDim.x + threadIdx.x;\nif (i < n ) Y[i] = a * X[i] + Y[i];\n}\n\u2026..\nsaxpy_gpu<<<1, 256>>>(1, 2.0, X, Y); /* X and Y are both sized 1! */\n\n\u25cf \u201cif (i < n)\u201d condition prevents writing beyond bounds of array.\n\u25cf But that requires some threads within a warp not performing the write.\no But a warp is a single vector instruction. How can you branch?\no \u201cif (i < n)\u201d creates a predicate \u201cmask\u201d vector to use for the write\no Only thread 0 has predicate turned on, rest has predicate turned off\n33\n\n\fGPUs Use Predication for Branches\n\n\u25cf Each thread computes own predicate for condition threadIdx.x < 4\n\u25cf Taken together, 32 threads of a warp create a 32-bit predicate mask\n\u25cf Mask is applied to warps for A, B, X, and Y.\n\u25cf Just like for VLIW processors, this can lead to low utilization.\n34\n\n\fGPU Performance\n\n35\n\n\fLesson 1:\nParallelism is Important\n\n36\n\n\fThread Level Parallelism\n\u25cf Superscalars and VLIWs are useful only if\u2026\no Program exhibits ILP (Instruction Level Parallelism) in the code\n\u25cf GPUs are useful only if\u2026\no Program has TLP (Thread Level Parallelism) in the code\no TLP can be expressed as the number of threads in the code\n\u25cf How that TLP is laid out in the kernel is also important\no How many threads are in a thread block\n\u25aa If less than threads in warp, some SPs may get unused\no How many thread blocks are in the grid\n\u25aa If less than number of SMs, some SMs may get unused\n\u2192 If not careful, your GPU may get underutilized\n37\n\n\fExample: Kernels with Bad Layout\n\u25cf Suppose there are 4 SMs in GPU with 32 SPs in each SM.\no Case 1, 2 below have enough TLP (1024 threads) but bad layout.\no Utilized threads are marked in red. Rest are unused.\n\u25cf Case 1: Not enough threads\nkernel<<<1024, 1>>(\u2026) ;\n\u25cf Case 2: Not enough blocks\nkernel<<<1, 1024>>(\u2026) ;\n\u25cf Balanced threads and blocks\nkernel<<<32, 32>>(\u2026) ;\nkernel<<<16, 64>>(\u2026) ;\nkernel<<<4, 256>>(\u2026) ;\n\nSM 0\n\nSM 1\n\nSM 2\n\nSM 3\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\nSM 0\n\nSM 1\n\nSM 2\n\nSM 3\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\nSM 0\n\nSM 1\n\nSM 2\n\nSM 3\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n0 1 2 \u2026 31\n\n38\n\n\fLesson 2:\nBandwidth is Important\n\n39\n\n\fExample: Computing \ud835\udc9a \ud835\udc8a = \ud835\udc68 \ud835\udc8a, \ud835\udc8b \u2217 \ud835\udc99 \ud835\udc8b\nC program (on CPU)\nvoid mv_cpu(float* y, float* A,\nfloat* x, int n) {\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\ny[i] += A[i*n + j] * x[j];\n}\n\nvoid main ()\n{\n\u2026\nmv_cpu(y, A, x, n);\n\u2026\n}\n\nCUDA program (on CPU+GPU)\nvoid mv_gpu(float* y, float* A, float* x, int n) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\nif (i < n) {\nfor (int j = 0; j < n; j++)\ny[i] += A[i*n + j] * x[j];\n}\n}\nvoid main ()\n{\n\u2026\nint nblocks = (n + block_size - 1) / block_size;\nmv_gpu <<<nblocks, block_size>>> (y, A, x, n);\n\u2026\n}\n40\n\n\fPerformance Results for \ud835\udc9a \ud835\udc8a = \ud835\udc68 \ud835\udc8a, \ud835\udc8b \u2217 \ud835\udc99 \ud835\udc8b\n\u25cf Guess what? CPU is faster than GPU!\n\nBut even comparing pure compute\ntime, CPU is still faster than GPU.\nWhat the\u2026?\n\n* Run on netlab-1.cs.pitt.edu with n=8192:\n- Intel Core i7-4770 CPU\n- NVIDIA GF119-825-A1 GPU\n\nA lot of time is spent copying back and\nforth between CPU and GPU memories.\n\n41\n\n\fPerformance Results for \ud835\udc9a \ud835\udc8a = \ud835\udc68 \ud835\udc8a, \ud835\udc8b \u2217 \ud835\udc99 \ud835\udc8b\n\u25cf Was it because the GPU was wimpy and can\u2019t do enough FLOPS?\n\n\u25cf NVIDIA GF119-825-A1 is a Fermi GPU Capability 2.1\no Clock rate: 1046 MHz (X 2 for warp execution)\no Number of SMs: 1\no Number of SPs per SM: 48\no Max FLOPS = 1046 MHz * 2 * 1 * 48 = 100.4 GFLOPS\n\u25cf What was the FLOPS achieved?\no y[i] += A[i*n + j] * x[j] = 2 FP ops each iteration for n * n iterations\no n = 8192, so FP ops = 8192 * 8192 * 2 = 134 M\no Time = 0.27 seconds (shortest at 32 thread block size)\no FLOPS = 134 M / 0.27 = 496 MFLOPS\no Not even close to the limit!\n42\n\n\fPerformance Results for \ud835\udc9a \ud835\udc8a = \ud835\udc68 \ud835\udc8a, \ud835\udc8b \u2217 \ud835\udc99 \ud835\udc8b\n\u25cf Could it be that the GPU didn\u2019t have enough memory bandwidth?\n\n\u25cf NVIDIA GF119-825-A1 is a Fermi GPU Capability 2.1\no Memory Type: DDR3\no Memory Bandwidth: 14.00 GB/s\n\u25cf GPUs also have Performance Monitoring Units (PMUs)\no NVIDIA Profiler (nvprof) provides an easy way to read them:\nhttps://docs.nvidia.com/cuda/profiler-users-guide/index.html\no Let\u2019s use the PMU to profile the following:\n\u25aa DRAM Transfer Rate (GB/s)\n\u25aa L1 Hit Rate (%)\n\u25aa L2 Hit Rate (%)\n43\n\n\fMemory Wall Hits Again\n\n44\n\n\fMemory Wall Hits Again\n\nDRAM bandwidth not saturated:\nBenefits from more parallelism\ndue to larger thread block sizes\n\nDRAM bandwidth saturated:\nLarger thread blocks \u2192 increased working set size \u2192\nhigher cache miss rates \u2192 worse bandwidth problem\nDRAM\nBandwidth\nLimit (14 GB/s)\n\n45\n\n\fIs there a way we can reach max FLOPS?\n\u25cf Let\u2019s take a look at the GPU design metrics again:\no Max FLOPS = 100.4 GFLOPS\no Memory Bandwidth: 14.00 GB/s\n\u25cf To sustain max FLOPS, you need to do a lot of work per byte\no 100.4 GFLOPS / 14.00 GB/s = 7.17 FP ops / byte\no Or, about 28 FP ops / float (4 bytes) fetched from memory\no Otherwise, the memory bandwidth cannot sustain the FLOPS\n\u25cf All GPUs have this problem with memory bandwidth:\no It\u2019s easy to put in more SMs using transistors for Moore\u2019s Law\no Your memory bandwidth is limited due to your DDR interface\n\n46\n\n\fArithmetic Intensity: A property of the program\n\u25cf How many FP ops / float for our mat-vec multiplication?\no y[i] += A[i*n + j] * x[j] each iteration with n * n iterations\no FP ops = 2 * n * n (one multiply and one add)\no Float accesses = n * n + 2n (1 matrix and 2 vector accesses)\n\u25aa That\u2019s counting only cold misses but could be even more\no So approx. 2 FP ops / float (a far cry from 28 FP ops / float)\no This metric is called arithmetic intensity\n\n\u25cf Arithmetic intensity is a property of the program needed by GPUs\no Just like TLP (thread-level-parallelism) is needed by GPUs\no Matrix-vector multiplication has low intensity\n\u2192 Fundamentally not suited for fast GPU computation\n\n47\n\n\fArithmetic Intensity: A property of the program\n\n* Courtesy of Lawrence Berkeley National Laboratory:\nhttps://crd.lbl.gov/departments/computer-science/par/research/roofline/introduction/\n\n\u25cf BLAS: Basic Linear Algebra Subprograms\no BLAS 1: Vector operations only (e.g. saxpy) \u2192 Bad intensity\no BLAS 2: General Matrix-Vector Multiplication (GeMV) \u2192 Bad intensity\no BLAS 3: General Matrix Multiplication (GeMM) \u2192 Good intensity\n48\n\n\fMatrix-Matrix Multiply: Good Arithmetic Intensity\n\u25cf Matrix-multiplication:\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\nfor (int k=0; k<n; k++)\nC[i*n + j] += A[i*n + k] * B[k*n + j];\n\u25cf What\u2019s the arithmetic intensity for this program?\no FP ops = 2 * n * n * n (one multiply and one add)\no Float accesses = 3 * n * n (3 matrix accesses)\n\u25aa If we only have cold misses and no capacity misses\no Arithmetic intensity = 2 * n / 3 = 0.66 * n = O(n)\no Implication: The larger the matrix size, the better suited for GPUs!\n\u25aa Important result for deep learning and other apps\n49\n\n\fExample: Computing \ud835\udc6a \ud835\udc8a, \ud835\udc8b = \ud835\udc68 \ud835\udc8a, \ud835\udc8c \u2217 \ud835\udc69 \ud835\udc8c, \ud835\udc8b\nC program (on CPU)\nvoid mm_cpu(float* C, float* A, float* B,\nint n) {\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\nfor (int k=0; k<n; k++)\nC[i*n + j] += A[i*n + k] * B[k*n + j];\n}\n\nvoid main ()\n{\nmm_cpu(C, A, B, n);\n}\n\nCUDA program (on CPU+GPU)\nvoid mm_gpu(float* C, float* A, float* B, int n) {\nfloat Cvalue = 0;\nint i = blockIdx.y * blockDim.y + threadIdx.y;\nint j = blockIdx.x * blockDim.x + threadIdx.x;\nfor (int k = 0; k < n; ++k)\nCvalue += A[i * n + k] * B[k * n + j];\nC[i * n + j] = Cvalue;\n}\nvoid main ()\n{\ndim3 dimBlock(block_size, block_size);\ndim3 dimGrid(n / dimBlock.x, n / dimBlock.y);\nmm_gpu <<<dimGrid, dimBlock>>> (C, A, B, n);\n}\n50\n\n\fPerformance Results for \ud835\udc6a \ud835\udc8a, \ud835\udc8b = \ud835\udc68 \ud835\udc8a, \ud835\udc8c \u2217 \ud835\udc69 \ud835\udc8c, \ud835\udc8b\n\nNow GPU is much faster\nthan CPU given enough TLP\n\nWhat\u2019s this? This version of\nmatrix-multiply reduces\ncache capacity misses using\nshared memory in the GPU.\n\n51\n\n\fCapacity Misses can Reduce Arithmetic Intensity\nfor (int i=0; i<n; i++)\nfor (int j=0; j<n; j++)\nfor (int k=0; k<n; k++)\nC[i*n + j] += A[i*n + k] * B[k*n + j];\n\n\u25cf The ideal arithmetic intensity for this program was:\no FP ops = 2 * n * n * n (one multiply and one add)\no Float accesses = 3 * n * n (3 matrix accesses)\no Arithmetic intensity = 2 * n / 3 = 0.66 * n = O(n)\n\u25cf Only if we have no capacity misses. What if we did have them?\no For B[k*n + j], reuse distance is the entire matrix of B\no If B[k*n + j] always misses, memory accesses is in the order of n3!\n52\n\n\fSo what is Shared Memory?\nIF/ID\n\nIF/ID\n\nIF/ID\n\nL1 cache\n\nL1 cache\n\nL1 cache\n\nShared\nmemory\n\nShared\nmemory\n\nShared\nmemory\n\nL2 cache\n\nGlobal (GPU) memory\n\n\u25cf Shared Memory: memory shared among threads in a thread block\no Variables declared with __shared__ modifier live in shared memory\no Is same as L1 cache in terms of latency and bandwidth!\no Storing frequently used data in shared memory can save on bandwidth\n53\n\n\fLoop Tiling with Shared Memory\n\u25cf Store a \u201ctile\u201d within matrix in shared memory while operating on it\no Can reduce accesses to DRAM memory\n\u25cf Code in: https://docs.nvidia.com/cuda/cuda-c-best-practicesguide/index.html#shared-memory-in-matrix-multiplication-c-ab\n__shared__ float aTile[TILE_DIM][TILE_DIM],\nbTile[TILE_DIM][TILE_DIM];\nint row = blockIdx.y * blockDim.y + threadIdx.y;\nint col = blockIdx.x * blockDim.x + threadIdx.x;\nfloat sum = 0.0f;\naTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];\nbTile[threadIdx.y][threadIdx.x] = b[threadIdx.y*N+col];\n__syncthreads();\nfor (int i = 0; i < TILE_DIM; i++)\nsum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];\nc[row*N+col] = sum;\n\u25cf Assumption: TILE_DIM = w. What if w > TILE_DIM?\n\n54\n\n\fLoop Tiling with Shared Memory\n\u25cf TILE_DIM is limited by amount of shared memory. What if w > TILE_DIM?\no Now must load A and B tiles w / TILE_DIM times per thread block\n\u25cf Now code will look like:\nN\n__shared__ float aTile[TILE_DIM][TILE_DIM],\nB\nbTile[TILE_DIM][TILE_DIM];\nfloat sum = 0.0f;\nfor (int t = 0; t < w / TILE_DIM; t++) {\n\u2026\naTile[threadIdx.y][threadIdx.x] = \u2026;\nA\nC\nbTile[threadIdx.y][threadIdx.x] = \u2026;\n__syncthreads();\nN\nfor (int i = 0; i < TILE_DIM; i++)\nsum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];\nw\n}\nc[row*N+col] = sum;\n55\n\nw\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}