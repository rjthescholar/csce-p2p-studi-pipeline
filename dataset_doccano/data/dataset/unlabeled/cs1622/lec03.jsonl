{"id": 244, "segment": "unlabeled", "course": "cs1622", "lec": "lec03", "text": "Lexing and Grammars\nCS/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n\u25cf project 1 is out if you didn't see the announcement(s)\no remember it's due by this Saturday evening\no or Sunday for late credit (-10%)\n\u25cf finally today we're getting into the COMPILER STUFF!\n\u25cf couple new examples in the examples repo for today too!\n\n2\n\n\fLexing\n\n3\n\n\fWhat is it?\n\u25cf when a compiler reads the source code, it's just a long string.\n\u25cf lexing (or \"scanning\") is the process of splitting that string into small,\nmeaningful pieces in order to simplify the next step, parsing.\n\n\"3*x + (y / 1.9)\"\n\nthe lexer's input is\nthe source code.\n\nLexer\n\nIntLit(3, Dec), Times,\nId(\"x\"), Plus, ...\n\nthe lexer's output\nis a list of tokens.\n\neach token is like one \"word\" in the source\nlanguage \u2013 the smallest unit of meaning.\n4\n\n\fTokens, and\u2026 not-tokens\n\u25cf tokens have meaning to the programming language. but not\neverything you type in the source code is meaningful.\nif x < 10 {\nf();\n}\nif x<10{f();}\nif x < 10\n{\n// call\nf ( );\n}\n\nall three of these would lex to the same\nsequence of tokens. but what differs?\nspacing and indentation \u2013 together called\nwhitespace \u2013 is meaningless in most modern\nlanguages, and the lexer strips it out.\ncomments are another common kind of\nmeaningless text that can be ignored.\n5\n\n\fNot-so-whitespace\n\u25cf whitespace can't be ignored in all situations, though.\n\"hello, world\"\n\"hello,\nworld\"\n\nthese are different strings, right?\n\nso we have to remember whether or not we're inside \"quotes\".\nalso, fortress and for tress mean different things.\nPython uses indentation to structure code, instead of { braces }.\nif x < 10:\nf()\nprint(\"done\")\n\nthe print call is outside of the if. the only thing\nthat indicates that is the indentation.\n\nand JavaScript (aka ECMAScript) does weird things with newlines\nso you can avoid writing semicolons\u2026\n6\n\n\fWhere are we?\n\u25cf a secondary (but super important) job of the lexer is to produce\nlocation info so the compiler can give good error messages.\nline 1, column 1\n\n1:13\n\nline 1, col 4\n\n1:19\n1:16\n\nif num_cats == 10 {\nprintln(\"yay!\");\n2:5\n\n2:12\n2:13\n\n2:19\n2:20\n\nthe line number is\nbased on how many\nnewlines it's seen.\nthe column number is\nbased on how many\ncharacters it's seen\nsince the last newline.\n\nthis information can be carried forward through the\nrest of the compilation process for a number of uses.\n7\n\n\fLexing from Intuition\n\n8\n\n\fGetting a feel for it\n\u25cf a lot of problem-solving works by getting a feel for what a solution\nmight look like, then formalizing that intuition into an algorithm.\n\u25cf let's start by looking at what our input language looks like.\nstruct S {\nx: int,\ny: bool,\n}\n\nthis is the toy language we'll be using for\nprojects and examples, called Truss.\n\nfn main() {\nprintln_s(\"hi!\");\nlet s = new S();\ns.x = 10;\ns.y = true;\nprintln_i(s.x);\n}\n\nit has similarities to both Rust and Java,\nbut is a lot simpler in many ways.\nwhat are some kinds of tokens you\ncan pick out from this example?\n\n9\n\n\fCommon classes of tokens\n\u25cf most languages today use similar rules for their classes of tokens.\nthere are tokens that look like words.\nstruct int\nreturn while\n\nsome have special meaning in the\nlanguage. these are called keywords.\n\nS num_cats\nprintln main\n\nothers are written by the programmer to\nname things. these are called identifiers.\n\nthere are often\nmany symbols.\n\nand there are literals: a way of embedding\nconstant values directly into your code.\n\n+\n\n-\n\n* /\n\n=\n\n. , ; { } ==\n&& || +=\n\n\"hello,\\nworld!\"\n'c'\ntrue\n0xDEADBEEF\n1.9e6\n345\n10\n\n\fA first attempt\n\u25cf we might come up with some simple rules for these tokens:\no identifiers are a sequence of letters, underscores, and digits.\n\u25aa e.g. x, x2, _lift, THIS_IS_A_TEST, fort, o_o\no keywords are a fixed subset of identifiers.\n\u25aa e.g. if, else, int, private, for\no symbols are a fixed set of sequences of symbol characters.\n\u25aa e.g. +, +=, =, ==, &, &=, &&\n\u25cf but then we get to the literals, and it gets harder.\no here are some floats: 1.2, 10e9, 6.28E+23, 4., 5f\no do you think you could come up with a concise rule for that?\no and then strings\u2026 \"hello\\nworld\"\n\u25cf oh, but it gets worse!\n\n11\n\n\fWe keep running into each other\n\u25cf let's look at some awkward situations!\n\nabc123\n123abc\nx.y\n4.y\nx.5\n4.5\n1.2.3\n\nabc123\n123abc? or 123, abc? or error?\nx, ., y\n4, ., y or 4., y ?\nx, ., 5 or x, .5 ?\n4.5 or 4., 5 or 4, ., 5 ?\nor\nor\nor\nor\n\n1.2, ., 3\n1.2, .3\n1., 2.3\n1., 2., 3\n1, ., 2, ., 3\n\n12\n\n\fuHH\n\u25cf even with such a simple set of tokens, we're already running into\nproblems, and it's not entirely clear where we went wrong.\n\u25cf well, let's first learn a bit about grammars, which can give us some\ntools to talk about these things more rigorously.\n\n13\n\n\fGrammars\n\n14\n\n\fLanguages and alphabets\n\u25cf a language is a set of strings. (most useful languages are infinite sets.)\n\u25cf each string is a sequence of symbols chosen from an alphabet.\n\u25cf for example, here's a really dumb language and its alphabet:\n\nL = { \"hi\", \"bye\" }\nA = { 'a', 'b', \u2026, 'z' }\n\neach symbol in the alphabet\nis called a terminal.\n\nif we generate strings from this alphabet, it's\nkind of obvious whether or not they are in L.\n\n\"hi\" \u2208 L\n\"hii\" \u2209 L\n\"cat\" \u2209 L\n\n\u2026ok, what about a\nmore complicated\nexample?\n15\n\n\fGrammars\n\u25cf for more complex or infinite languages, you need a grammar: a set\nof rules to decide if a string is in the language.\n\nL = { \"a\", \"aa\", \"aaa\", \u2026 }\nA = { 'a', 'b', \u2026, 'z' }\n\nwhat is/are the rule(s)\nfor this language?\n\n\"1 or more 'a's\"\n\nokay. but can we write that grammar rule in a more rigorous way?\n\nL: 'a'+\nthis is a nonterminal. it\nisn't part of the alphabet;\nit stands in for some\ncombination of terminals.\n\nthis + says \"repeat the\nprevious thing 1 or\nmore times.\"\n16\n\n\fA language for specifying languages\n\u25cf we usually specify the lexical and syntactic form of a language with a\nmetalanguage: a concise way of specifying rules.\n\u25cf there are some common patterns for these rules:\n\nL: A B\nR: A | B\nX: A*\nY: A+\nU: A?\nD: (A B)+\n\nA followed by B. (sequencing)\nan A or a B. (alternation)\n\nzero or more As. (repetition)\none or more As. (repetition\u2026 again)\nzero or one A, or \"an optional A\".\nparens can group things together.\n\nin these examples, A and B can be terminals (from the\nalphabet) or nonterminals (names of other rules).\n17\n\n\fLexing a programming language\n\u25cf the alphabet is the character set that the source code is in.\no in our case, that's Unicode!\n\u25cf the language isn't the entire programming language\u2026\no instead, it's the tokens we want to produce.\n\u25cf so, we'll write a rule for each kind of token.\n\nSymbol: '+' | '-' | '{' | '}' | '=' | ('=' '=')\n'quoted' things are terminals, picked from the alphabet.\n\nKeyword: ('i' 'f') | ('e' 'l' 's' 'e') |...\nthat's ugly, but we can make our metalanguage look any way we like.\n\nKeyword: \"if\" | \"else\" | \"fn\" | \"return\"\nthat's better!\n\n18\n\n\fGetting more complex\n\u25cf rules can refer to other rules. like here:\n\nId:\nIdStart IdCont*\nIdStart: Alphabetic | '_' | '$'\nIdCont: IdStart | Digit\nDigit:\n'0'|'1'|'2'|'3'|'4'|'5'|'6'|'7'|'8'|'9'\nAlphabetic: (a whole bunch of characters)\nso, an identifier starts with a letter or underscore or dollar sign; and\nthat is followed by zero or more of those, plus digits. neat.\nand we could keep going for all the tokens!\n\n19\n\n\fLiterals!\n\u25cf numeric and string literals can get pretty complicated, but let\u2019s keep\nthem simple for now:\n\nIntLit: Digit+\nStrLit: '\"' StrChar* '\"'\nStrChar: <any character except '\"'>\ninteger literals are sequences of digits, and string literals are\nsequences of characters surrounded by double quotes. also it\u2019s fine\nto be a bit hand-wavey like in StrChar, as long as the intent is clear.\nbut: what if the input contains the number\n100000000000000000000? isn\u2019t it too big\u2026?\ndefining the valid ranges of numbers within the metalanguage\nisn\u2019t possible. so we could define that as an additional rule.\n20\n\n\fUh oh, whitespace.\n\u25cf whitespace isn't a token, but we do have to deal with it somehow.\n\u25cf we can write rules for it, like a real token:\n\nWhitespace: ' ' | '\\t' | '\\n' | Comment\nComment:\n\"//\" CommentChar* CommentEnd\nCommentChar: <any character except '\\n' or Eof>\nCommentEnd: ('\\n' | Eof)non-consuming\nwoah what the heck is that\n\nthis is a lookahead: it checks if the next character is a newline\nor EOF, but it does not make it part of the Comment.\nit feels a little kludgey, but sometimes\nlookaheads are just what you need.\n\njust\u2026 don\u2019t use them too much.\n\n21\n\n\fWrapping it up and putting a bow on top\n\u25cf once we have a rule for each kind of token, we can do this:\n\nToken:\nSymbol | Keyword | Id | IntLit | StrLit\nProgram: (Whitespace? Token)* Whitespace? Eof\nhere, Program is a special rule: it's our top-level or start rule.\nif we want to translate this grammar into code that\nlexes, it's where we start \u2013 it's the outermost loop.\n\nbut wait. if we spent all this time writing the\nlexical rules in a formalized, rigorous way\u2026\ncouldn't we just have a program compile\nthe grammar itself into a lexer?\n22\n\n\fCOMPILER COMPILERS \ud83e\udd2f\n\u25cf yes, this is A Thing.\n\u25cf there are many tools which will take some kind of metalanguage as\nthe input, and produce a lexer program as an output.\no (or a parser, but we haven't gotten to those yet)\n\n\u25cf however\u2026\no most are tightly tied to the language whose code they output.\no they're really complex, to be as flexible as possible.\no \u2026but, it can be awkward to fit your grammars into their rules.\no they can give really confusing/terrible error messages on invalid\nlexer input.\n\u25cf and honestly, writing your own lexer is not that complicated.\no and, yanno, it's a compilers course, so you have to do it. (:\n\n23\n\n\fDealing with Ambiguity\n\n24\n\n\fDealing with ambiguity\n\u25cf ambiguity is when you have > 1 possible \"correct\" tokenizations.\n\u25cf consider these snippets of Java.\n\nif(x == 3) is this [==] or [=, =]?\n3.4\n\nis this [3.4] or [3., 4]?\n\nabc123\n123abc\n\nis this [abc123] or [abc, 123] or [ab, c123] or\u2026?\nwhy do we think this is an error when the above is not?\n\non this line, >> is a right shift\u2026\nint y = x >> 3;\nList<List<String>> l; but here, it's two right angle brackets?\nthere are three ways I can think of to resolve ambiguity.\n25\n\n\fApproach 1: Maximal Munch\n\u25cf maximal munch is a strategy that says: always try to make the\nbiggest token possible, going left-to-right.\n\nif(x == 3) this is [==].\n3.4\n\nthis is [3.4].\n\nabc123\n123abc\n\nthis is [abc123].\nthis is an error because we start lexing a number, and\nthen hit a non-numerical character.\nalthough, it doesn't have to be; we could say \"3x\" is another way of writing \"3 * x\", couldn't we?\n\nint y = x >> 3;\nList<List<String>> l; but this still presents an issue\u2026\n26\n\n\fApproach 2: Deal with it later!\n\u25cf we can have the parser (the next step) disambiguate things that\nwould be impossible to detect in the lexer.\nlexers are usually specified as regular languages,\nwhich cannot detect arbitrarily nested brackets.\n\nList<List<String>> l;\nthe lexer cannot know that >> is a pair of closing brackets\nwithout knowing that there are two unpaired open brackets.\n\nso, we have the parser say \"oh, I currently have two open brackets;\nthis >> must be a pair of closing brackets and not a right-shift.\"\nbut why bother making things harder for ourselves?\n27\n\n\fApproach 3: \"Doctor, it hurts when I do this\"\n\u25cf we could just define our rules so they're unambiguous.\nList!(List!(String)) l;\n\nin the D language, templates\n(generics) use parens instead.\n\nthe parser then knows !( is a \"template open paren\".\nl :: List List String\nm :: List (List String)\n\nin Haskell, generics don't require\nany symbols, but can use parens.\n\nx shr 3\n\nor, we could redefine right shift to\nuse some spelling other than >>.\n\n28\n\n\fReally bad cases of ambiguity\n\u25cf if you aren't careful, you can end up with nasty situations.\n\nLexer\n\nParser\n\nSemantic\nAnalyzer\n\nC and C++ have this situation, where you cannot correctly lex\nthe source code until you've done (some) semantic analysis.\n\nthis sucks! it makes the compiler slower,\nharder to write, and harder to reason about.\nfortunately, we have the hindsight of decades\nof language design and can avoid this stuff :)\n29\n\n\fThe examples\n\u25cf first, there\u2019s an example showing how Rust\u2019s match statement works,\ncalled rust_match.\n\u25cf there's a small example, lexing_toy, that shows how simple it can be\nto make a lexer.\no it only has a few kinds of tokens, and is written in a different style\nfrom the project, cause I don\u2019t wanna give too much away (:\no but it shows the general \u201cshape\u201d of a lexing algorithm.\n\n30\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}