{"id": 203, "segment": "unlabeled", "course": "cs1622", "lec": "lec06", "text": "Bottom-up Parsing\nCS/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n\u25cf today we're finishing parsing!! yaaay\no so after today you\u2019ll have everything you need for project 2!!!!!!\n\u25cf bottom-up parsing is (imo) best used for parsing expressions, so we\nshould start by talking about\u2026\n\n2\n\n\fOperators\n\n3\n\n\fEvaluating (and producing) trees\n\u25cf we saw before that given an AST, we could evaluate it:\n*\n\n5\n\nI think you would agree that the order of operations\nhere is to do the addition, then the multiplication.\n\n+\n2\n\n3\n\nbut we're talking about parsing now. what source\ncode would I write to produce this AST?\n\n5 * (2 + 3)\n\n+\n\nwhy did we have to put parentheses there?\nbecause of the order of operations:\nwithout them, 5 * 2 + 3 would parse as:\n\n*\n\n5\n\n3\n\n2\n4\n\n\fPEMDAS, BODMAS, BIDMAS\u2026\n\u25cf officially, the order of operations used by mathematicians is:\n1. parentheses/brackets\n2. exponentiation/roots\n3. multiplication/division\n4. addition/subtraction\n\u25cf the order of the letters in the acronym (MD vs. DM) does not say\nthat e.g. multiplication happens before division.\n\u25cf when writing math horizontally like 27 / 3 / 9, there's a problem:\no even among mathematicians, there is no consensus on what\norder horizontal division should be done!\n\u25cf in programming languages, all math is written horizontally\u2026\no and you have e.g. %, &&, ||, &, |, ^, !, ~, <, >, <=, >=, ==, !=, \u2026\n\u25cf so we need a more rigorous way of dealing with this.\n5\n\n\fOperator Precedence\n\u25cf here's a more formalized, algorithmically-checkable method:\no you rank the operators by giving each a number, its precedence\no that precedence decides the order of evaluation\n\u25cf another way of thinking of it is: if you wrote the expression without\nparentheses, the precedence tells you where they should go:\nOperator Prec\n\na + b * c\n\na + (b * c)\n\n**\n\n1st\n\na ** b * c\n\n(a ** b) * c\n\n* / %\n\n2nd\n\n+ -\n\n3rd\n\nbut this still isn't enough information to\ndisambiguate when operators of the same\nprecedence appear next to each other:\n\n27 / 3 / 9\n\n(27 / 3) / 9 or\n27 / (3 / 9) ?\n6\n\n\fOperator Associativity\n\u25cf in a precedence tie, an operator's associativity breaks the tie.\nleft-associative operators\n(e.g. +, -, *, /, %) evaluate\nthings left-to-right.\n\nand right-associative\noperators (e.g. **, =) evaluate\nthings right-to-left.\n\n27 / 3 / 9\n\n2 ** 3 ** 4\n\n(27 / 3) / 9\n\n2 ** (3 ** 4)\n\nin other words, associativity says which way the tree branches.\n/\n\n/\n27\n\n**\n\n9\n3\n\n2\n\n**\n\n3\n\n4\n7\n\n\fMore about associativity\n\u25cf to be clear: associativity only applies when you have adjacent\noperators of the same precedence.\no a ** b + c parses as (a ** b) + c, because exponentiation's\nprecedence ranks higher than addition's.\n\u25cf it's also possible for operators to be non-associative.\no that means it's an error for them to be adjacent.\n\u25cf for example, what does x < y < z mean?\no in Java, you get a type error.\no in Python, it's parsed as (x < y) && (y < z).\no in the language we're designing, we could say that comparisons are\nnon-associative, and block this potentially-confusing code as a\nparsing error instead.\n\n8\n\n\fUnary and postfix operators\n\u25cf operators that take two operands are called binary. but many\n(especially in C-style languages) are unary and operate on one value.\no -x, ~x, !x, &x, *x, ++x, --x, (int)x\no these are given a precedence higher than any binary operator.\n\u25aa this matches our intuition: -3 + 5 should give 2, not -8.\no these are all right-associative, but it doesn't come up much.\n\u25aa -~x would bitwise-complement x first, then negate that.\n\u25cf finally there are postfix operators which come after their operand.\no x++, x--, a[i], obj.field, f(x)\no and these have the highest precedence of all!\n\u25aa -f(x) calls the function first, then negates the return value.\no these are all left-associative, and that's pretty common to see.\n\u25aa f(x)[i].field calls f first, then indexes the returned array, then\naccesses the field from the array value.\n9\n\n\fSummarizing operators\n\u25cf a language's specification will typically have all the operators, their\nprecedences, and their associativities listed in a table:\nOperator\n\nPrec Assoc\n\nf() a[i] o.f\n\n1\n\nL\n\n!x -x *x &x\n++x --x\n\n2\n\nR\n\n* / %\n\n3\n\nL\n\n+ -\n\n4\n\nL\n\n< <= > >=\n\n5\n\nL\n\n== !=\n\n6\n\nL\n\n&&\n\n7\n\nL\n\n||\n\n8\n\nL\n\n=\n\n9\n\nR\n\nthis is a pretty typical arrangement for a\nC-style language (excluding bitwise operators, casts,\nreflexive assignments, and some other obscure operators)\n\nbut where are parentheses?\n\nwell, they're not operators. parentheses\nonly control the order of evaluation and\nthe way the AST is built; they don't do\nanything otherwise.\n10\n\n\fBuilding a grammar\nfor expressions\n\n11\n\n\fA grammar from the bottom up\n\u25cf rather than starting with the \"overall\" structure of an expression\u2026\no let's start with the most basic parts and gradually combine them.\n\u25cf a primary expression is the most basic kind.\nliterals fall into this category.\n\n318\n\nidentifiers are another kind:\nthey name single things.\n\n\"hello\"\n9.5\n\n'x'\n\nx\n\nmain\n\nSystem\n\nlast, we'll include parenthesized expressions, because they\ncontain whole sub-expressions. so, our grammar is:\nPrimaryExp: IdExp | IntLitExp | ParenExp\nIdExp:\n<identifier token>\nIntLitExp: <int literal token>\n(Exp is the top-level expression\nrule that we're building up to)\nParenExp:\n'(' Exp ')'\n\n12\n\n\fTerms: primaries with decorations\n\u25cf since unary and postfix operators are so closely associated with their\noperands, we can define a Term rule like this:\nTerm: UnaryOp* PrimaryExp PostfixOp*\n\u25cf that is, 0 or more unary operators before a primary \"core,\" followed\nby 0 or more postfix operators.\n\u25cf UnaryOp and PostfixOp might be defined like:\nUnaryOp:\n'-' | '!'\nPostfixOp: CallOp | FieldOp | IndexOp\nCallOp:\n'(' ')' | '(' Exp (',' Exp)* ')'\nFieldOp:\n'.' <identifier token>\nIndexOp:\n'[' Exp ']'\n\u25cf example productions of this grammar are x, -x, f(), !f(x).z, !!x\n\n13\n\n\fExpressions: the big picture\n\u25cf really, every expression looks like this:\nterm op term op term op term op term\nwhere each op is a binary operator.\nso, we'll define these rules:\nExp:\nTerm (BinaryOp Term)*\nBinaryOp: '+'|'-'|'*'|'/'|'%'|\u2026etc.\n\nbut what about the associativity and precedence??\nwell, we just\u2026 uh\u2026 won't encode that into the grammar.\n\n14\n\n\fWait. What?\n\u25cf it is absolutely possible to encode precedence and associativity into\nthe expression grammar, and then write a fully recursive-descent\nparser for expressions. (I know, because I've done it.)\n\u25cf but the result is just\u2026 well. here's a simple mathematical language:\nExp: Add\nAdd: Mul (('+'|'-') Mul)*\nMul: Neg (('*'|'/'|'%') Neg)*\nNeg: Pow | ('-' Neg)\nPow: Pri ('**' Pow)*\nPri: Id | IntLit | '(' Exp ')'\n\nis it obvious that this accurately\ncaptures all expressions?\ncan you identify the\nassociativity and precedence\nof these operators?\n\nwhat if, like C or C++, we had 15 precedence levels and\ndozens of operators? how readable would that be?\n15\n\n\fSo no! We won't do it!\n\u25cf because we're defining our language's grammar a bit loosely, it\ndoesn't really matter if our grammar is 100% accurate.\no instead, the ambiguities in the grammar are resolved by the table\nof precedence and associativity.\n\u25cf any complex idea can be expressed in multiple different ways.\no unless you have some kind of externally-imposed reason to stick\nwith one way of writing something\u2026\no it makes sense to use different ways according to their strengths.\n\u25cf so\u2026\no how do we write an algorithm to parse this stuff??\n\n16\n\n\fBottom-up parsing\n\n17\n\n\fThe core of the algorithm\n\u25cf let's assume we only have left-associative binary operators.\n\u25cf so, to parse Term (BinaryOp Term)*, we can write something like:\nlet mut LHS = parse_term();\nwhile current token is a binary operator {\nlet op = current token;\nmove to next token;\nlet RHS = parse_term();\nLHS = AstNode::new(LHS, op, RHS);\n}\nreturn LHS;\n\u25cf it might seem strange to replace the contents of the LHS variable\ninside the loop, but this is how it builds up the tree.\n\n18\n\n\fOkay, let's try it (animated)\n\u25cf given this sequence of tokens, let's see what happens:\n\nx + y - z <eof>\nToken Action\n\nx\n+\ny\nz\n<eof>\n\nLHS = parse term\n\nLHS\n\n+\n\nstop looping!\n\ny\n\n(x + y)\n\nop = -, loop!\nRHS = parse term\nLHS = (LHS op RHS)\n\nRHS\n\nx\n\nop = +, loop!\nRHS = parse term\nLHS = (LHS op RHS)\n\nop\n\n-\n\nz\n((x + y) - z)\n19\n\n\fThat's why it's called bottom-up (animated)\n\u25cf this algorithm produces a tree that branches to the left, like this.\n\na + b + c + d + e\nbut if we allow all the operators, what\nwould be a problem input for this?\n\n+\n+\n\n+\n+\na\n\nx + y * z\n\ne\n\n*\n\nd\n+\n\nc\nb\n\nand it builds it\u2026\nbottom up!\n\nx\n\nz\ny\n\nit was a little too\neager to produce\n(x + y), when it\ndidn't know that\nthere was a *\ncoming up.\n20\n\n\fAccounting for precedence\n\u25cf we have to tweak our algorithm a bit.\nlet mut LHS = parse_term();\nwhile current token is a binary operator {\nlet op = current token;\nmove to next token;\nat this point, we don't know if RHS is\nlet RHS = parse_term();\nour second operand, or if it's the next\n\noperator's first operand.\n\nso, we'll have to check if the current token is a binary operator\nwhose precedence is higher than op's.\nand if that's the case, we um. uh.\n\nwait, what do we do then?\n\nLHS = AstNode::new(LHS, op, RHS);\n}\n\n21\n\n\fImplicit parentheses\n\u25cf if you think about what we're doing in a different way\u2026\n\na +(b * c * d * e)+ f\nwe're sort of inserting parentheses as we go.\nso when we see the first *, we should say \"oh, the left\nparenthesis goes before b\u2026\n\n\"\u2026and the right parenthesis goes before the first operator that has\na lower precedence than multiplication.\"*\nsince we want to put the parsing of + on hold, the most\nnatural way to parse the multiplications is to recurse.\n\n(let's look at the example to see how this is done.)\n22\n\n\fHow that parses (animated)\n\u25cf Looking at this input again, let's see how the AST is built for it:\n+\n\na + b * c * d * e + f\n1. a is parsed.\n\nf\n\n+\n\n2. b is parsed.\n3. * is seen, causing us to recurse. + is put \"on hold.\"\n\n*\n\n4. the subtree of multiplications is parsed.\n\n5. that subtree is returned and becomes +'s RHS.\n6. the final + f is parsed.\nRight-branching is done by the\nRecursion;\nLeft-branching is done by the\nLoop.\n\n*\n\n*\na\n\nb\n\ne\n\nd\nc\n23\n\n\fFinishing up by parsing terms\n\u25cf finally, these are more straightforward.\nTerm: UnaryOp* PrimaryExp PostfixOp*\n\u25cf since the unary operators are Right-associative\u2026\no you got it: we have to Recurse to build the AST for those.\n\u25cf after that, primary expressions are straightforward.\no parenthesized expressions will work just fine, because ')' is not an\noperator, and will cause the operator parsing loop to terminate.\n\u25cf and finally, postfix expressions come after the primaries.\no the primary has to be passed to the postfix parser, so that it can\nbecome the LHS of those postfix operators.\no postfix operators are Left-associative, so we can use a Loop.\no also, it's not an error to see 0 postfix operators. they\u2019re optional!\n\n24\n\n\fWhat about right-associative binary operators?\n\u25cf right-associativity is another kind of right-branching.\no so, we would have to change the recursion condition in the binary\noperator parsing.\n\u25cf currently we recurse when we see an operator of higher precedence.\no we will also recurse when we see a right-associative operator of\nthe same or higher precedence.\n\u25cf this is so if you have multiple right-associative operators in a row of\nthe same precedence, you still get the right-branching structure.\n\u25cf in a ** b ** c\u2026\no we see that the second ** has the same precedence as the first\no so we recurse, giving us an RHS of b ** c\no ultimately giving us a ** (b ** c)\n\n25\n\n\fThings to think about\n\n26\n\n\fBottom-up parsing without recursion\n\u25cf by using auxiliary stack data structures, we could remove the need for\nrecursion entirely.\n\u25cf did any of you do the expression evaluation assignment in 445?\no where you had an operator stack and an operand stack?\n\u25cf if you still have that code, try going back and looking at it.\n\u25cf what if the operand stack held AST nodes, instead of Doubles?\no and when you handle names/operands, you pushed AST nodes?\n\u25cf and in the parts where you pop operators and evaluate\u2026\no what if, instead of evaluating, you created/pushed AST nodes?\n\u25cf that's it, that's a parser, you already made most of an expression parser\nand you didn't even know it\n\n27\n\n\fCan we parse everything with bottom-up parsing?\n\u25cf Yes!\n\u25cf actually, generalized bottom-up parsing is more powerful than topdown parsing!\no there are some grammars that top-down parsers can never parse.\n\u25cf however\u2026\no once you move beyond expressions it gets stupidly complex\no the complexity is not really justified for programming languages\n\u25aa their grammars are typically not that complicated\no so it's mostly out of the scope of this course, IMO\n\u25cf for our language, we can parse everything using a hybrid approach:\no use recursive descent for the program's broad structure\no use bottom-up for expressions\n\n28\n\n\fHow does that work though?\n\u25cf well, expressions only appear inside other pieces of code.\n\u25cf so, a grammar might look like this:\nProgram:\nFunction*\nFunction:\n'fn' Id '(' ')' Block\nBlock:\n'{' Stmt* '}'\nStmt:\nIfStmt | ExpStmt | AssignStmt\nIfStmt:\n'if' Exp Block ('else' Block)?\nExpStmt:\nExp ';'\nAssignStmt: Exp '=' Exp ';'\n\u25cf all of these rules would be parsed with recursive descent\u2026\n\u25cf and whenever we need to parse an Exp, we use bottom-up parsing.\n\n29\n\n\fSyntactic Sugar\n\n30\n\n\fYummy\n\u25cf each language feature adds complexity in every stage of the\ncompiler: lexing, parsing, semantics, optimization, code generation\u2026\n\u25cf so, one approach to adding features to our languages is by defining\nthem in terms of other, simpler features that already exist.\nlet lhs = self.parse_term()?;\nthis is syntactic sugar, and it's called that because\nit usually makes your code easier to write.\nlet lhs = match self.parse_term() {\nOk(x) => x,\nErr(e) => return Err(e),\n};\n31\n\n\fAnother example\n\u25cf Java's generic for loop is just sugar for using an iterator.\nfor(int i : someCollection) { \u2026 }\n\n\u25cf becomes something like:\nfor(Iterator<Integer> iter = someCollection.iterator();\niter.hasNext(); )\n{\nint i = iter.next().intValue();\n\u2026\n}\n\n32\n\n\fHow is it implemented?\n\u25cf by desugaring: rewriting the AST after parsing.\no we saw previously that trees are easy to create and manipulate.\n\u25cf but we have to be a little careful.\no up next is semantic analysis, and if we desugar the code before\nthen, it might give really confusing errors about code that the\nprogrammer didn't write!\n\u25cf but that's it for parsing!\n\n33\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}