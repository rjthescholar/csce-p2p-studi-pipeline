{"id": 243, "segment": "unlabeled", "course": "cs1622", "lec": "lec07", "text": "Semantic Analysis\nCS/COE 1622\nJarrett Billingsley\n\n\fClass Announcements\n\u25cf\u00f6\n\n2\n\n\fSemantics\n\n3\n\n\fWhat does it all MEAN\n\u25cf semantics means\u2026 meaning.\n\u25cf a language's semantics are the rules that make it that language.\n\nint x = 10;\nprintf(\"x = \" + x);\n\nthis code lexes and parses exactly\nthe same in both C and Java.\n\nbut the semantics differ wildly.\n\nin Java, this says: \"create a new string object that is the\nconcatenation of \"x = \" and the result of calling x.toString().\"\nin C, this says: \"calculate a pointer starting at the address of \"x = \"\nand adding the value in x times the size of one char.\"\nthe semantics define what each piece of the language does.\n4\n\n\fStatic vs. Dynamic\n\u25cf you'll see these words used a lot in talking about semantics\n\u25cf if we look at a timeline of the process of compiling and running\u2026\nexecution begins here!\ncompilation, linking, loading\u2026\n\nprogram is running\n\nstatic means something that is\ndone before execution begins,\nand it's only done once.\n\ndynamic means something that\nis done during execution,\npossibly over and over.\n\nyou'll also hear it called\n\"compile-time.\"\n\nyou'll also hear it called\n\"runtime.\"\n\n(or \"link-time\" or \"load-time\")\n\n5\n\n\fAbstraction\n\u25cf HLLs provide programmers with useful sets of abstractions.\n\u25cf semantics are about enforcing the rules of those abstractions\u2026\no even if the target CPU doesn't make a distinction!\n\nint i\n= 0;\nObject o = null;\nchar c\n= '\\0';\nboolean b = false;\n\nin this Java, all four lines here will\nprobably compile down to the\nexact same code, something like:\n\nsw\n\n$zero, i\n\nthe CPU doesn't know or care about the difference\nbetween these types, but the language does.\n6\n\n\fCompile-time and runtime guarantees\n\u25cf the compiler is responsible for enforcing many of the language's\nrules, but those rules can extend beyond the compiler's reach!\nint[] a = new int[5];\na[10] = 0; // oops\n\na Java compiler might be able to\ncatch this contrived case.\n\nbut in the general case, this error can't be detected until runtime.\nstatic checks done by the compiler try to\ncatch as many mistakes as possible\u2026\n\u2026but they can't catch every mistake. doing so\nwould require solving the halting problem.\nstill, doing what static checks we can is better than\nleaving all the semantic analysis until runtime!\n7\n\n\fSemantic analysis can get tricky\n\u25cf in most nontrivial languages, semantic analysis is not just \"one step.\"\no there may be several phases or passes of semantic analysis\u2026\no and sometimes, those passes can be run multiple times, or be\nmutually recursive with one another!\n\u25cf the toy language we're working with won't get too scary to analyze\u2026\no but it's good to keep in mind that even seemingly-simple features\ncan cause the complexity of semantic analysis to explode.\n\u25cf one of the most powerful tools we have to define the rules and\nboundaries of a language is\u2026\n\n8\n\n\fTypes\n\n9\n\n\fTypes and typing\n\u25cf HLLs give us several kinds of values to work with.\n\u25cf we classify these values according to their type.\na type is a set of valid values\u2026\n\n{ 0, 1, -1, 2, -2, ... }\n\u2026along with a set of operations that can be performed on them.\n\n{ +, -, *, /, %, parse, ...}\ntypes set the boundaries on what you\ncan and can't do within the language.\n10\n\n\fYou have to listen to the notes they don't play\n\u25cf down at the CPU level, types don't really exist.\no you can do almost anything with anything! the CPU doesn't care!\n\u25cf we say that assembly is untyped.\nli\nsw\nli\njr\n\nt0, 10\nzero, (t0)\nt0, 'a'\nt0\n\n# oh, t0 is an int\n# wait we're using it as an address?\n# uhh now it's a character?\n# and now we're jumping to it??\n\n\u25cf in an untyped language, the programmer can do anything.\no but is that really a good idea\u2026?\n\n11\n\n\fA vast sea of garbage\n\u25cf a program is any sequence of instructions.\n\u25cf so if we consider all possible sequences of instructions\u2026\nalmost all of them do nothing useful at all.\nthis is Useful Program Island.\nType System\nTown\n\n\u00d7\n\ntype systems keep you\nsafely on the island\u2026\n\u2026at the cost of preventing you from being able\nto write all useful programs. like this one.\nbut maybe these kinds of programs are too hard for\nhumans to understand anyway, so no big loss?\n12\n\n\fThe soul of a programming language\n\u25cf the type system affects every operation in the language.\n\u25cf the type system decides\u2026\no what kinds of variables can exist\no what operators and methods you can use on them\no how those operators do their work\no how control structures work\n\u25cf the main difference between C and Java is their type systems.\no even though their syntax can look superficially similar\u2026\no the boundaries drawn by their type systems are completely\ndifferent.\n\n13\n\n\fType systems\n\n14\n\n\fPrimitive types\n\u25cf every language considers some types to be primitive, or\nfundamental. they cannot be broken into any simpler parts.\nsome, like ints and floats, are lw $t0, int_var\nderived from the CPU's abilities. l.s $f0, float_var\n\nothers, like bools and chars, are simple\nabstractions on top of those\u2026\nbut they help us encode intent into our\nprograms, and intent helps us better understand\nour own code (and the code that others write).\nint isDone = 0; // ??\n\nbool isDone = false; // better.\n15\n\n\fCompound types\n\u25cf compound types are composed of two or more primitive types.\no you can think of primitive and compound types like atoms and\nmolecules: multiple atoms make up a molecule.\nstruct Point {\nx: i32,\nstructs and classes are common ways to let the\ny: i32,\nprogrammer define their own compound types.\n}\nlet p: (i32, i32) = (4, 5);\ntuples are a little obscure, but work similarly to structs.\nlet a: [i32; 3] = [1, 2, 3];\n\narrays also count, but they're a little more interesting\u2026\n16\n\n\fGeneric types\n\u25cf a generic type is a special kind of function which takes types as its\ninputs, and produces a type as its output.\no we call that a type constructor.\n\u25cf if that sounds really out there, well, how about ArrayList?\no it takes type arguments in angle brackets: ArrayList<Integer>\n\u25cf and if you give it different type arguments, you get a different type.\no ArrayList<Integer> is different from ArrayList<Double>.\n\u25cf another example in Java is the square brackets: []\no you can't just have a []. it has to be an array of something.\n\u25cf you can put any type before [] and get a new type, so it's a type\nconstructor.\no int[] is a new type. int[][] is another new type. and so on,\nforever!\n17\n\n\fAnd further still\u2026\n\u25cf there are even type constructors that can take values as arguments.\n\u25cf there's a very concrete example for that: arrays\no see, an array has a length\no but\u2026 what if that length were part of the type? like int[10]?\n\u25cf could you make a language where you could never have an\nArrayIndexOutOfBoundsException?\no yes!\no it's possible to track the length of every array and the range of\nevery integer at compile time, and ensure they can't happen.\n\u25cf but there's a big downside to this:\no in the general case, these type systems are undecidable\n(meaning you have to solve the halting problem).\n\u25aa but in some limited cases, it is decidable, and maybe useful?\n18\n\n\fNaming\n\n19\n\n\fHello, my name is\u2026\n\u25cf computers don't know anything about names, language, or strings\u2026\no but humans love words. we can't get enough of em.\n\u25cf one of the reasons people very quickly invented assembly language\nwas to be able to name things in their programs, because dealing\nwith addresses is confusing, tedious, and error-prone.\no can you imagine having to refer to everything by a number?\no and where changing one part of the program changed the\nnumbers of everything after it, and so you had to go back and\nchange all the references to all those numbers\u2026\n\u25aa it was not doable.\n\n20\n\n\fScoping and name resolution\n\u25cf name resolution matches names to the things they refer to.\n\u25cf scoping determines which names are candidates during resolution.\nvoid main() {\nint m = 5;\nf();\n}\n\nyou know that this is wrong, but not\nevery language works like this!\n\nvoid f() {\nprintf(\"m = %d\\n\", m);\n}\n\nin some languages, you can\naccess the local variables of any\nfunction on the call stack.\n\nthis used to be somewhat common in older languages, but eventually we realized \"wow that's confusing as hell\"\n\nwe've kind of settled on a common set of name resolution\nrules, but some languages can still surprise you.\n21\n\n\fStatic vs. dynamic name resolution\n\u25cf some names' referents can't be decided until runtime.\nclass A { String toString() { return \"A\"; } }\nclass B { String toString() { return \"B\"; } }\nObject[] objs = new Object []{ new A(), new B() };\nfor(Object o : objs)\nSystem.out.println(o.toString());\nwhich toString() does o.toString() refer to?\n\non the other hand, all the class names,\nvariable names etc. are statically resolved.\nin some languages, all names are dynamically resolved.\n22\n\n\fRound and round\n\u25cf name resolution can create a complex or even cyclical graph of\ndependencies between the parts of a program.\nclass A {\nvoid print(B b) { println(b.value()); }\nint value()\n{ return 10; }\n}\nA depends on B, and B depends on A.\nclass B {\nvoid print(A a) { println(a.value()); }\nint value()\n{ return 20; }\n}\nI can't check that all of A's code is correct until I check B's\u2026\nbut I can't check that B's is correct until I check A's\u2026???\n\nwell. it's possible to do things in phases, like I said before.\n23\n\n\fScoping\n\n24\n\n\fWhat is it?\n\u25cf the programmer can declare named things like variables, etc.\n\u25cf the scope is where those names can be seen in the program.\nif(x == 10) {\nint y = 20;\nprintf(\"%d\", y); this is alright!\n}\n\nwell, that's how you\nlearned it. not every\nlanguage has the\nsame rules.\n\nprintf(\"%d\", y); this is not.\n\n# Python\nif x == 10:\ny = 20\nprint(y) # works!\nprint(y)\n\n# works!\n25\n\n\fStatic vs. dynamic\n\u25cf the majority of languages use static scope: the syntactic structure\nof the language defines where a name can be seen.\n\u25cf Python and many other dynamically-typed languages are instead\ndynamically scoped: the variable is not looked up until runtime.\ndynamic scope can lead to really\nconfusing problems; the same piece\nof code can either work or not work\nbased on what variables are\nf()\n# runtime error!\ndeclared around it.\ng = 10 # g is a global\nf()\n# prints 10\n# Python\ndef f():\nprint(g)\n\nbut I think most dynamic languages do it like\nthis cause it's easy to implement!\n26\n\n\fShadowing\n\u25cf names aren't always unique.\n\u25cf sometimes this is clearly a mistake, but in other cases\u2026\nclass A {\nin this case we say that the local\nint x;\nvariable shadows the member variable.\nA(int x) {\n// which x?\nx = 5;\n} this refers to\u2026\n}\nx\nthat argument.\n\n27\n\n\fShadowing isn't always a mistake\n\u25cf sometimes it just makes things more convenient.\no if you write a piece of code, and then change the environment\naround it (like adding a global), you don't want it to suddenly\nchange behavior or get new compiler errors!\n\u25cf or in Rust, where variables are immutable by default, this is a\ncommon pattern:\nlet ast = parse(lex(input)?)?;\nlet types = typecheck(ast)?;\nlet ast = const_fold(ast, types)?;\nlet ast = inline_calls(ast, types)?;\ncodegen(ast, types)?;\n\u25cf subsequent declarations of ast shadow the previous ones.\n\n28\n\n\fEvaluation and Execution\n\n29\n\n\fEvaluation and Side Effects\n\u25cf evaluation means figuring out the value that an expression has.\no it might be trivial, like for literals: 10 evaluates to\u2026 10!\n\u25cf but it might require execution of an unknown amount of code\u2026\n\u25cf and in the process, that code may also produce side effects:\nchanges to things outside of the code's local variables, like:\no modifying global variables\no modifying objects that were passed to it by reference\no performing input or output\no making your computer explode\n\u25cf side effects make it hard for us to reason about code\u2026\no they can cause the same piece of code to do different things\ndepending on how many times you run it.\n\u25cf but without side effects, code can't do anything useful!\no imagine having no input or output!\n30\n\n\fEvaluation Order\n\u25cf here's some Java code:\nint f(int x) { println(\"got \" + x); return x; }\nint add(int a, int b) { return a + b; }\nprintln(\"3 + 5 = \" + add(f(3), f(5)));\n\u25cf which one does it print?\ngot 3\ngot 5\n3 + 5 = 8\n\ngot 5\ngot 3\n3 + 5 = 8\n\nwe have an expectation that code executes left-to-right, so\nwe expect this left one\u2026 and this is correct. for Java.\nevaluation order is defined by a language's semantics, and Java\nhappens to define this order, but not all languages agree!\n31\n\n\fEager vs. Lazy\n\u25cf a less common distinction is eager versus lazy evaluation.\nint x = someLongComputation();\nSystem.out.println(\"done.\");\nSystem.out.println(\"x = \" + x);\n\u25cf in the above Java code, someLongComputation() is called, and its\nreturn value is assigned into x after it returns.\no this is eager evaluation, in which the function call occurs at the\npoint where you write it.\n\u25cf but another way is lazy evaluation, in which the variable declaration\ndoesn't call the function, the use of x on the last line does.\no that might sound bizarre, but is actually very popular in functional\nlanguages, where side-effect-free functions are the norm!\n\u25cf eager evaluation is by far the more popular strategy today, but\u2026\n32\n\n\fYou never knew you were using it\n\u25cf in this code:\nif(condOne() && condTwo()) { println(\"woo\"); }\n\u25cf if condOne() returns false, does condTwo() ever run?\no nope.\n\u25cf this is lazy evaluation.\no in all C-style languages, && and || lazily evaluate their second\noperand: they only execute that code if needed.\n\u25cf lazy evaluation can be really useful when using higher-order\nfunctions (passing functions to other functions), too.\n\u25cf so don't write it off as academic nonsense!\n\n33\n\n\fLanguages without\nSemantic Analysis?\n\n34\n\n\fWho needs semantic analysis anyway\n\u25cf depending on how the language is designed, we may not have to\ndo any semantic analysis in the compiler.\ndef test():\nPython is a dynamically-typed language.\nx = 10\nx = \"hello\" that means the semantic rules of its type system\nare checked at runtime, as the code executes.\nx = print\nx(\"hi!\")\nthis is totally valid Python code, but there's no\ntest()\nway for the compiler to tell. so, it\u2026 doesn't.\ncompilers for dynamic languages can be pretty\nsimple: lex, parse, and then\u2026 well, right to execution!\n35\n\n\fExecuting the AST\n\u25cf we saw this already with the ast_math example!\no once you build the AST, you just write some recursive methods to\nvisit all the AST nodes in order, evaluating them as you go.\no it's not hard to extend this concept to AST nodes for loops,\nconditionals, function calls etc.\n\u25cf this is an AST interpreter, and is a really quick-and-dirty way to get\nsomething running.\no from what I understand, Ruby used exactly this execution strategy\nfor over 10 years!\n\u25cf but as you might imagine, it is not fast at all.\no consider all the interpreter code that needs to run just to, say, add\na couple numbers together.\n\u25cf but this topic is better left for after the midterm\u2026\n36\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}