{"id": 238, "segment": "unlabeled", "course": "cs1622", "lec": "lec12", "text": "Dynamic Memory\nManagement\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n\u25cf we're gonna talk about some stuff a bit abstractly today\no but I think it's a really important topic\no dynamic memory management has a H U G E impact on language\nand compiler design\no and it will also be important to know about this for when we start\ntalking about code generation and the runtime library!\n\n2\n\n\fAllocation, Deallocation,\nLifetime, and Ownership\n\n3\n\n\fMaking room\n\u25cf every value in your program takes up space in memory.\n\u25cf allocation sets aside a piece of memory as a value's \"home.\"\n\nlet g = 10;\nlet h = 20;\nfn main() {\ng = g + h;\n}\n\nglobal variables are the easiest to\nallocate, because the compiler knows\nhow many globals there are.\nit does static allocation: it gives each\nglobal a unique location, which is encoded\nin the output machine code.\n\nstatic allocation is also used for certain kinds of\nconstants, like \"quoted string literals\".\n4\n\n\fBut that's boring\n\u25cf what about locals? they \"appear\" and \"disappear\" with function calls.\n\u25cf that means we have to do dynamic allocation: we come up with the\nlocations of variables at runtime.\n\nfn r(x: int) {\nthis is easier than it sounds at\nif x > 1 {\nfirst, because every function has a\nfixed number of variables.\nreturn x + r(x-1);\n} else {\nreturn x;\nbecause of how function calls work, the\nvariables can be allocated using a stack.\n}\n}\nso, each local variable gets a statically-determined\nlocation within the stack frame, relative to the sp.\n5\n\n\fDeallocation\n\u25cf memory is a finite resource. we have to reuse it when we can.\n\u25cf deallocation marks a previously-used location as ready for reuse.\nstacks are natural recyclers.\n3\n\n3\n\n3\n\n3\n\n10\n\n10\n\n10\n\n10\n\nas we push values,\nthey are placed in\nnew locations.\n\n99\n\n99\n\n433\n\n4\n\n4\n\n4\n\n163\n\n163\n\n163\n\nwhen they're popped, the\nvalues are deallocated, but\nthe locations stick around\u2026 and can be easily reused\nby the next push.\n6\n\n\fLifetime and Ownership\n\u25cf a value's lifetime is the span of time from allocation to deallocation.\n\u25cf its owner is what decides when it's safe to deallocate it.\n\nlet g = 10;\nfn main() {\nlet x = g + 5;\nprintln_i(x);\n}\n\nglobal variables are owned by the\nprogram: they can't be deallocated\nuntil the program ends.\nlocal variables are owned by the\nenclosing function: they can be\ndeallocated when it returns.\n\n\u2026but this isn't the whole story, is it?\n7\n\n\fIndirection\ntime check < 20 min\n\n8\n\n\fIndirection, the concept\n\u25cf indirection means using a symbol to stand in for something, instead\nof using that thing directly.\no yes, this is really abstract, but it's an abstract concept!\nlinguistic symbols \u2013 spoken or written\n\u2013 all stand in for something else.\n\n\ud83d\udc49\nif I point at a cat, my\nfinger is the symbol\nwhich refers to the cat.\n\nif I want to refer to houses on my street, I\ncan refer to them by number.\n121\n\n123\n\n125\n\n127\n\n129\n\n9\n\n\fIt works for houses, and it works for objects\n\u25cf if I have a program which deals with very large objects (kB-MB?),\no I can put them in an array and access them by their index.\n\n64kB 64kB 64kB 64kB 64kB 64kB 64kB\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nthe number 3 is small enough to fit into a register.\nof course, memory itself is an array (of bytes), so we can refer\nto anything in memory by its index: its memory address.\n\n01\n\n00\n\n00\n\n80\n\nF8\n\nFF\n\nFF\n\n8000 8001 8002 8003 8004 8005 8006\n\n10\n\n\fPointers and references\n\u25cf programs manipulate values, like ints, strings, objects, etc.\n\u25cf a pointer is a value which is the memory address of another value.\no \"reference\" is another name for a pointer, with a \"safer\" connotation.\n\nlet x = 1; 07FF8FFC\nlet y = 5; 07FF8FF8\nlet r = &x; 07FF8FF4\n\n00000001\n\n00000005\n07FF8FFC\n\nwe say,\n\"r points to x.\" r is\nthe pointer, and x\nis its referent.\n\nwhen you run this code, the variables\nmight look something like this in memory.\n\nif I now write this:\n\nlet z = *r;\n\nthe asterisk follows the arrow from r to x,\nand loads the value stored there. so z will be\u2026?\n11\n\n\fDereferencing\n\u25cf dereferencing means \u201caccessing the referent of a reference.\u201d\n\u25cf it does not mean \u201cto deallocate a piece of memory.\u201d\n\nA a = new A();\na.x = 10;\na.method();\narr[i] = a;\n\nlet mut x = 1;\nlet r = &mut x;\nlet z = *r;\n*r = 10;\n\nin Java, the dot and square\nbracket operators perform\ndereferencing, to access\nvalues and methods within\nan object or array.\n\nRust has those, as well as the\nprefix asterisk operator, which\ncan get or set the value at the\nother end of the reference.\n\n12\n\n\fThe necessity of indirection\n\u25cf when you write a program, you have a fixed number of variables\u2026\no but the size of most data structures is dynamic.\no you don't know how many values there will be until runtime.\n\u25cf so, we have to dynamically allocate memory:\nint[] a = new int[100]; // gimme 100 new variables\n\u25cf but we won't know the array's address until runtime!\no so the only way to access it is indirectly.\n\u25cf the variable above is a pointer to an value on the heap, the area of\nmemory where dynamic allocation happens.\no from now on, I'll use the term object to refer to heap-allocated\nvalues, but not necessarily in the \"object-oriented\" sense.\n\n13\n\n\fThe power of the heap\n\u25cf the heap lets us create data structures of any practical size at runtime.\n\u25cf the lifetime of heap-allocated values (objects) is also dynamic.\n\nfn f(): Cat {\nreturn new Cat();\n}\nfn main() {\nlet c = f();\nc.meow();\n}\n\nan object can outlive the\nfunction which allocated it!\nbut this presents a problem:\nwhen does the lifetime of\nan object end?\n(who's the owner??)\n\n14\n\n\fHeap memory\nmanagement\ntime check < 40 min\n\n15\n\n\fMemory safety (slightly animated)\n\u25cf a program is memory-safe if, when you follow an arrow:\no there is exactly one possible value on the other end; and\no that value is guaranteed to be alive.\nfn main() {\nlet a = new A();\na.method();\na = new A();\na.method();\n}\n\na\n\nStack\n\nHeap\n\n00000000\n00804000\n00804018\n\ninstance of A\ninstance of A\n\nat all times, a is pointing to a live object,\nso calling a method on it is safe.\n\nfor memory safety to hold, as long as at least one arrow is\npointing to an object, it must not be deallocated.\nbut that sounds hard to prove\u2026\n16\n\n\fSo let's not prove it!\n\u25cf one way to ensure arrows always point to live objects is\u2026\no you never deallocate them!\n\u25cf what?? didn't we say memory was finite, and we have to reuse it?\no well, yeah, but not every program needs to reuse memory.\n\u25cf if you have a short-lived program which allocates memory, does its\nwork, and exits, there may be no need to deallocate at all.\no for example, a program that reads some text, parses it into a tree,\ndoes some processing on that tree, and spits out different text.\n\u25aa what could I be talking about? :^)\n\n\u25cf this idea can be applied at smaller scales within programs by using\narena allocation: tying heap allocation to the stack by deallocating a\ngroup of objects when a function returns.\n\u25cf buuuuut obviously this isn't a general solution.\n17\n\n\fManual memory management\n\u25cf since the programmer decides when object lifetimes begin\u2026\n\u25cf let's also have them decide when their lifetimes end!\no\n\nhahaha what could possibly go wrong?\n\nthis is not a solution to the problem.\nwe've just pushed it somewhere else.\n\n// C++\nvoid main() {\nauto a = new A();\na->method();\ndelete a;\neasy peasy!\na->method(); oh fu---}\n\nhumans suck at knowing when is the right time\nto deallocate an object. what makes it hard?\n18\n\n\fIt's those dang arrows (slightly animated)\n\u25cf lots of languages make it really, really easy to create arrows\u2026\no but they offer little or no help in ensuring that those arrows are\nvalid (that is, they point to a live object)\n// C\nint* p = malloc(40);\nint* q = p + 256;\np\n*q = 5000; // UB\nq\nfree(p);\nx\n*p = 10;\n// UB\nint x = 3;\np = &x;\nfree(p);\n// UB\n(UB = Undefined Behavior)\n\nHeap\n\nStack\n\n07FF8FF8\n00804000\n00804400\n\n40 bytes\n???????\n(10 ints)\n\n00000003\n\n???????\n\nit gets worse.\n19\n\n\fAliasing\n\u25cf this is when two or more arrows point to the same value.\n\u25cf it's very powerful: it lets you create any directed graph (digraph).\nStack\n\n1\n\n1\n\n1\n\nlists\u2026\n\n1\n\n1\n\ntrees\u2026\n\n1\n1\n\n2\n\n2\n\ndoubly-linked lists\u2026\n\n1\n3\n\n1\n\ntrees with\nparent links\u2026\n\n1\n2\n\n1\n1\n\ndirected acyclic\ngraphs (DAGs)\u2026\n\na node's in-degree is how many arrows point to it.\n\n20\n\n\fWITH GREAT POWER etc.\n\u25cf unrestricted pointer aliasing can cause a lot of problems.\n\np\nt1\n\ndelete p;\n\nt2\nq\n\ntwo threads of execution\nboth accessing the same\nobject at the same time\ncan cause race conditions,\ndeadlocks, and more.\n\nif the programmer can\ndeallocate objects, two pointers\npointing at the same thing can\nlead to dangling pointers.\n\nother sources of dangling/invalid pointers are\npointer arithmetic and pointers to stack values.\n21\n\n\fAutomatic heap\nmemory management\ntime check \u2264 70 min\n\n22\n\n\fHard for humans, easy for computers (animated)\n\u25cf at a high level, the problem is not that complicated.\nStack\n\nobjects and pointers form graphs.\n\nGlobals\n\nthe roots are the\nstack(s) and globals.\n\nreachable objects are pointed\nto \u2013 directly or transitively \u2013\nfrom the roots.\nany objects that become\nunreachable can never be used\nby the program again, and are\ntherefore safe to deallocate.\nthis gives us a simple* rule: an\nobject's lifetime ends when its\nin-degree reaches 0.\n\n23\n\n\fCounting the arrows by\u2026 counting them (animated)\n\u25cf reference counting (refcounting) explicitly tracks the number of\narrows pointing to an object at any given time.\nwhen an object is\nwhenever a reference to it is\nallocated, its refcount is 0.\ncreated, the count is incremented.\nAnt a = new Ant();\nAnt b = a;\na = new Ant();\nb = null;\na = null;\n\na\nb\n\nwhen a variable is reassigned, the\nprevious object's count is decremented.\n\nvtbl: \u2026\nrefs: 1\n0\n2\nspecies: \"ant\"\n\nvtbl: \u2026\nrefs: 0\n1\nspecies: \"ant\"\n\nwhen the count reaches 0, the object is deallocated.\n24\n\n\fThe compiler's job\n\u25cf the compiler knows when the references are created and destroyed.\n\u25cf so, around each assignment, it inserts those 'crements.\nAnt a = new Ant();\ntricky details to consider:\nincRef(a);\nwhen a local variable goes out of scope,\nAnt b = a;\nwe need to decRef() it.\nincRef(b);\ndecRef(a);\nreturning a reference or passing one as\na = new Ant();\nan argument will increment the refcount.\nincRef(a);\ndecRef(b);\nbefore an object is deallocated, any\nb = null;\nreferences it has to other objects must\ndecRef(a);\nbe recursively decremented first.\na = null;\n25\n\n\fSo what's the catch? (animated)\n\u25cf \"when an object's in-degree reaches 0\" may never happen.\n\n1\n2\n\nnow what????????\n\n1\n\nthese objects are unreachable and should\nbe deallocated. but they won't be.\n\nthis is a memory leak: these objects take\nup space that can never be deallocated.\nit's 2!\n\n2!!!\n\n1!!!\n\nt1\n\n:c\n\nit's also bad for multithreaded\nprograms, since the threads will\nfight over the refcount.\n\nit's 1!\n\nt2\n\nknowing the in-degree is necessary for automatic memory\nmanagement, but it isn't sufficient: it's not all you need to know.\n26\n\n\fTracing Garbage\nCollection\ntime check \u2264 90 min\n\n27\n\n\fThe idea (animated)\n\u25cf same ideas as before: roots, graph, reachability.\nStack\n\nGlobals\n\n\u2705\n\u2705\n\n\u2705\n\u2705\n\nyour program runs normally, with\nno reference counting. this is\ncalled the mutation phase.\nperiodically, your program is paused\nand the collection phase begins.\nthe collector starts at the roots, and\nfollows every arrow it can find, marking\neach object it encounters as live.\nwhen there are no more arrows to\nfollow, the remaining objects are\ngarbage and are swept away.\n\n28\n\n\fThe correctness\n\u25cf this algorithm is mark-and-sweep and is the basis of tracing GC.\n\u25cf it works because an object is not garbage when its in-degree is 0\u2026\no it's garbage when it is unreachable from the roots.\n\u25cf but wait, how does the collector get to the dead objects?\no simple: you are no longer the objects' owner. the collector is.\n\u25cf every time you allocate an object, the collector remembers it.\no the collector has a list of every object on the heap.\no in the mutation phase, it adds to this list when you new.\no the collection phase, it uses this list to find disconnected objects.\n\u25cf this works great for:\no cycles, cause the collector doesn't care about in-degree.\no multithreading, cause there are no ref counts to argue over, and\nthe collector considers all threads' stacks as roots.\n29\n\n\fConcessions we make\n\u25cf for the tracing algorithm to work well, there are some requirements.\nlet x = 0x08004030;\nlet a = new A();\nx 08004030\na 08004030\n\nwhat if this happened by\nchance? how can the collector\nknow a is a pointer but x is not?\n\nthe compiler must produce information for\nevery global, local, and class field saying\nwhether or not something is a pointer.\n\nint x = 0x08004030;\nint *p = (int*)x;\n\ntype safety is crucial. being able to\nfreely change the types of values will\nthrow the collector way off.\n\npointer arithmetic is a no-go. nuh uh. no way.\n30\n\n\fWhat's the downside?\n\u25cf well, the mark-and-sweep algorithm I showed is\u2026 too simple.\no it doesn't have great performance.\n\u25cf getting good performance means making it way more complex.\no concurrent collectors do some/most of their work during the\nmutation phase, making the collection phase shorter.\no multithreaded collectors extend that to multiple mutators.\no generational collectors take advantage of the fact that most objects\nare deallocated very quickly after they're allocated, and focus more\neffort on newer objects.\no copying collectors reduce the amount of memory needed by\nmoving objects around.\n\n31\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}