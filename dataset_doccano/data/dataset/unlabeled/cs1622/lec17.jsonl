{"id": 210, "segment": "unlabeled", "course": "cs1622", "lec": "lec17", "text": "Local Optimization\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n\u25cf buhhhhhhh??????\n\n2\n\n\fOptimization\n\n3\n\n\fWhat is it?\n\u25cf optimization is the name for any technique which:\no rewrites part of a program\u2026\no in order to improve resource utilization\u2026\no without changing its behavior.\n\u25cf \"resource utilization\" can mean many things, such as:\no time (how quickly the code executes)\no code size (how many bytes the code takes up)\no memory size (how many bytes the data/variables take up)\no power (how many watts the code uses when executed)\no registers (how many/what kinds of registers are used)\n\u25cf some of these goals are even contradictory!\no so compilers let you control which optimizations are performed.\n\n4\n\n\fWe do it all the time\n\u25cf many of the optimizations we'll discuss are just automated versions\nof things human programmers do.\nfunc1(some.long + code[i]);\nfunc2(some.long + code[i]);\n\nlet v = some.long + code[i];\nfunc1(v);\nfunc2(v);\n\nthis technique is called common subexpression elimination (CSE).\nlet x = 1024 * 4;\n\nlet x = 4096;\n\nthis is constant folding.\nlet x = pow(y, 2);\n\nlet x = y * y;\n\nthis is strength reduction: using a simpler, faster operation\nthat gives the same answer as a more complex one.\n5\n\n\fOptimization scope\n\u25cf we'll be doing optimizations on our IR, rather than the AST.\nfunc\n\nlocal optimizations\nwork on code within\na single BB.\n\nglobal optimizations\nwork on one function's\nwhole CFG. (I know, \"global\" sounds\n\nmain\n\ninterprocedural\noptimizations work\non all functions in\nthe program.\n\nlike it should be the whole program but it's not)\n\nas you might imagine, local is the simplest; global is more\ncomplex; and interprocedural is the most complex.\n\n6\n\n\fDead code elimination (DCE)\n\u25cf a common technique in Java is to use static final variables to\nenable or disable pieces of code. in those cases, the condition is a\nconstant, and one or more control paths become \u201cdead code.\u201d\nstatic final boolean DEBUG = false;\npublic int getWhatever() {\nif(DEBUG)\nSystem.err.println(\"here\");\n\npublic int getWhatever() {\nreturn this.whatever;\n}\n\nreturn this.whatever;\n}\n\nthis is a global optimization, as it operates on the CFG of the\nmethod. some BBs were removed from this method.\n7\n\n\fFunction (or method) inlining\n\u25cf a very common interprocedural optimization is function inlining:\neliminating a call by copying the code of the callee into the caller.\npublic void printWhatever() {\nint w = this.getWhatever();\nSystem.out.println(w);\n}\npublic int getWhatever() {\nreturn this.whatever;\n}\n\npublic void printWhatever() {\nint w = this.whatever;\nSystem.out.println(w);\n}\n\nthink about all the work this is saving\n\u2013 no more passing arguments, doing\nthe function prologue and epilogue,\nmessing with the stack\u2026\n\nbut knowing when it\u2019s okay to do this, and knowing whether doing\nthis will save time or waste time is really, really complicated.\n8\n\n\fCareful now\u2026\n\u25cf remember the \"without changing its behavior\" part of the definition?\nfunc1(problem(1));\nfunc2(problem(1));\n\nlet v = problem(1);\nfunc1(v);\nfunc2(v);\n\nwhat if problem were defined like this:\nfn problem(x: int): int {\nprint_s(\"problem: \");\nprintln_i(x);\nreturn x;\n}\n\nnow the original and \"optimized\"\nversions do different things!\n\nwhen performing optimizations, we must respect the\nsemantics and evaluation rules of the source language,\nand we cannot ignore side effects.\n9\n\n\fTo optimize, or not to optimize?\n\u25cf optimization might sound like a no-brainer. why not do it? well\u2026\nthey can slow down\ncompilation, a lot.\n\nthey can be difficult to\nimplement correctly.\n\nFinished [release] target(s) in 1859.3s\n\n$ ./hello\nHello, world!\n$ ./hello_optimized\nSegmentation fault\n\nthey may only give a\nsmall improvement.\n\nthey can make it harder to\ndebug a running program.\n\n$ ./bench\n18.93s\n$ ./bench_optimized\n18.11s\n\n$ gdb bench_optimized\n(gdb) b main\nFunction \"main\" not defined.\n(gdb) _\n\nor some combination of all of these!\n10\n\n\fPeephole Optimizations\n\n11\n\n\fStarting small\n\u25cf the simplest kinds of optimizations are peephole optimizations:\nthey work on the level of one or two instructions at a time.\n\u25cf despite their simplicity, they are the foundation of all the more\ncomplicated kinds of optimizations.\na = 5 == 5\n\na = true\n\ndoing peephole\noptimizations\u2026\n\na = true\nif a then bb1\nelse bb2\n\nif true then bb1\nelse bb2\n\n\u2026can open up\nopportunities for larger\nlocal optimizations\u2026\n\nif true then bb1\nelse bb2\n\ngoto bb1\n<delete bb2>\n\n\u2026which are the\nbasis for global\noptimizations.\n12\n\n\fOperations vs. Moves (assignments)\n\u25cf each instruction in our IR looks (basically) like one of these two:\nx = y + z\na = b\n\u25cf the first is an operation: it requires computation to complete.\n\u25cf the second is a move: copying a value from place to place.\no but, a lot of moves are unnecessary and could be eliminated.\n\u25cf furthermore, if we can somehow convert operations into moves\u2026\no we can eliminate entire steps of the program.\n\u25cf all the local optimizations basically follow these principles:\no make operations simpler than what was written;\no turn operations into moves if possible; and then\no eliminate the moves.\n\u25cf and then repeat!\n13\n\n\fConstant Folding\n\u25cf this one is easy: if you see arithmetic being done on constants,\nreplace the arithmetic with the result of the operation.\n\na = 640 * 480\n\na = 307200\n\nb = not true\n\nb = false\n\nc = \"hi\" + \"bye\"\n\nc = \"hibye\"\n\nthis turns each operation into a move. nice! but what about:\n\n$t1 = 640 * 480\nx\n= $t1 * 32\n\n$t1 = 307200\nx\n= $t1 * 32\n\nclearly there's more work to be done, but we'll come back to this.\n14\n\n\fDanger ahead\n\u25cf some mathematical operations can cause\u2026 problems.\n\na = 1000 / 0\n\nuh oh. don't try to simplify this\nor you'll crash the compiler!\n\na = 1000000000 * 5\n\nthat doesn't fit in a 32-bit int\u2026\n\nconfusingly, these may or may not be errors in the code, so we\nprobably shouldn't give an error here; just give up instead.\ndue to interactions with control flow analysis,\nthese may not even get executed at runtime.\n\nlast, unless we're explicitly looking for errors, we really should\njust let the code through. we can't catch every mistake. it\u2019s\nnever wrong to leave code the way the programmer wrote it.\n\n15\n\n\fAlgebraic Simplification\n\u25cf this uses algebraic laws to simplify or even eliminate operations.\n\na = b * 1\n\na = b\n\na = b + 0\n\na = b\n\na = b * 0\n\na = 0\n\na = a + 0\n\n(nothing!)\n\n$t1 = a == a\n\n$t1 = true\n\n$t1 = a < a\n\n$t1 = false\n16\n\n\fStrength Reduction\n\u25cf strength reduction might not reduce complexity, but it can improve\nperformance by using cheaper operations to do the same task.\n\na = b * 2\n\na = b + b\n\na = pow(b, 2)\n\na = b * b\n\na = b * 8\n\na = b << 3\n\na = b * 9\n\na = b << 3\na = a + b\n\na = b % 8\n\na = b & 7\n\n(many compilers \"know\" about\nstandard library math functions\nand can optimize them out.)\n\ntwo instructions may\nstill be faster than a\nsingle multiplication!\n\n17\n\n\fOptimization: not just for IR\n\u25cf peephole optimizations can be done on the target code too.\n\u25cf here are some Silly Sequences my code generator is producing:\naddi sp, sp, -4\naddi sp, sp, -4\n\nli s2, 5\nadd s1, s1, s2\n\nshows up in nested\ncalls like f(g(x))\n\nadding a constant to\na variable, x + 5\n\njal func\nmove s0, v0\nsw\ns0, -16(fp)\n\njal func\nmove s0, v0\nmove v0, s0\n\nassigning a return\nvalue to a variable\n\nreturning the value\nthat a call returned\n18\n\n\fMake it better, do it faster\n\u25cf a final pass after codegen can go through and replace these silly\nsequences with simpler equivalent sequences.\naddi sp, sp, -4\naddi sp, sp, -4\n\naddi sp, sp, -8\n\nli s2, 5\nadd s1, s1, s2\n\naddi s1, s1, 5\n\njal func\nmove s0, v0\nsw\ns0, -16(fp)\n\njal\nsw\n\nfunc\nv0, -16(fp)\n\njal func\nmove s0, v0\nmove v0, s0\n\njal\n\nfunc\n19\n\n\fSingle Static\nAssignment Form (SSA)\n\n20\n\n\fHitting a wall\n\u25cf let's apply all the optimizations we've seen to this bit of code.\nx = 10 * 16 constant folding\u2026\nprintln_i(x)\nstrength reduction\u2026\nx = x * 4\nprintln_i(x)\nx = x * 1\nalgebraic simplification\u2026\nprintln_i(x)\n$t0 = x\nkind of leaves something\nreturn\nto be desired, no?\n\nx = 160\nprintln_i(x)\nx = x << 2\nprintln_i(x)\n\nprintln_i(x)\n$t0 = x\nreturn\n\nthink about it: do we really even need this x variable?\nwell, to make things easier on ourselves, first we're going to\nconvert the code into a single assignment form.\n21\n\n\fI AM LEAVING OUT A LOT OF DETAILS HERE\n\u25cf single static assignment (SSA) form rewrites the code so that each\nlocation is only assigned once, and is never reassigned.\n\u25cf this does not \"optimize\" the code at all, but it does make several\nother optimizations much easier to perform.\nx = 10 * 16\nprintln_i(x)\nx = x * 4\nprintln_i(x)\nx = x * 1\nprintln_i(x)\n$t0 = x\nreturn\n\nSSA!\n\nx1 = 10 * 16\nprintln_i(x1)\nx2 = x 1 * 4\nprintln_i(x2)\nx3 = x 2 * 1\nprintln_i(x3)\n$t0 = x3\nreturn\n\neach time x is assigned,\nwe replace it with a new,\nunique variable.\n\nreferences to x are\nreplaced with the \"most\nrecent version\" of x.\n\n22\n\n\fNot a requirement\n\u25cf SSA is a super useful form for optimization buuuuuuuuuut\u2026\no it has to be done on the whole CFG, which is complicated.\no consider a for loop. how do you represent the counter variable in\nthis form at all?? (heheheheh hahah hohohoho \u0278)\n\u25cf then, once you're done optimizing, you have to convert back out of\nSSA to do the final codegen.\no so it's a bit of a mixed bag, depending on what kinds of\noptimizations you want to do.\n\u25cf it's not required for all optimizations, it just simplifies a lot of them.\no the other local optimizations we\u2019ll talk about can be implemented\nwith or without SSA, but the algorithms for detecting and applying\nthem are more complicated without it.\n\n23\n\n\fMore advanced\nlocal optimizations\n\n24\n\n\fCopy Propagation\n\u25cf if the code is in SSA form, we \"unlock\" this optimization:\n\u25cf if we see a move x = y, then we can replace all uses of x with y.\n\nx = a\nb = x + c\n\nx = a\nb = a + c\n\nx = 10\na = x + 5\n\nx = 10\na = 10 + 5\n\ntwo things to notice here:\n\nin the second example, we've produced something\nthat can be further optimized to a = 15!\nin both examples, x is no longer used anywhere.\nso what do we do with it?\n25\n\n\fDead store elimination\n\u25cf if a variable is assigned a value, and that value is never read, then\nthe assignment is a dead store and can be removed.\no \u2026as long as the assignment has no side effects!\n\u25cf so continuing with the improved code from last slide:\n\nbut:\n\nx = a\nb = a + c\n\nb = a + c\n\nx = 10\na = 10 + 5\n\na = 10 + 5\n\nx = f()\n$t0 = 5\nreturn\n\nwe can't remove the assignment to x,\nbecause f() may have side effects.\n26\n\n\fCommon subexpression elimination (CSE)\n\u25cf if the code is in SSA form\u2026\no and two variable assignments have the same rhs\u2026\no and the rhs has no side effects\u2026\n\u25cf then the second variable assignment can be changed to a move\nfrom the first variable.\n\nx = a + c\nprintln_i(x)\ny = a + c\nprintln_i(y)\n\nx = a + c\nprintln_i(x)\ny = x\nprintln_i(y)\n\nbecause we're using SSA, the variables on the RHS are\nguaranteed not to change value between the assignments.\n(if we weren't using SSA, we'd have to check that.)\n\n27\n\n\fTeamwork makes the dream work\n\u25cf each local optimization only does a little work.\n\u25cf but every time you run one, it can make it possible for another one\nto do a little more work\u2026\na copy propagation makes a\nconstant folding possible;\nwhich makes another copy\npropagation possible, followed\nby a dead store elimination;\n\naround and around until\nthe code is as \"simple\" as\nwe can make it.\n\n28\n\n\fHow the compiler does it\n\u25cf it just does a simple round-robin scheme:\ndo {\nconst_fold(bb);\nstrength_reduce(bb);\nalgebraic_simpl(bb);\ncopy_propagate(bb);\ndead_store_elim(bb);\n} while(the bb changed);\n\neach optimization is so simple and\nquick that you just\u2026 keep doing them\nuntil they don't do anything anymore.\nit is literally more work to \"check\nif the optimization is possible\"\nthan it is to just try to do it\n\nif this makes you feel uneasy (couldn't it get stuck in an\ninfinite loop??), I'm with you! but if it works, it works.\n(the compiler might stop after some n iterations anyway).\n\n29\n\n\fAn example\n\u25cf let's take that code from before and optimize it real good.\nx1 = 10 * 16\nprintln_i(x1)\nx2 = x1 * 4\nprintln_i(x2)\nx3 = x2 * 1\nprintln_i(x3)\n$t0 = x3\nreturn\n\nx1 = 160\nprintln_i(x1)\nx2 = x1 << 2\nprintln_i(x2)\nx3 = x2\nprintln_i(x3)\n$t0 = x3\nreturn\n\nx1 = 160\nprintln_i(160)\nx2 = 160 << 2\nprintln_i(x2)\nx3 = x2\nprintln_i(x2)\n$t0 = x2\nreturn\n\nprintln_i(160)\nx2 = 640\nprintln_i(x2)\nprintln_i(x2)\n$t0 = x2\nreturn\n\nprintln_i(160)\nx2 = 640\nprintln_i(640)\nprintln_i(640)\n$t0 = 640\nreturn\n\nprintln_i(160)\nprintln_i(640)\nprintln_i(640)\n$t0 = 640\nreturn\n\nprintln_i(160)\nx2 = 160 << 2\nprintln_i(x2)\nprintln_i(x2)\n$t0 = x2\nreturn\n\nwhew!\n\n30\n\n\fA bigger example of CSE\n\u25cf even larger common subexpressions can be eliminated.\nfunc_1(a.x + b.y - c);\nfunc_2(a.x + b.y - c);\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t3 = a.x + b.y\n$t4 = $t3 - c\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t3 = $t1\n$t4 = $t3 - c\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t3 = $t1\n$t4 = $t1 - c\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t4 = $t2\nfunc_2($t4)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t4 = $t2\nfunc_2($t2)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\nfunc_2($t2)\n\n$t1 = a.x + b.y\n$t2 = $t1 - c\nfunc_1($t2)\n$t4 = $t1 - c\nfunc_2($t4)\n\n31\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}