{"id": 226, "segment": "unlabeled", "course": "cs1622", "lec": "lec19", "text": "Liveness and\nRegister Allocation\nCS 1622\nJarrett Billingsley\n\n\fClass Announcements\n\u25cf uhhhhhh exam Wednesday! woo!\n\u25cf keep in mind this stuff from today, while interesting, is not likely to\nbe covered in-depth on the exam for obvious practical reasons\n\u25cf also I should acknowledge Stephen Chong of Harvard\u2019s CS\ndepartment as I would not have understood this stuff myself nearly\nas well without the slides from his compilers course\n\n2\n\n\fThe liveness algorithm\n\n3\n\n\fBackwards Analyses\n\u25cf it sounds silly, but some analyses make more sense to do backwards.\nout-state\nbefore-state\ninst1\ninst2\ninst3\ninst4\ninst5\nif x bb2 else bb3\n\nin-state\nafter-state\njoin(s1, s2)\n\na backwards analysis works exactly the same\nway, but in the opposite direction: state flows\nfrom the successors into the join function.\n\nthe terms \"in-state\" and \"out-state\" take on\ndifferent meanings, so it can get confusing\u2026\nso I'm going to use before and after\ninstead of in and out.\n\nin forward, state goes before \u2192 after;\nin backward, state goes after \u2192 before.\n4\n\n\fStates and transfer function\n\u25cf let's just focus on one variable x for now.\n\u25cf x can be in one of two states: DEAD or LIVE.\no DEAD is also the initial value, so the whole CFG is filled with it.\no DEAD is also pulling triple duty as the \"unvisited/unknown\" value.\n\u25cf the transfer function works like this:\nx = LIVE\nprintln_i(x)\n\nx = DEAD\nx = 5\n\nif an instruction\nuses x, then x is\nLIVE before it.\n\nelse, if an instruction\ndefs x, then x is\nDEAD before it.\n\nx = DEAD\ny = 5\nx = DEAD\n\nx = LIVE\ny = 5\nx = LIVE\n\nelse, before = after\n(just copy the state).\n\n(in x = x + 1, the first rule takes\nprecedence, so x is LIVE before it.)\n5\n\n\fThe join function\n\u25cf the join function is pretty simple.\nif x is LIVE at the beginning\nof any successor, then x is\nLIVE at the end of this block.\nLIVE\nDEAD\ns1\n\nelse, x is DEAD at the\nend of this block.\n\nDEAD\nLIVE\ns2\n\nDEAD\ns3\n\nDEAD\ns1\n\nDEAD\ns2\n\nDEAD\ns3\n\n(yeah, in our IR, BBs can have at most 2 successors,\nbut this rule works for any number of successors.)\n6\n\n\fTrying it out (animated)\n\u25cf let's try it on this simple, one-BB function.\nwe initialize it by setting x\nto DEAD everywhere.\n\nthen we run the transfer\nfunction on each instruction.\n\nx\nDEAD\nDEAD\nLIVE\nDEAD\nLIVE\nDEAD\nDEAD\n\nx = arg + 10\n\ndefs x\u2026\n\nprintln_i(x) uses x\u2026\n$t0 = x\n\nuses x\u2026\n\nreturn doesn't touch x\u2026\n\nand there we go!\nnotice how on the last\nstep (the first instruction),\nx was marked dead before\nit, so we didn't have to\nchange that state.\nand? why's that matter?\n7\n\n\fIt matters for termination\n\u25cf we have a finite number of states (2)\u2026\n\u25cf and the states only change monotonically:\no every location starts as DEAD.\no a location's DEAD can become LIVE, but not the other way around.\no \"changing\" DEAD to DEAD like in that last step is not breaking the\nrule, because you're not changing anything!\n\u25cf so we've satisfied the conditions for termination!\no let's try it on some more complex functions to convince ourselves.\n\n8\n\n\fOne with a diamond (animated)\n\u25cf a diamond shape will force us to visit some nodes twice.\njust a def here,\nno change.\n\nDEAD\nx = 10\n\nDEAD\nLIVE\nif arg bb1 else bb2\n\njoin DEAD with\nDEAD, get DEAD.\n\njoin DEAD with\nLIVE, get LIVE!\n\nDEAD\nLIVE\nLIVE\nDEAD\n\nDEAD\n\nprint_s(\"x = \")\n\nprintln_s(\"argh!\")\n\nLIVE\nDEAD\n\nDEAD\n\nprintln_i(x)\n\nooh, a use!\n\nDEAD\n\nlet's go left..\n\nwe start at the\nreturn node.\n\nDEAD\n\nwait, we have\nthis other path.\n\nreturn\n\nDEAD\n\n9\n\n\fOne with a loop (animated)\n\u25cf this should be interesting\u2026\nprintln_s(\"start!\")\n\nDEAD x = 0\nLIVE i = 0\n\nLIVE\nDEAD\nDEAD $t1 = i < 10\nLIVE\nif $t1 bb2 else bb3\n\nfollow the\nloop back\u2026\n\nLIVE\nprintln_i(x)\ni = i + 1\n\noh that's\ninteresting!\n\nthe liveness changes in\nthe middle of this BB.\n\njoin DEAD\nwith DEAD,\nthings\nare different\nthe\ngettime\nDEAD.\nsecond\naround!\n\nDEAD\n$t0 = 5\nreturn\n\nx isn't mentioned, so it's\ndead before this BB.\n10\n\n\fWait, is that right\u2026?\n\u25cf these might look wrong at first, but the liveness algorithm is telling\nyou something important about x.\nfn f(a: int) {\nlet x = 10;\nlet x = whatever();\n\nx = 20;\nprintln_i(a);\nprintln_i(x);\n}\n\nx is always dead??\nyeah, the code never used x.\nmaybe the compiler would\nreport a warning/error here.\n\nx is only live for one instruction??\nthe code never uses the first\nvalue assigned to x. maybe a\nwarning/error again!\n11\n\n\fSome implementation thoughts\n\u25cf there are only two states.\no so we could store the state of one variable with one bit. efficient!\n\u25cf therefore, to track n variables' states, we'd need n bits per location.\no e.g. if we have 3 variables x, y, z\u2026\n\u25aa then 000 would mean all 3 are dead\n\u25aa and 010 would mean y is live and x/z are dead.\n\u25cf this places liveness into a special class of analysis problems called bit\nvector problems, and you can guess why they're called that.\no bitwise operations are fast, and bits are tiny!\no think about the join function: if any of the successors are live (1)\u2026\n\u25aa that means just ORing together all the successors.\n\u25cf okay. enough about liveness. how do we allocate registers?\n\n12\n\n\fRegister Allocation\ntime check \u2264 27\n\n13\n\n\fState of the art circa 1980\n\u25cf currently, our compiler places all locals (including args) in memory.\no so, accessing them requires loads and stores.\nx = y + z;\n\nlw s0, -12(fp)\nlw s1, -16(fp)\nadd s0, s0, s1\nsw s0, -20(fp)\n\nthis is definitely correct (our main goal),\nbut it has serious performance problems.\nin, say, 1980, the memory in your\ncomputer was faster than the CPU.\nso loads, stores, and adds all took the\nsame amount of time.\n\nthat is absolutely not the case anymore.\n\nin the absolute worst case, a load or store can\ntake around 100 times longer than an add.\n14\n\n\fNot all doom and gloom\n\u25cf due to Reasons You Learn In 1541\u2122, that's just the worst case\nperformance. some memory accesses are as fast as an add!\n\u25cf but memory accesses still cause issues for the CPU's pipeline.\n\u25cf and it's still, like, more instructions. cause what if our variables lived in\nregisters instead?\nx = y + z;\n\nadd s0, s1, s2\n\nthis is one of the reasons RISC ISAs like\nMIPS have so many registers to begin with:\nso you can use them for stuff like this.\n\nmore values in registers\nmeans fewer in memory\nmeans fewer loads/stores\nmeans higher performance.\n15\n\n\fReal ABIs use registers\n\u25cf as you learned in 447, the real MIPS ABI has four main kinds:\no a registers, for passing arguments (which we aren't using)\no v registers, for returning values\no t registers, for temporaries which do not cross jals\no s registers, for temporaries which do cross jals\n\u25cf our compiler isn't even using a or t registers! how wasteful.\n\u25cf register allocation algorithms allow us to set up constraints to say\n\"hey, this value must go in a0\" or similar, so that we can generate\ncode that follows these real ABIs.\n\n16\n\n\fUnder pressure\n\u25cf the core idea of register allocation is simple:\no a function has v variables, and the CPU has r registers.\no if v \u2264 r, it's easy: assign each variable to a register. done!\no if v > r, put (v - r) variables in memory, and the rest in registers.\n\u25cf but this is too simplistic, even for simple functions.\no our IR creates a lot of temporary variables.\no this increases register pressure: the number of values we \"want\"\nto keep in registers at any given time.\n\u25cf some optimization passes also increase register pressure!\no e.g. function inlining can really blow up the number of locals.\n\u25cf so, we need something that tells us which variables are \"in use\" at\nany given time, and assign registers based on that.\no\u2026\no \u2026oh right, that's liveness :)\n17\n\n\fThe Register\nInterference Graph\ntime check \u2264 40\n\n18\n\n\fOh boy, more graphs! Whee!\n\u25cf the Register Interference Graph (RIG) is the data structure used for\nregister allocation. it is an undirected graph with:\no one node for each local in the function; and\no an edge between any two locals which are live at the same time.\n\u25cf here is a small function and its RIG.\nfn riggy(a: int): int {\nlet x = a + 10;\nprintln_i(x);\nlet y = \"---\";\na\nprintln_s(y);\nreturn a;\n}\nx\n\nthese two edges indicate that a\nand x are live at the same time,\nand so are a and y, but x and y\nare not live at the same time.\ny\n\nclearly, \"live at the same\ntime\" is derived directly from\nthe liveness we computed!\n19\n\n\fBuilding the RIG (animated)\n\u25cf after you've computed liveness, it's really straightforward.\n\u25cf you start with a graph with no edges.\nat every point in the function, if two locals are\nlive at that point, add an edge between them.\nn\n\nobj\n\ni\n\nobj\n\ni\n\nval\n\nfound\nn\nval\nl\n\nl\n\nfound\n\nfn has(l: List, val: int): bool {\nlet n = l.length();\nlet found = false;\nfor i in 0, n {\nlet obj = l.get(i);\nif obj.value() == val {\nfound = true;\n}\n}\nreturn found;\n}\n20\n\n\fOk but what's it mean and why is it a graph\n\u25cf it's the register interference graph: an edge between two nodes\nmeans that those two locals cannot go in the same register!\no but the absence of an edge means they can go in the same register.\n\u25cf as for \"why a graph,\" well, lots of things reduce to graph algorithms.\nl\nval\n\nn\n\nobj\n\ni\nfound\n\nthis is graph coloring: we assign each\nnode a color such that no two nodes of\nthe same color share an edge (touch).\nthe \"color\" is just an arbitrary label or set:\nfor our purposes, \"color\" will mean the\nregister used to store the variable.\n\nhere, n and obj are the same color,\nso they can use the same register!\n21\n\n\fGraph Coloring\ntime check \u2264 56\n\n22\n\n\fColorability and chromatic number\n\u25cf because our \"colors\" represent the registers on the target CPU\u2026\no we have a fixed number of colors to use.\n\u25cf we say a graph is k-colorable if it can be colored with k colors.\na\n\nx\n\na\n\ny\n\nx\n\na\n\ny\n\nwe could color this\ngraph with 3 colors\u2026\n\nx\n\ny\n\nor just 2. but 2 is\nthe minimum.\n\nthe chromatic number of a graph is the\nminimum number of colors needed to color it.\nif we have r registers, we'd really like the\nchromatic number of the RIG to be \u2264 r!\n\n23\n\n\fYou didn't think it would be THAT easy, did you\n\u25cf unfortunately, determining k-colorability is NP-complete.\no (and determining the chromatic number is NP-hard!)\n\n\u25cf so, what do we do?\no sometimes, an imperfect solution is good enough.\n\u25cf we're going to use a heuristic to figure out a coloring order that is\nlikely to lead to a successful coloring.\no and if that fails, we can tweak the graph a bit and try it again.\n\u25cf this leads to an iterative solution that is near-linear for most\ncommon cases.\no yeah we might not find the perfect register allocation, but\nperfection is the enemy of good.\n\n24\n\n\fThe coloring heuristic (animated)\n\u25cf let's say r = 5 (where r = number of CPU registers).\n\u25cf if the RIG has a node with < r neighbors, and you remove that node\u2026\no and the resulting graph is r-colorable\u2026\no then the original graph is r-colorable.\nStack\nwe'll repeatedly remove\nl\nnodes with < 5 neighbors\n(edges), and put them in\nval\nn\nthis stack as we do so.\n\nobj\n\ni\nfound\n25\n\n\fPop'n'color (animated)\n\u25cf now we pop the nodes off the stack, and as we do so, color them\naccording to what their neighbors are.\nStack\ni\n\nn\n\nval\n\nl has red and green neighbors,\nso blue it is.\nl\nn has no colored neighbors,\nso let's make it red too.\nn\n\nfound\n\nval needs to be a fourth color\u2026\n\nl\n\nand obj needs to be a fifth.\n\nval\n\nobj\n\nobj\n\ni\nfound\n\ni has no colored neighbors,\nso let's make it red.\n\nfound has two red neighbors,\nso let's make it green.\n\n26\n\n\fThis works??\n\u25cf it's a little mind-blowing, but it does work, because that heuristic\nactually goes both ways: it's an \"if and only if.\"\no because we were able to 5-color each sub-graph of the RIG as we\nbuilt it back up by popping\u2026\no then the whole RIG was 5-colorable.\n\u25cf BUT: it doesn't always work. if the RIG is not actually r-colorable, we\nwill run into one of two things:\no no nodes with < r neighbors on the removal phase; or:\no impossible-to-color nodes in the popping phase!\n\n27\n\n\fFailure to color\n\u25cf let's see what happens on the same RIG if r = 4 instead.\no we have to remove nodes with < 4 neighbors now.\nStack\nl\n\nfound\nobj\n\nval\n\naaaand we're stuck.\nwe can't color l!\nn\n\nuh oh. everyone has \u2265 4\nneighbors. let's just remove\none and move on anyway.\n\nval\ni\nl\nn\n\nobj\n\ni\nfound\n28\n\n\fSpilling\ntime check \u2264 85\n\n29\n\n\fSo what do we do??\n\u25cf a failure to color the RIG means that there are more values being\nused at one time than will fit into the CPU's registers.\no it's a setback, but it's not catastrophic, right? we can use memory.\n\u25cf so in this case we turn to spilling: picking one or more locals to live\non the stack instead of in registers.\n\u25cf ideally, we\u2019d like to pick some local that isn\u2019t used very often to be\nplaced into memory instead of a register\u2026 but which one?\no if we pick poorly, we could slow down our program a lot by\ncausing a bunch of excess loads and stores\u2026\n\n30\n\n\fStop interfering with me!\n\nobj\n\ni\n\nfound\nn\nval\nl\n\n\u25cf picking which local to spill is, again, something we can\u2019t do perfectly.\n\u25cf so we can again use heuristics to pick a likely candidate. maybe some\nvariable that interferes with a lot of others but isn\u2019t used much\u2026\nlocals with long live ranges\n\nfn has(l: List, val: int): bool {\ntend to interfere a lot.\nlet n = l.length();\nlet found = false;\nwe also have to consider\nfor i in 0, n {\nthings like loops \u2013 how many\nlet obj = l.get(i);\naccesses to the local will\nif obj.value() == val {\nfound = true;\nhappen because of the loop?\n}\ncan we estimate that?\n}\nreturn found;\nfound seems like a good\n}\n\nspilling candidate here.\n\n31\n\n\fSo how does it work?\n\u25cf if we get stuck when removing nodes from the RIG\u2026\no we pick a local to maybe spill. but we don\u2019t spill it yet.\no we push that node on the stack, along with a note that says,\n\u201cmaybe spill this?\u201d\n\u25cf then, when we pop-and-color\u2026\no if we get to a maybe-spill entry on the stack...\no AND that node cannot be colored\u2026\no then we know that we have to spill it, and we rewrite the code!\n\u25cf why do we wait to rewrite until the pop-and-color phase?\no because we\u2019re just using heuristics here. it\u2019s entirely possible that\nthe graph is colorable when we remove a node with \u2265 r neighbors!\nso, we have to be conservative.\n\n32\n\n\fHow the code is rewritten\n\u25cf in our IR, we might add new instructions to represent loads/stores.\n\u25cf then, each use and def of the spilled variable is replaced with loads\nand stores to the stack.\nfound = false\n\nfound_1 = false\nStore(fp-12, found_1)\n\nfound = true\n\nfound_2 = true\nStore(fp-12, found_2)\n\n$t0 = found\n\nfound_3 = Load(fp-12)\n$t0 = found_3\n\nwe also rename the spilled local at each use location with a\nunique name. each of these new locals has a very short live range!\n33\n\n\fThen what?\n\u25cf then\u2026 we start all over again! compute liveness, build the RIG\u2026\n\u25cf this is the iterative register allocation algorithm in a nutshell:\nRewrite\nspill\nLiveness\nand RIG\n\nRemove\nnodes\n\nPop and\ncolor\n\nDone!\n\nPick spill\ncandidate\n\n34\n\n\fAnd it keeps going\u2026\n\u25cf this is just the basic shape of the algorithm.\n\u25cf there are several more features, such as:\no coalescing pairs of nodes that are likely to end up in the same\nregister anyway\n\u25aa e.g. in a = b where their liveness doesn\u2019t overlap\no pre-coloring some nodes to indicate they must be in certain\nregisters. this is used for:\n\u25aa argument and return value registers (MIPS \u201ca\u201d and \u201cv\u201d registers)\n\u25aa saved temporary registers (MIPS \u201cs\u201d registers)\no and more\u2026\n\n35\n\n\f", "label": [[-2, -1, "Concept"]], "Comments": []}