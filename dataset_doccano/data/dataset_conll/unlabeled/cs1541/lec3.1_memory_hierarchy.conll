unlabeled|cs1541|lec3.1_memory_hierarchy
-DOCSTART- -X- -X- O

Memory _ _ O
Hierarchy _ _ O
CS _ _ O
1541 _ _ O
Wonsun _ _ O
Ahn _ _ O

Using _ _ O
the _ _ O
PMU _ _ O
to _ _ O
Understand _ _ O
Performance _ _ O
2 _ _ O

Experiment _ _ O
on _ _ O
kernighan.cs.pitt.edu _ _ O
● _ _ O
The _ _ O
source _ _ O
code _ _ O
for _ _ O
the _ _ O
experiments _ _ O
are _ _ O
available _ _ O
at _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
github.com _ _ O
/ _ _ O
wonsunahn _ _ O
/ _ _ O
CS1541_Spring2022 _ _ O
/ _ _ O
tree _ _ O
/ _ _ O
main _ _ O
/ _ _ O
res _ _ O
ources _ _ O
/ _ _ O
cache_experiments _ _ O
● _ _ O
Or _ _ O
on _ _ O
the _ _ O
following _ _ O
directory _ _ O
at _ _ O
linux.cs.pitt.edu _ _ O
: _ _ O
/afs _ _ O
/ _ _ O
cs.pitt.edu _ _ O
/ _ _ O
courses _ _ O
/ _ _ O
1541 _ _ O
/ _ _ O
cache_experiments _ _ O
/ _ _ O
● _ _ O
You _ _ O
can _ _ O
run _ _ O
the _ _ O
experiments _ _ O
by _ _ O
doing _ _ O
‘ _ _ O
make _ _ O
’ _ _ O
at _ _ O
the _ _ O
root _ _ O
o _ _ O
It _ _ O
will _ _ O
take _ _ O
a _ _ O
few _ _ O
minutes _ _ O
to _ _ O
run _ _ O
all _ _ O
the _ _ O
experiments _ _ O
o _ _ O
In _ _ O
the _ _ O
end _ _ O
, _ _ O
you _ _ O
get _ _ O
two _ _ O
plots _ _ O
: _ _ O
IPC.pdf _ _ O
and _ _ O
MemStalls.pdf _ _ O
3 _ _ O

Four _ _ O
benchmarks _ _ O
● _ _ O
linked-list.c _ _ O
o _ _ O
Traverses _ _ O
a _ _ O
linked _ _ O
list _ _ O
from _ _ O
beginning _ _ O
to _ _ O
end _ _ O
over _ _ O
and _ _ O
over _ _ O
again _ _ O
o _ _ O
Each _ _ O
node _ _ O
has _ _ O
120 _ _ O
bytes _ _ O
of _ _ O
data _ _ O
● _ _ O
array.c _ _ O
o _ _ O
Traverses _ _ O
an _ _ O
array _ _ O
from _ _ O
beginning _ _ O
to _ _ O
end _ _ O
over _ _ O
and _ _ O
over _ _ O
again _ _ O
o _ _ O
Each _ _ O
element _ _ O
has _ _ O
120 _ _ O
bytes _ _ O
of _ _ O
data _ _ O
● _ _ O
linked-list_nodata.c _ _ O
o _ _ O
Same _ _ O
as _ _ O
linked-list _ _ O
but _ _ O
nodes _ _ O
have _ _ O
no _ _ O
data _ _ O
inside _ _ O
them _ _ O
● _ _ O
array_nodata.c _ _ O
o _ _ O
Same _ _ O
as _ _ O
array _ _ O
but _ _ O
elements _ _ O
have _ _ O
no _ _ O
data _ _ O
inside _ _ O
them _ _ O
4 _ _ O

Code _ _ O
for _ _ O
linked-list.c _ _ O
/ _ _ O
/ _ _ O
Define _ _ O
a _ _ O
linked _ _ O
list _ _ O
node _ _ O
type _ _ O
with _ _ O
data _ _ O
typedef _ _ O
struct _ _ O
node _ _ O
{ _ _ O
struct _ _ O
node _ _ O
* _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
8 _ _ O
bytes _ _ O
int _ _ O
data _ _ O
[ _ _ O
30 _ _ O
] _ _ O
; _ _ O
/ _ _ O
/ _ _ O
120 _ _ O
bytes _ _ O
} _ _ O
node_t _ _ O
; _ _ O
… _ _ O
/ _ _ O
/ _ _ O
Create _ _ O
a _ _ O
linked _ _ O
list _ _ O
of _ _ O
length _ _ O
items _ _ O
void _ _ O
* _ _ O
create _ _ O
( _ _ O
void _ _ O
* _ _ O
unused _ _ O
) _ _ O
{ _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
items _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
node_t _ _ O
* _ _ O
n _ _ O
= _ _ O
( _ _ O
node_t* _ _ O
) _ _ O
malloc _ _ O
( _ _ O
sizeof _ _ O
( _ _ O
node_t _ _ O
) _ _ O
) _ _ O
; _ _ O
if _ _ O
( _ _ O
last _ _ O
= _ _ O
= _ _ O
NULL _ _ O
) _ _ O
{ _ _ O
/ _ _ O
/ _ _ O
Is _ _ O
the _ _ O
list _ _ O
empty _ _ O
? _ _ O
If _ _ O
so _ _ O
, _ _ O
the _ _ O
new _ _ O
node _ _ O
is _ _ O
the _ _ O
head _ _ O
and _ _ O
tail _ _ O
head _ _ O
= _ _ O
n _ _ O
; _ _ O
last _ _ O
= _ _ O
n _ _ O
; _ _ O
} _ _ O
else _ _ O
{ _ _ O
last- _ _ O
> _ _ O
next _ _ O
= _ _ O
n _ _ O
; _ _ O
last _ _ O
= _ _ O
n _ _ O
; _ _ O
} _ _ O
} _ _ O
} _ _ O
5 _ _ O

Code _ _ O
for _ _ O
linked-list.c _ _ O
# _ _ O
define _ _ O
ACCESSES _ _ O
1000000000 _ _ O
/ _ _ O
/ _ _ O
MEASUREMENT _ _ O
BEGIN _ _ O
/ _ _ O
/ _ _ O
Traverse _ _ O
list _ _ O
over _ _ O
and _ _ O
over _ _ O
until _ _ O
we _ _ O
’ve _ _ O
visited _ _ O
` _ _ O
ACCESSES _ _ O
` _ _ O
nodes _ _ O
node_t _ _ O
* _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
ACCESSES _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
if _ _ O
( _ _ O
current _ _ O
= _ _ O
= _ _ O
NULL _ _ O
) _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
/ _ _ O
/ _ _ O
reached _ _ O
the _ _ O
end _ _ O
else _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
next _ _ O
node _ _ O
} _ _ O
/ _ _ O
/ _ _ O
MEASUREMENT _ _ O
END _ _ O
● _ _ O
Note _ _ O
: _ _ O
executed _ _ O
instructions _ _ O
are _ _ O
equivalent _ _ O
regardless _ _ O
of _ _ O
list _ _ O
length _ _ O
● _ _ O
So _ _ O
we _ _ O
expect _ _ O
performance _ _ O
to _ _ O
be _ _ O
same _ _ O
regardless _ _ O
of _ _ O
length _ _ O
. _ _ O
Is _ _ O
it _ _ O
? _ _ O
6 _ _ O

Code _ _ O
for _ _ O
array.c _ _ O
/ _ _ O
/ _ _ O
Define _ _ O
a _ _ O
linked _ _ O
list _ _ O
node _ _ O
type _ _ O
with _ _ O
data _ _ O
typedef _ _ O
struct _ _ O
node _ _ O
{ _ _ O
struct _ _ O
node _ _ O
* _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
8 _ _ O
bytes _ _ O
int _ _ O
data _ _ O
[ _ _ O
30 _ _ O
] _ _ O
; _ _ O
/ _ _ O
/ _ _ O
120 _ _ O
bytes _ _ O
} _ _ O
node_t _ _ O
; _ _ O
… _ _ O
/ _ _ O
/ _ _ O
Create _ _ O
a _ _ O
linked _ _ O
list _ _ O
but _ _ O
allocate _ _ O
nodes _ _ O
in _ _ O
an _ _ O
array _ _ O
void _ _ O
* _ _ O
create _ _ O
( _ _ O
void _ _ O
* _ _ O
unused _ _ O
) _ _ O
{ _ _ O
head _ _ O
= _ _ O
( _ _ O
node_t _ _ O
* _ _ O
) _ _ O
malloc _ _ O
( _ _ O
sizeof _ _ O
( _ _ O
node_t _ _ O
) _ _ O
* _ _ O
items _ _ O
) _ _ O
; _ _ O
last _ _ O
= _ _ O
head _ _ O
+ _ _ O
items _ _ O
- _ _ O
1 _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
items _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
node_t _ _ O
* _ _ O
n _ _ O
= _ _ O
& _ _ O
head _ _ O
[ _ _ O
items _ _ O
] _ _ O
; _ _ O
n- _ _ O
> _ _ O
next _ _ O
= _ _ O
& _ _ O
head _ _ O
[ _ _ O
items+1 _ _ O
] _ _ O
; _ _ O
/ _ _ O
/ _ _ O
Next _ _ O
node _ _ O
is _ _ O
next _ _ O
element _ _ O
in _ _ O
array _ _ O
} _ _ O
last- _ _ O
> _ _ O
next _ _ O
= _ _ O
NULL _ _ O
; _ _ O
} _ _ O
7 _ _ O

Code _ _ O
for _ _ O
array.c _ _ O
# _ _ O
define _ _ O
ACCESSES _ _ O
1000000000 _ _ O
/ _ _ O
/ _ _ O
MEASUREMENT _ _ O
BEGIN _ _ O
/ _ _ O
/ _ _ O
Traverse _ _ O
list _ _ O
over _ _ O
and _ _ O
over _ _ O
until _ _ O
we _ _ O
’ve _ _ O
visited _ _ O
` _ _ O
ACCESSES _ _ O
` _ _ O
nodes _ _ O
node_t _ _ O
* _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
ACCESSES _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
if _ _ O
( _ _ O
current _ _ O
= _ _ O
= _ _ O
NULL _ _ O
) _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
/ _ _ O
/ _ _ O
reached _ _ O
the _ _ O
end _ _ O
else _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
next _ _ O
node _ _ O
} _ _ O
/ _ _ O
/ _ _ O
MEASUREMENT _ _ O
END _ _ O
● _ _ O
Note _ _ O
: _ _ O
same _ _ O
exact _ _ O
loop _ _ O
as _ _ O
the _ _ O
linked-list.c _ _ O
loop _ _ O
. _ _ O
● _ _ O
So _ _ O
we _ _ O
expect _ _ O
performance _ _ O
to _ _ O
be _ _ O
exactly _ _ O
the _ _ O
same _ _ O
. _ _ O
Is _ _ O
it _ _ O
? _ _ O
8 _ _ O

kernighan.cs.pitt.edu _ _ O
specs _ _ O
● _ _ O
Two _ _ O
CPU _ _ O
sockets _ _ O
. _ _ O
Each _ _ O
CPU _ _ O
: _ _ O
o _ _ O
Intel _ _ O
( _ _ O
R _ _ O
) _ _ O
Xeon _ _ O
( _ _ O
R _ _ O
) _ _ O
CPU _ _ O
E5 _ _ O
- _ _ O
2640 _ _ O
v4 _ _ O
o _ _ O
10 _ _ O
cores _ _ O
, _ _ O
with _ _ O
2 _ _ O
threads _ _ O
per _ _ O
each _ _ O
core _ _ O
( _ _ O
SMT _ _ O
) _ _ O
o _ _ O
L1 _ _ O
i-cache _ _ O
: _ _ O
32 _ _ O
KB _ _ O
8-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
per _ _ O
core _ _ O
) _ _ O
o _ _ O
L1 _ _ O
d-cache _ _ O
: _ _ O
32 _ _ O
KB _ _ O
8-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
per _ _ O
core _ _ O
) _ _ O
o _ _ O
L2 _ _ O
cache _ _ O
: _ _ O
256 _ _ O
KB _ _ O
8-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
per _ _ O
core _ _ O
) _ _ O
o _ _ O
L3 _ _ O
cache _ _ O
: _ _ O
25 _ _ O
MB _ _ O
20-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
shared _ _ O
) _ _ O
● _ _ O
Memory _ _ O
o _ _ O
128 _ _ O
GB _ _ O
DRAM _ _ O
● _ _ O
Information _ _ O
obtained _ _ O
from _ _ O
o _ _ O
“ _ _ O
cat _ _ O
/proc _ _ O
/ _ _ O
cpuinfo _ _ O
” _ _ O
on _ _ O
Linux _ _ O
server _ _ O
o _ _ O
“ _ _ O
cat _ _ O
/proc _ _ O
/ _ _ O
meminfo _ _ O
” _ _ O
on _ _ O
Linux _ _ O
server _ _ O
o _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
en.wikichip.org _ _ O
/ _ _ O
wiki _ _ O
/ _ _ O
intel _ _ O
/ _ _ O
xeon_e5 _ _ O
/ _ _ O
e5 _ _ O
- _ _ O
2640_v4 _ _ O
9 _ _ O

Experimental _ _ O
data _ _ O
collection _ _ O
● _ _ O
Collected _ _ O
using _ _ O
CPU _ _ O
Performance _ _ O
Monitoring _ _ O
Unit _ _ O
( _ _ O
PMU _ _ O
) _ _ O
o _ _ O
PMU _ _ O
provides _ _ O
performance _ _ O
counters _ _ O
for _ _ O
a _ _ O
lot _ _ O
of _ _ O
things _ _ O
o _ _ O
Cycles _ _ O
, _ _ O
instructions _ _ O
, _ _ O
various _ _ O
types _ _ O
of _ _ O
stalls _ _ O
, _ _ O
branch _ _ O
mispredictions _ _ O
, _ _ O
cache _ _ O
misses _ _ O
, _ _ O
bandwidth _ _ O
usage _ _ O
, _ _ O
… _ _ O
● _ _ O
Linux _ _ O
perf _ _ O
utility _ _ O
summarizes _ _ O
this _ _ O
info _ _ O
in _ _ O
easy _ _ O
to _ _ O
read _ _ O
format _ _ O
o _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
perf.wiki.kernel.org _ _ O
/ _ _ O
index.php _ _ O
/ _ _ O
Tutorial _ _ O
10 _ _ O

CPI _ _ O
( _ _ O
Cycles _ _ O
Per _ _ O
Instruction _ _ O
) _ _ O
Results _ _ O
Why _ _ O
the _ _ O
three _ _ O
“ _ _ O
plateaus _ _ O
” _ _ O
? _ _ O
Why _ _ O
increase _ _ O
in _ _ O
CPI _ _ O
with _ _ O
larger _ _ O
size _ _ O
? _ _ O
Why _ _ O
is _ _ O
array _ _ O
faster _ _ O
than _ _ O
linked _ _ O
list _ _ O
? _ _ O
Why _ _ O
constant _ _ O
IPC _ _ O
, _ _ O
regardless _ _ O
of _ _ O
size _ _ O
? _ _ O
11 _ _ O

Memory _ _ O
Stall _ _ O
Cycle _ _ O
Percentage _ _ O
The _ _ O
three _ _ O
“ _ _ O
plateaus _ _ O
” _ _ O
are _ _ O
in _ _ O
memory _ _ O
stalls _ _ O
too _ _ O
! _ _ O
IPC _ _ O
decrease _ _ O
due _ _ O
to _ _ O
memory _ _ O
stall _ _ O
increases _ _ O
! _ _ O
Array _ _ O
has _ _ O
less _ _ O
memory _ _ O
stalls _ _ O
than _ _ O
linked _ _ O
list _ _ O
No _ _ O
IPC _ _ O
decrease _ _ O
because _ _ O
no _ _ O
increase _ _ O
in _ _ O
memory _ _ O
stalls _ _ O
! _ _ O
12 _ _ O

Data _ _ O
Structure _ _ O
Performance _ _ O
∝ _ _ O
Memory _ _ O
Stalls _ _ O
● _ _ O
Data _ _ O
structure _ _ O
performance _ _ O
is _ _ O
proportional _ _ O
to _ _ O
memory _ _ O
stalls _ _ O
o _ _ O
Applies _ _ O
to _ _ O
other _ _ O
data _ _ O
structures _ _ O
such _ _ O
as _ _ O
trees _ _ O
, _ _ O
graphs _ _ O
, _ _ O
… _ _ O
● _ _ O
In _ _ O
general _ _ O
, _ _ O
more _ _ O
data _ _ O
leads _ _ O
to _ _ O
worse _ _ O
performance _ _ O
o _ _ O
But _ _ O
why _ _ O
? _ _ O
Does _ _ O
more _ _ O
data _ _ O
make _ _ O
MEM _ _ O
stalls _ _ O
longer _ _ O
? _ _ O
( _ _ O
Hint _ _ O
: _ _ O
yes _ _ O
) _ _ O
o _ _ O
And _ _ O
why _ _ O
is _ _ O
an _ _ O
array _ _ O
not _ _ O
affected _ _ O
by _ _ O
data _ _ O
size _ _ O
? _ _ O
( _ _ O
I _ _ O
wonder _ _ O
… _ _ O
) _ _ O
● _ _ O
You _ _ O
will _ _ O
be _ _ O
able _ _ O
to _ _ O
answer _ _ O
all _ _ O
these _ _ O
questions _ _ O
when _ _ O
we _ _ O
are _ _ O
done _ _ O
. _ _ O
13 _ _ O

Memory _ _ O
Technologies _ _ O
14 _ _ O

Static _ _ O
RAM _ _ O
( _ _ O
SRAM _ _ O
) _ _ O
● _ _ O
SRAM _ _ O
uses _ _ O
a _ _ O
loop _ _ O
of _ _ O
NOT _ _ O
gates _ _ O
to _ _ O
store _ _ O
a _ _ O
single _ _ O
bit _ _ O
● _ _ O
This _ _ O
is _ _ O
usually _ _ O
called _ _ O
a _ _ O
6 _ _ O
T _ _ O
SRAM _ _ O
cell _ _ O
since _ _ O
it _ _ O
uses _ _ O
... _ _ O
6 _ _ O
Transistors _ _ O
! _ _ O
● _ _ O
Pros _ _ O
: _ _ O
o _ _ O
Very _ _ O
fast _ _ O
to _ _ O
read _ _ O
/ _ _ O
write _ _ O
● _ _ O
Cons _ _ O
: _ _ O
o _ _ O
Volatile _ _ O
( _ _ O
loses _ _ O
data _ _ O
without _ _ O
power _ _ O
) _ _ O
o _ _ O
Relatively _ _ O
many _ _ O
transistors _ _ O
needed _ _ O
- _ _ O
> _ _ O
expensive _ _ O
15 _ _ O

Dynamic _ _ O
RAM _ _ O
( _ _ O
DRAM _ _ O
) _ _ O
● _ _ O
DRAM _ _ O
uses _ _ O
one _ _ O
transistor _ _ O
and _ _ O
one _ _ O
capacitor _ _ O
o _ _ O
The _ _ O
bit _ _ O
is _ _ O
stored _ _ O
as _ _ O
a _ _ O
charge _ _ O
in _ _ O
the _ _ O
capacitor _ _ O
o _ _ O
Capacitor _ _ O
leaks _ _ O
charge _ _ O
over _ _ O
time _ _ O
- _ _ O
> _ _ O
Must _ _ O
be _ _ O
periodically _ _ O
recharged _ _ O
( _ _ O
called _ _ O
refresh _ _ O
) _ _ O
- _ _ O
> _ _ O
During _ _ O
refresh _ _ O
, _ _ O
DRAM _ _ O
ca _ _ O
n’t _ _ O
be _ _ O
accessed _ _ O
o _ _ O
Accesses _ _ O
are _ _ O
slower _ _ O
- _ _ O
> _ _ O
Small _ _ O
charge _ _ O
must _ _ O
be _ _ O
amplified _ _ O
to _ _ O
be _ _ O
read _ _ O
- _ _ O
> _ _ O
Also _ _ O
after _ _ O
read _ _ O
, _ _ O
capacitor _ _ O
needs _ _ O
recharging _ _ O
again _ _ O
o _ _ O
Reading _ _ O
a _ _ O
DRAM _ _ O
cell _ _ O
is _ _ O
slower _ _ O
than _ _ O
reading _ _ O
SRAM _ _ O
● _ _ O
Pros _ _ O
: _ _ O
o _ _ O
Higher _ _ O
density _ _ O
- _ _ O
> _ _ O
less _ _ O
silicon _ _ O
- _ _ O
> _ _ O
much _ _ O
cheaper _ _ O
than _ _ O
SRAM _ _ O
● _ _ O
Cons _ _ O
: _ _ O
o _ _ O
Still _ _ O
volatile _ _ O
( _ _ O
even _ _ O
more _ _ O
volatile _ _ O
than _ _ O
SRAM _ _ O
) _ _ O
o _ _ O
Slower _ _ O
access _ _ O
time _ _ O
16 _ _ O

Spinning _ _ O
magnetic _ _ O
disks _ _ O
( _ _ O
HDD _ _ O
) _ _ O
● _ _ O
Spinning _ _ O
platter _ _ O
coated _ _ O
with _ _ O
a _ _ O
ferromagnetic _ _ O
substance _ _ O
magnetized _ _ O
to _ _ O
represent _ _ O
bits _ _ O
o _ _ O
Has _ _ O
a _ _ O
mechanical _ _ O
arm _ _ O
with _ _ O
a _ _ O
head _ _ O
o _ _ O
Reads _ _ O
by _ _ O
placing _ _ O
arm _ _ O
in _ _ O
correct _ _ O
cylinder _ _ O
, _ _ O
and _ _ O
waiting _ _ O
for _ _ O
platter _ _ O
to _ _ O
rotate _ _ O
● _ _ O
Pros _ _ O
: _ _ O
o _ _ O
Nonvolatile _ _ O
( _ _ O
magnetization _ _ O
persists _ _ O
without _ _ O
power _ _ O
) _ _ O
o _ _ O
Extremely _ _ O
cheap _ _ O
( _ _ O
1 _ _ O
TB _ _ O
for _ _ O
$ _ _ O
50 _ _ O
) _ _ O
● _ _ O
Cons _ _ O
: _ _ O
o _ _ O
Extremely _ _ O
slow _ _ O
( _ _ O
it _ _ O
has _ _ O
a _ _ O
mechanical _ _ O
arm _ _ O
, _ _ O
enough _ _ O
said _ _ O
) _ _ O
17 _ _ O

Other _ _ O
technology _ _ O
● _ _ O
Flash _ _ O
Memory _ _ O
o _ _ O
Works _ _ O
using _ _ O
a _ _ O
special _ _ O
MOSFET _ _ O
with _ _ O
“ _ _ O
floating _ _ O
gate _ _ O
” _ _ O
o _ _ O
Pros _ _ O
: _ _ O
nonvolatile _ _ O
, _ _ O
much _ _ O
faster _ _ O
than _ _ O
HDD _ _ O
o _ _ O
Cons _ _ O
: _ _ O
▪ _ _ O
Slower _ _ O
than _ _ O
DRAM _ _ O
▪ _ _ O
More _ _ O
expensive _ _ O
than _ _ O
HDDs _ _ O
( _ _ O
1 _ _ O
TB _ _ O
for _ _ O
$ _ _ O
250 _ _ O
) _ _ O
▪ _ _ O
Writing _ _ O
is _ _ O
destructive _ _ O
and _ _ O
shortens _ _ O
lifespan _ _ O
● _ _ O
Experimental _ _ O
technology _ _ O
o _ _ O
Ferroelectric _ _ O
RAM _ _ O
( _ _ O
FeRAM _ _ O
) _ _ O
, _ _ O
Magnetoresistive _ _ O
RAM _ _ O
( _ _ O
MRAM _ _ O
) _ _ O
, _ _ O
Phase-change _ _ O
memory _ _ O
( _ _ O
PRAM _ _ O
) _ _ O
, _ _ O
carbon _ _ O
nanotubes _ _ O
... _ _ O
o _ _ O
In _ _ O
varying _ _ O
states _ _ O
of _ _ O
development _ _ O
and _ _ O
maturity _ _ O
o _ _ O
Nonvolatile _ _ O
and _ _ O
close _ _ O
to _ _ O
DRAM _ _ O
speeds _ _ O
18 _ _ O

Memory _ _ O
/ _ _ O
storage _ _ O
technologies _ _ O
Volatile _ _ O
Nonvolatile _ _ O
SRAM _ _ O
DRAM _ _ O
HDDs _ _ O
Flash _ _ O
Speed _ _ O
FAST _ _ O
OK _ _ O
SLOW _ _ O
Pretty _ _ O
good _ _ O
! _ _ O
Price _ _ O
Expensive _ _ O
OK _ _ O
Cheap _ _ O
! _ _ O
Meh _ _ O
Power _ _ O
Good _ _ O
! _ _ O
Meh _ _ O
Bad _ _ O
OK _ _ O
Durability _ _ O
Good _ _ O
! _ _ O
Good _ _ O
! _ _ O
Good _ _ O
! _ _ O
OK _ _ O
Reliability _ _ O
Good _ _ O
! _ _ O
Pretty _ _ O
good _ _ O
! _ _ O
Meh _ _ O
Pretty _ _ O
good _ _ O
! _ _ O
I _ _ O
’m _ _ O
using _ _ O
Durability _ _ O
to _ _ O
mean _ _ O
“ _ _ O
how _ _ O
well _ _ O
it _ _ O
holds _ _ O
data _ _ O
after _ _ O
repeated _ _ O
use _ _ O
. _ _ O
” _ _ O
I _ _ O
’m _ _ O
using _ _ O
Reliability _ _ O
to _ _ O
mean _ _ O
“ _ _ O
how _ _ O
resistant _ _ O
is _ _ O
it _ _ O
to _ _ O
external _ _ O
shock _ _ O
. _ _ O
” _ _ O
19 _ _ O

Do _ _ O
you _ _ O
notice _ _ O
a _ _ O
trend _ _ O
? _ _ O
● _ _ O
The _ _ O
faster _ _ O
the _ _ O
memory _ _ O
the _ _ O
more _ _ O
expensive _ _ O
and _ _ O
lower _ _ O
density _ _ O
. _ _ O
● _ _ O
The _ _ O
slower _ _ O
the _ _ O
memory _ _ O
the _ _ O
less _ _ O
expensive _ _ O
and _ _ O
higher _ _ O
density _ _ O
. _ _ O
● _ _ O
There _ _ O
exists _ _ O
a _ _ O
hierarchy _ _ O
in _ _ O
program _ _ O
data _ _ O
: _ _ O
o _ _ O
Small _ _ O
set _ _ O
of _ _ O
data _ _ O
that _ _ O
is _ _ O
accessed _ _ O
very _ _ O
frequently _ _ O
o _ _ O
Large _ _ O
set _ _ O
of _ _ O
data _ _ O
that _ _ O
is _ _ O
accessed _ _ O
very _ _ O
infrequently _ _ O
● _ _ O
Thus _ _ O
, _ _ O
memory _ _ O
should _ _ O
also _ _ O
be _ _ O
constructed _ _ O
as _ _ O
a _ _ O
hierarchy _ _ O
: _ _ O
o _ _ O
Fast _ _ O
and _ _ O
small _ _ O
memory _ _ O
at _ _ O
the _ _ O
upper _ _ O
levels _ _ O
o _ _ O
Slow _ _ O
and _ _ O
big _ _ O
memory _ _ O
at _ _ O
the _ _ O
lower _ _ O
levels _ _ O
20 _ _ O

Larger _ _ O
capacity _ _ O
memories _ _ O
are _ _ O
slower _ _ O
Thoziyoor _ _ O
, _ _ O
Shyamkumar _ _ O
& _ _ O
Muralimanohar _ _ O
, _ _ O
Naveen _ _ O
& _ _ O
Ahn _ _ O
, _ _ O
Jung _ _ O
Ho _ _ O
& _ _ O
Jouppi _ _ O
, _ _ O
Norman _ _ O
. _ _ O
( _ _ O
2008 _ _ O
) _ _ O
. _ _ O
CACTI _ _ O
5.1 _ _ O
. _ _ O
21 _ _ O

DRAM _ _ O
faster _ _ O
than _ _ O
SRAM _ _ O
at _ _ O
high _ _ O
capacity _ _ O
due _ _ O
to _ _ O
density _ _ O
● _ _ O
Access _ _ O
Time _ _ O
= _ _ O
Memory _ _ O
Cell _ _ O
Delay _ _ O
+ _ _ O
Address _ _ O
Decode _ _ O
Delay _ _ O
+ _ _ O
Wire _ _ O
Delay _ _ O
● _ _ O
With _ _ O
high _ _ O
capacity _ _ O
, _ _ O
Wire _ _ O
Delay _ _ O
( _ _ O
word _ _ O
lines _ _ O
+ _ _ O
bit _ _ O
lines _ _ O
) _ _ O
starts _ _ O
to _ _ O
dominate _ _ O
● _ _ O
Wire _ _ O
Delay _ _ O
∝ _ _ O
Memory _ _ O
Structure _ _ O
Area _ _ O
● _ _ O
DRAM _ _ O
density _ _ O
> _ _ O
SRAM _ _ O
density _ _ O
→ _ _ O
DRAM _ _ O
Wire _ _ O
Delay _ _ O
< _ _ O
SRAM _ _ O
Wire _ _ O
Delay _ _ O
22 _ _ O

The _ _ O
Memory _ _ O
Hierarchy _ _ O
23 _ _ O

System _ _ O
Memory _ _ O
Hierarchy _ _ O
● _ _ O
Use _ _ O
fast _ _ O
memory _ _ O
( _ _ O
SRAM _ _ O
) _ _ O
to _ _ O
store _ _ O
frequently _ _ O
used _ _ O
data _ _ O
inside _ _ O
the _ _ O
CPU _ _ O
● _ _ O
Use _ _ O
slow _ _ O
memory _ _ O
( _ _ O
e.g. _ _ O
DRAM _ _ O
) _ _ O
to _ _ O
store _ _ O
rest _ _ O
of _ _ O
the _ _ O
data _ _ O
outside _ _ O
the _ _ O
CPU _ _ O
CPU _ _ O
Pipeline _ _ O
Memory _ _ O
bus _ _ O
delay _ _ O
SRAM _ _ O
( _ _ O
regs _ _ O
) _ _ O
PCIe _ _ O
bus _ _ O
delay _ _ O
DRAM _ _ O
( _ _ O
memory _ _ O
) _ _ O
HDD _ _ O
/ _ _ O
SDD _ _ O
( _ _ O
files _ _ O
, _ _ O
swapped _ _ O
out _ _ O
memory _ _ O
) _ _ O
● _ _ O
Registers _ _ O
are _ _ O
used _ _ O
frequently _ _ O
for _ _ O
computation _ _ O
so _ _ O
are _ _ O
stored _ _ O
in _ _ O
SRAM _ _ O
● _ _ O
Memory _ _ O
pages _ _ O
used _ _ O
frequently _ _ O
are _ _ O
stored _ _ O
in _ _ O
DRAM _ _ O
● _ _ O
Memory _ _ O
pages _ _ O
used _ _ O
infrequently _ _ O
are _ _ O
stored _ _ O
in _ _ O
HDD _ _ O
/ _ _ O
SDD _ _ O
( _ _ O
in _ _ O
swap _ _ O
space _ _ O
) _ _ O
● _ _ O
Note _ _ O
: _ _ O
Memories _ _ O
outside _ _ O
CPU _ _ O
suffers _ _ O
from _ _ O
bus _ _ O
delay _ _ O
as _ _ O
well _ _ O
24 _ _ O

System _ _ O
Memory _ _ O
Hierarchy _ _ O
● _ _ O
Use _ _ O
fast _ _ O
memory _ _ O
( _ _ O
SRAM _ _ O
) _ _ O
to _ _ O
store _ _ O
frequently _ _ O
used _ _ O
data _ _ O
inside _ _ O
the _ _ O
CPU _ _ O
● _ _ O
Use _ _ O
slow _ _ O
memory _ _ O
( _ _ O
e.g. _ _ O
DRAM _ _ O
) _ _ O
to _ _ O
store _ _ O
rest _ _ O
of _ _ O
the _ _ O
data _ _ O
outside _ _ O
the _ _ O
CPU _ _ O
CPU _ _ O
Pipeline _ _ O
Memory _ _ O
bus _ _ O
delay _ _ O
SRAM _ _ O
( _ _ O
regs _ _ O
) _ _ O
PCIe _ _ O
bus _ _ O
delay _ _ O
DRAM _ _ O
( _ _ O
memory _ _ O
) _ _ O
HDD _ _ O
/ _ _ O
SDD _ _ O
( _ _ O
files _ _ O
, _ _ O
swapped _ _ O
out _ _ O
memory _ _ O
) _ _ O
● _ _ O
Drawback _ _ O
: _ _ O
Memory _ _ O
access _ _ O
is _ _ O
much _ _ O
slower _ _ O
compared _ _ O
to _ _ O
registers _ _ O
● _ _ O
Q _ _ O
: _ _ O
Can _ _ O
we _ _ O
make _ _ O
memory _ _ O
access _ _ O
speed _ _ O
comparable _ _ O
to _ _ O
register _ _ O
access _ _ O
? _ _ O
25 _ _ O

System _ _ O
Memory _ _ O
Hierarchy _ _ O
● _ _ O
Use _ _ O
fast _ _ O
memory _ _ O
( _ _ O
SRAM _ _ O
) _ _ O
to _ _ O
store _ _ O
frequently _ _ O
used _ _ O
data _ _ O
inside _ _ O
the _ _ O
CPU _ _ O
● _ _ O
Use _ _ O
slow _ _ O
memory _ _ O
( _ _ O
e.g. _ _ O
DRAM _ _ O
) _ _ O
to _ _ O
store _ _ O
rest _ _ O
of _ _ O
the _ _ O
data _ _ O
outside _ _ O
the _ _ O
CPU _ _ O
CPU _ _ O
SRAM _ _ O
( _ _ O
registers _ _ O
) _ _ O
DRAM _ _ O
( _ _ O
memory _ _ O
) _ _ O
Pipeline _ _ O
SRAM _ _ O
( _ _ O
cache _ _ O
) _ _ O
HDD _ _ O
/ _ _ O
SDD _ _ O
( _ _ O
files _ _ O
, _ _ O
swapped _ _ O
out _ _ O
memory _ _ O
) _ _ O
● _ _ O
Drawback _ _ O
: _ _ O
Memory _ _ O
access _ _ O
is _ _ O
much _ _ O
slower _ _ O
compared _ _ O
to _ _ O
registers _ _ O
● _ _ O
Q _ _ O
: _ _ O
Can _ _ O
we _ _ O
make _ _ O
memory _ _ O
access _ _ O
speed _ _ O
comparable _ _ O
to _ _ O
register _ _ O
access _ _ O
? _ _ O
o _ _ O
How _ _ O
about _ _ O
storing _ _ O
frequently _ _ O
used _ _ O
memory _ _ O
data _ _ O
in _ _ O
SRAM _ _ O
too _ _ O
? _ _ O
o _ _ O
This _ _ O
is _ _ O
called _ _ O
caching _ _ O
. _ _ O
The _ _ O
hardware _ _ O
structure _ _ O
is _ _ O
called _ _ O
a _ _ O
cache _ _ O
. _ _ O
26 _ _ O

Caching _ _ O
● _ _ O
Caching _ _ O
: _ _ O
keeping _ _ O
a _ _ O
temporary _ _ O
copy _ _ O
of _ _ O
data _ _ O
for _ _ O
faster _ _ O
access _ _ O
● _ _ O
DRAM _ _ O
is _ _ O
in _ _ O
a _ _ O
sense _ _ O
also _ _ O
caching _ _ O
frequently _ _ O
used _ _ O
pages _ _ O
from _ _ O
swap _ _ O
space _ _ O
o _ _ O
We _ _ O
are _ _ O
just _ _ O
extending _ _ O
that _ _ O
idea _ _ O
to _ _ O
bring _ _ O
cache _ _ O
data _ _ O
inside _ _ O
the _ _ O
CPU _ _ O
! _ _ O
● _ _ O
Now _ _ O
instructions _ _ O
like _ _ O
lw _ _ O
or _ _ O
sw _ _ O
never _ _ O
directly _ _ O
access _ _ O
DRAM _ _ O
o _ _ O
They _ _ O
first _ _ O
search _ _ O
the _ _ O
cache _ _ O
to _ _ O
see _ _ O
if _ _ O
there _ _ O
is _ _ O
a _ _ O
hit _ _ O
in _ _ O
the _ _ O
cache _ _ O
o _ _ O
Only _ _ O
if _ _ O
they _ _ O
miss _ _ O
will _ _ O
they _ _ O
access _ _ O
DRAM _ _ O
to _ _ O
bring _ _ O
data _ _ O
into _ _ O
the _ _ O
cache _ _ O
CPU _ _ O
SRAM _ _ O
( _ _ O
registers _ _ O
) _ _ O
DRAM _ _ O
( _ _ O
memory _ _ O
) _ _ O
Pipeline _ _ O
SRAM _ _ O
( _ _ O
cache _ _ O
) _ _ O
HDD _ _ O
/ _ _ O
SDD _ _ O
( _ _ O
files _ _ O
, _ _ O
swapped _ _ O
out _ _ O
memory _ _ O
) _ _ O
27 _ _ O

Cache _ _ O
Flow _ _ O
Chart _ _ O
● _ _ O
Cache _ _ O
block _ _ O
: _ _ O
unit _ _ O
of _ _ O
data _ _ O
used _ _ O
to _ _ O
cache _ _ O
data _ _ O
o _ _ O
What _ _ O
page _ _ O
is _ _ O
to _ _ O
memory _ _ O
paging _ _ O
o _ _ O
Cache _ _ O
block _ _ O
size _ _ O
is _ _ O
typically _ _ O
multiple _ _ O
words _ _ O
( _ _ O
e.g. _ _ O
32 _ _ O
bytes _ _ O
or _ _ O
64 _ _ O
bytes _ _ O
. _ _ O
You _ _ O
’ll _ _ O
see _ _ O
why _ _ O
. _ _ O
) _ _ O
● _ _ O
Good _ _ O
: _ _ O
Memory _ _ O
Wall _ _ O
can _ _ O
be _ _ O
surmounted _ _ O
o _ _ O
On _ _ O
cache _ _ O
hit _ _ O
, _ _ O
no _ _ O
need _ _ O
to _ _ O
go _ _ O
to _ _ O
DRAM _ _ O
! _ _ O
● _ _ O
Bad _ _ O
: _ _ O
MEM _ _ O
stage _ _ O
has _ _ O
variable _ _ O
latency _ _ O
o _ _ O
Typically _ _ O
, _ _ O
only _ _ O
a _ _ O
few _ _ O
cycles _ _ O
if _ _ O
cache _ _ O
hit _ _ O
o _ _ O
More _ _ O
than _ _ O
a _ _ O
100 _ _ O
cycles _ _ O
if _ _ O
cache _ _ O
miss _ _ O
! _ _ O
( _ _ O
Processor _ _ O
must _ _ O
go _ _ O
all _ _ O
the _ _ O
way _ _ O
to _ _ O
DRAM _ _ O
! _ _ O
) _ _ O
o _ _ O
Makes _ _ O
performance _ _ O
very _ _ O
unpredictable _ _ O
28 _ _ O

Cache _ _ O
Locality _ _ O
: _ _ O
Temporal _ _ O
and _ _ O
Spatial _ _ O
• _ _ O
Temporal _ _ O
Locality _ _ O
• _ _ O
Spatial _ _ O
Locality _ _ O
Cache _ _ O
Block _ _ O
Data _ _ O
Item _ _ O
0 _ _ O
Data _ _ O
Item _ _ O
Miss _ _ O
! _ _ O
Hit _ _ O
! _ _ O
Time _ _ O
Hit _ _ O
! _ _ O
Miss _ _ O
! _ _ O
Data _ _ O
Item _ _ O
1 _ _ O
Hit _ _ O
! _ _ O
Data _ _ O
Item _ _ O
2 _ _ O
Hit _ _ O
! _ _ O
Time _ _ O
29 _ _ O

Cache _ _ O
Locality _ _ O
: _ _ O
Temporal _ _ O
and _ _ O
Spatial _ _ O
● _ _ O
Caching _ _ O
works _ _ O
because _ _ O
there _ _ O
is _ _ O
locality _ _ O
in _ _ O
program _ _ O
data _ _ O
accesses _ _ O
o _ _ O
Temporal _ _ O
locality _ _ O
▪ _ _ O
Same _ _ O
data _ _ O
item _ _ O
is _ _ O
accessed _ _ O
many _ _ O
times _ _ O
in _ _ O
succession _ _ O
▪ _ _ O
1st _ _ O
access _ _ O
will _ _ O
miss _ _ O
but _ _ O
following _ _ O
accesses _ _ O
will _ _ O
hit _ _ O
in _ _ O
the _ _ O
cache _ _ O
o _ _ O
Spatial _ _ O
locality _ _ O
▪ _ _ O
Different _ _ O
data _ _ O
items _ _ O
spatially _ _ O
close _ _ O
are _ _ O
accessed _ _ O
in _ _ O
succession _ _ O
▪ _ _ O
1st _ _ O
access _ _ O
will _ _ O
miss _ _ O
but _ _ O
bring _ _ O
in _ _ O
an _ _ O
entire _ _ O
cache _ _ O
block _ _ O
▪ _ _ O
Accesses _ _ O
to _ _ O
other _ _ O
items _ _ O
within _ _ O
same _ _ O
cache _ _ O
block _ _ O
will _ _ O
hit _ _ O
▪ _ _ O
E.g. _ _ O
, _ _ O
fields _ _ O
in _ _ O
an _ _ O
object _ _ O
, _ _ O
elements _ _ O
in _ _ O
an _ _ O
array _ _ O
, _ _ O
… _ _ O
● _ _ O
Locality _ _ O
, _ _ O
like _ _ O
ILP _ _ O
, _ _ O
is _ _ O
a _ _ O
property _ _ O
of _ _ O
the _ _ O
program _ _ O
30 _ _ O

Cold _ _ O
Misses _ _ O
and _ _ O
Capacity _ _ O
Misses _ _ O
● _ _ O
Cold _ _ O
miss _ _ O
( _ _ O
a.k.a _ _ O
. _ _ O
compulsory _ _ O
miss _ _ O
) _ _ O
o _ _ O
Miss _ _ O
suffered _ _ O
when _ _ O
data _ _ O
is _ _ O
accessed _ _ O
for _ _ O
the _ _ O
first _ _ O
time _ _ O
by _ _ O
program _ _ O
o _ _ O
Cold _ _ O
miss _ _ O
since _ _ O
cache _ _ O
has _ _ O
n’t _ _ O
been _ _ O
warmed _ _ O
up _ _ O
with _ _ O
accesses _ _ O
o _ _ O
Compulsory _ _ O
miss _ _ O
since _ _ O
there _ _ O
is _ _ O
no _ _ O
way _ _ O
you _ _ O
can _ _ O
hit _ _ O
on _ _ O
the _ _ O
first _ _ O
access _ _ O
o _ _ O
Subsequent _ _ O
accesses _ _ O
will _ _ O
be _ _ O
hits _ _ O
since _ _ O
now _ _ O
data _ _ O
is _ _ O
fetched _ _ O
into _ _ O
cache _ _ O
▪ _ _ O
Unless _ _ O
it _ _ O
is _ _ O
replaced _ _ O
to _ _ O
make _ _ O
space _ _ O
for _ _ O
more _ _ O
frequently _ _ O
used _ _ O
data _ _ O
● _ _ O
Capacity _ _ O
miss _ _ O
o _ _ O
Miss _ _ O
suffered _ _ O
when _ _ O
data _ _ O
is _ _ O
accessed _ _ O
for _ _ O
the _ _ O
second _ _ O
or _ _ O
third _ _ O
times _ _ O
o _ _ O
This _ _ O
miss _ _ O
occurred _ _ O
because _ _ O
data _ _ O
was _ _ O
replaced _ _ O
to _ _ O
make _ _ O
space _ _ O
o _ _ O
If _ _ O
there _ _ O
had _ _ O
been _ _ O
more _ _ O
capacity _ _ O
, _ _ O
miss _ _ O
would _ _ O
n’t _ _ O
have _ _ O
happened _ _ O
o _ _ O
Capacity _ _ O
decides _ _ O
how _ _ O
much _ _ O
temporal _ _ O
locality _ _ O
you _ _ O
can _ _ O
leverage _ _ O
31 _ _ O

Reducing _ _ O
Cold _ _ O
Misses _ _ O
and _ _ O
Capacity _ _ O
Misses _ _ O
● _ _ O
Reducing _ _ O
capacity _ _ O
misses _ _ O
is _ _ O
straightforward _ _ O
o _ _ O
Increase _ _ O
capacity _ _ O
, _ _ O
that _ _ O
is _ _ O
cache _ _ O
size _ _ O
! _ _ O
● _ _ O
But _ _ O
how _ _ O
do _ _ O
you _ _ O
reduce _ _ O
cold _ _ O
misses _ _ O
? _ _ O
Is _ _ O
it _ _ O
even _ _ O
possible _ _ O
? _ _ O
o _ _ O
Yes _ _ O
! _ _ O
By _ _ O
taking _ _ O
advantage _ _ O
of _ _ O
spatial _ _ O
locality _ _ O
. _ _ O
o _ _ O
Have _ _ O
a _ _ O
large _ _ O
cache _ _ O
block _ _ O
so _ _ O
you _ _ O
bring _ _ O
in _ _ O
other _ _ O
items _ _ O
on _ _ O
a _ _ O
miss _ _ O
o _ _ O
Those _ _ O
other _ _ O
items _ _ O
may _ _ O
be _ _ O
accessed _ _ O
for _ _ O
the _ _ O
first _ _ O
time _ _ O
but _ _ O
will _ _ O
hit _ _ O
! _ _ O
● _ _ O
Large _ _ O
cache _ _ O
blocks _ _ O
can _ _ O
… _ _ O
o _ _ O
Potentially _ _ O
reduce _ _ O
cold _ _ O
misses _ _ O
( _ _ O
and/or _ _ O
reduce _ _ O
capacity _ _ O
misses _ _ O
) _ _ O
( _ _ O
given _ _ O
some _ _ O
spatial _ _ O
locality _ _ O
, _ _ O
can _ _ O
bring _ _ O
in _ _ O
more _ _ O
data _ _ O
on _ _ O
a _ _ O
miss _ _ O
) _ _ O
o _ _ O
Potentially _ _ O
increase _ _ O
capacity _ _ O
misses _ _ O
( _ _ O
with _ _ O
no _ _ O
spatial _ _ O
locality _ _ O
, _ _ O
can _ _ O
store _ _ O
less _ _ O
data _ _ O
items _ _ O
in _ _ O
same _ _ O
capacity _ _ O
) _ _ O
→ _ _ O
Each _ _ O
program _ _ O
has _ _ O
a _ _ O
sweet _ _ O
spot _ _ O
. _ _ O
Architects _ _ O
choose _ _ O
a _ _ O
compromise _ _ O
. _ _ O
32 _ _ O

So _ _ O
how _ _ O
big _ _ O
do _ _ O
we _ _ O
want _ _ O
the _ _ O
cache _ _ O
to _ _ O
be _ _ O
? _ _ O
● _ _ O
Uber _ _ O
big _ _ O
! _ _ O
Caches _ _ O
● _ _ O
On _ _ O
the _ _ O
right _ _ O
is _ _ O
a _ _ O
diagram _ _ O
of _ _ O
the _ _ O
Xeon _ _ O
Broadwell _ _ O
CPU _ _ O
used _ _ O
in _ _ O
kernighan.cs.pitt.edu _ _ O
. _ _ O
o _ _ O
Caches _ _ O
take _ _ O
up _ _ O
almost _ _ O
as _ _ O
much _ _ O
real _ _ O
estate _ _ O
as _ _ O
cores _ _ O
! _ _ O
o _ _ O
A _ _ O
cache _ _ O
miss _ _ O
is _ _ O
that _ _ O
painful _ _ O
. _ _ O
● _ _ O
But _ _ O
having _ _ O
a _ _ O
big _ _ O
cache _ _ O
comes _ _ O
with _ _ O
its _ _ O
own _ _ O
set _ _ O
of _ _ O
problems _ _ O
o _ _ O
Cache _ _ O
itself _ _ O
gets _ _ O
slower _ _ O
33 _ _ O

Bigger _ _ O
caches _ _ O
are _ _ O
slower _ _ O
● _ _ O
Below _ _ O
is _ _ O
a _ _ O
diagram _ _ O
of _ _ O
a _ _ O
Nehalem _ _ O
CPU _ _ O
( _ _ O
an _ _ O
older _ _ O
Intel _ _ O
CPU _ _ O
) _ _ O
● _ _ O
How _ _ O
long _ _ O
do _ _ O
you _ _ O
think _ _ O
it _ _ O
takes _ _ O
for _ _ O
data _ _ O
to _ _ O
make _ _ O
it _ _ O
from _ _ O
here _ _ O
... _ _ O
● _ _ O
... _ _ O
to _ _ O
here _ _ O
? _ _ O
● _ _ O
It _ _ O
must _ _ O
be _ _ O
routed _ _ O
through _ _ O
all _ _ O
this _ _ O
. _ _ O
● _ _ O
Can _ _ O
we _ _ O
cache _ _ O
the _ _ O
data _ _ O
in _ _ O
the _ _ O
far _ _ O
away _ _ O
“ _ _ O
L3 _ _ O
Cache _ _ O
” _ _ O
to _ _ O
a _ _ O
nearby _ _ O
“ _ _ O
L2 _ _ O
Cache _ _ O
” _ _ O
? _ _ O
34 _ _ O

Multi-level _ _ O
Caching _ _ O
● _ _ O
This _ _ O
is _ _ O
the _ _ O
structure _ _ O
of _ _ O
the _ _ O
kernighan.cs.pitt.edu _ _ O
Xeon _ _ O
CPU _ _ O
: _ _ O
L1 _ _ O
I-Cache _ _ O
L1 _ _ O
D-Cache _ _ O
L2 _ _ O
Cache _ _ O
L3 _ _ O
Cache _ _ O
● _ _ O
L1 _ _ O
cache _ _ O
: _ _ O
Small _ _ O
but _ _ O
fast _ _ O
. _ _ O
Interfaces _ _ O
with _ _ O
CPU _ _ O
pipeline _ _ O
MEM _ _ O
stage _ _ O
. _ _ O
o _ _ O
Split _ _ O
to _ _ O
i-cache _ _ O
and _ _ O
d-cache _ _ O
to _ _ O
avoid _ _ O
structural _ _ O
hazard _ _ O
● _ _ O
L2 _ _ O
cache _ _ O
: _ _ O
Middle-sized _ _ O
and _ _ O
middle-fast _ _ O
. _ _ O
Intermediate _ _ O
level _ _ O
. _ _ O
● _ _ O
L3 _ _ O
cache _ _ O
: _ _ O
Big _ _ O
but _ _ O
slow _ _ O
. _ _ O
Last _ _ O
line _ _ O
of _ _ O
defense _ _ O
against _ _ O
memory _ _ O
access _ _ O
. _ _ O
● _ _ O
Allows _ _ O
performance _ _ O
to _ _ O
degrade _ _ O
gracefully _ _ O
35 _ _ O

Revisiting _ _ O
Our _ _ O
Experiments _ _ O
36 _ _ O

Revisiting _ _ O
our _ _ O
CPI _ _ O
Results _ _ O
with _ _ O
the _ _ O
new _ _ O
perspective _ _ O
Correspond _ _ O
to _ _ O
multiple _ _ O
levels _ _ O
of _ _ O
memory _ _ O
But _ _ O
this _ _ O
is _ _ O
just _ _ O
conjecture _ _ O
. _ _ O
Is _ _ O
it _ _ O
actually _ _ O
true _ _ O
? _ _ O
CPI _ _ O
increases _ _ O
with _ _ O
bigger _ _ O
size _ _ O
: _ _ O
more _ _ O
of _ _ O
data _ _ O
structure _ _ O
is _ _ O
stored _ _ O
in _ _ O
lower _ _ O
memory _ _ O
37 _ _ O

kernighan.cs.pitt.edu _ _ O
cache _ _ O
specs _ _ O
● _ _ O
On _ _ O
a _ _ O
Xeon _ _ O
E5 _ _ O
- _ _ O
2640 _ _ O
v4 _ _ O
CPU _ _ O
( _ _ O
10 _ _ O
cores _ _ O
) _ _ O
: _ _ O
o _ _ O
L1 _ _ O
i-cache _ _ O
: _ _ O
32 _ _ O
KB _ _ O
8-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
per _ _ O
core _ _ O
) _ _ O
o _ _ O
L1 _ _ O
d-cache _ _ O
: _ _ O
32 _ _ O
KB _ _ O
8-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
per _ _ O
core _ _ O
) _ _ O
o _ _ O
L2 _ _ O
cache _ _ O
: _ _ O
256 _ _ O
KB _ _ O
8-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
per _ _ O
core _ _ O
) _ _ O
o _ _ O
L3 _ _ O
cache _ _ O
: _ _ O
25 _ _ O
MB _ _ O
20-way _ _ O
set _ _ O
associative _ _ O
( _ _ O
shared _ _ O
) _ _ O
Ref _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
en.wikichip.org _ _ O
/ _ _ O
wiki _ _ O
/ _ _ O
intel _ _ O
/ _ _ O
xeon_e5 _ _ O
/ _ _ O
e5 _ _ O
- _ _ O
2640_v4 _ _ O
● _ _ O
Access _ _ O
latencies _ _ O
( _ _ O
each _ _ O
level _ _ O
includes _ _ O
latency _ _ O
of _ _ O
previous _ _ O
levels _ _ O
) _ _ O
: _ _ O
o _ _ O
L1 _ _ O
: _ _ O
~3 _ _ O
cycles _ _ O
o _ _ O
L2 _ _ O
: _ _ O
~8 _ _ O
cycles _ _ O
o _ _ O
L3 _ _ O
: _ _ O
~16 _ _ O
cycles _ _ O
o _ _ O
DRAM _ _ O
Memory _ _ O
: _ _ O
~67 _ _ O
cycles _ _ O
Ref _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
www.nas.nasa.gov _ _ O
/ _ _ O
assets _ _ O
/ _ _ O
pdf _ _ O
/ _ _ O
papers _ _ O
/ _ _ O
NAS_Technical_Report_NAS-2015 _ _ O
- _ _ O
05.pdf _ _ O
38 _ _ O

Cache _ _ O
Specs _ _ O
Reverse _ _ O
Engineering _ _ O
● _ _ O
Why _ _ O
do _ _ O
I _ _ O
have _ _ O
to _ _ O
refer _ _ O
to _ _ O
a _ _ O
NASA _ _ O
technical _ _ O
report _ _ O
for _ _ O
latencies _ _ O
? _ _ O
o _ _ O
Ref _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
www.nas.nasa.gov _ _ O
/ _ _ O
assets _ _ O
/ _ _ O
pdf _ _ O
/ _ _ O
papers _ _ O
/ _ _ O
NAS_Technical_Report_NAS-2015 _ _ O
- _ _ O
05.pdf _ _ O
o _ _ O
Because _ _ O
Intel _ _ O
does _ _ O
n’t _ _ O
publish _ _ O
detailed _ _ O
cache _ _ O
specs _ _ O
on _ _ O
data _ _ O
sheet _ _ O
● _ _ O
In _ _ O
the _ _ O
technical _ _ O
report _ _ O
( _ _ O
does _ _ O
the _ _ O
step _ _ O
function _ _ O
look _ _ O
familiar _ _ O
? _ _ O
) _ _ O
: _ _ O
Why _ _ O
just _ _ O
Loads _ _ O
and _ _ O
not _ _ O
Stores _ _ O
? _ _ O
39 _ _ O

Loads _ _ O
have _ _ O
more _ _ O
impact _ _ O
on _ _ O
performance _ _ O
● _ _ O
Suppose _ _ O
we _ _ O
added _ _ O
a _ _ O
store _ _ O
to _ _ O
our _ _ O
original _ _ O
loop _ _ O
: _ _ O
node_t _ _ O
* _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
ACCESSES _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
if _ _ O
( _ _ O
current _ _ O
= _ _ O
= _ _ O
NULL _ _ O
) _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
/ _ _ O
/ _ _ O
reached _ _ O
the _ _ O
end _ _ O
else _ _ O
{ _ _ O
current- _ _ O
> _ _ O
data _ _ O
[ _ _ O
0 _ _ O
] _ _ O
= _ _ O
100 _ _ O
; _ _ O
/ _ _ O
/ _ _ O
store _ _ O
to _ _ O
node _ _ O
data _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
load _ _ O
next _ _ O
node _ _ O
address _ _ O
} _ _ O
● _ _ O
Which _ _ O
would _ _ O
have _ _ O
more _ _ O
impact _ _ O
on _ _ O
performance _ _ O
? _ _ O
The _ _ O
load _ _ O
or _ _ O
the _ _ O
store _ _ O
? _ _ O
o _ _ O
A _ _ O
: _ _ O
The _ _ O
load _ _ O
because _ _ O
it _ _ O
is _ _ O
on _ _ O
the _ _ O
critical _ _ O
path _ _ O
. _ _ O
… _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
… _ _ O
current- _ _ O
> _ _ O
data _ _ O
[ _ _ O
0 _ _ O
] _ _ O
= _ _ O
100 _ _ O
current- _ _ O
> _ _ O
data _ _ O
[ _ _ O
0 _ _ O
] _ _ O
= _ _ O
100 _ _ O
current- _ _ O
> _ _ O
data _ _ O
[ _ _ O
0 _ _ O
] _ _ O
= _ _ O
100 _ _ O
… _ _ O
40 _ _ O

Loads _ _ O
have _ _ O
more _ _ O
impact _ _ O
on _ _ O
performance _ _ O
● _ _ O
Loads _ _ O
produce _ _ O
values _ _ O
needed _ _ O
for _ _ O
computation _ _ O
to _ _ O
proceed _ _ O
o _ _ O
Stalled _ _ O
loads _ _ O
delays _ _ O
computation _ _ O
and _ _ O
possibly _ _ O
the _ _ O
critical _ _ O
path _ _ O
● _ _ O
Stores _ _ O
write _ _ O
computation _ _ O
results _ _ O
back _ _ O
to _ _ O
memory _ _ O
o _ _ O
As _ _ O
long _ _ O
as _ _ O
the _ _ O
results _ _ O
are _ _ O
written _ _ O
back _ _ O
eventually _ _ O
, _ _ O
no _ _ O
big _ _ O
hurry _ _ O
o _ _ O
If _ _ O
store _ _ O
results _ _ O
in _ _ O
a _ _ O
cache _ _ O
miss _ _ O
, _ _ O
store _ _ O
is _ _ O
marked _ _ O
as _ _ O
“ _ _ O
pending _ _ O
” _ _ O
and _ _ O
CPU _ _ O
moves _ _ O
on _ _ O
to _ _ O
next _ _ O
computation _ _ O
o _ _ O
Pending _ _ O
stores _ _ O
are _ _ O
maintained _ _ O
in _ _ O
a _ _ O
write _ _ O
buffer _ _ O
hardware _ _ O
structure _ _ O
● _ _ O
What _ _ O
if _ _ O
the _ _ O
next _ _ O
computation _ _ O
reads _ _ O
from _ _ O
a _ _ O
pending _ _ O
store _ _ O
? _ _ O
o _ _ O
First _ _ O
check _ _ O
the _ _ O
write _ _ O
buffer _ _ O
and _ _ O
read _ _ O
in _ _ O
the _ _ O
value _ _ O
if _ _ O
it _ _ O
’s _ _ O
there _ _ O
o _ _ O
Again _ _ O
, _ _ O
performing _ _ O
the _ _ O
store _ _ O
is _ _ O
not _ _ O
on _ _ O
the _ _ O
critical _ _ O
path _ _ O
41 _ _ O

How _ _ O
Write _ _ O
Buffer _ _ O
Maintains _ _ O
Pending _ _ O
Stores _ _ O
● _ _ O
sw _ _ O
p0 _ _ O
, _ _ O
4 _ _ O
( _ _ O
p1 _ _ O
) _ _ O
is _ _ O
about _ _ O
to _ _ O
commit _ _ O
. _ _ O
4 _ _ O
( _ _ O
p1 _ _ O
) _ _ O
= _ _ O
= _ _ O
0xdeadbeef _ _ O
, _ _ O
p0 _ _ O
= _ _ O
= _ _ O
10 _ _ O
● _ _ O
Unfortunately _ _ O
, _ _ O
address _ _ O
0xdeadbeef _ _ O
is _ _ O
not _ _ O
in _ _ O
the _ _ O
L1 _ _ O
d-cache _ _ O
and _ _ O
it _ _ O
misses _ _ O
L1 _ _ O
d-cache _ _ O
… _ _ O
… _ _ O
Miss _ _ O
! _ _ O
… _ _ O
… _ _ O
Store _ _ O
Queue _ _ O
Address _ _ O
Value _ _ O
0xdeadbeef _ _ O
10 _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
commit _ _ O
in _ _ O
order _ _ O
Instruction _ _ O
Decoder _ _ O
Write _ _ O
Buffer _ _ O
Address _ _ O
Value _ _ O
Retirement _ _ O
Register _ _ O
File _ _ O
t0 _ _ O
s0 _ _ O
a0 _ _ O
t1 _ _ O
s1 _ _ O
a1 _ _ O
t2 _ _ O
s2 _ _ O
a2 _ _ O
… _ _ O
… _ _ O
… _ _ O
Instruction _ _ O
Queue _ _ O
instruction _ _ O
dest _ _ O
done _ _ O
? _ _ O
sw _ _ O
p0 _ _ O
, _ _ O
4 _ _ O
( _ _ O
p1 _ _ O
) _ _ O
Y _ _ O
… _ _ O
… _ _ O
… _ _ O
Load _ _ O
/ _ _ O
Store _ _ O
Int _ _ O
ALU _ _ O
1 _ _ O
Int _ _ O
ALU _ _ O
2 _ _ O
Float _ _ O
ALU _ _ O
42 _ _ O

How _ _ O
Write _ _ O
Buffer _ _ O
Maintains _ _ O
Pending _ _ O
Stores _ _ O
● _ _ O
sw _ _ O
p0 _ _ O
, _ _ O
4 _ _ O
( _ _ O
p1 _ _ O
) _ _ O
commits _ _ O
successfully _ _ O
anyway _ _ O
● _ _ O
The _ _ O
store _ _ O
is _ _ O
moved _ _ O
to _ _ O
the _ _ O
Write _ _ O
Buffer _ _ O
and _ _ O
stays _ _ O
there _ _ O
until _ _ O
store _ _ O
completes _ _ O
L1 _ _ O
d-cache _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
Store _ _ O
Queue _ _ O
Address _ _ O
Value _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
Retirement _ _ O
Register _ _ O
File _ _ O
t0 _ _ O
s0 _ _ O
a0 _ _ O
t1 _ _ O
s1 _ _ O
a1 _ _ O
t2 _ _ O
s2 _ _ O
a2 _ _ O
… _ _ O
… _ _ O
… _ _ O
commit _ _ O
in _ _ O
order _ _ O
Instruction _ _ O
Decoder _ _ O
Write _ _ O
Buffer _ _ O
Address _ _ O
Value _ _ O
0xdeadbeef _ _ O
10 _ _ O
Instruction _ _ O
Queue _ _ O
instruction _ _ O
dest _ _ O
done _ _ O
? _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
Load _ _ O
/ _ _ O
Store _ _ O
Int _ _ O
ALU _ _ O
1 _ _ O
Int _ _ O
ALU _ _ O
2 _ _ O
Float _ _ O
ALU _ _ O
43 _ _ O

How _ _ O
Write _ _ O
Buffer _ _ O
Maintains _ _ O
Pending _ _ O
Stores _ _ O
● _ _ O
Later _ _ O
, _ _ O
when _ _ O
lw _ _ O
p3 _ _ O
, _ _ O
0 _ _ O
( _ _ O
p2 _ _ O
) _ _ O
comes _ _ O
along _ _ O
, _ _ O
it _ _ O
checks _ _ O
Write _ _ O
Buffer _ _ O
first _ _ O
● _ _ O
If _ _ O
0 _ _ O
( _ _ O
p2 _ _ O
) _ _ O
= _ _ O
= _ _ O
0xdeadbeef _ _ O
, _ _ O
Write _ _ O
Buffer _ _ O
provides _ _ O
value _ _ O
instead _ _ O
of _ _ O
memory _ _ O
L1 _ _ O
d-cache _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
Store _ _ O
Queue _ _ O
Address _ _ O
Value _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
… _ _ O
Retirement _ _ O
Register _ _ O
File _ _ O
t0 _ _ O
s0 _ _ O
a0 _ _ O
t1 _ _ O
s1 _ _ O
a1 _ _ O
t2 _ _ O
s2 _ _ O
a2 _ _ O
… _ _ O
… _ _ O
… _ _ O
commit _ _ O
in _ _ O
order _ _ O
Instruction _ _ O
Decoder _ _ O
Write _ _ O
Buffer _ _ O
Address _ _ O
Value _ _ O
0xdeadbeef _ _ O
10 _ _ O
Instruction _ _ O
Queue _ _ O
instruction _ _ O
dest _ _ O
done _ _ O
? _ _ O
lw _ _ O
p3 _ _ O
, _ _ O
0 _ _ O
( _ _ O
p2 _ _ O
) _ _ O
t0 _ _ O
N _ _ O
… _ _ O
… _ _ O
… _ _ O
Load _ _ O
/ _ _ O
Store _ _ O
Int _ _ O
ALU _ _ O
1 _ _ O
Int _ _ O
ALU _ _ O
2 _ _ O
Float _ _ O
ALU _ _ O
44 _ _ O

So _ _ O
are _ _ O
stores _ _ O
never _ _ O
on _ _ O
the _ _ O
critical _ _ O
path _ _ O
? _ _ O
● _ _ O
If _ _ O
we _ _ O
had _ _ O
an _ _ O
infinitely _ _ O
sized _ _ O
write _ _ O
buffer _ _ O
, _ _ O
no _ _ O
, _ _ O
never _ _ O
. _ _ O
● _ _ O
In _ _ O
real _ _ O
life _ _ O
, _ _ O
write _ _ O
buffer _ _ O
is _ _ O
limited _ _ O
and _ _ O
can _ _ O
suffer _ _ O
structural _ _ O
hazards _ _ O
o _ _ O
If _ _ O
write _ _ O
buffer _ _ O
is _ _ O
full _ _ O
of _ _ O
pending _ _ O
stores _ _ O
, _ _ O
you _ _ O
ca _ _ O
n’t _ _ O
insert _ _ O
more _ _ O
. _ _ O
→ _ _ O
That _ _ O
will _ _ O
prevent _ _ O
a _ _ O
missing _ _ O
store _ _ O
from _ _ O
committing _ _ O
from _ _ O
i-queue _ _ O
→ _ _ O
That _ _ O
will _ _ O
prevent _ _ O
all _ _ O
subsequent _ _ O
instructions _ _ O
from _ _ O
committing _ _ O
→ _ _ O
That _ _ O
will _ _ O
eventually _ _ O
stall _ _ O
the _ _ O
entire _ _ O
pipeline _ _ O
● _ _ O
But _ _ O
with _ _ O
ample _ _ O
write _ _ O
buffer _ _ O
size _ _ O
, _ _ O
happens _ _ O
rarely _ _ O
o _ _ O
And _ _ O
if _ _ O
it _ _ O
does _ _ O
happen _ _ O
can _ _ O
be _ _ O
detected _ _ O
using _ _ O
PMUs _ _ O
● _ _ O
Hence _ _ O
, _ _ O
we _ _ O
will _ _ O
focus _ _ O
on _ _ O
loads _ _ O
to _ _ O
analyze _ _ O
performance _ _ O
45 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
But _ _ O
what _ _ O
do _ _ O
these _ _ O
lines _ _ O
actually _ _ O
mean _ _ O
? _ _ O
46 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
A _ _ O
few _ _ O
L2 _ _ O
cold _ _ O
misses _ _ O
result _ _ O
in _ _ O
high _ _ O
miss _ _ O
rate _ _ O
. _ _ O
But _ _ O
very _ _ O
few _ _ O
L2 _ _ O
accesses _ _ O
to _ _ O
begin _ _ O
with _ _ O
. _ _ O
Almost _ _ O
only _ _ O
L1 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
Do _ _ O
the _ _ O
miss _ _ O
rates _ _ O
correspond _ _ O
to _ _ O
CPI _ _ O
results _ _ O
? _ _ O
Let _ _ O
’s _ _ O
compare _ _ O
with _ _ O
our _ _ O
own _ _ O
eyes _ _ O
! _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
+ _ _ O
Memory _ _ O
L1 _ _ O
+ _ _ O
Memory _ _ O
( _ _ O
mostly _ _ O
) _ _ O
Miss _ _ O
! _ _ O
Miss _ _ O
! _ _ O
Miss _ _ O
! _ _ O
Hit _ _ O
! _ _ O
L1 _ _ O
Cache _ _ O
Miss _ _ O
! _ _ O
Miss _ _ O
! _ _ O
L2 _ _ O
Cache _ _ O
Miss _ _ O
! _ _ O
Hit _ _ O
! _ _ O
Hit _ _ O
! _ _ O
Hit _ _ O
! _ _ O
L3 _ _ O
Cache _ _ O
DRAM _ _ O
Memory _ _ O
47 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
vs _ _ O
CPI _ _ O
Almost _ _ O
only _ _ O
L1 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
+ _ _ O
Memory _ _ O
L1 _ _ O
+ _ _ O
Memory _ _ O
( _ _ O
mostly _ _ O
) _ _ O
48 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
– _ _ O
Other _ _ O
Questions _ _ O
Almost _ _ O
only _ _ O
L1 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
+ _ _ O
Memory _ _ O
L1 _ _ O
+ _ _ O
Memory _ _ O
( _ _ O
mostly _ _ O
) _ _ O
● _ _ O
Why _ _ O
the _ _ O
step _ _ O
up _ _ O
in _ _ O
L1 _ _ O
cache _ _ O
misses _ _ O
between _ _ O
500 _ _ O
– _ _ O
1000 _ _ O
nodes _ _ O
? _ _ O
● _ _ O
Why _ _ O
the _ _ O
step _ _ O
up _ _ O
in _ _ O
L2 _ _ O
cache _ _ O
misses _ _ O
between _ _ O
1000 _ _ O
– _ _ O
5000 _ _ O
nodes _ _ O
? _ _ O
● _ _ O
Why _ _ O
the _ _ O
step _ _ O
up _ _ O
in _ _ O
L3 _ _ O
cache _ _ O
misses _ _ O
between _ _ O
100 _ _ O
k _ _ O
– _ _ O
500 _ _ O
k _ _ O
nodes _ _ O
? _ _ O
● _ _ O
Also _ _ O
, _ _ O
why _ _ O
do _ _ O
cache _ _ O
miss _ _ O
increases _ _ O
look _ _ O
like _ _ O
step _ _ O
functions _ _ O
in _ _ O
general _ _ O
? _ _ O
49 _ _ O

Working _ _ O
Set _ _ O
Overflow _ _ O
can _ _ O
cause _ _ O
Step _ _ O
Function _ _ O
● _ _ O
The _ _ O
size _ _ O
of _ _ O
a _ _ O
node _ _ O
is _ _ O
128 _ _ O
bytes _ _ O
: _ _ O
typedef _ _ O
struct _ _ O
node _ _ O
{ _ _ O
struct _ _ O
node _ _ O
* _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
8 _ _ O
bytes _ _ O
int _ _ O
data _ _ O
[ _ _ O
30 _ _ O
] _ _ O
; _ _ O
/ _ _ O
/ _ _ O
120 _ _ O
bytes _ _ O
} _ _ O
node_t _ _ O
; _ _ O
● _ _ O
Working _ _ O
set _ _ O
: _ _ O
amount _ _ O
of _ _ O
memory _ _ O
program _ _ O
accesses _ _ O
during _ _ O
a _ _ O
phase _ _ O
o _ _ O
For _ _ O
linked-list.c _ _ O
, _ _ O
working _ _ O
set _ _ O
is _ _ O
the _ _ O
entire _ _ O
linked _ _ O
list _ _ O
▪ _ _ O
Program _ _ O
accesses _ _ O
entire _ _ O
linked _ _ O
list _ _ O
in _ _ O
a _ _ O
loop _ _ O
over _ _ O
and _ _ O
over _ _ O
again _ _ O
o _ _ O
If _ _ O
there _ _ O
are _ _ O
8 _ _ O
nodes _ _ O
in _ _ O
linked _ _ O
list _ _ O
, _ _ O
working _ _ O
set _ _ O
size _ _ O
= _ _ O
128 _ _ O
* _ _ O
8 _ _ O
= _ _ O
1 _ _ O
KB _ _ O
● _ _ O
When _ _ O
working _ _ O
set _ _ O
overflows _ _ O
cache _ _ O
capacity _ _ O
, _ _ O
start _ _ O
to _ _ O
see _ _ O
cache _ _ O
misses _ _ O
o _ _ O
Miss _ _ O
increase _ _ O
can _ _ O
be _ _ O
drastic _ _ O
, _ _ O
almost _ _ O
like _ _ O
a _ _ O
step _ _ O
function _ _ O
o _ _ O
Suppose _ _ O
cache _ _ O
size _ _ O
is _ _ O
1 _ _ O
KB _ _ O
and _ _ O
nodes _ _ O
increase _ _ O
from _ _ O
8 _ _ O
→ _ _ O
9 _ _ O
▪ _ _ O
When _ _ O
8 _ _ O
nodes _ _ O
: _ _ O
always _ _ O
hit _ _ O
( _ _ O
since _ _ O
entire _ _ O
list _ _ O
in _ _ O
contained _ _ O
in _ _ O
cache _ _ O
) _ _ O
▪ _ _ O
When _ _ O
9 _ _ O
nodes _ _ O
: _ _ O
always _ _ O
miss _ _ O
( _ _ O
if _ _ O
least _ _ O
recent _ _ O
node _ _ O
is _ _ O
replaced _ _ O
first _ _ O
) _ _ O
50 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
– _ _ O
Other _ _ O
Questions _ _ O
● _ _ O
Why _ _ O
the _ _ O
step _ _ O
up _ _ O
in _ _ O
L2 _ _ O
cache _ _ O
misses _ _ O
between _ _ O
1000 _ _ O
– _ _ O
5000 _ _ O
nodes _ _ O
? _ _ O
o _ _ O
L2 _ _ O
cache _ _ O
size _ _ O
is _ _ O
256 _ _ O
KB _ _ O
o _ _ O
Number _ _ O
of _ _ O
nodes _ _ O
that _ _ O
can _ _ O
fit _ _ O
= _ _ O
256 _ _ O
KB _ _ O
/ _ _ O
128 _ _ O
= _ _ O
2048 _ _ O
● _ _ O
Why _ _ O
the _ _ O
step _ _ O
up _ _ O
in _ _ O
L3 _ _ O
cache _ _ O
misses _ _ O
between _ _ O
100 _ _ O
k _ _ O
– _ _ O
500 _ _ O
k _ _ O
nodes _ _ O
? _ _ O
o _ _ O
L3 _ _ O
cache _ _ O
size _ _ O
is _ _ O
25 _ _ O
MB _ _ O
o _ _ O
Number _ _ O
of _ _ O
nodes _ _ O
that _ _ O
can _ _ O
fit _ _ O
= _ _ O
25 _ _ O
MB _ _ O
/ _ _ O
128 _ _ O
≈ _ _ O
200 _ _ O
k _ _ O
● _ _ O
Why _ _ O
the _ _ O
step _ _ O
up _ _ O
in _ _ O
L1 _ _ O
cache _ _ O
misses _ _ O
between _ _ O
500 _ _ O
– _ _ O
1000 _ _ O
nodes _ _ O
? _ _ O
o _ _ O
L1 _ _ O
d-cache _ _ O
size _ _ O
is _ _ O
32 _ _ O
KB _ _ O
o _ _ O
Number _ _ O
of _ _ O
nodes _ _ O
that _ _ O
can _ _ O
fit _ _ O
= _ _ O
32 _ _ O
KB _ _ O
/ _ _ O
128 _ _ O
= _ _ O
256 _ _ O
o _ _ O
So _ _ O
, _ _ O
in _ _ O
theory _ _ O
you _ _ O
should _ _ O
already _ _ O
see _ _ O
a _ _ O
step _ _ O
up _ _ O
at _ _ O
500 _ _ O
nodes _ _ O
o _ _ O
Apparently _ _ O
, _ _ O
CPU _ _ O
does _ _ O
n’t _ _ O
use _ _ O
least-recently-used _ _ O
( _ _ O
LRU _ _ O
) _ _ O
replacement _ _ O
o _ _ O
According _ _ O
to _ _ O
another _ _ O
reverse _ _ O
engineering _ _ O
paper _ _ O
, _ _ O
Intel _ _ O
uses _ _ O
PLRU _ _ O
Ref _ _ O
: _ _ O
“ _ _ O
CacheQuery _ _ O
: _ _ O
Learning _ _ O
Replacement _ _ O
Policies _ _ O
from _ _ O
Hardware _ _ O
Caches _ _ O
” _ _ O
by _ _ O
Vila _ _ O
et _ _ O
al _ _ O
. _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
arxiv.org _ _ O
/ _ _ O
pdf _ _ O
/ _ _ O
1912.09770.pdf _ _ O
51 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
– _ _ O
Other _ _ O
Questions _ _ O
Almost _ _ O
only _ _ O
L1 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
L1 _ _ O
+ _ _ O
L2 _ _ O
+ _ _ O
L3 _ _ O
+ _ _ O
Memory _ _ O
L1 _ _ O
+ _ _ O
Memory _ _ O
( _ _ O
mostly _ _ O
) _ _ O
● _ _ O
Why _ _ O
did _ _ O
L1 _ _ O
cache _ _ O
miss _ _ O
rate _ _ O
saturate _ _ O
at _ _ O
around _ _ O
20 _ _ O
% _ _ O
? _ _ O
o _ _ O
Should _ _ O
n’t _ _ O
it _ _ O
keep _ _ O
increasing _ _ O
with _ _ O
more _ _ O
nodes _ _ O
like _ _ O
L2 _ _ O
and _ _ O
L3 _ _ O
? _ _ O
52 _ _ O

Linked _ _ O
List _ _ O
Cache _ _ O
Load _ _ O
Miss _ _ O
Rates _ _ O
– _ _ O
Other _ _ O
Questions _ _ O
[ _ _ O
linked-list.c _ _ O
] _ _ O
void _ _ O
* _ _ O
run _ _ O
( _ _ O
void _ _ O
* _ _ O
unused _ _ O
) _ _ O
{ _ _ O
node_t _ _ O
* _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
ACCESSES _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
if _ _ O
( _ _ O
current _ _ O
= _ _ O
= _ _ O
NULL _ _ O
) _ _ O
current _ _ O
= _ _ O
head _ _ O
; _ _ O
else _ _ O
current _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
; _ _ O
} _ _ O
} _ _ O
[ _ _ O
objdump _ _ O
-S _ _ O
linked-list _ _ O
] _ _ O
0000000000400739 _ _ O
< _ _ O
run _ _ O
> _ _ O
: _ _ O
... _ _ O
400741 _ _ O
: _ _ O
mov _ _ O
0x200920 _ _ O
( _ _ O
%rip _ _ O
) _ _ O
, _ _ O
%rax _ _ O
# _ _ O
% _ _ O
rax _ _ O
= _ _ O
head _ _ O
400748 _ _ O
: _ _ O
mov _ _ O
% _ _ O
rax _ _ O
, _ _ O
-0x8 _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
current _ _ O
= _ _ O
% _ _ O
rax _ _ O
40074c _ _ O
: _ _ O
movl _ _ O
$ _ _ O
0x0 _ _ O
, _ _ O
-0xc _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
i _ _ O
= _ _ O
0 _ _ O
400753 _ _ O
: _ _ O
jmp _ _ O
400778 _ _ O
# _ _ O
jump _ _ O
to _ _ O
i _ _ O
< _ _ O
ACCESSES _ _ O
comparison _ _ O
400755 _ _ O
: _ _ O
cmpq _ _ O
$ _ _ O
0x0 _ _ O
, _ _ O
-0x8 _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
current _ _ O
= _ _ O
= _ _ O
NULL _ _ O
? _ _ O
40075a _ _ O
: _ _ O
jne _ _ O
400769 _ _ O
# _ _ O
jump _ _ O
to _ _ O
else _ _ O
branch _ _ O
if _ _ O
not _ _ O
equal _ _ O
Within _ _ O
a _ _ O
typical _ _ O
iteration _ _ O
in _ _ O
for _ _ O
loop _ _ O
: _ _ O
40075c _ _ O
: _ _ O
mov _ _ O
0x200905 _ _ O
( _ _ O
%rip _ _ O
) _ _ O
, _ _ O
%rax _ _ O
# _ _ O
% _ _ O
rax _ _ O
= _ _ O
head _ _ O
4 _ _ O
blue _ _ O
loads _ _ O
that _ _ O
hit _ _ O
in _ _ O
L1 _ _ O
: _ _ O
400763 _ _ O
: _ _ O
mov _ _ O
% _ _ O
rax _ _ O
, _ _ O
-0x8 _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
current _ _ O
= _ _ O
% _ _ O
rax _ _ O
• _ _ O
2 _ _ O
loads _ _ O
each _ _ O
of _ _ O
local _ _ O
vars _ _ O
current _ _ O
, _ _ O
i _ _ O
400767 _ _ O
: _ _ O
jmp _ _ O
400774 _ _ O
# _ _ O
jump _ _ O
to _ _ O
end _ _ O
of _ _ O
if-then-else _ _ O
• _ _ O
Read _ _ O
frequently _ _ O
so _ _ O
never _ _ O
replaced _ _ O
400769 _ _ O
: _ _ O
mov _ _ O
-0x8 _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
, _ _ O
%rax _ _ O
# _ _ O
% _ _ O
rax _ _ O
= _ _ O
current _ _ O
1 _ _ O
red _ _ O
load _ _ O
that _ _ O
misses _ _ O
in _ _ O
L1 _ _ O
: _ _ O
40076d _ _ O
: _ _ O
mov _ _ O
( _ _ O
% _ _ O
rax _ _ O
) _ _ O
, _ _ O
%rax _ _ O
# _ _ O
% _ _ O
rax _ _ O
= _ _ O
current- _ _ O
> _ _ O
next _ _ O
• _ _ O
current- _ _ O
> _ _ O
next _ _ O
( _ _ O
next _ _ O
field _ _ O
of _ _ O
node _ _ O
) _ _ O
400770 _ _ O
: _ _ O
mov _ _ O
% _ _ O
rax _ _ O
, _ _ O
-0x8 _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
current _ _ O
= _ _ O
% _ _ O
rax _ _ O
• _ _ O
Node _ _ O
may _ _ O
not _ _ O
be _ _ O
in _ _ O
cache _ _ O
and _ _ O
miss _ _ O
400774 _ _ O
: _ _ O
addl _ _ O
$ _ _ O
0x1 _ _ O
, _ _ O
-0xc _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
i++ _ _ O
( _ _ O
e.g. _ _ O
due _ _ O
to _ _ O
a _ _ O
capacity _ _ O
miss _ _ O
) _ _ O
400778 _ _ O
: _ _ O
cmpl _ _ O
$ _ _ O
0x3b9ac9ff _ _ O
, _ _ O
-0xc _ _ O
( _ _ O
%rbp _ _ O
) _ _ O
# _ _ O
i _ _ O
< _ _ O
ACCESSES _ _ O
? _ _ O
 _ _ O
1 _ _ O
miss _ _ O
/ _ _ O
5 _ _ O
loads _ _ O
= _ _ O
20 _ _ O
% _ _ O
miss _ _ O
rate _ _ O
40077f _ _ O
: _ _ O
jle _ _ O
400755 _ _ O
# _ _ O
jump _ _ O
to _ _ O
head _ _ O
of _ _ O
loop _ _ O
if _ _ O
less _ _ O
than _ _ O
53 _ _ O

How _ _ O
about _ _ O
Linked _ _ O
List _ _ O
with _ _ O
No _ _ O
Data _ _ O
? _ _ O
● _ _ O
Linked _ _ O
list _ _ O
, _ _ O
no _ _ O
data _ _ O
suffered _ _ O
almost _ _ O
no _ _ O
CPI _ _ O
degradation _ _ O
. _ _ O
Why _ _ O
? _ _ O
54 _ _ O

Linked _ _ O
List _ _ O
w _ _ O
/ _ _ O
Data _ _ O
vs. _ _ O
w/o _ _ O
Data _ _ O
● _ _ O
Linked _ _ O
list _ _ O
with _ _ O
data _ _ O
● _ _ O
Linked _ _ O
list _ _ O
with _ _ O
no _ _ O
data _ _ O
55 _ _ O

Linked _ _ O
List _ _ O
w _ _ O
/ _ _ O
Data _ _ O
vs. _ _ O
w/o _ _ O
Data _ _ O
. _ _ O
Why _ _ O
? _ _ O
● _ _ O
The _ _ O
size _ _ O
of _ _ O
a _ _ O
node _ _ O
with _ _ O
no _ _ O
data _ _ O
is _ _ O
only _ _ O
8 _ _ O
bytes _ _ O
: _ _ O
typedef _ _ O
struct _ _ O
node _ _ O
{ _ _ O
struct _ _ O
node _ _ O
* _ _ O
next _ _ O
; _ _ O
/ _ _ O
/ _ _ O
8 _ _ O
bytes _ _ O
/ _ _ O
/ _ _ O
no _ _ O
data _ _ O
} _ _ O
node_t _ _ O
; _ _ O
● _ _ O
Compared _ _ O
to _ _ O
128 _ _ O
bytes _ _ O
with _ _ O
data _ _ O
, _ _ O
can _ _ O
fit _ _ O
in _ _ O
16X _ _ O
more _ _ O
nodes _ _ O
in _ _ O
cache _ _ O
o _ _ O
Temporal _ _ O
locality _ _ O
: _ _ O
More _ _ O
likely _ _ O
that _ _ O
a _ _ O
node _ _ O
will _ _ O
be _ _ O
present _ _ O
in _ _ O
cache _ _ O
● _ _ O
How _ _ O
about _ _ O
L1 _ _ O
cache _ _ O
miss _ _ O
rate _ _ O
that _ _ O
hovers _ _ O
around _ _ O
10 _ _ O
% _ _ O
instead _ _ O
of _ _ O
20 _ _ O
% _ _ O
? _ _ O
o _ _ O
By _ _ O
107 _ _ O
nodes _ _ O
, _ _ O
there _ _ O
is _ _ O
no _ _ O
temporal _ _ O
locality _ _ O
with _ _ O
respect _ _ O
to _ _ O
the _ _ O
L1 _ _ O
cache _ _ O
o _ _ O
Spatial _ _ O
locality _ _ O
must _ _ O
be _ _ O
responsible _ _ O
for _ _ O
the _ _ O
reduction _ _ O
in _ _ O
miss _ _ O
rate _ _ O
56 _ _ O

Linked _ _ O
List _ _ O
w _ _ O
/ _ _ O
Data _ _ O
vs. _ _ O
w/o _ _ O
Data _ _ O
. _ _ O
Why _ _ O
? _ _ O
● _ _ O
Nodes _ _ O
of _ _ O
the _ _ O
linked _ _ O
list _ _ O
are _ _ O
malloced _ _ O
one _ _ O
by _ _ O
one _ _ O
in _ _ O
a _ _ O
loop _ _ O
: _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
items _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
node_t _ _ O
* _ _ O
n _ _ O
= _ _ O
( _ _ O
node_t* _ _ O
) _ _ O
malloc _ _ O
( _ _ O
sizeof _ _ O
( _ _ O
node_t _ _ O
) _ _ O
) _ _ O
; _ _ O
… _ _ O
} _ _ O
o _ _ O
I _ _ O
have _ _ O
no _ _ O
idea _ _ O
where _ _ O
glibc _ _ O
malloc _ _ O
decides _ _ O
to _ _ O
allocate _ _ O
each _ _ O
node _ _ O
● _ _ O
But _ _ O
knowing _ _ O
each _ _ O
cache _ _ O
block _ _ O
is _ _ O
64 _ _ O
bytes _ _ O
long _ _ O
in _ _ O
the _ _ O
Xeon _ _ O
E5 _ _ O
processor _ _ O
o _ _ O
Let _ _ O
’s _ _ O
say _ _ O
multiple _ _ O
nodes _ _ O
are _ _ O
allocated _ _ O
on _ _ O
same _ _ O
cache _ _ O
block _ _ O
: _ _ O
node _ _ O
1 _ _ O
meta-data _ _ O
meta-data _ _ O
node _ _ O
37 _ _ O
meta-data _ _ O
meta-data _ _ O
node _ _ O
23 _ _ O
meta-data _ _ O
o _ _ O
Then _ _ O
even _ _ O
if _ _ O
access _ _ O
to _ _ O
node _ _ O
1 _ _ O
misses _ _ O
, _ _ O
due _ _ O
to _ _ O
a _ _ O
capacity _ _ O
miss _ _ O
, _ _ O
accesses _ _ O
to _ _ O
nodes _ _ O
37 _ _ O
and _ _ O
23 _ _ O
that _ _ O
soon _ _ O
follow _ _ O
will _ _ O
hit _ _ O
! _ _ O
o _ _ O
This _ _ O
is _ _ O
assuming _ _ O
there _ _ O
is _ _ O
some _ _ O
spatial _ _ O
locality _ _ O
in _ _ O
how _ _ O
malloc _ _ O
allocates _ _ O
57 _ _ O

Data _ _ O
structure _ _ O
with _ _ O
most _ _ O
spatial _ _ O
locality _ _ O
: _ _ O
Array _ _ O
● _ _ O
Elements _ _ O
of _ _ O
an _ _ O
array _ _ O
are _ _ O
guaranteed _ _ O
to _ _ O
be _ _ O
in _ _ O
contiguous _ _ O
memory _ _ O
: _ _ O
void _ _ O
* _ _ O
create _ _ O
( _ _ O
void _ _ O
* _ _ O
unused _ _ O
) _ _ O
{ _ _ O
head _ _ O
= _ _ O
( _ _ O
node_t _ _ O
* _ _ O
) _ _ O
malloc _ _ O
( _ _ O
sizeof _ _ O
( _ _ O
node_t _ _ O
) _ _ O
* _ _ O
items _ _ O
) _ _ O
; _ _ O
… _ _ O
} _ _ O
● _ _ O
Each _ _ O
cache _ _ O
block _ _ O
is _ _ O
64 _ _ O
bytes _ _ O
long _ _ O
in _ _ O
the _ _ O
Xeon _ _ O
E5 _ _ O
processor _ _ O
o _ _ O
Now _ _ O
8 _ _ O
elements _ _ O
are _ _ O
guaranteed _ _ O
to _ _ O
be _ _ O
on _ _ O
same _ _ O
cache _ _ O
line _ _ O
, _ _ O
in _ _ O
order _ _ O
: _ _ O
node _ _ O
0 _ _ O
node _ _ O
1 _ _ O
node _ _ O
2 _ _ O
node _ _ O
3 _ _ O
node _ _ O
4 _ _ O
node _ _ O
5 _ _ O
node _ _ O
6 _ _ O
node _ _ O
7 _ _ O
o _ _ O
Even _ _ O
with _ _ O
cold _ _ O
cache _ _ O
, _ _ O
only _ _ O
1 _ _ O
out _ _ O
of _ _ O
8 _ _ O
nodes _ _ O
miss _ _ O
( _ _ O
1 _ _ O
/ _ _ O
8 _ _ O
= _ _ O
12.5 _ _ O
% _ _ O
miss _ _ O
rate _ _ O
) _ _ O
o _ _ O
Assuming _ _ O
that _ _ O
nodes _ _ O
are _ _ O
accessed _ _ O
sequentially _ _ O
▪ _ _ O
If _ _ O
accessed _ _ O
in _ _ O
random _ _ O
order _ _ O
, _ _ O
no _ _ O
spatial _ _ O
locality _ _ O
, _ _ O
even _ _ O
with _ _ O
array _ _ O
o _ _ O
True _ _ O
regardless _ _ O
of _ _ O
capacity _ _ O
( _ _ O
even _ _ O
if _ _ O
cache _ _ O
contained _ _ O
only _ _ O
a _ _ O
single _ _ O
block _ _ O
) _ _ O
58 _ _ O

Let _ _ O
’s _ _ O
look _ _ O
at _ _ O
the _ _ O
CPI _ _ O
of _ _ O
arrays _ _ O
finally _ _ O
● _ _ O
Array _ _ O
, _ _ O
no _ _ O
data _ _ O
did _ _ O
very _ _ O
well _ _ O
as _ _ O
expected _ _ O
o _ _ O
The _ _ O
most _ _ O
spatial _ _ O
locality _ _ O
of _ _ O
the _ _ O
four _ _ O
benchmarks _ _ O
( _ _ O
contiguous _ _ O
nodes _ _ O
) _ _ O
o _ _ O
Smallest _ _ O
memory _ _ O
footprint _ _ O
so _ _ O
can _ _ O
also _ _ O
leverage _ _ O
temporal _ _ O
locality _ _ O
the _ _ O
best _ _ O
● _ _ O
Array _ _ O
performed _ _ O
the _ _ O
same _ _ O
as _ _ O
array _ _ O
, _ _ O
no _ _ O
data _ _ O
. _ _ O
How _ _ O
come _ _ O
? _ _ O
o _ _ O
No _ _ O
spatial _ _ O
locality _ _ O
since _ _ O
each _ _ O
node _ _ O
takes _ _ O
up _ _ O
two _ _ O
64-byte _ _ O
cache _ _ O
blocks _ _ O
o _ _ O
Has _ _ O
much _ _ O
larger _ _ O
memory _ _ O
footprint _ _ O
compared _ _ O
to _ _ O
array _ _ O
, _ _ O
no _ _ O
data _ _ O
o _ _ O
This _ _ O
is _ _ O
the _ _ O
real _ _ O
mystery _ _ O
. _ _ O
We _ _ O
will _ _ O
learn _ _ O
more _ _ O
about _ _ O
it _ _ O
as _ _ O
we _ _ O
go _ _ O
on _ _ O
. _ _ O
☺ _ _ O
59 _ _ O

Impact _ _ O
of _ _ O
Memory _ _ O
On _ _ O
Performance _ _ O
60 _ _ O

Impact _ _ O
of _ _ O
Memory _ _ O
on _ _ O
Performance _ _ O
● _ _ O
CPU _ _ O
Cycles _ _ O
= _ _ O
CPU _ _ O
Compute _ _ O
Cycles _ _ O
+ _ _ O
Memory _ _ O
Stall _ _ O
Cycles _ _ O
o _ _ O
CPU _ _ O
Compute _ _ O
Cycles _ _ O
= _ _ O
cycles _ _ O
where _ _ O
CPU _ _ O
is _ _ O
not _ _ O
stalled _ _ O
on _ _ O
memory _ _ O
o _ _ O
Memory _ _ O
Stall _ _ O
Cycles _ _ O
= _ _ O
cycles _ _ O
where _ _ O
CPU _ _ O
is _ _ O
waiting _ _ O
for _ _ O
memory _ _ O
● _ _ O
Why _ _ O
do _ _ O
we _ _ O
need _ _ O
to _ _ O
differentiate _ _ O
between _ _ O
the _ _ O
two _ _ O
? _ _ O
o _ _ O
Because _ _ O
each _ _ O
are _ _ O
improved _ _ O
by _ _ O
different _ _ O
design _ _ O
features _ _ O
! _ _ O
● _ _ O
HW _ _ O
/ _ _ O
SW _ _ O
design _ _ O
features _ _ O
that _ _ O
impact _ _ O
CPU _ _ O
Compute _ _ O
Cycles _ _ O
: _ _ O
o _ _ O
HW _ _ O
: _ _ O
Pipelining _ _ O
, _ _ O
branch _ _ O
prediction _ _ O
, _ _ O
wide _ _ O
execution _ _ O
, _ _ O
out-of-order _ _ O
o _ _ O
SW _ _ O
: _ _ O
Optimizing _ _ O
the _ _ O
computation _ _ O
in _ _ O
the _ _ O
program _ _ O
● _ _ O
HW _ _ O
/ _ _ O
SW _ _ O
design _ _ O
features _ _ O
that _ _ O
impact _ _ O
Memory _ _ O
Stall _ _ O
Cycles _ _ O
: _ _ O
o _ _ O
HW _ _ O
: _ _ O
Caches _ _ O
, _ _ O
write _ _ O
buffer _ _ O
, _ _ O
prefetcher _ _ O
( _ _ O
we _ _ O
have _ _ O
n’t _ _ O
learned _ _ O
this _ _ O
yet _ _ O
) _ _ O
o _ _ O
SW _ _ O
: _ _ O
Optimizing _ _ O
memory _ _ O
access _ _ O
pattern _ _ O
of _ _ O
program _ _ O
( _ _ O
Taking _ _ O
into _ _ O
consideration _ _ O
temporal _ _ O
and _ _ O
spatial _ _ O
locality _ _ O
) _ _ O
61 _ _ O

How _ _ O
about _ _ O
overclocking _ _ O
using _ _ O
DVFS _ _ O
? _ _ O
● _ _ O
CPU _ _ O
Time _ _ O
= _ _ O
CPU _ _ O
Cycles _ _ O
* _ _ O
Cycle _ _ O
Time _ _ O
= _ _ O
( _ _ O
CPU _ _ O
Compute _ _ O
Cycles _ _ O
+ _ _ O
Memory _ _ O
Stall _ _ O
Cycles _ _ O
) _ _ O
* _ _ O
Cycle _ _ O
Time _ _ O
● _ _ O
What _ _ O
if _ _ O
we _ _ O
halved _ _ O
the _ _ O
Cycle _ _ O
Time _ _ O
using _ _ O
DVFS _ _ O
? _ _ O
o _ _ O
Memory _ _ O
Stall _ _ O
Cycles _ _ O
could _ _ O
increase _ _ O
by _ _ O
up _ _ O
to _ _ O
2X _ _ O
! _ _ O
o _ _ O
Why _ _ O
? _ _ O
DRAM _ _ O
speed _ _ O
remains _ _ O
the _ _ O
same _ _ O
, _ _ O
so _ _ O
you _ _ O
need _ _ O
twice _ _ O
the _ _ O
cycles _ _ O
. _ _ O
▪ _ _ O
The _ _ O
bus _ _ O
( _ _ O
wire _ _ O
) _ _ O
that _ _ O
connects _ _ O
CPU _ _ O
to _ _ O
DRAM _ _ O
is _ _ O
not _ _ O
getting _ _ O
any _ _ O
faster _ _ O
▪ _ _ O
The _ _ O
DRAM _ _ O
chip _ _ O
itself _ _ O
is _ _ O
not _ _ O
getting _ _ O
clocked _ _ O
any _ _ O
faster _ _ O
o _ _ O
How _ _ O
about _ _ O
caches _ _ O
? _ _ O
Same _ _ O
number _ _ O
of _ _ O
cycles _ _ O
regardless _ _ O
of _ _ O
DVFS _ _ O
. _ _ O
● _ _ O
If _ _ O
most _ _ O
of _ _ O
your _ _ O
time _ _ O
is _ _ O
spent _ _ O
accessing _ _ O
DRAM _ _ O
, _ _ O
then _ _ O
overclocking _ _ O
is _ _ O
useless _ _ O
o _ _ O
Memory _ _ O
Stall _ _ O
Time _ _ O
( _ _ O
Memory _ _ O
Stall _ _ O
Cycles _ _ O
* _ _ O
Cycle _ _ O
Time _ _ O
) _ _ O
is _ _ O
mostly _ _ O
constant _ _ O
62 _ _ O

Memory _ _ O
Latency _ _ O
vs. _ _ O
Memory _ _ O
Bandwidth _ _ O
● _ _ O
Memory _ _ O
stall _ _ O
cycles _ _ O
comes _ _ O
from _ _ O
two _ _ O
sources _ _ O
: _ _ O
o _ _ O
Memory _ _ O
latency _ _ O
: _ _ O
seconds _ _ O
to _ _ O
handle _ _ O
a _ _ O
single _ _ O
memory _ _ O
request _ _ O
vs. _ _ O
o _ _ O
Memory _ _ O
bandwidth _ _ O
: _ _ O
maximum _ _ O
requests _ _ O
handled _ _ O
per _ _ O
second _ _ O
vs. _ _ O
63 _ _ O

Memory _ _ O
bandwidth _ _ O
puts _ _ O
a _ _ O
ceiling _ _ O
on _ _ O
performance _ _ O
● _ _ O
Memory _ _ O
latency _ _ O
can _ _ O
be _ _ O
surmounted _ _ O
by _ _ O
smart _ _ O
scheduling _ _ O
o _ _ O
Either _ _ O
by _ _ O
compiler _ _ O
or _ _ O
by _ _ O
instruction _ _ O
queue _ _ O
scheduling _ _ O
by _ _ O
CPU _ _ O
● _ _ O
No _ _ O
real _ _ O
way _ _ O
to _ _ O
surmount _ _ O
memory _ _ O
bandwidth _ _ O
limit _ _ O
o _ _ O
No _ _ O
matter _ _ O
how _ _ O
much _ _ O
scheduling _ _ O
you _ _ O
do _ _ O
, _ _ O
same _ _ O
bandwidth _ _ O
● _ _ O
When _ _ O
you _ _ O
get _ _ O
a _ _ O
traffic _ _ O
jam _ _ O
, _ _ O
performance _ _ O
is _ _ O
dictated _ _ O
by _ _ O
bandwidth _ _ O
o _ _ O
How _ _ O
quickly _ _ O
you _ _ O
pull _ _ O
data _ _ O
in _ _ O
, _ _ O
rather _ _ O
than _ _ O
how _ _ O
fast _ _ O
you _ _ O
process _ _ O
it _ _ O
o _ _ O
Operational _ _ O
intensity _ _ O
= _ _ O
work _ _ O
( _ _ O
FLOP _ _ O
) _ _ O
per _ _ O
memory _ _ O
access _ _ O
( _ _ O
byte _ _ O
) _ _ O
o _ _ O
Performance _ _ O
= _ _ O
work _ _ O
/ _ _ O
second _ _ O
= _ _ O
work _ _ O
/ _ _ O
byte _ _ O
* _ _ O
byte _ _ O
/ _ _ O
second _ _ O
= _ _ O
operational _ _ O
intensity _ _ O
* _ _ O
memory _ _ O
bandwidth _ _ O
→ _ _ O
Linear _ _ O
relationship _ _ O
between _ _ O
performance _ _ O
and _ _ O
operational _ _ O
intensity _ _ O
64 _ _ O

The _ _ O
Roofline _ _ O
Model _ _ O
: _ _ O
A _ _ O
bird _ _ O
’s _ _ O
eye _ _ O
view _ _ O
of _ _ O
performance _ _ O
● _ _ O
Roofline _ _ O
: _ _ O
theoretical _ _ O
performance _ _ O
achievable _ _ O
given _ _ O
an _ _ O
intensity _ _ O
o _ _ O
Formed _ _ O
by _ _ O
memory _ _ O
bandwidth _ _ O
bounds _ _ O
+ _ _ O
peak _ _ O
CPU _ _ O
performance _ _ O
Where _ _ O
do _ _ O
these _ _ O
gaps _ _ O
come _ _ O
from _ _ O
? _ _ O
65 _ _ O

Memory _ _ O
bound _ _ O
vs. _ _ O
compute _ _ O
bound _ _ O
apps _ _ O
● _ _ O
I _ _ O
= _ _ O
intensity _ _ O
, _ _ O
β _ _ O
= _ _ O
peak _ _ O
bandwidth _ _ O
, _ _ O
π _ _ O
= _ _ O
peak _ _ O
performance _ _ O
66 _ _ O

Effect _ _ O
of _ _ O
Cache _ _ O
on _ _ O
Roofline _ _ O
Model _ _ O
● _ _ O
With _ _ O
caching _ _ O
, _ _ O
apps _ _ O
shift _ _ O
upwards _ _ O
and _ _ O
rightwards _ _ O
Reduction _ _ O
in _ _ O
memory _ _ O
latency _ _ O
makes _ _ O
CPU _ _ O
efficient _ _ O
Reduction _ _ O
in _ _ O
memory _ _ O
accesses _ _ O
increases _ _ O
operational _ _ O
intensity _ _ O
67 _ _ O



