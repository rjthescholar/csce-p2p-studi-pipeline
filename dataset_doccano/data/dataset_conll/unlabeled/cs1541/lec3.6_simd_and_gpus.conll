unlabeled|cs1541|lec3.6_simd_and_gpus
-DOCSTART- -X- -X- O

SIMD _ _ O
and _ _ O
GPUs _ _ O
CS _ _ O
1541 _ _ O
Wonsun _ _ O
Ahn _ _ O

SIMD _ _ O
Architectures _ _ O
2 _ _ O

ISA _ _ O
not _ _ O
optimized _ _ O
for _ _ O
data _ _ O
parallel _ _ O
workloads _ _ O
‚óè _ _ O
This _ _ O
loop _ _ O
does _ _ O
multiply _ _ O
accumulate _ _ O
( _ _ O
MAC _ _ O
) _ _ O
: _ _ O
for _ _ O
( _ _ O
int _ _ O
i _ _ O
= _ _ O
0 _ _ O
; _ _ O
i _ _ O
< _ _ O
64 _ _ O
; _ _ O
i++ _ _ O
) _ _ O
{ _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
a _ _ O
* _ _ O
x _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
} _ _ O
o _ _ O
A _ _ O
common _ _ O
operation _ _ O
in _ _ O
digital _ _ O
signal _ _ O
processing _ _ O
( _ _ O
DAXPY _ _ O
) _ _ O
‚óè _ _ O
Note _ _ O
how _ _ O
we _ _ O
apply _ _ O
the _ _ O
same _ _ O
MAC _ _ O
operation _ _ O
on _ _ O
each _ _ O
data _ _ O
item _ _ O
o _ _ O
This _ _ O
is _ _ O
how _ _ O
many _ _ O
data _ _ O
parallel _ _ O
workloads _ _ O
look _ _ O
like _ _ O
‚óè _ _ O
A _ _ O
conventional _ _ O
ISA _ _ O
( _ _ O
likes _ _ O
MIPS _ _ O
) _ _ O
is _ _ O
not _ _ O
optimal _ _ O
for _ _ O
encoding _ _ O
this _ _ O
o _ _ O
Results _ _ O
in _ _ O
wasted _ _ O
work _ _ O
and _ _ O
suboptimal _ _ O
performance _ _ O
o _ _ O
Let _ _ O
‚Äôs _ _ O
look _ _ O
at _ _ O
the _ _ O
actual _ _ O
MIPS _ _ O
translation _ _ O
3 _ _ O

MIPS _ _ O
code _ _ O
for _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùíÇ _ _ O
‚àó _ _ O
ùíô _ _ O
ùíä _ _ O
+ _ _ O
ùíö _ _ O
ùíä _ _ O
l.d _ _ O
$ _ _ O
f0 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$sp _ _ O
) _ _ O
; _ _ O
$ _ _ O
f0 _ _ O
= _ _ O
a _ _ O
addi _ _ O
$ _ _ O
s2 _ _ O
, _ _ O
$s0 _ _ O
, _ _ O
512 _ _ O
; _ _ O
64 _ _ O
elements _ _ O
( _ _ O
64*8=512 _ _ O
bytes _ _ O
) _ _ O
loop _ _ O
: _ _ O
l.d _ _ O
$ _ _ O
f2 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$s0 _ _ O
) _ _ O
; _ _ O
$ _ _ O
f2 _ _ O
= _ _ O
x _ _ O
( _ _ O
i _ _ O
) _ _ O
mul.d _ _ O
$ _ _ O
f2 _ _ O
, _ _ O
$f2 _ _ O
, _ _ O
$f0 _ _ O
; _ _ O
$ _ _ O
f2 _ _ O
= _ _ O
a _ _ O
* _ _ O
x _ _ O
( _ _ O
i _ _ O
) _ _ O
l.d _ _ O
$ _ _ O
f4 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$s1 _ _ O
) _ _ O
; _ _ O
$ _ _ O
f4 _ _ O
= _ _ O
y _ _ O
( _ _ O
i _ _ O
) _ _ O
add.d _ _ O
$ _ _ O
f4 _ _ O
, _ _ O
$f4 _ _ O
, _ _ O
$f2 _ _ O
; _ _ O
$ _ _ O
f4 _ _ O
= _ _ O
a _ _ O
* _ _ O
x _ _ O
( _ _ O
i _ _ O
) _ _ O
+ _ _ O
y _ _ O
( _ _ O
i _ _ O
) _ _ O
s.d _ _ O
$ _ _ O
f4 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$s1 _ _ O
) _ _ O
; _ _ O
y _ _ O
( _ _ O
i _ _ O
) _ _ O
= _ _ O
$ _ _ O
f4 _ _ O
addi _ _ O
$ _ _ O
s0 _ _ O
, _ _ O
$s0 _ _ O
, _ _ O
8 _ _ O
; _ _ O
increment _ _ O
index _ _ O
to _ _ O
x _ _ O
addi _ _ O
$ _ _ O
s1 _ _ O
, _ _ O
$s1 _ _ O
, _ _ O
8 _ _ O
; _ _ O
increment _ _ O
index _ _ O
to _ _ O
y _ _ O
subu _ _ O
$ _ _ O
t0 _ _ O
, _ _ O
$s2 _ _ O
, _ _ O
$s0 _ _ O
; _ _ O
evaluate _ _ O
i _ _ O
< _ _ O
64 _ _ O
loop _ _ O
condition _ _ O
bne _ _ O
$ _ _ O
t0 _ _ O
, _ _ O
$zero _ _ O
, _ _ O
loop _ _ O
; _ _ O
loop _ _ O
if _ _ O
not _ _ O
done _ _ O
‚óè _ _ O
Blue _ _ O
instructions _ _ O
do _ _ O
n‚Äôt _ _ O
do _ _ O
actual _ _ O
computation _ _ O
. _ _ O
There _ _ O
for _ _ O
indexing _ _ O
and _ _ O
loop _ _ O
control _ _ O
. _ _ O
o _ _ O
Is _ _ O
there _ _ O
a _ _ O
way _ _ O
to _ _ O
avoid _ _ O
? _ _ O
Loop _ _ O
unrolling _ _ O
yes _ _ O
. _ _ O
But _ _ O
that _ _ O
causes _ _ O
code _ _ O
bloat _ _ O
! _ _ O
‚óè _ _ O
Red _ _ O
instructions _ _ O
do _ _ O
computation _ _ O
. _ _ O
But _ _ O
why _ _ O
decode _ _ O
them _ _ O
over _ _ O
and _ _ O
over _ _ O
again _ _ O
? _ _ O
o _ _ O
Is _ _ O
there _ _ O
a _ _ O
way _ _ O
to _ _ O
fetch _ _ O
and _ _ O
decode _ _ O
once _ _ O
and _ _ O
apply _ _ O
to _ _ O
all _ _ O
data _ _ O
items _ _ O
? _ _ O
4 _ _ O

SIMD _ _ O
( _ _ O
Single _ _ O
Instruction _ _ O
Multiple _ _ O
Data _ _ O
) _ _ O
‚óè _ _ O
SIMD _ _ O
( _ _ O
Single _ _ O
Instruction _ _ O
Multiple _ _ O
Data _ _ O
) _ _ O
o _ _ O
An _ _ O
architecture _ _ O
for _ _ O
applying _ _ O
one _ _ O
instruction _ _ O
on _ _ O
multiple _ _ O
data _ _ O
items _ _ O
o _ _ O
ISA _ _ O
includes _ _ O
vector _ _ O
instructions _ _ O
for _ _ O
doing _ _ O
just _ _ O
that _ _ O
‚ñ™ _ _ O
Along _ _ O
with _ _ O
vector _ _ O
registers _ _ O
to _ _ O
hold _ _ O
multiple _ _ O
data _ _ O
items _ _ O
‚óè _ _ O
Using _ _ O
MIPS _ _ O
vector _ _ O
instruction _ _ O
extensions _ _ O
: _ _ O
l.d _ _ O
$ _ _ O
f0 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$sp _ _ O
) _ _ O
lv _ _ O
$ _ _ O
v1 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$s0 _ _ O
) _ _ O
mulvs.d _ _ O
$ _ _ O
v2 _ _ O
, _ _ O
$v1 _ _ O
, _ _ O
$f0 _ _ O
lv _ _ O
$ _ _ O
v3 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$s1 _ _ O
) _ _ O
addv.d _ _ O
$ _ _ O
v4 _ _ O
, _ _ O
$v2 _ _ O
, _ _ O
$v3 _ _ O
sv _ _ O
$ _ _ O
v4 _ _ O
, _ _ O
0 _ _ O
( _ _ O
$s1 _ _ O
) _ _ O
; _ _ O
$ _ _ O
f0 _ _ O
= _ _ O
scalar _ _ O
a _ _ O
; _ _ O
$ _ _ O
v1 _ _ O
= _ _ O
vector _ _ O
x _ _ O
( _ _ O
64 _ _ O
values _ _ O
) _ _ O
; _ _ O
$ _ _ O
v2 _ _ O
= _ _ O
a _ _ O
* _ _ O
vector _ _ O
x _ _ O
; _ _ O
$ _ _ O
v3 _ _ O
= _ _ O
vector _ _ O
y _ _ O
( _ _ O
64 _ _ O
values _ _ O
) _ _ O
; _ _ O
$ _ _ O
v4 _ _ O
= _ _ O
a _ _ O
* _ _ O
vector _ _ O
x _ _ O
+ _ _ O
vector _ _ O
y _ _ O
; _ _ O
vector _ _ O
y _ _ O
= _ _ O
$ _ _ O
v4 _ _ O
o _ _ O
Note _ _ O
: _ _ O
no _ _ O
indexing _ _ O
and _ _ O
loop _ _ O
control _ _ O
overhead _ _ O
o _ _ O
Note _ _ O
: _ _ O
each _ _ O
instruction _ _ O
is _ _ O
fetched _ _ O
and _ _ O
decoded _ _ O
only _ _ O
once _ _ O
5 _ _ O

SIMD _ _ O
Processor _ _ O
Design _ _ O
‚óè _ _ O
How _ _ O
would _ _ O
you _ _ O
design _ _ O
a _ _ O
processor _ _ O
for _ _ O
the _ _ O
vector _ _ O
instructions _ _ O
? _ _ O
data _ _ O
1 _ _ O
. _ _ O
One _ _ O
processing _ _ O
element _ _ O
( _ _ O
PE _ _ O
) _ _ O
o _ _ O
Fetch _ _ O
and _ _ O
decode _ _ O
instruction _ _ O
once _ _ O
o _ _ O
PE _ _ O
applies _ _ O
op _ _ O
on _ _ O
each _ _ O
data _ _ O
item _ _ O
‚ñ™ _ _ O
Item _ _ O
may _ _ O
be _ _ O
in _ _ O
vector _ _ O
register _ _ O
program _ _ O
‚ñ™ _ _ O
Item _ _ O
may _ _ O
be _ _ O
in _ _ O
data _ _ O
memory _ _ O
2 _ _ O
. _ _ O
Multiple _ _ O
PEs _ _ O
in _ _ O
parallel _ _ O
program _ _ O
o _ _ O
Fetch _ _ O
and _ _ O
decode _ _ O
instruction _ _ O
once _ _ O
o _ _ O
PEs _ _ O
apply _ _ O
op _ _ O
in _ _ O
parallel _ _ O
‚ñ™ _ _ O
In _ _ O
synchronous _ _ O
lockstep _ _ O
PE _ _ O
‚Üí _ _ O
The _ _ O
more _ _ O
PEs _ _ O
, _ _ O
the _ _ O
faster _ _ O
! _ _ O
data _ _ O
control _ _ O
PE _ _ O
control _ _ O
PE _ _ O
PE _ _ O
PE _ _ O
data _ _ O
data _ _ O
data _ _ O
6 _ _ O

Example _ _ O
: _ _ O
Adding _ _ O
Two _ _ O
Vectors _ _ O
‚óè _ _ O
Instead _ _ O
of _ _ O
having _ _ O
a _ _ O
single _ _ O
FP _ _ O
adder _ _ O
work _ _ O
on _ _ O
each _ _ O
item _ _ O
( _ _ O
a _ _ O
) _ _ O
‚óè _ _ O
Have _ _ O
four _ _ O
FP _ _ O
adders _ _ O
work _ _ O
on _ _ O
items _ _ O
in _ _ O
parallel _ _ O
( _ _ O
b _ _ O
) _ _ O
‚óè _ _ O
Each _ _ O
pipelined _ _ O
FP _ _ O
unit _ _ O
is _ _ O
in _ _ O
charge _ _ O
of _ _ O
pre-designated _ _ O
items _ _ O
in _ _ O
vector _ _ O
o _ _ O
For _ _ O
full _ _ O
parallelization _ _ O
, _ _ O
put _ _ O
as _ _ O
many _ _ O
FP _ _ O
units _ _ O
as _ _ O
there _ _ O
are _ _ O
items _ _ O
7 _ _ O

Vector _ _ O
Load-Store _ _ O
Unit _ _ O
‚óè _ _ O
Striding _ _ O
lets _ _ O
you _ _ O
load _ _ O
/ _ _ O
store _ _ O
non-contiguous _ _ O
data _ _ O
from _ _ O
memory _ _ O
at _ _ O
regular _ _ O
offsets _ _ O
. _ _ O
( _ _ O
e.g. _ _ O
the _ _ O
first _ _ O
member _ _ O
of _ _ O
each _ _ O
struct _ _ O
in _ _ O
an _ _ O
array _ _ O
) _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
5 _ _ O
0 _ _ O
4 _ _ O
8 _ _ O
C _ _ O
6 _ _ O
7 _ _ O
8 _ _ O
9 _ _ O
A _ _ O
B _ _ O
C _ _ O
D _ _ O
E _ _ O
F _ _ O
‚óè _ _ O
Gather-scatter _ _ O
lets _ _ O
you _ _ O
put _ _ O
pointers _ _ O
in _ _ O
a _ _ O
vector _ _ O
, _ _ O
then _ _ O
load _ _ O
/ _ _ O
store _ _ O
from _ _ O
arbitrary _ _ O
memory _ _ O
addresses _ _ O
. _ _ O
( _ _ O
gather _ _ O
= _ _ O
load _ _ O
, _ _ O
scatter _ _ O
= _ _ O
store _ _ O
) _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
5 _ _ O
2 _ _ O
E _ _ O
7 _ _ O
4 _ _ O
6 _ _ O
7 _ _ O
8 _ _ O
9 _ _ O
A _ _ O
B _ _ O
C _ _ O
D _ _ O
E _ _ O
F _ _ O
8 _ _ O

How _ _ O
does _ _ O
Gather-Scatter _ _ O
work _ _ O
? _ _ O
9 _ _ O

When _ _ O
does _ _ O
Gather-Scatter _ _ O
make _ _ O
sense _ _ O
? _ _ O
‚óè _ _ O
Often _ _ O
data _ _ O
for _ _ O
scientific _ _ O
or _ _ O
AI _ _ O
applications _ _ O
are _ _ O
sparse _ _ O
. _ _ O
o _ _ O
Time-sampled _ _ O
points _ _ O
where _ _ O
a _ _ O
sensor _ _ O
measurement _ _ O
changes _ _ O
o _ _ O
In _ _ O
social _ _ O
networking _ _ O
, _ _ O
connections _ _ O
in _ _ O
a _ _ O
N _ _ O
x _ _ O
N _ _ O
friendship _ _ O
matrix _ _ O
o _ _ O
In _ _ O
neural _ _ O
networks _ _ O
, _ _ O
weights _ _ O
in _ _ O
a _ _ O
filter _ _ O
layer _ _ O
that _ _ O
are _ _ O
non-zero _ _ O
‚óè _ _ O
To _ _ O
save _ _ O
memory _ _ O
, _ _ O
sparse _ _ O
data _ _ O
is _ _ O
stored _ _ O
in _ _ O
sparse _ _ O
format _ _ O
: _ _ O
How _ _ O
a _ _ O
sparse _ _ O
filter _ _ O
layer _ _ O
is _ _ O
stored _ _ O
in _ _ O
a _ _ O
Convolutional _ _ O
Neural _ _ O
Network _ _ O
( _ _ O
CNN _ _ O
) _ _ O
10 _ _ O

When _ _ O
does _ _ O
Gather-Scatter _ _ O
make _ _ O
sense _ _ O
? _ _ O
‚óè _ _ O
Convolution _ _ O
works _ _ O
by _ _ O
applying _ _ O
filter _ _ O
like _ _ O
a _ _ O
shifting _ _ O
window _ _ O
: _ _ O
row _ _ O
column _ _ O
weight _ _ O
Image _ _ O
Convolved _ _ O
Feature _ _ O
0 _ _ O
0 _ _ O
1 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
0 _ _ O
2 _ _ O
1 _ _ O
1 _ _ O
1 _ _ O
0 _ _ O
1 _ _ O
1 _ _ O
1 _ _ O
1 _ _ O
2 _ _ O
0 _ _ O
1 _ _ O
0 _ _ O
2 _ _ O
0 _ _ O
1 _ _ O
3 _ _ O
1 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
2 _ _ O
1 _ _ O
Filter _ _ O
( _ _ O
sparse _ _ O
matrix _ _ O
format _ _ O
) _ _ O
‚óè _ _ O
When _ _ O
applying _ _ O
filter _ _ O
on _ _ O
image _ _ O
, _ _ O
a _ _ O
gather _ _ O
needs _ _ O
to _ _ O
take _ _ O
place _ _ O
1 _ _ O
. _ _ O
Gather _ _ O
values _ _ O
in _ _ O
image _ _ O
in _ _ O
corresponding _ _ O
rows _ _ O
and _ _ O
columns _ _ O
2 _ _ O
. _ _ O
Create _ _ O
an _ _ O
image _ _ O
vector _ _ O
out _ _ O
of _ _ O
those _ _ O
gathered _ _ O
values _ _ O
3 _ _ O
. _ _ O
Do _ _ O
a _ _ O
dot _ _ O
( _ _ O
‚óè _ _ O
) _ _ O
product _ _ O
between _ _ O
image _ _ O
vector _ _ O
and _ _ O
filter _ _ O
vector _ _ O
11 _ _ O

SIMD _ _ O
instructions _ _ O
in _ _ O
real _ _ O
processors _ _ O
‚óè _ _ O
x86 _ _ O
vector _ _ O
extensions _ _ O
o _ _ O
Historically _ _ O
: _ _ O
MMX _ _ O
, _ _ O
SSE _ _ O
, _ _ O
AVX _ _ O
, _ _ O
AVX-2 _ _ O
o _ _ O
Current _ _ O
: _ _ O
AVX-512 _ _ O
( _ _ O
512-bit _ _ O
vector _ _ O
instructions _ _ O
) _ _ O
‚óè _ _ O
ARM _ _ O
vector _ _ O
extensions _ _ O
o _ _ O
Historically _ _ O
: _ _ O
VFP _ _ O
( _ _ O
Vector _ _ O
Floating _ _ O
Point _ _ O
) _ _ O
o _ _ O
Current _ _ O
: _ _ O
Neon _ _ O
( _ _ O
128-bit _ _ O
vector _ _ O
instructions _ _ O
) _ _ O
‚óè _ _ O
Vector _ _ O
instructions _ _ O
have _ _ O
progressively _ _ O
become _ _ O
wider _ _ O
historically _ _ O
o _ _ O
Due _ _ O
to _ _ O
increase _ _ O
of _ _ O
data _ _ O
parallel _ _ O
applications _ _ O
o _ _ O
Due _ _ O
to _ _ O
their _ _ O
power _ _ O
efficiency _ _ O
‚ñ™ _ _ O
Compared _ _ O
to _ _ O
fetching _ _ O
, _ _ O
decoding _ _ O
, _ _ O
scheduling _ _ O
multiple _ _ O
instructions _ _ O
‚ñ™ _ _ O
Good _ _ O
way _ _ O
to _ _ O
increase _ _ O
FLOPS _ _ O
while _ _ O
staying _ _ O
within _ _ O
TDP _ _ O
limit _ _ O
‚óè _ _ O
Enter _ _ O
GPUs _ _ O
for _ _ O
general _ _ O
computing _ _ O
( _ _ O
circa _ _ O
2001 _ _ O
) _ _ O
13 _ _ O

GPUs _ _ O
: _ _ O
Graphical _ _ O
Processing _ _ O
Units _ _ O
14 _ _ O

History _ _ O
of _ _ O
GPUs _ _ O
‚Ä¢ _ _ O
VGA _ _ O
( _ _ O
Video _ _ O
graphic _ _ O
array _ _ O
) _ _ O
has _ _ O
been _ _ O
around _ _ O
since _ _ O
the _ _ O
early _ _ O
90 _ _ O
‚Äôs _ _ O
‚Ä¢ _ _ O
A _ _ O
display _ _ O
generator _ _ O
connected _ _ O
to _ _ O
some _ _ O
( _ _ O
video _ _ O
) _ _ O
RAM _ _ O
‚Ä¢ _ _ O
By _ _ O
2000 _ _ O
, _ _ O
VGA _ _ O
controllers _ _ O
were _ _ O
handling _ _ O
almost _ _ O
all _ _ O
graphics _ _ O
computation _ _ O
‚Ä¢ _ _ O
Programmable _ _ O
through _ _ O
OpenGL _ _ O
, _ _ O
Direct _ _ O
3D _ _ O
API _ _ O
‚Ä¢ _ _ O
APIs _ _ O
allowed _ _ O
accelerated _ _ O
vertex _ _ O
/ _ _ O
pixel _ _ O
processing _ _ O
: _ _ O
‚Ä¢ _ _ O
Shading _ _ O
‚Ä¢ _ _ O
Texture _ _ O
mapping _ _ O
‚Ä¢ _ _ O
Rasterization _ _ O
‚Ä¢ _ _ O
Gained _ _ O
moniker _ _ O
Graphical _ _ O
Processing _ _ O
Unit _ _ O
‚Ä¢ _ _ O
2007 _ _ O
: _ _ O
First _ _ O
general _ _ O
purpose _ _ O
use _ _ O
of _ _ O
GPUs _ _ O
‚Ä¢ _ _ O
2007 _ _ O
: _ _ O
Release _ _ O
of _ _ O
CUDA _ _ O
language _ _ O
‚Ä¢ _ _ O
2011 _ _ O
: _ _ O
Release _ _ O
of _ _ O
OpenCL _ _ O
language _ _ O
15 _ _ O

GPU _ _ O
is _ _ O
Really _ _ O
a _ _ O
SIMD _ _ O
Processor _ _ O
GPU _ _ O
Streaming _ _ O
Multi-processor _ _ O
( _ _ O
SM _ _ O
) _ _ O
Streaming _ _ O
Processor _ _ O
( _ _ O
SP _ _ O
) _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
L1 _ _ O
cache _ _ O
L1 _ _ O
cache _ _ O
L1 _ _ O
cache _ _ O
Shared _ _ O
memory _ _ O
Shared _ _ O
memory _ _ O
Shared _ _ O
memory _ _ O
CPU _ _ O
L2 _ _ O
cache _ _ O
CPU _ _ O
( _ _ O
Host _ _ O
) _ _ O
memory _ _ O
L2 _ _ O
cache _ _ O
PCIe _ _ O
bus _ _ O
Global _ _ O
( _ _ O
GPU _ _ O
) _ _ O
memory _ _ O
‚óè _ _ O
Logically _ _ O
, _ _ O
a _ _ O
GPU _ _ O
is _ _ O
composed _ _ O
of _ _ O
SMs _ _ O
( _ _ O
Streaming _ _ O
Multi-processors _ _ O
) _ _ O
o _ _ O
An _ _ O
SM _ _ O
is _ _ O
a _ _ O
vector _ _ O
unit _ _ O
that _ _ O
can _ _ O
process _ _ O
multiple _ _ O
pixels _ _ O
( _ _ O
or _ _ O
data _ _ O
items _ _ O
) _ _ O
‚óè _ _ O
Each _ _ O
SM _ _ O
is _ _ O
composed _ _ O
of _ _ O
SPs _ _ O
which _ _ O
work _ _ O
on _ _ O
each _ _ O
pixel _ _ O
or _ _ O
data _ _ O
item _ _ O
16 _ _ O

CPU-GPU _ _ O
architecture _ _ O
GPU _ _ O
Streaming _ _ O
Multi-processor _ _ O
( _ _ O
SM _ _ O
) _ _ O
Streaming _ _ O
Processor _ _ O
( _ _ O
SP _ _ O
) _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
L1 _ _ O
cache _ _ O
L1 _ _ O
cache _ _ O
L1 _ _ O
cache _ _ O
Shared _ _ O
memory _ _ O
Shared _ _ O
memory _ _ O
Shared _ _ O
memory _ _ O
CPU _ _ O
L2 _ _ O
cache _ _ O
CPU _ _ O
( _ _ O
Host _ _ O
) _ _ O
memory _ _ O
L2 _ _ O
cache _ _ O
PCIe _ _ O
bus _ _ O
Global _ _ O
( _ _ O
GPU _ _ O
) _ _ O
memory _ _ O
‚óè _ _ O
Dedicated _ _ O
GPU _ _ O
memory _ _ O
separate _ _ O
from _ _ O
system _ _ O
memory _ _ O
‚óè _ _ O
Code _ _ O
and _ _ O
data _ _ O
must _ _ O
be _ _ O
transferred _ _ O
to _ _ O
GPU _ _ O
memory _ _ O
for _ _ O
it _ _ O
to _ _ O
work _ _ O
on _ _ O
it _ _ O
o _ _ O
Through _ _ O
PCI-Express _ _ O
bus _ _ O
connecting _ _ O
GPU _ _ O
to _ _ O
CPU _ _ O
17 _ _ O

Modern _ _ O
GPU _ _ O
architecture _ _ O
SM=streaming _ _ O
multiprocessor _ _ O
TPC _ _ O
= _ _ O
Texture _ _ O
Processing _ _ O
TPC _ _ O
= _ _ O
texture _ _ O
Cluster _ _ O
processing _ _ O
cluster _ _ O
ROP _ _ O
= _ _ O
raster _ _ O
operations _ _ O
pipeline _ _ O
SFU _ _ O
= _ _ O
special _ _ O
function _ _ O
unit _ _ O

GPU _ _ O
Programming _ _ O
Model _ _ O
Copy _ _ O
data _ _ O
from _ _ O
CPU _ _ O
memory _ _ O
to _ _ O
GPU _ _ O
memory _ _ O
CPU _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
CPU _ _ O
memory _ _ O
Launch _ _ O
the _ _ O
kernel _ _ O
CPU _ _ O
CPU _ _ O
CPU _ _ O
memory _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
Global _ _ O
( _ _ O
GPU _ _ O
) _ _ O
memory _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
CPU _ _ O
memory _ _ O
Copy _ _ O
data _ _ O
from _ _ O
GPU _ _ O
memory _ _ O
to _ _ O
CPU _ _ O
memory _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
Global _ _ O
( _ _ O
GPU _ _ O
) _ _ O
memory _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
Global _ _ O
( _ _ O
GPU _ _ O
) _ _ O
memory _ _ O
19 _ _ O

GPU _ _ O
Programming _ _ O
Model _ _ O
CPU _ _ O
program _ _ O
( _ _ O
serial _ _ O
code _ _ O
) _ _ O
cudaMemcpy _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
Copy _ _ O
data _ _ O
from _ _ O
CPU _ _ O
memory _ _ O
to _ _ O
GPU _ _ O
memory _ _ O
Function _ _ O
< _ _ O
< _ _ O
< _ _ O
nb _ _ O
, _ _ O
nt _ _ O
> _ _ O
> _ _ O
> _ _ O
Launch _ _ O
kernel _ _ O
on _ _ O
GPU _ _ O
cudaMemcpy _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
_ _ _ O
global _ _ O
_ _ _ O
Function _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
Copy _ _ O
results _ _ O
from _ _ O
GPU _ _ O
memory _ _ O
to _ _ O
CPU _ _ O
memory _ _ O
Implementation _ _ O
of _ _ O
GPU _ _ O
kernel _ _ O
kernel _ _ O
: _ _ O
Function _ _ O
executed _ _ O
on _ _ O
the _ _ O
GPU _ _ O
20 _ _ O

GPU _ _ O
Programming _ _ O
Model _ _ O
: _ _ O
Copying _ _ O
Data _ _ O
/ _ _ O
* _ _ O
malloc _ _ O
in _ _ O
GPU _ _ O
global _ _ O
memory _ _ O
* _ _ O
/ _ _ O
cudaMalloc _ _ O
( _ _ O
void _ _ O
* _ _ O
* _ _ O
pointer _ _ O
, _ _ O
size_t _ _ O
nbytes _ _ O
) _ _ O
; _ _ O
/ _ _ O
* _ _ O
free _ _ O
malloced _ _ O
GPU _ _ O
global _ _ O
memory _ _ O
* _ _ O
/ _ _ O
cudaFree _ _ O
( _ _ O
void _ _ O
* _ _ O
* _ _ O
pointer _ _ O
) _ _ O
; _ _ O
/ _ _ O
* _ _ O
initialize _ _ O
GPU _ _ O
global _ _ O
memory _ _ O
with _ _ O
value _ _ O
* _ _ O
/ _ _ O
cudaMemset _ _ O
( _ _ O
void _ _ O
* _ _ O
pointer _ _ O
, _ _ O
int _ _ O
value _ _ O
, _ _ O
size_t _ _ O
count _ _ O
) _ _ O
; _ _ O
/ _ _ O
* _ _ O
copy _ _ O
to _ _ O
and _ _ O
from _ _ O
between _ _ O
CPU _ _ O
and _ _ O
GPU _ _ O
memory _ _ O
* _ _ O
/ _ _ O
cudaMemcpy _ _ O
( _ _ O
void _ _ O
* _ _ O
dest _ _ O
, _ _ O
void _ _ O
* _ _ O
src _ _ O
, _ _ O
size_t _ _ O
nbytes _ _ O
, _ _ O
enum _ _ O
cudaMemcopyKind _ _ O
dir _ _ O
) _ _ O
; _ _ O
enum _ _ O
cudaMemcpyKind _ _ O
‚Ä¢ _ _ O
cudaMemcpyHostToDevice _ _ O
‚Ä¢ _ _ O
cudaMemcpyDeviceToHost _ _ O
‚Ä¢ _ _ O
cudaMemcpyDeviceToDevice _ _ O
CPU _ _ O
memory _ _ O
PCIe _ _ O
bus _ _ O
GPU _ _ O
Global _ _ O
memory _ _ O
21 _ _ O

Example _ _ O
: _ _ O
Copying _ _ O
array _ _ O
a _ _ O
to _ _ O
array _ _ O
b _ _ O
using _ _ O
the _ _ O
GPU _ _ O
22 _ _ O

GPU _ _ O
Programming _ _ O
Model _ _ O
: _ _ O
Launching _ _ O
the _ _ O
Kernel _ _ O
CPU _ _ O
program _ _ O
( _ _ O
serial _ _ O
code _ _ O
) _ _ O
cudaMemcpy _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
Copy _ _ O
data _ _ O
from _ _ O
CPU _ _ O
memory _ _ O
to _ _ O
GPU _ _ O
memory _ _ O
Function _ _ O
< _ _ O
< _ _ O
< _ _ O
nb _ _ O
, _ _ O
nt _ _ O
> _ _ O
> _ _ O
> _ _ O
Launch _ _ O
a _ _ O
kernel _ _ O
with _ _ O
nb _ _ O
blocks _ _ O
, _ _ O
each _ _ O
with _ _ O
nt _ _ O
threads _ _ O
cudaMemcpy _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
_ _ _ O
global _ _ O
_ _ _ O
Function _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
Copy _ _ O
results _ _ O
from _ _ O
GPU _ _ O
memory _ _ O
to _ _ O
CPU _ _ O
memory _ _ O
Implementation _ _ O
of _ _ O
kernel _ _ O
( _ _ O
the _ _ O
function _ _ O
run _ _ O
by _ _ O
each _ _ O
GPU _ _ O
thread _ _ O
) _ _ O
23 _ _ O

The _ _ O
Execution _ _ O
Model _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
L1 _ _ O
cache _ _ O
Shared _ _ O
memory _ _ O
‚Ä¢ _ _ O
The _ _ O
thread _ _ O
blocks _ _ O
are _ _ O
dispatched _ _ O
to _ _ O
SMs _ _ O
‚Ä¢ _ _ O
The _ _ O
number _ _ O
of _ _ O
blocks _ _ O
dispatched _ _ O
to _ _ O
an _ _ O
SM _ _ O
depends _ _ O
on _ _ O
the _ _ O
SM _ _ O
‚Äôs _ _ O
resources _ _ O
( _ _ O
registers _ _ O
, _ _ O
shared _ _ O
memory _ _ O
, _ _ O
‚Ä¶ _ _ O
) _ _ O
. _ _ O
Blocks _ _ O
not _ _ O
dispatched _ _ O
initially _ _ O
are _ _ O
dispatched _ _ O
when _ _ O
an _ _ O
SM _ _ O
frees _ _ O
up _ _ O
after _ _ O
finishing _ _ O
a _ _ O
block _ _ O
‚Ä¢ _ _ O
When _ _ O
a _ _ O
block _ _ O
is _ _ O
dispatched _ _ O
to _ _ O
an _ _ O
SM _ _ O
, _ _ O
each _ _ O
of _ _ O
its _ _ O
threads _ _ O
executes _ _ O
on _ _ O
an _ _ O
SP _ _ O
in _ _ O
the _ _ O
SM _ _ O
. _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
L1 _ _ O
cache _ _ O
Shared _ _ O
memory _ _ O
24 _ _ O

A _ _ O
thread _ _ O
block _ _ O
is _ _ O
executed _ _ O
in _ _ O
warps _ _ O
‚óè _ _ O
Each _ _ O
block _ _ O
( _ _ O
up _ _ O
to _ _ O
1 _ _ O
K _ _ O
threads _ _ O
) _ _ O
is _ _ O
divided _ _ O
into _ _ O
groups _ _ O
of _ _ O
32 _ _ O
threads _ _ O
( _ _ O
called _ _ O
warps _ _ O
) _ _ O
‚Äì _ _ O
empty _ _ O
threads _ _ O
are _ _ O
used _ _ O
as _ _ O
fillers _ _ O
. _ _ O
‚óè _ _ O
A _ _ O
warp _ _ O
executes _ _ O
as _ _ O
a _ _ O
SIMD _ _ O
vector _ _ O
instruction _ _ O
on _ _ O
the _ _ O
SM _ _ O
. _ _ O
‚óè _ _ O
Depending _ _ O
on _ _ O
the _ _ O
number _ _ O
of _ _ O
SPs _ _ O
per _ _ O
SM _ _ O
: _ _ O
0 _ _ O
1 _ _ O
30 _ _ O
31 _ _ O
o _ _ O
If _ _ O
32 _ _ O
SP _ _ O
per _ _ O
SM _ _ O
‚Üí _ _ O
1 _ _ O
thread _ _ O
of _ _ O
a _ _ O
warp _ _ O
executes _ _ O
on _ _ O
1 _ _ O
SP _ _ O
( _ _ O
32 _ _ O
lanes _ _ O
of _ _ O
execution _ _ O
, _ _ O
one _ _ O
thread _ _ O
per _ _ O
lane _ _ O
) _ _ O
0 _ _ O
1 _ _ O
30 _ _ O
31 _ _ O
01 _ _ O
30 _ _ O
31 _ _ O
o _ _ O
If _ _ O
16 _ _ O
SP _ _ O
per _ _ O
SM _ _ O
‚Üí _ _ O
2 _ _ O
threads _ _ O
are _ _ O
time _ _ O
multiplexed _ _ O
on _ _ O
1 _ _ O
SP _ _ O
( _ _ O
16 _ _ O
lanes _ _ O
of _ _ O
execution _ _ O
, _ _ O
2 _ _ O
threads _ _ O
per _ _ O
lane _ _ O
) _ _ O
0 _ _ O
15 _ _ O
0123 _ _ O
31 _ _ O
0 _ _ O
o _ _ O
If _ _ O
8 _ _ O
SP _ _ O
per _ _ O
SM _ _ O
‚Üí _ _ O
4 _ _ O
threads _ _ O
are _ _ O
time _ _ O
multiplexed _ _ O
on _ _ O
1 _ _ O
SP _ _ O
( _ _ O
8 _ _ O
lanes _ _ O
of _ _ O
execution _ _ O
, _ _ O
4 _ _ O
threads _ _ O
per _ _ O
lane _ _ O
) _ _ O
7 _ _ O
25 _ _ O

A _ _ O
SM _ _ O
is _ _ O
composed _ _ O
of _ _ O
one _ _ O
or _ _ O
more _ _ O
warp _ _ O
schedulers _ _ O
‚óè _ _ O
In _ _ O
this _ _ O
SM _ _ O
, _ _ O
there _ _ O
are _ _ O
4 _ _ O
warp _ _ O
schedulers _ _ O
. _ _ O
‚óè _ _ O
Warps _ _ O
in _ _ O
thread _ _ O
block _ _ O
are _ _ O
divided _ _ O
statically _ _ O
among _ _ O
warp _ _ O
schedulers _ _ O
. _ _ O
‚óè _ _ O
E.g. _ _ O
for _ _ O
4 _ _ O
schedulers _ _ O
: _ _ O
o _ _ O
Scheduler _ _ O
1 _ _ O
: _ _ O
Warp _ _ O
0 _ _ O
, _ _ O
Warp _ _ O
4 _ _ O
, _ _ O
‚Ä¶ _ _ O
o _ _ O
Scheduler _ _ O
2 _ _ O
: _ _ O
Warp _ _ O
1 _ _ O
, _ _ O
Warp _ _ O
5 _ _ O
, _ _ O
‚Ä¶ _ _ O
o _ _ O
Scheduler _ _ O
3 _ _ O
: _ _ O
Warp _ _ O
2 _ _ O
, _ _ O
Warp _ _ O
6 _ _ O
, _ _ O
‚Ä¶ _ _ O
o _ _ O
Scheduler _ _ O
4 _ _ O
: _ _ O
Warp _ _ O
3 _ _ O
, _ _ O
Warp _ _ O
7 _ _ O
, _ _ O
‚Ä¶ _ _ O
‚óè _ _ O
Round _ _ O
robin _ _ O
assignment _ _ O
to _ _ O
ensure _ _ O
equal _ _ O
distribution _ _ O
of _ _ O
warps _ _ O
26 _ _ O

Warp _ _ O
scheduling _ _ O
enables _ _ O
latency _ _ O
hiding _ _ O
‚óè _ _ O
Warp _ _ O
scheduler _ _ O
can _ _ O
hide _ _ O
bubbles _ _ O
( _ _ O
just _ _ O
like _ _ O
a _ _ O
superscalar _ _ O
scheduler _ _ O
) _ _ O
o _ _ O
But _ _ O
without _ _ O
an _ _ O
instruction _ _ O
queue _ _ O
and _ _ O
out-of-order _ _ O
execution _ _ O
‚óè _ _ O
How _ _ O
? _ _ O
o _ _ O
In-order _ _ O
execution _ _ O
. _ _ O
o _ _ O
Switch _ _ O
to _ _ O
different _ _ O
warp _ _ O
when _ _ O
a _ _ O
bubble _ _ O
is _ _ O
about _ _ O
to _ _ O
form _ _ O
. _ _ O
‚óè _ _ O
Warp _ _ O
can _ _ O
come _ _ O
from _ _ O
any _ _ O
thread _ _ O
block _ _ O
in _ _ O
SM _ _ O
o _ _ O
More _ _ O
thread _ _ O
blocks _ _ O
can _ _ O
lead _ _ O
to _ _ O
higher _ _ O
utilization _ _ O
. _ _ O
27 _ _ O

All _ _ O
threads _ _ O
execute _ _ O
the _ _ O
same _ _ O
code _ _ O
‚óè _ _ O
Launched _ _ O
using _ _ O
Kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
1 _ _ O
, _ _ O
64 _ _ O
> _ _ O
> _ _ O
> _ _ O
: _ _ O
1 _ _ O
block _ _ O
with _ _ O
64 _ _ O
threads _ _ O
threadIdx.x _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
60 _ _ O
61 _ _ O
62 _ _ O
63 _ _ O
int _ _ O
i _ _ O
= _ _ O
threadIdx.x _ _ O
; _ _ O
B _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
A _ _ O
[ _ _ O
63-i _ _ O
] _ _ O
; _ _ O
C _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
B _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
A _ _ O
[ _ _ O
i _ _ O
] _ _ O
A _ _ O
[ _ _ O
0 _ _ O
, _ _ O
‚Ä¶ _ _ O
, _ _ O
63 _ _ O
] _ _ O
B _ _ O
[ _ _ O
0 _ _ O
, _ _ O
‚Ä¶ _ _ O
, _ _ O
63 _ _ O
] _ _ O
C _ _ O
[ _ _ O
0 _ _ O
, _ _ O
‚Ä¶ _ _ O
, _ _ O
63 _ _ O
] _ _ O
GPU _ _ O
memory _ _ O
‚óè _ _ O
Each _ _ O
thread _ _ O
in _ _ O
a _ _ O
thread _ _ O
block _ _ O
has _ _ O
a _ _ O
unique _ _ O
‚Äú _ _ O
thread _ _ O
index _ _ O
‚Äù _ _ O
‚Üí _ _ O
threadIdx.x _ _ O
‚óè _ _ O
The _ _ O
same _ _ O
sequence _ _ O
of _ _ O
instructions _ _ O
can _ _ O
apply _ _ O
to _ _ O
different _ _ O
data _ _ O
items _ _ O
. _ _ O
28 _ _ O

Blocks _ _ O
of _ _ O
Threads _ _ O
‚óè _ _ O
Launched _ _ O
using _ _ O
Kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
2 _ _ O
, _ _ O
32 _ _ O
> _ _ O
> _ _ O
> _ _ O
: _ _ O
2 _ _ O
blocks _ _ O
of _ _ O
32 _ _ O
threads _ _ O
blockIdx.x _ _ O
= _ _ O
1 _ _ O
blockIdx.x _ _ O
= _ _ O
0 _ _ O
threadIdx.x _ _ O
0 _ _ O
1 _ _ O
30 _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
30 _ _ O
31 _ _ O
int _ _ O
i _ _ O
= _ _ O
32 _ _ O
* _ _ O
blockIdx.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
B _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
A _ _ O
[ _ _ O
63-i _ _ O
] _ _ O
; _ _ O
C _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
B _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
A _ _ O
[ _ _ O
i _ _ O
] _ _ O
A _ _ O
[ _ _ O
0 _ _ O
, _ _ O
‚Ä¶ _ _ O
, _ _ O
63 _ _ O
] _ _ O
B _ _ O
[ _ _ O
0 _ _ O
, _ _ O
‚Ä¶ _ _ O
, _ _ O
63 _ _ O
] _ _ O
C _ _ O
[ _ _ O
0 _ _ O
, _ _ O
‚Ä¶ _ _ O
, _ _ O
63 _ _ O
] _ _ O
GPU _ _ O
memory _ _ O
‚óè _ _ O
Each _ _ O
thread _ _ O
block _ _ O
has _ _ O
a _ _ O
unique _ _ O
‚Äú _ _ O
block _ _ O
index _ _ O
‚Äù _ _ O
‚Üí _ _ O
blockIdx.x _ _ O
‚óè _ _ O
Each _ _ O
thread _ _ O
has _ _ O
a _ _ O
unique _ _ O
threadIdx.x _ _ O
within _ _ O
its _ _ O
own _ _ O
block _ _ O
‚óè _ _ O
Can _ _ O
compute _ _ O
a _ _ O
global _ _ O
index _ _ O
from _ _ O
the _ _ O
blockIdx.x _ _ O
and _ _ O
threadIdx.x _ _ O
29 _ _ O

Two-dimensions _ _ O
grids _ _ O
and _ _ O
blocks _ _ O
‚óè _ _ O
Launched _ _ O
using _ _ O
Kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
( _ _ O
2 _ _ O
, _ _ O
2 _ _ O
) _ _ O
, _ _ O
( _ _ O
4 _ _ O
, _ _ O
8) _ _ O
> _ _ O
> _ _ O
> _ _ O
: _ _ O
2X2 _ _ O
blocks _ _ O
of _ _ O
4X8 _ _ O
threads _ _ O
blockIdx.x _ _ O
= _ _ O
0 _ _ O
blockIdx.x _ _ O
= _ _ O
1 _ _ O
blockIdx.y _ _ O
= _ _ O
0 _ _ O
blockIdx.y _ _ O
= _ _ O
0 _ _ O
0 _ _ O
, _ _ O
0 _ _ O
0 _ _ O
, _ _ O
1 _ _ O
0 _ _ O
, _ _ O
2 _ _ O
0 _ _ O
, _ _ O
3 _ _ O
0 _ _ O
, _ _ O
4 _ _ O
0 _ _ O
, _ _ O
5 _ _ O
0 _ _ O
, _ _ O
6 _ _ O
0 _ _ O
, _ _ O
7 _ _ O
0 _ _ O
, _ _ O
0 _ _ O
0 _ _ O
, _ _ O
1 _ _ O
0 _ _ O
, _ _ O
2 _ _ O
0 _ _ O
, _ _ O
3 _ _ O
0 _ _ O
, _ _ O
4 _ _ O
0 _ _ O
, _ _ O
5 _ _ O
0 _ _ O
, _ _ O
6 _ _ O
0 _ _ O
, _ _ O
7 _ _ O
1 _ _ O
, _ _ O
0 _ _ O
1 _ _ O
, _ _ O
1 _ _ O
1 _ _ O
, _ _ O
2 _ _ O
1 _ _ O
, _ _ O
3 _ _ O
1 _ _ O
, _ _ O
4 _ _ O
1 _ _ O
, _ _ O
5 _ _ O
1 _ _ O
, _ _ O
6 _ _ O
1 _ _ O
, _ _ O
7 _ _ O
1 _ _ O
, _ _ O
0 _ _ O
1 _ _ O
, _ _ O
1 _ _ O
1 _ _ O
, _ _ O
2 _ _ O
1 _ _ O
, _ _ O
3 _ _ O
1 _ _ O
, _ _ O
4 _ _ O
1 _ _ O
, _ _ O
5 _ _ O
1 _ _ O
, _ _ O
6 _ _ O
1 _ _ O
, _ _ O
7 _ _ O
2 _ _ O
, _ _ O
0 _ _ O
2 _ _ O
, _ _ O
1 _ _ O
2 _ _ O
, _ _ O
2 _ _ O
2 _ _ O
, _ _ O
3 _ _ O
2 _ _ O
, _ _ O
4 _ _ O
2 _ _ O
, _ _ O
5 _ _ O
2 _ _ O
, _ _ O
6 _ _ O
2 _ _ O
, _ _ O
7 _ _ O
2 _ _ O
, _ _ O
0 _ _ O
2 _ _ O
, _ _ O
1 _ _ O
2 _ _ O
, _ _ O
2 _ _ O
2 _ _ O
, _ _ O
3 _ _ O
2 _ _ O
, _ _ O
4 _ _ O
2 _ _ O
, _ _ O
5 _ _ O
2 _ _ O
, _ _ O
6 _ _ O
2 _ _ O
, _ _ O
7 _ _ O
3 _ _ O
, _ _ O
0 _ _ O
3 _ _ O
, _ _ O
1 _ _ O
3 _ _ O
, _ _ O
2 _ _ O
3 _ _ O
, _ _ O
3 _ _ O
3 _ _ O
, _ _ O
4 _ _ O
3 _ _ O
, _ _ O
5 _ _ O
3 _ _ O
, _ _ O
6 _ _ O
3 _ _ O
, _ _ O
7 _ _ O
3 _ _ O
, _ _ O
0 _ _ O
3 _ _ O
, _ _ O
1 _ _ O
3 _ _ O
, _ _ O
2 _ _ O
3 _ _ O
, _ _ O
3 _ _ O
3 _ _ O
, _ _ O
4 _ _ O
3 _ _ O
, _ _ O
5 _ _ O
3 _ _ O
, _ _ O
6 _ _ O
3 _ _ O
, _ _ O
7 _ _ O
0 _ _ O
, _ _ O
0 _ _ O
0 _ _ O
, _ _ O
1 _ _ O
0 _ _ O
, _ _ O
2 _ _ O
0 _ _ O
, _ _ O
3 _ _ O
0 _ _ O
, _ _ O
4 _ _ O
0 _ _ O
, _ _ O
5 _ _ O
0 _ _ O
, _ _ O
6 _ _ O
0 _ _ O
, _ _ O
7 _ _ O
0 _ _ O
, _ _ O
0 _ _ O
0 _ _ O
, _ _ O
1 _ _ O
0 _ _ O
, _ _ O
2 _ _ O
0 _ _ O
, _ _ O
3 _ _ O
0 _ _ O
, _ _ O
4 _ _ O
0 _ _ O
, _ _ O
5 _ _ O
0 _ _ O
, _ _ O
6 _ _ O
0 _ _ O
, _ _ O
7 _ _ O
1 _ _ O
, _ _ O
0 _ _ O
1 _ _ O
, _ _ O
1 _ _ O
1 _ _ O
, _ _ O
2 _ _ O
1 _ _ O
, _ _ O
3 _ _ O
1 _ _ O
, _ _ O
4 _ _ O
1 _ _ O
, _ _ O
5 _ _ O
1 _ _ O
, _ _ O
6 _ _ O
1 _ _ O
, _ _ O
7 _ _ O
1 _ _ O
, _ _ O
0 _ _ O
1 _ _ O
, _ _ O
1 _ _ O
1 _ _ O
, _ _ O
2 _ _ O
1 _ _ O
, _ _ O
3 _ _ O
1 _ _ O
, _ _ O
4 _ _ O
1 _ _ O
, _ _ O
5 _ _ O
1 _ _ O
, _ _ O
6 _ _ O
1 _ _ O
, _ _ O
7 _ _ O
2 _ _ O
, _ _ O
0 _ _ O
2 _ _ O
, _ _ O
1 _ _ O
2 _ _ O
, _ _ O
2 _ _ O
2 _ _ O
, _ _ O
3 _ _ O
2 _ _ O
, _ _ O
4 _ _ O
2 _ _ O
, _ _ O
5 _ _ O
2 _ _ O
, _ _ O
6 _ _ O
2 _ _ O
, _ _ O
7 _ _ O
2 _ _ O
, _ _ O
0 _ _ O
2 _ _ O
, _ _ O
1 _ _ O
2 _ _ O
, _ _ O
2 _ _ O
2 _ _ O
, _ _ O
3 _ _ O
2 _ _ O
, _ _ O
4 _ _ O
2 _ _ O
, _ _ O
5 _ _ O
2 _ _ O
, _ _ O
6 _ _ O
2 _ _ O
, _ _ O
7 _ _ O
3 _ _ O
, _ _ O
0 _ _ O
3 _ _ O
, _ _ O
1 _ _ O
3 _ _ O
, _ _ O
2 _ _ O
3 _ _ O
, _ _ O
3 _ _ O
3 _ _ O
, _ _ O
4 _ _ O
3 _ _ O
, _ _ O
5 _ _ O
3 _ _ O
, _ _ O
6 _ _ O
3 _ _ O
, _ _ O
7 _ _ O
3 _ _ O
, _ _ O
0 _ _ O
3 _ _ O
, _ _ O
1 _ _ O
3 _ _ O
, _ _ O
2 _ _ O
3 _ _ O
, _ _ O
3 _ _ O
3 _ _ O
, _ _ O
4 _ _ O
3 _ _ O
, _ _ O
5 _ _ O
3 _ _ O
, _ _ O
6 _ _ O
3 _ _ O
, _ _ O
7 _ _ O
x _ _ O
0 _ _ O
, _ _ O
0 _ _ O
0 _ _ O
, _ _ O
1 _ _ O
0 _ _ O
, _ _ O
2 _ _ O
0 _ _ O
, _ _ O
3 _ _ O
0 _ _ O
, _ _ O
4 _ _ O
0 _ _ O
, _ _ O
5 _ _ O
0 _ _ O
, _ _ O
6 _ _ O
0 _ _ O
, _ _ O
7 _ _ O
1 _ _ O
, _ _ O
0 _ _ O
1 _ _ O
, _ _ O
1 _ _ O
1 _ _ O
, _ _ O
2 _ _ O
1 _ _ O
, _ _ O
3 _ _ O
1 _ _ O
, _ _ O
4 _ _ O
1 _ _ O
, _ _ O
5 _ _ O
1 _ _ O
, _ _ O
6 _ _ O
1 _ _ O
, _ _ O
7 _ _ O
2 _ _ O
, _ _ O
0 _ _ O
2 _ _ O
, _ _ O
1 _ _ O
2 _ _ O
, _ _ O
2 _ _ O
2 _ _ O
, _ _ O
3 _ _ O
2 _ _ O
, _ _ O
4 _ _ O
2 _ _ O
, _ _ O
5 _ _ O
2 _ _ O
, _ _ O
6 _ _ O
2 _ _ O
, _ _ O
7 _ _ O
3 _ _ O
, _ _ O
0 _ _ O
3 _ _ O
, _ _ O
1 _ _ O
3 _ _ O
, _ _ O
2 _ _ O
3 _ _ O
, _ _ O
3 _ _ O
3 _ _ O
, _ _ O
4 _ _ O
3 _ _ O
, _ _ O
5 _ _ O
3 _ _ O
, _ _ O
6 _ _ O
3 _ _ O
, _ _ O
7 _ _ O
y _ _ O
blockIdx.x _ _ O
= _ _ O
0 _ _ O
blockIdx.x _ _ O
= _ _ O
1 _ _ O
blockIdx.y _ _ O
= _ _ O
1 _ _ O
blockIdx.y _ _ O
= _ _ O
1 _ _ O
‚óè _ _ O
Each _ _ O
block _ _ O
has _ _ O
two _ _ O
indices _ _ O
( _ _ O
blockIdx.x _ _ O
, _ _ O
blockIdx.y _ _ O
) _ _ O
‚óè _ _ O
Each _ _ O
thread _ _ O
in _ _ O
a _ _ O
thread _ _ O
block _ _ O
has _ _ O
two _ _ O
indices _ _ O
( _ _ O
threadIdx.x _ _ O
, _ _ O
threadIdx.y _ _ O
) _ _ O
30 _ _ O

Example _ _ O
: _ _ O
Computing _ _ O
the _ _ O
global _ _ O
index _ _ O
threadIdx.x _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
cudaMalloc _ _ O
( _ _ O
int _ _ O
* _ _ O
& _ _ O
a _ _ O
, _ _ O
20*sizeof _ _ O
( _ _ O
int _ _ O
) _ _ O
) _ _ O
; _ _ O
blockIdx.x _ _ O
blockIdx.x _ _ O
blockIdx.x _ _ O
blockIdx.x _ _ O
cudaMalloc _ _ O
( _ _ O
int _ _ O
* _ _ O
& _ _ O
b _ _ O
, _ _ O
20*sizeof _ _ O
( _ _ O
int _ _ O
) _ _ O
) _ _ O
; _ _ O
= _ _ O
0 _ _ O
= _ _ O
1 _ _ O
= _ _ O
2 _ _ O
=3 _ _ O
cudaMalloc _ _ O
( _ _ O
int _ _ O
* _ _ O
& _ _ O
c _ _ O
, _ _ O
20*sizeof _ _ O
( _ _ O
int _ _ O
) _ _ O
) _ _ O
; _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
‚Ä¶ _ _ O
kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
4 _ _ O
, _ _ O
5 _ _ O
> _ _ O
> _ _ O
( _ _ O
a _ _ O
, _ _ O
b _ _ O
, _ _ O
c _ _ O
) _ _ O
; _ _ O
‚Ä¶ _ _ O
} _ _ O
NOTE _ _ O
: _ _ O
Each _ _ O
block _ _ O
will _ _ O
consist _ _ O
of _ _ O
one _ _ O
warp _ _ O
‚Äì _ _ O
_ _ _ O
global _ _ O
_ _ _ O
void _ _ O
kernel _ _ O
( _ _ O
int _ _ O
* _ _ O
a _ _ O
, _ _ O
* _ _ O
b _ _ O
, _ _ O
* _ _ O
c _ _ O
) _ _ O
{ _ _ O
int _ _ O
i _ _ O
= _ _ O
blockIdx.x _ _ O
* _ _ O
blockDim.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
only _ _ O
5 _ _ O
threads _ _ O
in _ _ O
warp _ _ O
will _ _ O
do _ _ O
useful _ _ O
work _ _ O
. _ _ O
( _ _ O
Other _ _ O
27 _ _ O
threads _ _ O
will _ _ O
execute _ _ O
no-ops _ _ O
. _ _ O
) _ _ O
a _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
i _ _ O
; _ _ O
b _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
blockIdx.x _ _ O
; _ _ O
c _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
threadIdx.x _ _ O
; _ _ O
} _ _ O
a _ _ O
[ _ _ O
] _ _ O
Global _ _ O
Memory _ _ O
b _ _ O
[ _ _ O
] _ _ O
c _ _ O
[ _ _ O
] _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
5 _ _ O
6 _ _ O
7 _ _ O
8 _ _ O
9 _ _ O
10 _ _ O
11 _ _ O
12 _ _ O
13 _ _ O
14 _ _ O
15 _ _ O
16 _ _ O
17 _ _ O
18 _ _ O
19 _ _ O
0 _ _ O
0 _ _ O
0 _ _ O
0 _ _ O
0 _ _ O
1 _ _ O
1 _ _ O
1 _ _ O
1 _ _ O
1 _ _ O
2 _ _ O
2 _ _ O
2 _ _ O
2 _ _ O
2 _ _ O
3 _ _ O
3 _ _ O
3 _ _ O
3 _ _ O
3 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
3 _ _ O
4 _ _ O
31 _ _ O

Example _ _ O
: _ _ O
Computing _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùíÇ _ _ O
‚àó _ _ O
ùíô _ _ O
ùíä _ _ O
+ _ _ O
ùíö _ _ O
ùíä _ _ O
C _ _ O
program _ _ O
( _ _ O
on _ _ O
CPU _ _ O
) _ _ O
CUDA _ _ O
program _ _ O
( _ _ O
on _ _ O
CPU+GPU _ _ O
) _ _ O
void _ _ O
saxpy_serial _ _ O
( _ _ O
int _ _ O
n _ _ O
, _ _ O
float _ _ O
a _ _ O
, _ _ O
float _ _ O
* _ _ O
x _ _ O
, _ _ O
float _ _ O
* _ _ O
y _ _ O
) _ _ O
{ _ _ O
for _ _ O
( _ _ O
int _ _ O
i _ _ O
= _ _ O
0 _ _ O
; _ _ O
i _ _ O
< _ _ O
n _ _ O
; _ _ O
i++ _ _ O
) _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
a _ _ O
* _ _ O
x _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
; _ _ O
} _ _ O
_ _ _ O
global _ _ O
_ _ _ O
void _ _ O
saxpy_gpu _ _ O
( _ _ O
int _ _ O
n _ _ O
, _ _ O
float _ _ O
a _ _ O
, _ _ O
float _ _ O
* _ _ O
x _ _ O
, _ _ O
float _ _ O
* _ _ O
y _ _ O
) _ _ O
{ _ _ O
int _ _ O
i _ _ O
= _ _ O
blockIdx.x*blockDim.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
if _ _ O
( _ _ O
i _ _ O
< _ _ O
n _ _ O
) _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
a _ _ O
* _ _ O
x _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
; _ _ O
} _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
‚Ä¶ _ _ O
saxpy_serial _ _ O
( _ _ O
n _ _ O
, _ _ O
2.0 _ _ O
, _ _ O
x _ _ O
, _ _ O
y _ _ O
) _ _ O
; _ _ O
‚Ä¶ _ _ O
} _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
‚Ä¶ _ _ O
/ _ _ O
/ _ _ O
cudaMalloc _ _ O
arrays _ _ O
X _ _ O
and _ _ O
Y _ _ O
/ _ _ O
/ _ _ O
cudaMemcpy _ _ O
data _ _ O
to _ _ O
X _ _ O
and _ _ O
Y _ _ O
int _ _ O
NB _ _ O
= _ _ O
( _ _ O
n _ _ O
+ _ _ O
255 _ _ O
) _ _ O
/ _ _ O
256 _ _ O
; _ _ O
saxpy_gpu _ _ O
< _ _ O
< _ _ O
< _ _ O
NB _ _ O
, _ _ O
256 _ _ O
> _ _ O
> _ _ O
> _ _ O
( _ _ O
n _ _ O
, _ _ O
2.0 _ _ O
, _ _ O
X _ _ O
, _ _ O
Y _ _ O
) _ _ O
; _ _ O
/ _ _ O
/ _ _ O
cudaMemcpy _ _ O
data _ _ O
from _ _ O
Y _ _ O
} _ _ O
32 _ _ O

Example _ _ O
: _ _ O
Computing _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùíÇ _ _ O
‚àó _ _ O
ùíô _ _ O
ùíä _ _ O
+ _ _ O
ùíö _ _ O
ùíä _ _ O
‚óè _ _ O
What _ _ O
happens _ _ O
when _ _ O
n _ _ O
= _ _ O
1 _ _ O
? _ _ O
_ _ _ O
global_void _ _ O
saxpy_gpu _ _ O
( _ _ O
int _ _ O
n _ _ O
, _ _ O
float _ _ O
a _ _ O
, _ _ O
float _ _ O
* _ _ O
X _ _ O
, _ _ O
float _ _ O
* _ _ O
Y _ _ O
) _ _ O
{ _ _ O
int _ _ O
i _ _ O
= _ _ O
blockIdx.x*blockDim.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
if _ _ O
( _ _ O
i _ _ O
< _ _ O
n _ _ O
) _ _ O
Y _ _ O
[ _ _ O
i _ _ O
] _ _ O
= _ _ O
a _ _ O
* _ _ O
X _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
Y _ _ O
[ _ _ O
i _ _ O
] _ _ O
; _ _ O
} _ _ O
‚Ä¶ _ _ O
.. _ _ O
saxpy_gpu _ _ O
< _ _ O
< _ _ O
< _ _ O
1 _ _ O
, _ _ O
256 _ _ O
> _ _ O
> _ _ O
> _ _ O
( _ _ O
1 _ _ O
, _ _ O
2.0 _ _ O
, _ _ O
X _ _ O
, _ _ O
Y _ _ O
) _ _ O
; _ _ O
/ _ _ O
* _ _ O
X _ _ O
and _ _ O
Y _ _ O
are _ _ O
both _ _ O
sized _ _ O
1 _ _ O
! _ _ O
* _ _ O
/ _ _ O
‚óè _ _ O
‚Äú _ _ O
if _ _ O
( _ _ O
i _ _ O
< _ _ O
n _ _ O
) _ _ O
‚Äù _ _ O
condition _ _ O
prevents _ _ O
writing _ _ O
beyond _ _ O
bounds _ _ O
of _ _ O
array _ _ O
. _ _ O
‚óè _ _ O
But _ _ O
that _ _ O
requires _ _ O
some _ _ O
threads _ _ O
within _ _ O
a _ _ O
warp _ _ O
not _ _ O
performing _ _ O
the _ _ O
write _ _ O
. _ _ O
o _ _ O
But _ _ O
a _ _ O
warp _ _ O
is _ _ O
a _ _ O
single _ _ O
vector _ _ O
instruction _ _ O
. _ _ O
How _ _ O
can _ _ O
you _ _ O
branch _ _ O
? _ _ O
o _ _ O
‚Äú _ _ O
if _ _ O
( _ _ O
i _ _ O
< _ _ O
n _ _ O
) _ _ O
‚Äù _ _ O
creates _ _ O
a _ _ O
predicate _ _ O
‚Äú _ _ O
mask _ _ O
‚Äù _ _ O
vector _ _ O
to _ _ O
use _ _ O
for _ _ O
the _ _ O
write _ _ O
o _ _ O
Only _ _ O
thread _ _ O
0 _ _ O
has _ _ O
predicate _ _ O
turned _ _ O
on _ _ O
, _ _ O
rest _ _ O
has _ _ O
predicate _ _ O
turned _ _ O
off _ _ O
33 _ _ O

GPUs _ _ O
Use _ _ O
Predication _ _ O
for _ _ O
Branches _ _ O
‚óè _ _ O
Each _ _ O
thread _ _ O
computes _ _ O
own _ _ O
predicate _ _ O
for _ _ O
condition _ _ O
threadIdx.x _ _ O
< _ _ O
4 _ _ O
‚óè _ _ O
Taken _ _ O
together _ _ O
, _ _ O
32 _ _ O
threads _ _ O
of _ _ O
a _ _ O
warp _ _ O
create _ _ O
a _ _ O
32-bit _ _ O
predicate _ _ O
mask _ _ O
‚óè _ _ O
Mask _ _ O
is _ _ O
applied _ _ O
to _ _ O
warps _ _ O
for _ _ O
A _ _ O
, _ _ O
B _ _ O
, _ _ O
X _ _ O
, _ _ O
and _ _ O
Y. _ _ O
‚óè _ _ O
Just _ _ O
like _ _ O
for _ _ O
VLIW _ _ O
processors _ _ O
, _ _ O
this _ _ O
can _ _ O
lead _ _ O
to _ _ O
low _ _ O
utilization _ _ O
. _ _ O
34 _ _ O

GPU _ _ O
Performance _ _ O
35 _ _ O

Lesson _ _ O
1 _ _ O
: _ _ O
Parallelism _ _ O
is _ _ O
Important _ _ O
36 _ _ O

Thread _ _ O
Level _ _ O
Parallelism _ _ O
‚óè _ _ O
Superscalars _ _ O
and _ _ O
VLIWs _ _ O
are _ _ O
useful _ _ O
only _ _ O
if _ _ O
‚Ä¶ _ _ O
o _ _ O
Program _ _ O
exhibits _ _ O
ILP _ _ O
( _ _ O
Instruction _ _ O
Level _ _ O
Parallelism _ _ O
) _ _ O
in _ _ O
the _ _ O
code _ _ O
‚óè _ _ O
GPUs _ _ O
are _ _ O
useful _ _ O
only _ _ O
if _ _ O
‚Ä¶ _ _ O
o _ _ O
Program _ _ O
has _ _ O
TLP _ _ O
( _ _ O
Thread _ _ O
Level _ _ O
Parallelism _ _ O
) _ _ O
in _ _ O
the _ _ O
code _ _ O
o _ _ O
TLP _ _ O
can _ _ O
be _ _ O
expressed _ _ O
as _ _ O
the _ _ O
number _ _ O
of _ _ O
threads _ _ O
in _ _ O
the _ _ O
code _ _ O
‚óè _ _ O
How _ _ O
that _ _ O
TLP _ _ O
is _ _ O
laid _ _ O
out _ _ O
in _ _ O
the _ _ O
kernel _ _ O
is _ _ O
also _ _ O
important _ _ O
o _ _ O
How _ _ O
many _ _ O
threads _ _ O
are _ _ O
in _ _ O
a _ _ O
thread _ _ O
block _ _ O
‚ñ™ _ _ O
If _ _ O
less _ _ O
than _ _ O
threads _ _ O
in _ _ O
warp _ _ O
, _ _ O
some _ _ O
SPs _ _ O
may _ _ O
get _ _ O
unused _ _ O
o _ _ O
How _ _ O
many _ _ O
thread _ _ O
blocks _ _ O
are _ _ O
in _ _ O
the _ _ O
grid _ _ O
‚ñ™ _ _ O
If _ _ O
less _ _ O
than _ _ O
number _ _ O
of _ _ O
SMs _ _ O
, _ _ O
some _ _ O
SMs _ _ O
may _ _ O
get _ _ O
unused _ _ O
‚Üí _ _ O
If _ _ O
not _ _ O
careful _ _ O
, _ _ O
your _ _ O
GPU _ _ O
may _ _ O
get _ _ O
underutilized _ _ O
37 _ _ O

Example _ _ O
: _ _ O
Kernels _ _ O
with _ _ O
Bad _ _ O
Layout _ _ O
‚óè _ _ O
Suppose _ _ O
there _ _ O
are _ _ O
4 _ _ O
SMs _ _ O
in _ _ O
GPU _ _ O
with _ _ O
32 _ _ O
SPs _ _ O
in _ _ O
each _ _ O
SM _ _ O
. _ _ O
o _ _ O
Case _ _ O
1 _ _ O
, _ _ O
2 _ _ O
below _ _ O
have _ _ O
enough _ _ O
TLP _ _ O
( _ _ O
1024 _ _ O
threads _ _ O
) _ _ O
but _ _ O
bad _ _ O
layout _ _ O
. _ _ O
o _ _ O
Utilized _ _ O
threads _ _ O
are _ _ O
marked _ _ O
in _ _ O
red _ _ O
. _ _ O
Rest _ _ O
are _ _ O
unused _ _ O
. _ _ O
‚óè _ _ O
Case _ _ O
1 _ _ O
: _ _ O
Not _ _ O
enough _ _ O
threads _ _ O
kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
1024 _ _ O
, _ _ O
1 _ _ O
> _ _ O
> _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
; _ _ O
‚óè _ _ O
Case _ _ O
2 _ _ O
: _ _ O
Not _ _ O
enough _ _ O
blocks _ _ O
kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
1 _ _ O
, _ _ O
1024 _ _ O
> _ _ O
> _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
; _ _ O
‚óè _ _ O
Balanced _ _ O
threads _ _ O
and _ _ O
blocks _ _ O
kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
32 _ _ O
, _ _ O
32 _ _ O
> _ _ O
> _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
; _ _ O
kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
16 _ _ O
, _ _ O
64 _ _ O
> _ _ O
> _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
; _ _ O
kernel _ _ O
< _ _ O
< _ _ O
< _ _ O
4 _ _ O
, _ _ O
256 _ _ O
> _ _ O
> _ _ O
( _ _ O
‚Ä¶ _ _ O
) _ _ O
; _ _ O
SM _ _ O
0 _ _ O
SM _ _ O
1 _ _ O
SM _ _ O
2 _ _ O
SM _ _ O
3 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
SM _ _ O
0 _ _ O
SM _ _ O
1 _ _ O
SM _ _ O
2 _ _ O
SM _ _ O
3 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
SM _ _ O
0 _ _ O
SM _ _ O
1 _ _ O
SM _ _ O
2 _ _ O
SM _ _ O
3 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
0 _ _ O
1 _ _ O
2 _ _ O
‚Ä¶ _ _ O
31 _ _ O
38 _ _ O

Lesson _ _ O
2 _ _ O
: _ _ O
Bandwidth _ _ O
is _ _ O
Important _ _ O
39 _ _ O

Example _ _ O
: _ _ O
Computing _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùë® _ _ O
ùíä _ _ O
, _ _ O
ùíã _ _ O
‚àó _ _ O
ùíô _ _ O
ùíã _ _ O
C _ _ O
program _ _ O
( _ _ O
on _ _ O
CPU _ _ O
) _ _ O
void _ _ O
mv_cpu _ _ O
( _ _ O
float _ _ O
* _ _ O
y _ _ O
, _ _ O
float _ _ O
* _ _ O
A _ _ O
, _ _ O
float _ _ O
* _ _ O
x _ _ O
, _ _ O
int _ _ O
n _ _ O
) _ _ O
{ _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
n _ _ O
; _ _ O
i++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
j=0 _ _ O
; _ _ O
j _ _ O
< _ _ O
n _ _ O
; _ _ O
j++ _ _ O
) _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
* _ _ O
x _ _ O
[ _ _ O
j _ _ O
] _ _ O
; _ _ O
} _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
‚Ä¶ _ _ O
mv_cpu _ _ O
( _ _ O
y _ _ O
, _ _ O
A _ _ O
, _ _ O
x _ _ O
, _ _ O
n _ _ O
) _ _ O
; _ _ O
‚Ä¶ _ _ O
} _ _ O
CUDA _ _ O
program _ _ O
( _ _ O
on _ _ O
CPU+GPU _ _ O
) _ _ O
void _ _ O
mv_gpu _ _ O
( _ _ O
float _ _ O
* _ _ O
y _ _ O
, _ _ O
float _ _ O
* _ _ O
A _ _ O
, _ _ O
float _ _ O
* _ _ O
x _ _ O
, _ _ O
int _ _ O
n _ _ O
) _ _ O
{ _ _ O
int _ _ O
i _ _ O
= _ _ O
blockIdx.x _ _ O
* _ _ O
blockDim.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
if _ _ O
( _ _ O
i _ _ O
< _ _ O
n _ _ O
) _ _ O
{ _ _ O
for _ _ O
( _ _ O
int _ _ O
j _ _ O
= _ _ O
0 _ _ O
; _ _ O
j _ _ O
< _ _ O
n _ _ O
; _ _ O
j++ _ _ O
) _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
* _ _ O
x _ _ O
[ _ _ O
j _ _ O
] _ _ O
; _ _ O
} _ _ O
} _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
‚Ä¶ _ _ O
int _ _ O
nblocks _ _ O
= _ _ O
( _ _ O
n _ _ O
+ _ _ O
block_size _ _ O
- _ _ O
1 _ _ O
) _ _ O
/ _ _ O
block_size _ _ O
; _ _ O
mv_gpu _ _ O
< _ _ O
< _ _ O
< _ _ O
nblocks _ _ O
, _ _ O
block_size _ _ O
> _ _ O
> _ _ O
> _ _ O
( _ _ O
y _ _ O
, _ _ O
A _ _ O
, _ _ O
x _ _ O
, _ _ O
n _ _ O
) _ _ O
; _ _ O
‚Ä¶ _ _ O
} _ _ O
40 _ _ O

Performance _ _ O
Results _ _ O
for _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùë® _ _ O
ùíä _ _ O
, _ _ O
ùíã _ _ O
‚àó _ _ O
ùíô _ _ O
ùíã _ _ O
‚óè _ _ O
Guess _ _ O
what _ _ O
? _ _ O
CPU _ _ O
is _ _ O
faster _ _ O
than _ _ O
GPU _ _ O
! _ _ O
But _ _ O
even _ _ O
comparing _ _ O
pure _ _ O
compute _ _ O
time _ _ O
, _ _ O
CPU _ _ O
is _ _ O
still _ _ O
faster _ _ O
than _ _ O
GPU _ _ O
. _ _ O
What _ _ O
the _ _ O
‚Ä¶ _ _ O
? _ _ O
* _ _ O
Run _ _ O
on _ _ O
netlab-1.cs.pitt.edu _ _ O
with _ _ O
n=8192 _ _ O
: _ _ O
- _ _ O
Intel _ _ O
Core _ _ O
i7 _ _ O
- _ _ O
4770 _ _ O
CPU _ _ O
- _ _ O
NVIDIA _ _ O
GF119 _ _ O
- _ _ O
825-A1 _ _ O
GPU _ _ O
A _ _ O
lot _ _ O
of _ _ O
time _ _ O
is _ _ O
spent _ _ O
copying _ _ O
back _ _ O
and _ _ O
forth _ _ O
between _ _ O
CPU _ _ O
and _ _ O
GPU _ _ O
memories _ _ O
. _ _ O
41 _ _ O

Performance _ _ O
Results _ _ O
for _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùë® _ _ O
ùíä _ _ O
, _ _ O
ùíã _ _ O
‚àó _ _ O
ùíô _ _ O
ùíã _ _ O
‚óè _ _ O
Was _ _ O
it _ _ O
because _ _ O
the _ _ O
GPU _ _ O
was _ _ O
wimpy _ _ O
and _ _ O
ca _ _ O
n‚Äôt _ _ O
do _ _ O
enough _ _ O
FLOPS _ _ O
? _ _ O
‚óè _ _ O
NVIDIA _ _ O
GF119 _ _ O
- _ _ O
825-A1 _ _ O
is _ _ O
a _ _ O
Fermi _ _ O
GPU _ _ O
Capability _ _ O
2.1 _ _ O
o _ _ O
Clock _ _ O
rate _ _ O
: _ _ O
1046 _ _ O
MHz _ _ O
( _ _ O
X _ _ O
2 _ _ O
for _ _ O
warp _ _ O
execution _ _ O
) _ _ O
o _ _ O
Number _ _ O
of _ _ O
SMs _ _ O
: _ _ O
1 _ _ O
o _ _ O
Number _ _ O
of _ _ O
SPs _ _ O
per _ _ O
SM _ _ O
: _ _ O
48 _ _ O
o _ _ O
Max _ _ O
FLOPS _ _ O
= _ _ O
1046 _ _ O
MHz _ _ O
* _ _ O
2 _ _ O
* _ _ O
1 _ _ O
* _ _ O
48 _ _ O
= _ _ O
100.4 _ _ O
GFLOPS _ _ O
‚óè _ _ O
What _ _ O
was _ _ O
the _ _ O
FLOPS _ _ O
achieved _ _ O
? _ _ O
o _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
* _ _ O
x _ _ O
[ _ _ O
j _ _ O
] _ _ O
= _ _ O
2 _ _ O
FP _ _ O
ops _ _ O
each _ _ O
iteration _ _ O
for _ _ O
n _ _ O
* _ _ O
n _ _ O
iterations _ _ O
o _ _ O
n _ _ O
= _ _ O
8192 _ _ O
, _ _ O
so _ _ O
FP _ _ O
ops _ _ O
= _ _ O
8192 _ _ O
* _ _ O
8192 _ _ O
* _ _ O
2 _ _ O
= _ _ O
134 _ _ O
M _ _ O
o _ _ O
Time _ _ O
= _ _ O
0.27 _ _ O
seconds _ _ O
( _ _ O
shortest _ _ O
at _ _ O
32 _ _ O
thread _ _ O
block _ _ O
size _ _ O
) _ _ O
o _ _ O
FLOPS _ _ O
= _ _ O
134 _ _ O
M _ _ O
/ _ _ O
0.27 _ _ O
= _ _ O
496 _ _ O
MFLOPS _ _ O
o _ _ O
Not _ _ O
even _ _ O
close _ _ O
to _ _ O
the _ _ O
limit _ _ O
! _ _ O
42 _ _ O

Performance _ _ O
Results _ _ O
for _ _ O
ùíö _ _ O
ùíä _ _ O
= _ _ O
ùë® _ _ O
ùíä _ _ O
, _ _ O
ùíã _ _ O
‚àó _ _ O
ùíô _ _ O
ùíã _ _ O
‚óè _ _ O
Could _ _ O
it _ _ O
be _ _ O
that _ _ O
the _ _ O
GPU _ _ O
did _ _ O
n‚Äôt _ _ O
have _ _ O
enough _ _ O
memory _ _ O
bandwidth _ _ O
? _ _ O
‚óè _ _ O
NVIDIA _ _ O
GF119 _ _ O
- _ _ O
825-A1 _ _ O
is _ _ O
a _ _ O
Fermi _ _ O
GPU _ _ O
Capability _ _ O
2.1 _ _ O
o _ _ O
Memory _ _ O
Type _ _ O
: _ _ O
DDR3 _ _ O
o _ _ O
Memory _ _ O
Bandwidth _ _ O
: _ _ O
14.00 _ _ O
GB _ _ O
/ _ _ O
s _ _ O
‚óè _ _ O
GPUs _ _ O
also _ _ O
have _ _ O
Performance _ _ O
Monitoring _ _ O
Units _ _ O
( _ _ O
PMUs _ _ O
) _ _ O
o _ _ O
NVIDIA _ _ O
Profiler _ _ O
( _ _ O
nvprof _ _ O
) _ _ O
provides _ _ O
an _ _ O
easy _ _ O
way _ _ O
to _ _ O
read _ _ O
them _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
docs.nvidia.com _ _ O
/ _ _ O
cuda _ _ O
/ _ _ O
profiler-users-guide _ _ O
/ _ _ O
index.html _ _ O
o _ _ O
Let _ _ O
‚Äôs _ _ O
use _ _ O
the _ _ O
PMU _ _ O
to _ _ O
profile _ _ O
the _ _ O
following _ _ O
: _ _ O
‚ñ™ _ _ O
DRAM _ _ O
Transfer _ _ O
Rate _ _ O
( _ _ O
GB _ _ O
/ _ _ O
s _ _ O
) _ _ O
‚ñ™ _ _ O
L1 _ _ O
Hit _ _ O
Rate _ _ O
( _ _ O
% _ _ O
) _ _ O
‚ñ™ _ _ O
L2 _ _ O
Hit _ _ O
Rate _ _ O
( _ _ O
% _ _ O
) _ _ O
43 _ _ O

Memory _ _ O
Wall _ _ O
Hits _ _ O
Again _ _ O
44 _ _ O

Memory _ _ O
Wall _ _ O
Hits _ _ O
Again _ _ O
DRAM _ _ O
bandwidth _ _ O
not _ _ O
saturated _ _ O
: _ _ O
Benefits _ _ O
from _ _ O
more _ _ O
parallelism _ _ O
due _ _ O
to _ _ O
larger _ _ O
thread _ _ O
block _ _ O
sizes _ _ O
DRAM _ _ O
bandwidth _ _ O
saturated _ _ O
: _ _ O
Larger _ _ O
thread _ _ O
blocks _ _ O
‚Üí _ _ O
increased _ _ O
working _ _ O
set _ _ O
size _ _ O
‚Üí _ _ O
higher _ _ O
cache _ _ O
miss _ _ O
rates _ _ O
‚Üí _ _ O
worse _ _ O
bandwidth _ _ O
problem _ _ O
DRAM _ _ O
Bandwidth _ _ O
Limit _ _ O
( _ _ O
14 _ _ O
GB _ _ O
/ _ _ O
s _ _ O
) _ _ O
45 _ _ O

Is _ _ O
there _ _ O
a _ _ O
way _ _ O
we _ _ O
can _ _ O
reach _ _ O
max _ _ O
FLOPS _ _ O
? _ _ O
‚óè _ _ O
Let _ _ O
‚Äôs _ _ O
take _ _ O
a _ _ O
look _ _ O
at _ _ O
the _ _ O
GPU _ _ O
design _ _ O
metrics _ _ O
again _ _ O
: _ _ O
o _ _ O
Max _ _ O
FLOPS _ _ O
= _ _ O
100.4 _ _ O
GFLOPS _ _ O
o _ _ O
Memory _ _ O
Bandwidth _ _ O
: _ _ O
14.00 _ _ O
GB _ _ O
/ _ _ O
s _ _ O
‚óè _ _ O
To _ _ O
sustain _ _ O
max _ _ O
FLOPS _ _ O
, _ _ O
you _ _ O
need _ _ O
to _ _ O
do _ _ O
a _ _ O
lot _ _ O
of _ _ O
work _ _ O
per _ _ O
byte _ _ O
o _ _ O
100.4 _ _ O
GFLOPS _ _ O
/ _ _ O
14.00 _ _ O
GB _ _ O
/ _ _ O
s _ _ O
= _ _ O
7.17 _ _ O
FP _ _ O
ops _ _ O
/ _ _ O
byte _ _ O
o _ _ O
Or _ _ O
, _ _ O
about _ _ O
28 _ _ O
FP _ _ O
ops _ _ O
/ _ _ O
float _ _ O
( _ _ O
4 _ _ O
bytes _ _ O
) _ _ O
fetched _ _ O
from _ _ O
memory _ _ O
o _ _ O
Otherwise _ _ O
, _ _ O
the _ _ O
memory _ _ O
bandwidth _ _ O
can _ _ O
not _ _ O
sustain _ _ O
the _ _ O
FLOPS _ _ O
‚óè _ _ O
All _ _ O
GPUs _ _ O
have _ _ O
this _ _ O
problem _ _ O
with _ _ O
memory _ _ O
bandwidth _ _ O
: _ _ O
o _ _ O
It _ _ O
‚Äôs _ _ O
easy _ _ O
to _ _ O
put _ _ O
in _ _ O
more _ _ O
SMs _ _ O
using _ _ O
transistors _ _ O
for _ _ O
Moore _ _ O
‚Äôs _ _ O
Law _ _ O
o _ _ O
Your _ _ O
memory _ _ O
bandwidth _ _ O
is _ _ O
limited _ _ O
due _ _ O
to _ _ O
your _ _ O
DDR _ _ O
interface _ _ O
46 _ _ O

Arithmetic _ _ O
Intensity _ _ O
: _ _ O
A _ _ O
property _ _ O
of _ _ O
the _ _ O
program _ _ O
‚óè _ _ O
How _ _ O
many _ _ O
FP _ _ O
ops _ _ O
/ _ _ O
float _ _ O
for _ _ O
our _ _ O
mat-vec _ _ O
multiplication _ _ O
? _ _ O
o _ _ O
y _ _ O
[ _ _ O
i _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
* _ _ O
x _ _ O
[ _ _ O
j _ _ O
] _ _ O
each _ _ O
iteration _ _ O
with _ _ O
n _ _ O
* _ _ O
n _ _ O
iterations _ _ O
o _ _ O
FP _ _ O
ops _ _ O
= _ _ O
2 _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
( _ _ O
one _ _ O
multiply _ _ O
and _ _ O
one _ _ O
add _ _ O
) _ _ O
o _ _ O
Float _ _ O
accesses _ _ O
= _ _ O
n _ _ O
* _ _ O
n _ _ O
+ _ _ O
2n _ _ O
( _ _ O
1 _ _ O
matrix _ _ O
and _ _ O
2 _ _ O
vector _ _ O
accesses _ _ O
) _ _ O
‚ñ™ _ _ O
That _ _ O
‚Äôs _ _ O
counting _ _ O
only _ _ O
cold _ _ O
misses _ _ O
but _ _ O
could _ _ O
be _ _ O
even _ _ O
more _ _ O
o _ _ O
So _ _ O
approx _ _ O
. _ _ O
2 _ _ O
FP _ _ O
ops _ _ O
/ _ _ O
float _ _ O
( _ _ O
a _ _ O
far _ _ O
cry _ _ O
from _ _ O
28 _ _ O
FP _ _ O
ops _ _ O
/ _ _ O
float _ _ O
) _ _ O
o _ _ O
This _ _ O
metric _ _ O
is _ _ O
called _ _ O
arithmetic _ _ O
intensity _ _ O
‚óè _ _ O
Arithmetic _ _ O
intensity _ _ O
is _ _ O
a _ _ O
property _ _ O
of _ _ O
the _ _ O
program _ _ O
needed _ _ O
by _ _ O
GPUs _ _ O
o _ _ O
Just _ _ O
like _ _ O
TLP _ _ O
( _ _ O
thread-level-parallelism _ _ O
) _ _ O
is _ _ O
needed _ _ O
by _ _ O
GPUs _ _ O
o _ _ O
Matrix-vector _ _ O
multiplication _ _ O
has _ _ O
low _ _ O
intensity _ _ O
‚Üí _ _ O
Fundamentally _ _ O
not _ _ O
suited _ _ O
for _ _ O
fast _ _ O
GPU _ _ O
computation _ _ O
47 _ _ O

Arithmetic _ _ O
Intensity _ _ O
: _ _ O
A _ _ O
property _ _ O
of _ _ O
the _ _ O
program _ _ O
* _ _ O
Courtesy _ _ O
of _ _ O
Lawrence _ _ O
Berkeley _ _ O
National _ _ O
Laboratory _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
crd.lbl.gov _ _ O
/ _ _ O
departments _ _ O
/ _ _ O
computer-science _ _ O
/ _ _ O
par _ _ O
/ _ _ O
research _ _ O
/ _ _ O
roofline _ _ O
/ _ _ O
introduction _ _ O
/ _ _ O
‚óè _ _ O
BLAS _ _ O
: _ _ O
Basic _ _ O
Linear _ _ O
Algebra _ _ O
Subprograms _ _ O
o _ _ O
BLAS _ _ O
1 _ _ O
: _ _ O
Vector _ _ O
operations _ _ O
only _ _ O
( _ _ O
e.g. _ _ O
saxpy _ _ O
) _ _ O
‚Üí _ _ O
Bad _ _ O
intensity _ _ O
o _ _ O
BLAS _ _ O
2 _ _ O
: _ _ O
General _ _ O
Matrix-Vector _ _ O
Multiplication _ _ O
( _ _ O
GeMV _ _ O
) _ _ O
‚Üí _ _ O
Bad _ _ O
intensity _ _ O
o _ _ O
BLAS _ _ O
3 _ _ O
: _ _ O
General _ _ O
Matrix _ _ O
Multiplication _ _ O
( _ _ O
GeMM _ _ O
) _ _ O
‚Üí _ _ O
Good _ _ O
intensity _ _ O
48 _ _ O

Matrix-Matrix _ _ O
Multiply _ _ O
: _ _ O
Good _ _ O
Arithmetic _ _ O
Intensity _ _ O
‚óè _ _ O
Matrix-multiplication _ _ O
: _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
n _ _ O
; _ _ O
i++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
j=0 _ _ O
; _ _ O
j _ _ O
< _ _ O
n _ _ O
; _ _ O
j++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
k=0 _ _ O
; _ _ O
k _ _ O
< _ _ O
n _ _ O
; _ _ O
k++ _ _ O
) _ _ O
C _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
k _ _ O
] _ _ O
* _ _ O
B _ _ O
[ _ _ O
k*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
; _ _ O
‚óè _ _ O
What _ _ O
‚Äôs _ _ O
the _ _ O
arithmetic _ _ O
intensity _ _ O
for _ _ O
this _ _ O
program _ _ O
? _ _ O
o _ _ O
FP _ _ O
ops _ _ O
= _ _ O
2 _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
( _ _ O
one _ _ O
multiply _ _ O
and _ _ O
one _ _ O
add _ _ O
) _ _ O
o _ _ O
Float _ _ O
accesses _ _ O
= _ _ O
3 _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
( _ _ O
3 _ _ O
matrix _ _ O
accesses _ _ O
) _ _ O
‚ñ™ _ _ O
If _ _ O
we _ _ O
only _ _ O
have _ _ O
cold _ _ O
misses _ _ O
and _ _ O
no _ _ O
capacity _ _ O
misses _ _ O
o _ _ O
Arithmetic _ _ O
intensity _ _ O
= _ _ O
2 _ _ O
* _ _ O
n _ _ O
/ _ _ O
3 _ _ O
= _ _ O
0.66 _ _ O
* _ _ O
n _ _ O
= _ _ O
O _ _ O
( _ _ O
n _ _ O
) _ _ O
o _ _ O
Implication _ _ O
: _ _ O
The _ _ O
larger _ _ O
the _ _ O
matrix _ _ O
size _ _ O
, _ _ O
the _ _ O
better _ _ O
suited _ _ O
for _ _ O
GPUs _ _ O
! _ _ O
‚ñ™ _ _ O
Important _ _ O
result _ _ O
for _ _ O
deep _ _ O
learning _ _ O
and _ _ O
other _ _ O
apps _ _ O
49 _ _ O

Example _ _ O
: _ _ O
Computing _ _ O
ùë™ _ _ O
ùíä _ _ O
, _ _ O
ùíã _ _ O
= _ _ O
ùë® _ _ O
ùíä _ _ O
, _ _ O
ùíå _ _ O
‚àó _ _ O
ùë© _ _ O
ùíå _ _ O
, _ _ O
ùíã _ _ O
C _ _ O
program _ _ O
( _ _ O
on _ _ O
CPU _ _ O
) _ _ O
void _ _ O
mm_cpu _ _ O
( _ _ O
float _ _ O
* _ _ O
C _ _ O
, _ _ O
float _ _ O
* _ _ O
A _ _ O
, _ _ O
float _ _ O
* _ _ O
B _ _ O
, _ _ O
int _ _ O
n _ _ O
) _ _ O
{ _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
n _ _ O
; _ _ O
i++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
j=0 _ _ O
; _ _ O
j _ _ O
< _ _ O
n _ _ O
; _ _ O
j++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
k=0 _ _ O
; _ _ O
k _ _ O
< _ _ O
n _ _ O
; _ _ O
k++ _ _ O
) _ _ O
C _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
k _ _ O
] _ _ O
* _ _ O
B _ _ O
[ _ _ O
k*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
; _ _ O
} _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
mm_cpu _ _ O
( _ _ O
C _ _ O
, _ _ O
A _ _ O
, _ _ O
B _ _ O
, _ _ O
n _ _ O
) _ _ O
; _ _ O
} _ _ O
CUDA _ _ O
program _ _ O
( _ _ O
on _ _ O
CPU+GPU _ _ O
) _ _ O
void _ _ O
mm_gpu _ _ O
( _ _ O
float _ _ O
* _ _ O
C _ _ O
, _ _ O
float _ _ O
* _ _ O
A _ _ O
, _ _ O
float _ _ O
* _ _ O
B _ _ O
, _ _ O
int _ _ O
n _ _ O
) _ _ O
{ _ _ O
float _ _ O
Cvalue _ _ O
= _ _ O
0 _ _ O
; _ _ O
int _ _ O
i _ _ O
= _ _ O
blockIdx.y _ _ O
* _ _ O
blockDim.y _ _ O
+ _ _ O
threadIdx.y _ _ O
; _ _ O
int _ _ O
j _ _ O
= _ _ O
blockIdx.x _ _ O
* _ _ O
blockDim.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
k _ _ O
= _ _ O
0 _ _ O
; _ _ O
k _ _ O
< _ _ O
n _ _ O
; _ _ O
+ _ _ O
+ _ _ O
k _ _ O
) _ _ O
Cvalue _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i _ _ O
* _ _ O
n _ _ O
+ _ _ O
k _ _ O
] _ _ O
* _ _ O
B _ _ O
[ _ _ O
k _ _ O
* _ _ O
n _ _ O
+ _ _ O
j _ _ O
] _ _ O
; _ _ O
C _ _ O
[ _ _ O
i _ _ O
* _ _ O
n _ _ O
+ _ _ O
j _ _ O
] _ _ O
= _ _ O
Cvalue _ _ O
; _ _ O
} _ _ O
void _ _ O
main _ _ O
( _ _ O
) _ _ O
{ _ _ O
dim3 _ _ O
dimBlock _ _ O
( _ _ O
block_size _ _ O
, _ _ O
block_size _ _ O
) _ _ O
; _ _ O
dim3 _ _ O
dimGrid _ _ O
( _ _ O
n _ _ O
/ _ _ O
dimBlock.x _ _ O
, _ _ O
n _ _ O
/ _ _ O
dimBlock.y _ _ O
) _ _ O
; _ _ O
mm_gpu _ _ O
< _ _ O
< _ _ O
< _ _ O
dimGrid _ _ O
, _ _ O
dimBlock _ _ O
> _ _ O
> _ _ O
> _ _ O
( _ _ O
C _ _ O
, _ _ O
A _ _ O
, _ _ O
B _ _ O
, _ _ O
n _ _ O
) _ _ O
; _ _ O
} _ _ O
50 _ _ O

Performance _ _ O
Results _ _ O
for _ _ O
ùë™ _ _ O
ùíä _ _ O
, _ _ O
ùíã _ _ O
= _ _ O
ùë® _ _ O
ùíä _ _ O
, _ _ O
ùíå _ _ O
‚àó _ _ O
ùë© _ _ O
ùíå _ _ O
, _ _ O
ùíã _ _ O
Now _ _ O
GPU _ _ O
is _ _ O
much _ _ O
faster _ _ O
than _ _ O
CPU _ _ O
given _ _ O
enough _ _ O
TLP _ _ O
What _ _ O
‚Äôs _ _ O
this _ _ O
? _ _ O
This _ _ O
version _ _ O
of _ _ O
matrix-multiply _ _ O
reduces _ _ O
cache _ _ O
capacity _ _ O
misses _ _ O
using _ _ O
shared _ _ O
memory _ _ O
in _ _ O
the _ _ O
GPU _ _ O
. _ _ O
51 _ _ O

Capacity _ _ O
Misses _ _ O
can _ _ O
Reduce _ _ O
Arithmetic _ _ O
Intensity _ _ O
for _ _ O
( _ _ O
int _ _ O
i=0 _ _ O
; _ _ O
i _ _ O
< _ _ O
n _ _ O
; _ _ O
i++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
j=0 _ _ O
; _ _ O
j _ _ O
< _ _ O
n _ _ O
; _ _ O
j++ _ _ O
) _ _ O
for _ _ O
( _ _ O
int _ _ O
k=0 _ _ O
; _ _ O
k _ _ O
< _ _ O
n _ _ O
; _ _ O
k++ _ _ O
) _ _ O
C _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
+ _ _ O
= _ _ O
A _ _ O
[ _ _ O
i*n _ _ O
+ _ _ O
k _ _ O
] _ _ O
* _ _ O
B _ _ O
[ _ _ O
k*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
; _ _ O
‚óè _ _ O
The _ _ O
ideal _ _ O
arithmetic _ _ O
intensity _ _ O
for _ _ O
this _ _ O
program _ _ O
was _ _ O
: _ _ O
o _ _ O
FP _ _ O
ops _ _ O
= _ _ O
2 _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
( _ _ O
one _ _ O
multiply _ _ O
and _ _ O
one _ _ O
add _ _ O
) _ _ O
o _ _ O
Float _ _ O
accesses _ _ O
= _ _ O
3 _ _ O
* _ _ O
n _ _ O
* _ _ O
n _ _ O
( _ _ O
3 _ _ O
matrix _ _ O
accesses _ _ O
) _ _ O
o _ _ O
Arithmetic _ _ O
intensity _ _ O
= _ _ O
2 _ _ O
* _ _ O
n _ _ O
/ _ _ O
3 _ _ O
= _ _ O
0.66 _ _ O
* _ _ O
n _ _ O
= _ _ O
O _ _ O
( _ _ O
n _ _ O
) _ _ O
‚óè _ _ O
Only _ _ O
if _ _ O
we _ _ O
have _ _ O
no _ _ O
capacity _ _ O
misses _ _ O
. _ _ O
What _ _ O
if _ _ O
we _ _ O
did _ _ O
have _ _ O
them _ _ O
? _ _ O
o _ _ O
For _ _ O
B _ _ O
[ _ _ O
k*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
, _ _ O
reuse _ _ O
distance _ _ O
is _ _ O
the _ _ O
entire _ _ O
matrix _ _ O
of _ _ O
B _ _ O
o _ _ O
If _ _ O
B _ _ O
[ _ _ O
k*n _ _ O
+ _ _ O
j _ _ O
] _ _ O
always _ _ O
misses _ _ O
, _ _ O
memory _ _ O
accesses _ _ O
is _ _ O
in _ _ O
the _ _ O
order _ _ O
of _ _ O
n3 _ _ O
! _ _ O
52 _ _ O

So _ _ O
what _ _ O
is _ _ O
Shared _ _ O
Memory _ _ O
? _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
IF _ _ O
/ _ _ O
ID _ _ O
L1 _ _ O
cache _ _ O
L1 _ _ O
cache _ _ O
L1 _ _ O
cache _ _ O
Shared _ _ O
memory _ _ O
Shared _ _ O
memory _ _ O
Shared _ _ O
memory _ _ O
L2 _ _ O
cache _ _ O
Global _ _ O
( _ _ O
GPU _ _ O
) _ _ O
memory _ _ O
‚óè _ _ O
Shared _ _ O
Memory _ _ O
: _ _ O
memory _ _ O
shared _ _ O
among _ _ O
threads _ _ O
in _ _ O
a _ _ O
thread _ _ O
block _ _ O
o _ _ O
Variables _ _ O
declared _ _ O
with _ _ O
_ _ _ O
_ _ _ O
shared _ _ O
_ _ _ O
_ _ _ O
modifier _ _ O
live _ _ O
in _ _ O
shared _ _ O
memory _ _ O
o _ _ O
Is _ _ O
same _ _ O
as _ _ O
L1 _ _ O
cache _ _ O
in _ _ O
terms _ _ O
of _ _ O
latency _ _ O
and _ _ O
bandwidth _ _ O
! _ _ O
o _ _ O
Storing _ _ O
frequently _ _ O
used _ _ O
data _ _ O
in _ _ O
shared _ _ O
memory _ _ O
can _ _ O
save _ _ O
on _ _ O
bandwidth _ _ O
53 _ _ O

Loop _ _ O
Tiling _ _ O
with _ _ O
Shared _ _ O
Memory _ _ O
‚óè _ _ O
Store _ _ O
a _ _ O
‚Äú _ _ O
tile _ _ O
‚Äù _ _ O
within _ _ O
matrix _ _ O
in _ _ O
shared _ _ O
memory _ _ O
while _ _ O
operating _ _ O
on _ _ O
it _ _ O
o _ _ O
Can _ _ O
reduce _ _ O
accesses _ _ O
to _ _ O
DRAM _ _ O
memory _ _ O
‚óè _ _ O
Code _ _ O
in _ _ O
: _ _ O
https _ _ O
: _ _ O
/ _ _ O
/ _ _ O
docs.nvidia.com _ _ O
/ _ _ O
cuda _ _ O
/ _ _ O
cuda-c-best-practicesguide _ _ O
/ _ _ O
index.html#shared-memory-in-matrix-multiplication-c-ab _ _ O
_ _ _ O
_ _ _ O
shared _ _ O
_ _ _ O
_ _ _ O
float _ _ O
aTile _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
, _ _ O
bTile _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
; _ _ O
int _ _ O
row _ _ O
= _ _ O
blockIdx.y _ _ O
* _ _ O
blockDim.y _ _ O
+ _ _ O
threadIdx.y _ _ O
; _ _ O
int _ _ O
col _ _ O
= _ _ O
blockIdx.x _ _ O
* _ _ O
blockDim.x _ _ O
+ _ _ O
threadIdx.x _ _ O
; _ _ O
float _ _ O
sum _ _ O
= _ _ O
0.0f _ _ O
; _ _ O
aTile _ _ O
[ _ _ O
threadIdx.y _ _ O
] _ _ O
[ _ _ O
threadIdx.x _ _ O
] _ _ O
= _ _ O
a _ _ O
[ _ _ O
row*TILE_DIM+threadIdx.x _ _ O
] _ _ O
; _ _ O
bTile _ _ O
[ _ _ O
threadIdx.y _ _ O
] _ _ O
[ _ _ O
threadIdx.x _ _ O
] _ _ O
= _ _ O
b _ _ O
[ _ _ O
threadIdx.y*N+col _ _ O
] _ _ O
; _ _ O
_ _ _ O
_ _ _ O
syncthreads _ _ O
( _ _ O
) _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
i _ _ O
= _ _ O
0 _ _ O
; _ _ O
i _ _ O
< _ _ O
TILE_DIM _ _ O
; _ _ O
i++ _ _ O
) _ _ O
sum _ _ O
+ _ _ O
= _ _ O
aTile _ _ O
[ _ _ O
threadIdx.y _ _ O
] _ _ O
[ _ _ O
i _ _ O
] _ _ O
* _ _ O
bTile _ _ O
[ _ _ O
i _ _ O
] _ _ O
[ _ _ O
threadIdx.x _ _ O
] _ _ O
; _ _ O
c _ _ O
[ _ _ O
row*N+col _ _ O
] _ _ O
= _ _ O
sum _ _ O
; _ _ O
‚óè _ _ O
Assumption _ _ O
: _ _ O
TILE_DIM _ _ O
= _ _ O
w. _ _ O
What _ _ O
if _ _ O
w _ _ O
> _ _ O
TILE_DIM _ _ O
? _ _ O
54 _ _ O

Loop _ _ O
Tiling _ _ O
with _ _ O
Shared _ _ O
Memory _ _ O
‚óè _ _ O
TILE_DIM _ _ O
is _ _ O
limited _ _ O
by _ _ O
amount _ _ O
of _ _ O
shared _ _ O
memory _ _ O
. _ _ O
What _ _ O
if _ _ O
w _ _ O
> _ _ O
TILE_DIM _ _ O
? _ _ O
o _ _ O
Now _ _ O
must _ _ O
load _ _ O
A _ _ O
and _ _ O
B _ _ O
tiles _ _ O
w _ _ O
/ _ _ O
TILE_DIM _ _ O
times _ _ O
per _ _ O
thread _ _ O
block _ _ O
‚óè _ _ O
Now _ _ O
code _ _ O
will _ _ O
look _ _ O
like _ _ O
: _ _ O
N _ _ O
_ _ _ O
_ _ _ O
shared _ _ O
_ _ _ O
_ _ _ O
float _ _ O
aTile _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
, _ _ O
B _ _ O
bTile _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
[ _ _ O
TILE_DIM _ _ O
] _ _ O
; _ _ O
float _ _ O
sum _ _ O
= _ _ O
0.0f _ _ O
; _ _ O
for _ _ O
( _ _ O
int _ _ O
t _ _ O
= _ _ O
0 _ _ O
; _ _ O
t _ _ O
< _ _ O
w _ _ O
/ _ _ O
TILE_DIM _ _ O
; _ _ O
t++ _ _ O
) _ _ O
{ _ _ O
‚Ä¶ _ _ O
aTile _ _ O
[ _ _ O
threadIdx.y _ _ O
] _ _ O
[ _ _ O
threadIdx.x _ _ O
] _ _ O
= _ _ O
‚Ä¶ _ _ O
; _ _ O
A _ _ O
C _ _ O
bTile _ _ O
[ _ _ O
threadIdx.y _ _ O
] _ _ O
[ _ _ O
threadIdx.x _ _ O
] _ _ O
= _ _ O
‚Ä¶ _ _ O
; _ _ O
_ _ _ O
_ _ _ O
syncthreads _ _ O
( _ _ O
) _ _ O
; _ _ O
N _ _ O
for _ _ O
( _ _ O
int _ _ O
i _ _ O
= _ _ O
0 _ _ O
; _ _ O
i _ _ O
< _ _ O
TILE_DIM _ _ O
; _ _ O
i++ _ _ O
) _ _ O
sum _ _ O
+ _ _ O
= _ _ O
aTile _ _ O
[ _ _ O
threadIdx.y _ _ O
] _ _ O
[ _ _ O
i _ _ O
] _ _ O
* _ _ O
bTile _ _ O
[ _ _ O
i _ _ O
] _ _ O
[ _ _ O
threadIdx.x _ _ O
] _ _ O
; _ _ O
w _ _ O
} _ _ O
c _ _ O
[ _ _ O
row*N+col _ _ O
] _ _ O
= _ _ O
sum _ _ O
; _ _ O
55 _ _ O
w _ _ O



