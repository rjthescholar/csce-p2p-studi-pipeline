{"id": 23, "segment": ["train_set", "labeled"], "course": "cs0441", "lec": "lec23", "text": "Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #23: Expected Value\n\nBased on materials developed by Dr. Adam Lee\n\nWhat is a random variable?\nDefinition: A random variable is a function X from the sample\nspace of an experiment to the set of real numbers R. That is, a\nrandom variable assigns a real number to each possible outcome.\n\nNote: Despite the name, X is not a variable,\nand is not random. X is a function!\n\nExample: Suppose that a coin is flipped three times. Let X(s) be\nthe random variable that equals the numbers of heads that\nappear when s is the outcome. Then X(s) takes the following\nvalues:\nl X(HHH) = 3\nl X(HHT) = X(HTH) = X(THH) = 2\nl X(TTH) = X(THT) = X(HTT) = 1\nl X(TTT) = 0\n\nRandom variables and distributions\nDefinition: The distribution of a random variable X on a sample\nspace S is the set of pairs (r, p(X=r)) for all r \u2208 X(S), where p(X=r)\nis the probability that X takes the value r.\nNote: A distribution is usually described by specifying p(X=r) for\neach r \u2208 X(S)\n\nExample: Assume that our coin flips from the previous slide were\nall equally likely to occur. We then get the following distribution\nfor the random variable X:\nl p(X=0) = 1/8\nl p(X=1) = 3/8\nl p(X=2) = 3/8\nl p(X=3) = 1/8\n\nMany times, we want to study the expected\nvalue of a random variable\nDefinition: The expected value (or expectation) of a random\nvariable X(s) on the sample space S is equal to:\nE X = %p s X s\n!\u2208#\n\nFor every outcome...\n\n... use the probability of\nthat outcome occuring...\n\n... to weight the value of the\nrandom variable for that\noutcome.\n\nNote: The expected value of a random variable defined on an\ninfinite sample space is defined iff the infinite series in the\ndefinition is absolutely convergent.\n\nA roll of the dice...\nExample: Let X be the number that comes up when a die is\nrolled. What is the expected value of X?\n\nSolution:\nl 6 possible outcomes: 1, 2, 3, 4, 5, 6\nl Each outcomes occurs with the probability 1/6\nl E(X) = 1/6 + 2/6 + 3/6 + 4/6 + 5/6 + 6/6\nl\n= 21/6\nl\n= 7/2\n\nA flip of the coin...\nExample: A fair coin is flipped three times. Let S be the sample\nspace of the eight possible outcomes, and X be the random\nvariable that assigns to an outcome the number of heads in that\noutcome. What is the expected value of X?\n\nSolution:\nl Since coin flips are independent, each outcome is equally likely\nl E(X) = 1/8[X(HHH) + X(HHT) + X(HTH) + X(THH) + X(TTH)\n+ X(THT) + X(HTT) + X(TTT)]\nl\n= 1/8[3 + 2 + 2 + 2 + 1 + 1 + 1 + 0]\nl\n= 12/8\nl\n= 3/2\n\nIf S is large, the definition of expected value can be\ndifficult to use directly\nDefinition: If X is a random variable and p(X=r) is the probability\nthat X = r (i.e., p(X=r) = \u2211s\u2208S,X(s)=r p(s)), then\nE X = % p X=r r\n$\u2208% #\n\nEach value of X...\n\n... is weighted by its probability of\noccurrence.\n\nProof:\nl Suppose that X is a random variable ranging over S\nl Note that p(X=r) is the probability that X takes the value r\nl This means that p(X=r) is the sum of the probabilities of the outcomes s\u2208S\nsuch that X(s) = r\nl It thus follows that E X = \u2211!\u2208# $ p X = r r \u274f\n\nRolling two dice\nExample: Let X be the sum of the numbers that appear when a pair of fair\ndice is rolled. What is the expected value of X?\n\nRecall from last week:\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n\nX(1,1) = 2\nX(1,2) = X(2, 1) = 3\nX(1,3) = X(2,2) = X(3,1) = 4\nX(1,4) = X(2,3) = X(3,2) = X(4,1) = 5\nX(1,5) = X(2,4) = X(3,3) = X(4,2) = X(5,1) = 6\nX(1,6) = X(2,5) = X(3,4) = X(4,3) = X(5,2) = X(6,1) = 7\nX(2,6) = X(3,5) = X(4,4) = X(5,3) = X(6,2) = 8\nX(3,6) = X(4,5) = X(5,4) = X(6,3) = 9\nX(4,6) = X(5,5) = X(6,4) = 10\nX(5,6) = X(6,5) = 11\nX(6,6) = 12\n\np(X=2) = 1/36\np(X=3) = 2/36 = 1/18\np(X=4) = 3/36 = 1/12\np(X=5) = 4/36 = 1/9\np(X=6) = 5/36\np(X=7) = 6/36 = 1/6\np(X=8) = 5/36\np(X=9) = 4/36 = 1/9\np(X=10) = 3/36 = 1/12\np(X=11) = 2/36 = 1/18\np(X=12) = 1/36\n\nSo we have that:\nl E(X) = 2(1/36) + 3(1/18) + 4(1/12) + 5(1/9) + 6(5/36) + 7(1/6) + 8(5/36)\n+ 9(1/9) + 10(1/12) + 11(1/18) + 12(1/36)\nl\n=7\n\nWe can apply this formula to reason about\nBernoulli trials!\nTheorem: The expected number of successes when n independent\nBernoulli trials are performed, in which p is the probability of\nsuccess, is np.\nThe proof of this theorem is straightforward (cf. Sec 7.4 of the text)\nBut let's think about it intuitively...\nl 6 coin flips, how many will be heads?\nl Bernoulli trials: n = 6, p = 0.5, q = 0.5\nl Intuitively, you'd expect half of your flips to be heads\nl Mathematically, 6 * 0.5 = 3\n\nExpected values are linear!\nTheorem: If X1, X2, ..., Xn are random variables on S and if a and b\nare real numbers, then\n1.\n2.\n\nE(X1 + X2 + ... + Xn) = E(X1) + E(X2) + ... + E(Xn)\nE(aX + b) = aE(X) + b\n\nProof:\nl\nl\nl\nl\nl\n\nTo prove the first result for n=2, note that\nE(X1 + X2) = \u2211s\u2208S p(s)(X1(s) + X2(s))\nDef'n of E(X)\n= \u2211s\u2208S p(s)X1(s) + \u2211s\u2208S p(s)X2(s)\nProperty of summations\n= E(X1) + E(X2)\nDef'n of E(X)\nThe case with n variables is an easy proof by induction\n\nl\nl\nl\nl\nl\n\nTo prove the second property, note that\nE(aX + b) = \u2211s\u2208S p(s)(aX(s) + b)\n= \u2211s\u2208S p(s)aX(s) + \u2211s\u2208S p(s)b\n= a\u2211s\u2208S p(s)X(s) + b\u2211s\u2208S p(s)\n= aE(X) + b \u274f\n\nDef'n of E(X)\nProperty of summations\nProperty of summations\nDef'n of E(X), \u2211s\u2208S p(s) = 1\n\nDice, revisited\nExample: What is the expected value of the sum of the numbers\nthat appear when two fair dice are rolled?\n\nSolution:\nl Let X1 and X2 be random variables indicating the value on the first and\nsecond die, respectively\nl Want to calculate E(X1+X2)\nl By the previous theorem, we have that E(X1+X2) = E(X1)+E(X2)\nl From earlier in lecture, we know that E(X1) = E(X2) = 7/2\nl So, E(X1+X2) = 7/2 + 7/2 = 7\n\nNote: This agrees with the (more complicated)\ncalculation that we made earlier in lecture.\n\nIn-class exercises\nTop Hat\n\nSometimes we need more information than the\nexpected value can give us\nThe expected value of a random variable doesn't tell\nus the whole story...\n\np(X(s)=r)\n\np(X(s)=r)\nX(s)\n\nX(s)\n\np(X(s)=r)\nX(s)\n\nThe variance of a random variable gives us information\nabout how wide it is spread\nDefinition: The variance of a random variable X on a\nsample space S is defined as:\nV X = % X s -E X\n\n$\n\np s\n\n!\u2208#\nSquared difference from\nexpected value\n\nWeighted by probability of\noccurrence\n\nDefinition: The standard deviation of a random\nvariable X on a sample space S is defined as\n\nV X .\n\nVariance of a die\nExample: A fair die is rolled. What is the variance of the random\nvariable X representing the face that appears?\n\nSolution:\nl Recall that E(X) = 3.5\nl X(1) = 1, p(1)=1/6\nl X(2) = 2, p(2)=1/6\nl X(3) = 3, p(3)=1/6\nl X(4) = 4, p(4)=1/6\nl X(5) = 5, p(5)=1/6\nl X(6) = 6, p(6)=1/6\nl Thus, V(X) = (1/6)(1-3.5)2 + (1/6)(2-3.5)2 + (1/6)(3-3.5)2 +\n(1/6)(4-3.5)2 + (1/6)(5-3.5)2 + (1/6)(6-3.5)2\nl V(X) = 6.25/6 + 2.25/6 + 0.25/6 + 0.25/6 + 2.25/6 + 6.25/6\nl V(X) = 17.5/6 \u2248 2.92\n\nVariance: The short form\nTheorem: If X is a random variable on a sample space S,\nthen V X = E X $ - E X $.\n\nProof:\nl V(X) = \u2211s\u2208S (X(s) - E(X))2p(s)\nl\n= \u2211s\u2208S X(s)2p(s) - 2E(X)\u2211s\u2208S X(s)p(s) + E(X)2\u2211s\u2208S p(s)\nl\n= E(X2) - 2E(X)E(X) + E(X)2\nl\n= E(X2) - E(X)2\n\u274f\n\nVariance of a die, revisited\nExample: A fair die is rolled. What is the variance of the random\nvariable X representing the face that appears?\n\nSolution:\nl Recall that E(X) = 3.5\nl X2(1) = 1, p(1)=1/6\nl X2(2) = 4, p(2)=1/6\nl X2(3) = 9, p(3)=1/6\nl X2(4) = 16, p(4)=1/6\nl X2(5) = 25, p(5)=1/6\nl X2(6) = 36, p(6)=1/6\nl Thus, E(X2) = (1/6)(1) + (1/6)(4) + (1/6)(9) + (1/6)(16) + (1/6)(25) +\n(1/6)(36)\nl E(X2) = 1/6 + 4/6 + 9/6 + 16/6 + 25/6 + 36/6 = 91/6\nl V(X) = E(X2) - E(X)2 = 91/6 - 3.52 \u2248 2.92\n\nMultiple Dice\nExample: Two dice are rolled. What is the variance of the\nrandom variable X((j,k)) = 2j, where j is the number\nappearing on the first die and k is the number appearing on\nthe second die.\n\nSolution:\nl V(X) = E(X2) - E(X)2\nl Note that p(X=m) = 1/6 for m = 2,4,6,8,10,12 and is 0 otherwise\nl E(X) = (2+4+6+8+10+12)/6 = 7\nl E(X2) = (22+42+62+82+102+122)/6 = 182/3\nl So V(X) = 182/3 - 49 = 35/3\n\nVariance of a Bernoulli Distribution\nExample: What is the variance of random variable X\nwith X(t)=1 if a Bernoulli trial is a success and X(t)=0\notherwise? Assume that the probability of success is p.\n\u00a77.4 also proves that the variance\nof n Bernoulli trials is npq\nSolution:\nl Note that X takes only the values 0 and 1\nl Hence, X(t) = X2(t)\nl V(X) = E(X2) - E(X)2\nl\n= p - p2\nl\n= p(1-p)\nl\n= pq\nThis tells us that the variance of\nANY Bernoulli distribution is pq!\n\nVariance of n Bernoulli trials\nExample: A fair die is rolled 5 times. Let X be the\nrandom variable that assigns to an outcome the\nnumber of throws less than 3. What is the variance of\nX?\n\nSolution:\nl n = 5, p = 1/3, q = 2/3\nl V(X) = npq = 5 * 1/3 * 2/3 \u2248 1.11\n\nIn-class exercises\nTop Hat\n\nFinal Thoughts\nn Analyzing the expected value of a random variable\nallows us to answer a range of interesting questions\nn The variance of a random variable tells us about the\nspread of values that the random variable can take\n\n", "label": [[181, 196, "Concept"], [212, 227, "Concept"], [324, 339, "Concept"], [540, 555, "Concept"], [820, 835, "Concept"], [1189, 1204, "Concept"], [1322, 1337, "Concept"], [1573, 1588, "Concept"], [1638, 1653, "Concept"], [2623, 2638, "Concept"], [2843, 2858, "Concept"], [5790, 5805, "Concept"], [5909, 5924, "Concept"], [6004, 6019, "Concept"], [6797, 6812, "Concept"], [7576, 7591, "Concept"], [7979, 7994, "Concept"], [8455, 8470, "Concept"], [8712, 8727, "Concept"], [8801, 8816, "Concept"], [8862, 8877, "Concept"], [1419, 1431, "Concept"], [1677, 1689, "Concept"], [6027, 6039, "Concept"], [6229, 6241, "Concept"], [6818, 6830, "Concept"], [802, 835, "Concept"], [109, 123, "Concept"], [1354, 1368, "Concept"], [1618, 1632, "Concept"], [1869, 1883, "Concept"], [2280, 2294, "Concept"], [2553, 2567, "Concept"], [3200, 3214, "Concept"], [5197, 5211, "Concept"], [5739, 5753, "Concept"], [5770, 5784, "Concept"], [6111, 6125, "Concept"], [8692, 8706, "Concept"], [1373, 1384, "Concept"], [5895, 5903, "Concept"], [5990, 5998, "Concept"], [6266, 6274, "Concept"], [6327, 6335, "Concept"], [6753, 6761, "Concept"], [7009, 7017, "Concept"], [7081, 7089, "Concept"], [7560, 7568, "Concept"], [7909, 7917, "Concept"], [7967, 7975, "Concept"], [8136, 8144, "Concept"], [8325, 8333, "Concept"], [8372, 8380, "Concept"], [8544, 8552, "Concept"], [8787, 8795, "Concept"], [6182, 6200, "Concept"], [4005, 4021, "Concept"], [4084, 4100, "Concept"], [4318, 4334, "Concept"], [8150, 8166, "Concept"], [8386, 8402, "Concept"], [7923, 7945, "Concept"], [8341, 8363, "Concept"], [7909, 7945, "Concept"], [8372, 8402, "Concept"], [4036, 4064, "Concept"]]}