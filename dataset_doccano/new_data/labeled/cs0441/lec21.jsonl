{"id": 21, "segment": ["train_set", "labeled"], "course": "cs0441", "lec": "lec21", "text": "Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #21: Probability Theory\n\nBased on materials developed by Dr. Adam Lee\n\nNot all events are equally likely to occur...\n\nSporting events\n\nGames of strategy\n\nNature\n\nInvestments\n\nWe can model these types of real-life situations\nby relaxing our model of probability\nAs before, let S be our sample space. Unlike before,\nwe will allow S to be either finite or countable.\nWe will require that the following conditions hold:\n1. 0 \u2264 p(s) \u2264 1 for each s \u2208 S\n2.\n\nNo event can have a negative likelihood of\noccurrence, or more than a 100% chance\nof occurence\n\nIn any given experiment, some event will\noccur\n\nThe function p : S \u2192 [0,1] is called a probability\ndistribution\n\nRecap our formulas for the probability of\ncombinations of events\nProperty 1: p(E) = 1 - p(E)\n\nS=\n\nE\n\nl Recall that S = E \u222a E for any event E\nl Further, \u2211!\u2208# p s = 1\nl So, p(S) = p(E) + p(E) = 1\nl Thus, p(E) = 1 - p(E)\n\nE\n\nProperty 2: p(E1 \u222a E2) = p(E1) + p(E2) - p(E1 \u2229 E2)\nl Recall that p E = \u2211!\u2208$ p s\nS=\nl Let x be some outcome in E1 \u222a E2\nl If x is in one of E1 or E2, then p(x) is counted\nonce on the RHS of the equation\nl If x is in both E1 or E2, then p(x) is counted\n1 + 1 - 1 = 1 times on the RHS of the equation\n\nE1\n\nE2\n\nA formula for the probability of pairwise disjoint events\nTheorem: If E1, E2, ..., En is a sequence of pairwise disjoint events in\na sample space S, then we have:\n$\n\n$\n\np # E! = & p E!\n!\"#\n\n!\"#\n\nRecall: E1, E2, ..., En are pairwise disjoint iff Ei \u2229 Ej = \u2205 for 1 \u2264 i,j \u2264 n\nS=\nE1\n\nE2\n\n...\n\nEn\n\nWe can prove this theorem using mathematical induction!\n\nHow can we incorporate prior knowledge?\nSometimes we want to know the probability of some event given\nthat another event has occurred.\n\nExample: A fair coin is flipped three times. The first flip turns\nup tails. Given this information, what is the probability that an\nodd number of tails appear in the three flips?\n\nSolution:\nl Let F = \"the first flip of three comes up tails\"\nl Let E = \"tails comes up an odd number of times in three flips\"\nl Since F has happened, S is reduced to {THH, THT, TTH, TTT}\nl We know:\nl p(E) = |E|/|S|\nl\n= |{THH, TTT}| / |{THH, THT, TTH, TTT}|\nl\n= 2/4\nl\n= 1/2\n\nConditional Probability\nDefinition: Let E and F be events with p(F) > 0. The conditional\nprobability of E given F, denoted p(E | F), is defined as:\n\np E\u2229F\np E F =\np F\n\nIntuition:\nl Think of the event F as reducing the sample space that can be considered\nl The numerator looks at the likelihood of the outcomes in E that overlap\nthose in F\nl The denominator accounts for the reduction in sample size indicated by our\nprior knowledge that F has occurred\n\nBit strings\nExample: Suppose that a bit string of length 4 is generated at\nrandom so that each of the 16 possible 4-bit strings is equally\nlikely to occur. What is the probability that it contains at least\ntwo consecutive 0s, given that the first bit in the string is a 0?\n\nSolution:\nl Let E = \"A 4-bit string has at least two consecutive zeros\"\nl Let F = \"The first bit of a 4-bit string is a zero\"\nl Want to calculate p(E | F) = p(E \u2229 F)/p(F)\nl E \u2229 F = {0000, 0001, 0010, 0011, 0100}\nl So, p(E \u2229 F) = 5/16\nl Since each bit string is equally likely to occur,\np(F) = 8/16 = 1/2\nl So p(E | F) = (5/16)/(1/2) = 10/16 = 5/8\n\nKids\nExample: What is the conditional probability that a family with\ntwo kids has two boys, given that they have at least one boy?\nAssume that each of the possibilities BB, BG, GB, GG is equally\nlikely to occur.\nBoy is older\n\nSolution:\nl Let E = \"A family with 2 kids has 2 boys\"\nl E = {BB}\nl Let F = \"A family with 2 kids has at least 1 boy\"\nl F = {BB, BG, GB}\nl E \u2229 F = {BB}\nl So p(E | F) = p(E \u2229 F)/p(F)\nl\nl\n\n= (1/4) / (3/4)\n= 1/3\n\nGirl is older\n\nDoes prior knowledge always help us?\nExample: Suppose a fair coin is flipped twice. Does knowing that\nthe coin comes up tails on the first flip help you predict whether\nthe coin will be tails on the second flip?\n\nSolution:\nl S = {HH, HT, TH, TT}\nl F = \"Coin was tails on the first flip\" = {TH, TT}\nl E = \"Coin is tails on the second flip\" = {TT, HT}\nl p(E) = 2/4 = 1/2\nl p(E | F) = p(E \u2229 F)/p(F)\nl\n= (1/4) / (2/4)\nl\n= 1/2\nl Knowing the first flip does not help you guess the second flip!\n\nIndependent Events\nDefinition: We say that events E and F are independent if and\nonly if p(E \u2229 F) = p(E)p(F).\n\nRecall: In our last example...\nl S = {HH, HT, TH, TT}\nl F = {TH, TT}\nl E = {HT, TT}\nl E \u2229 F = {TT}\n\nSo:\nl p(E \u2229 F) = |E \u2229 F|/|S|\nl\n= 1/4\nl p(E)p(F) = 1/2 \u00d7 1/2\nl\n= 1/4\n\nThis checks out!\n\nExample: Bit Strings\nExample: Suppose that E is the event that a randomly generated\nbit string of length four begins with a 1, and F is the event that\nthis bit string contains an even number of 1s. Are E and F\nindependent if all 4-bit strings are equally likely to occur?\n\nSolution:\nl By the product rule, |S| = 24 = 16\nl E = {1111, 1110, 1101, 1011, 1100, 1010, 1001, 1000}\nl F = {0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111}\nl So p(E) = p(F) = 8/16 = 1/2\nl p(E)p(F) = 1/4\nl E \u2229 F = {1111, 1100, 1010, 1001}\nl p(E \u2229 F) = 4/16 = 1/4\nl Since p(E \u2229 F) = p(E)p(F), E and F are independent events\n\nExample: Distribution of kids\nExample: Assume that each of the four ways that a family can\nhave two children are equally likely. Are the events E that a\nfamily with two children has two boys, and F that a family with\ntwo children has at least one boy independent?\n\nSolution:\nl E = {BB}\nl F = {BB, BG, GB}\nl p(E) = 1/4\nl p(F) = 3/4\nl p(E)p(F) = 3/16\nl E \u2229 F = {BB}\nl p(E \u2229 F) = 1/4\nl Since 1/4 \u2260 3/16, E and F are not independent\n\nIf probabilities are independent, we can use the product rule to\ndetermine the probabilities of combinations of events\nExample: What is the probability of flipping heads 4 times in a\nrow using a fair coin?\n\nAnswer: p(H) = 1/2, so p(HHHH) = (1/2)4 = 1/16\n\nExample: What is the probability of rolling the same number 3\ntimes in a row using an unbiased 6-sided die?\n\nAnswer:\nl\nl\nl\nl\n\nFirst roll agrees with itself with probability 1\n2nd roll agrees with first with probability 1/6\n3rd roll agrees with first two with probability 1/6\nSo probability of rolling the same number 3 times is 1 \u00d7 1/6 \u00d7 1/6 = 1/36\n\nIn-class exercises\nTop Hat\n\nMany experiments only have two outcomes\n\nP(x)\nCoin flips: Heads or tails?\n\nBit strings: 0 or 1?\n\nPredicates: T or F?\n\nThese types of experiments are called Bernoulli trials\nTwo outcomes:\nl Success\nl Failure\n\nProbability p\nProbability q = 1 - p\n\nMany problems can be solved by examining the probability of k\nsuccesses in an experiment consisting of mutuallyindependent Bernoulli trials\n\nExample: Coin flips\nExample: A coin is biased so that the probability of heads is 2/3.\nWhat is the probability that exactly four heads come up when the\ncoin is flipped seven times, assuming that each flip is independent?\n\nSolution:\nl 27 = 128 possible outcomes for seven flips\nl There are C(7,4) ways that heads can be flipped four times\nl Since each flip is independent, the probability of each of these outcomes is\n(2/3)4(1/3)3\nl So, the probability of exactly 4 heads occurring in 7 flips of this biased coin\nis C(7,4)(2/3)4(1/3)3 = 560/2187\n\n7 Choose 4 outcomes\nto make heads\n\nProbability of each tails\ncombined using product rule\nProbability of each heads\ncombined using product rule\n\nThis general reasoning provides us with a nice formula...\nTheorem: The probability of exactly k successes in n independent\nBernoulli trials, with probability of success p and probability of\nfailure q = 1-p, is C(n,k)pkqn-k.\n\nProof:\nl The outcome of n Bernoulli trials is an n-tuple (t1, t2, ..., tn)\nl Each ti is either S (for success) or F (for failure)\nl C(n,k) ways to choose k tis to label S\nl Since each trial is independent, the probability of each outcome with k\nsuccesses and n-k failures is pkqn-k\nl So, the probability of exactly k successes is C(n,k)pkqn-k. \u274f\n\nNotation: We denote the probability of k successes in n independent\nBernoulli trials with probability of success p as b(k; n, p).\n\nBits (Again)\nExample: Suppose that the probability that a 0 bit is generated is\n0.9, that the probability that a 1 bit is generated is 0.1, and that\nbits are generated independently. What is the probability that\nexactly eight 0 bits are generated when ten random bits are\ngenerated?\n\nSolution:\nl Number of trials\nl Number of successes\nl Probability of success\nl Probability of failure\nl Want to compute b(k; 10, 0.9)\nl\n= C(10, 8)0.980.12\nl\n= 0.1937102445\n\nn = 10\nk=8\np = 0.9\nq = 1 - 0.9 = 0.1\n\nMany probability questions are concerned with some\nnumerical value associated with an experiment\n\nNumber of boys in a family\nNumber of 1 bits generated\n\nBeats per minute of a heart\n\nLongevity of a chicken\n\nNumber of \"heads\" flips\n\nWhat is a random variable?\nDefinition: A random variable is a function X from the sample\nspace of an experiment to the set of real numbers R. That is, a\nrandom variable assigns a real number to each possible outcome.\n\nNote: Despite the name, X is not a variable,\nand is not random. X is a function!\n\nExample: Suppose that a coin is flipped three times. Let X(s) be\nthe random variable that equals the numbers of heads that\nappear when s is the outcome. Then X(s) takes the following\nvalues:\nl X(HHH) = 3\nl X(HHT) = X(HTH) = X(THH) = 2\nl X(TTH) = X(THT) = X(HTT) = 1\nl X(TTT) = 0\n\nRandom variables and distributions\nDefinition: The distribution of a random variable X on a sample\nspace S is the set of pairs (r, p(X=r)) for all r \u2208 X(S), where p(X=r)\nis the probability that X takes the value r.\nNote: A distribution is usually described by specifying p(X=r) for\neach r \u2208 X(S)\n\nExample: Assume that our coin flips from the previous slide were\nall equally likely to occur. We then get the following distribution\nfor the random variable X:\nl p(X=0) = 1/8\nl p(X=1) = 3/8\nl p(X=2) = 3/8\nl p(X=3) = 1/8\n\nExample: Rolling dice\nLet X be the sum of the numbers that appear when a pair of fair dice\nis rolled. What are the values of this random variable for the 36\npossible outcomes (i, j) where i and j are the numbers that appear on\nthe first and second die, respectively?\n\nAnswer:\nl X(1,1) = 2\nl X(1,2) = X(2, 1) = 3\nl X(1,3) = X(2,2) = X(3,1) = 4\nl X(1,4) = X(2,3) = X(3,2) = X(4,1) = 5\nl X(1,5) = X(2,4) = X(3,3) = X(4,2) = X(5,1) = 6\nl X(1,6) = X(2,5) = X(3,4) = X(4,3) = X(5,2) = X(6,1) = 7\nl X(2,6) = X(3,5) = X(4,4) = X(5,3) = X(6,2) = 8\nl X(3,6) = X(4,5) = X(5,4) = X(6,3) = 9\nl X(4,6) = X(5,5) = X(6,4) = 10\nl X(5,6) = X(6,5) = 11\nl X(6,6) = 12\n\np(X=2) = 1/36\np(X=3) = 2/36\np(X=4) = 3/36\np(X=5) = 4/36\np(X=6) = 5/36\np(X=7) = 6/36\np(X=8) = 5/36\np(X=9) = 4/36\np(X=10) = 3/36\np(X=11) = 2/36\np(X=12) = 1/36\n\nSometimes probabilistic reasoning can lead us to some\ninteresting and unexpected conclusions...\nQuestion: How many people need to be in the same room so that\nthe probability of two people sharing the same birthday is greater\nthan 1/2?\n\nAssumptions:\n1. There are 366 possible birthdays\n2. All birthdays are equally likely to occur\n3. Birthdays are independent\n\nSolution tactic:\nl Find the probability pn that the n people in a room all have\ndifferent birthdays\nl Then compute 1-pn, which is the probability that at least two\npeople share the same birthday\n\nLet's figure this out...\nLet's assess probabilities as people enter the room\nl Person 1 clearly doesn't have the same birthday as anyone else in the\nroom\nl P2 has a different birthday than P1 with probability 365/366\nl P3 has a different birthday than P1 and P2 with probability 364/366\nl ...\n\nIn general, Pj has a different birthday than P1, P2, ..., Pj-1 with\nprobability [366-(j-1)]/366 = (367-j)/366\nRecall that pn is the probability that n people in the room all have\ndifferent birthdays. Using our above observations, this means:\n\nBut we're interested in 1-pn ...\n\nTo check the minimum number of people need in the room to\nensure that pn > 1/2, we'll use trial and error:\nl If n = 22, then 1 - pn \u2248 0.475\nl If n = 23, then 1 - pn \u2248 0.506\n\nSo, you need only 23 people in a room to have a better than 50%\nchance that two people share the same birthday!\n\nIn-class exercises\nProblem 3: What is the probability that exactly 2 heads occur\nwhen a fair coin is flipped 7 times?\nProblem 4: Consider a game between Alice and Bob. Over time,\nAlice has been shown to win this game (against Bob) 75% of the\ntime. If Alice and Bob play 6 games in a row, what is the\nprobability that Alice wins every game?\nProblem 5: Consider generating a uniformly-random 4-character\nbit string. Also consider R, a random variable that measures the\nlongest run of 1 bits in the generated string. Determine the\ndistribution of R.\n\nFinal Thoughts\nn Today we covered\nl Conditional probability\nl Independence\nl Bernoulli trials\nl Random variables\nl Probabilistic analysis\n\nn Next time:\nl Bayes' Theorem (Section 7.3)\n\nThe proof...\nP(n) \u2261 p \u22c3$!\"# E! = \u2211$!\"# p E!\nBase case: P(2): Let E1, E2 be disjoint events.\nBy definition, p(E1 \u222a E2) = p(E1) + p(E2) - p(E1 \u2229 E2).\nSince E1 \u2229 E2 = \u2205, p(E1 \u222a E2) = p(E1) + p(E2)\nI.H.: Assume that P(k) holds for an arbitrary integer k\nInductive step: We will now show that P(k) \u2192 P(k+1)\nn Consider E = E1 \u222a E2 \u222a ... \u222a Ek \u222a Ek+1\nn Let J = E1 \u222a E2 \u222a ... \u222a Ek, so E = J \u222a Ek+1\nn p(E) = p(J \u222a Ek+1)\nn\n= p(J) \u222a p(Ek+1)\nn\n= p(E1 \u222a E2 \u222a ... \u222a Ek) \u222a p(Ek+1)\nn\n= p(E1) \u222a p(E2) \u222a ... \u222a p(Ek) \u222a p(Ek+1)\n\nby definition of E\nby I.H.\nby definition of J\nby I.H.\n\nConclusion: Since we have proved the base case and the inductive\ncase, the claim holds by mathematical induction \u274f\n\n", "label": [[389, 401, "Concept"], [1426, 1438, "Concept"], [2451, 2463, "Concept"], [2233, 2256, "Concept"], [3334, 3357, "Concept"], [12612, 12635, "Concept"], [4247, 4265, "Concept"], [5122, 5140, "Concept"], [12638, 12650, "Concept"], [1326, 1350, "Concept"], [1396, 1420, "Concept"], [1335, 1350, "Concept"], [1405, 1420, "Concept"], [12835, 12850, "Concept"], [6361, 6377, "Concept"], [6573, 6589, "Concept"], [7404, 7420, "Concept"], [7532, 7548, "Concept"], [7921, 7937, "Concept"], [12653, 12669, "Concept"], [6394, 6401, "Concept"], [7442, 7449, "Concept"], [7608, 7615, "Concept"], [7958, 7965, "Concept"], [8336, 8343, "Concept"], [6404, 6411, "Concept"], [7471, 7478, "Concept"], [7627, 7634, "Concept"], [8361, 8368, "Concept"], [7427, 7449, "Concept"], [7943, 7965, "Concept"], [8321, 8343, "Concept"], [8346, 8368, "Concept"], [6361, 6377, "Concept"], [6573, 6589, "Concept"], [7404, 7420, "Concept"], [7532, 7548, "Concept"], [7921, 7937, "Concept"], [12653, 12669, "Concept"], [7971, 7981, "Concept"], [8719, 8734, "Concept"], [8750, 8765, "Concept"], [8862, 8877, "Concept"], [9078, 9093, "Concept"], [9358, 9373, "Concept"], [9727, 9742, "Concept"], [9937, 9952, "Concept"], [12461, 12476, "Concept"], [9340, 9373, "Concept"], [4837, 4849, "Concept"], [5621, 5633, "Concept"], [7213, 7225, "Concept"], [7267, 7279, "Concept"], [2722, 2732, "Concept"], [2985, 2995, "Concept"], [3064, 3074, "Concept"], [3207, 3217, "Concept"], [4629, 4639, "Concept"], [4701, 4711, "Concept"], [12430, 12440, "Concept"], [1618, 1640, "Concept"], [13413, 13435, "Concept"], [12804, 12813, "Concept"], [13360, 13369, "Concept"], [13010, 13024, "Concept"], [12691, 12713, "Concept"]]}