{"id": 22, "segment": ["train_set", "labeled"], "course": "cs0441", "lec": "lec22", "text": "Discrete Structures for Computer\nScience\n\nWilliam Garrison\nbill@cs.pitt.edu\n6311 Sennott Square\nLecture #22: Bayes' Theorem\n\nBased on materials developed by Dr. Adam Lee\n\nToday's Topics\nn Bayes' Theorem\nl What do we compute conditional probabilities with\nincomplete information?\n\nConditional Probability\nDefinition: Let E and F be events with p(F) > 0. The conditional\nprobability of E given F, denoted p(E | F), is defined as:\n\np E\u2229F\np E F =\np F\n\nIntuition:\nl Think of the event F as reducing the sample space that can be considered\nl The numerator looks at the likelihood of the outcomes in E that overlap\nthose in F\nl The denominator accounts for the reduction in sample size indicated by our\nprior knowledge that F has occurred\n\nBayes' Theorem\nBayes' Theorem allows us to relate the conditional and marginal\nprobabilities of two random events.\n\n?\nIn English: Bayes' Theorem will help us assess the probability\nthat an event occurred given only partial evidence.\nDoesn't our formula for conditional probability do this already?\n\nWe can't always use this\nformula directly...\n\nA Motivating Example\nSuppose that a certain drug test correctly identifies a person who\nuse the drug as testing positive 99% of the time, and will correctly\nidentify a non-user as testing negative 99% of the time. If a\ncompany suspects that 0.5% of its employees are users of the\ndrug, what is the probability that an employee that tests positive\nfor this drug is actually a user?\n\nQuestion: Can we use our simple\nconditional probability formula?\np EF =\n\nX is a user\n\n!(#\u2229%)\n!(%)\n\nX tested positive\n\nThe 1,000 foot view...\nIn situations like those on the last slide, Bayes' theorem can help!\nEssentially, Bayes' theorem will allow us to calculate P(E|F)\nassuming that we know (or can derive):\nl P E\nl P FE\n&\nl P(F|E)\n\nProbability that X is a user\nTest success rate\nTest false positive rate\n\nProbability that X is a user of the\ndrug, given a positive test\n\nReturning to our earlier example:\nl Let E = \"Person X is a user of the drug\"\nl Let F = \"Person X tested positive for the drug\"\n\nIt sounds like Bayes' Theorem could help in this case...\n\nNew Notation\nTo simplify expressions, we will use the notation EC to\ndenote the complementary event of E\nThat is:\n\nC\nE=E\n\nA Simple Example\nWe have two boxes. The first contains two green balls and seven\nred balls. The second contains four green balls and three red\nballs. Bob selects a ball by first choosing a box at random. He\nthen selects one of the balls from that box at random. If Bob has\nselected a red ball, what is the probability that he took it from\nthe first box?\n\n1\n\n2\n\nPicking the problem apart...\nFirst, let's define a few events relevant to this problem:\nl Let E = Bob has chosen a red ball\nl By definition EC = Bob has chosen a green ball\nl Let F = Bob chose his ball from this first box\nl Therefore, FC = Bob chose his ball from the second box\n\nWe want to find the probability that Bob chose from the first box,\ngiven that he picked a red ball. That is, we want p(F|E).\nGoal: Given that p(F|E) = p(F \u2229 E)/p(E), use what we know to\nderive p(F \u2229 E) and p(E).\n\nWhat do we know?\nWe have two boxes. The first contains two green balls and seven red balls.\nThe second contains four green balls and three red balls. Bob selects a ball\nby first choosing a box at random. He then selects one of the balls from that\nbox at random. If Bob has selected a red ball, what is the probability that he\ntook it from the first box?\nStatement: Bob selects a ball by first choosing a box at random\nl Bob is equally likely to choose the first box, or the second box\nl p(F) = p(FC) = 1/2\n\nStatement: The first contains two green balls and seven red balls\nl The first box has nine balls, seven of which are red\nl p(E|F) = 7/9\n\nStatement: The second contains four green balls and three red balls\nl The second box contains seven balls, three of which are red\nl p(E|FC) = 3/7\n\nNow, for a little algebra...\nThe end goal: Compute p(F|E) = p(F \u2229 E)/p(E)\nNote that p(E|F) = p(E \u2229 F)/p(F)\nl If we multiply by p(F), we get p(E \u2229 F) = p(E|F) p(F)\nl Further, we know that p(E|F) = 7/9 and p(F) = 1/2\nl So p(E \u2229 F) = 7/9 \u00d7 1/2 = 7/18\n\nRecall:\n\u2022p(F) = p(FC) = 1/2\n\u2022p(E|F) = 7/9\n\u2022p(E|FC) = 3/7\n\nSimilarly, p(E \u2229 FC) = p(E|FC) p(FC) = 3/7 \u00d7 1/2 = 3/14\nObservation: E = (E \u2229 F) \u222a (E \u2229 FC)\nl This means that p(E) = p(E \u2229 F) + p(E \u2229 FC)\nl\nl\nl\nl\n\n= 7/18 + 3/14\n= 49/126 + 27/126\n= 76/126\n= 38/63\n\nDenouement\nThe end goal: Compute p(F|E) = p(F \u2229 E)/p(E)\nSo, p(F|E) = (7/18) / (38/63) \u2248 0.645\n\nRecall:\n\u2022p(F) = p(FC) = 1/2\n\u2022p(E|F) = 7/9\n\u2022p(E|FC) = 3/7\n\u2022p(E \u2229 F) = 7/18\n\u2022p(E) = 38/63\n\nHow did we get here?\n1.\n2.\n\nExtract what we could from the problem definition itself\nRearrange terms to derive p(F \u2229 E) and p(E)\n\n3.\n\nUse our trusty definition of conditional probability to do the rest!\n\nThe reasoning that we used in the last problem\nessentially derives Bayes' Theorem for us!\nBayes' Theorem: Suppose that E and F are events from some\nsample space S such that p(E) \u2260 0 and p(F) \u2260 0. Then:\np E F p F\np F E =\nP E F p F + p E F' p F'\n\nProof:\nl The definition of conditional probability says that\n\u27a3 p(F|E) = p(F \u2229 E)/p(E)\n\u27a3 p(E|F) = p(E \u2229 F)/p(F)\n\nl This means that\n\u27a3 p(E \u2229 F) = p(F|E)p(E)\n\u27a3 p(E \u2229 F) = p(E|F)p(F)\n\nl So p(F|E)p(E) = p(E|F)p(F)\nl Therefore, p(F|E) = p(E|F)p(F)/p(E)\n\nProof (continued)\nNote: To finish, we must prove p(E) = p(E | F)p(F) + p(E | FC)p(FC)\nl Observe that E = E \u2229 S\nl\n= E \u2229 (F \u222a FC)\nl\n= (E \u2229 F) \u222a (E \u2229 FC)\nl Note also that (E \u2229 F) and (E \u2229 FC) are disjoint (i.e., no x can be in both F and FC)\nl This means that p(E) = p(E \u2229 F) + p(E \u2229 FC)\nl We already have shown that p(E \u2229 F) = p(E|F)p(F)\nl Further, since p(E | FC) = p(E \u2229 FC)/p(FC), we have that p(E \u2229 FC) = p(E|FC)p(FC)\nl So p(E) = p(E \u2229 F) + p(E \u2229 FC) = p(E|F)p(F) + p(E|FC)p(FC)\n\nPutting everything together, we get:\np E F p F\np F E =\nP E F p F + p E F' p F'\n\u274f\n\nAnd why is this useful?\nIn a nutshell, Bayes' Theorem is useful if you want to find p(F|E),\nbut you don't know p(E \u2229 F) or p(E).\n\nHere's a general solution tactic\nStep 1: Identify the independent events that are being\ninvestigated. For example:\nl F = Bob chooses the first box, FC = Bob chooses the second box\nl E = Bob chooses a red ball, EC = Bob chooses a green ball\n\nStep 2: Record the probabilities identified in the problem\nstatement. For example:\nl p(F) = p(FC) = 1/2\nl p(E|F) = 7/9\nl p(E|FC) = 3/7\n\nStep 3: Plug into Bayes' formula and solve\n\nExample: Pants and Skirts\nSuppose there is a co-ed school having 60% boys and 40% girls as\nstudents. The girl students wear pants or skirts in equal numbers;\nthe boys all wear pants. An observer sees a (random) student\nfrom a distance; all they can see is that this student is wearing\npants. What is the probability this student is a girl?\nStep 1: Set up events\nl\nl\nl\nl\n\nE = X is wearing pants\nEC = X is wearing a skirt\nF = X is a girl\nFC = X is a boy\n\nStep 2: Extract probabilities from problem definition\nl\nl\nl\nl\n\np(F) = 0.4\np(FC) = 0.6\np(E|F) = p(EC|F) = 0.5\np(E|FC) = 1\n\nPants and Skirts (continued)\np F E =\n\np E F p F\nP E F p F + p E F! p F!\n\nStep 3: Plug in to Bayes' Theorem\n\nRecall:\n\u2022 p(F) = 0.4\n\u2022 p(FC) = 0.6\n\u2022 p(E|F) = p(EC|F) = 0.5\n\u2022 p(E|FC) = 1\n\nl p(F|E) = (0.5 \u00d7 0.4)/(0.5 \u00d7 0.4 + 1 \u00d7 0.6)\nl\n= 0.2/0.8\nl\n= 1/4\n\nConclusion: There is a 25% chance that the person seen was a\ngirl, given that they were wearing pants.\n\nDrug screening, revisited\nSuppose that a certain drug test correctly identifies a person who\nuses the drug as testing positive 99% of the time, and will\ncorrectly identify a non-user as testing negative 99% of the time.\nIf a company suspects that 0.5% of its employees are users of the\ndrug, what is the probability that an employee that tests positive\nfor this drug is actually a user?\nStep 1: Set up events\nl\nl\nl\nl\n\nF = X is a user\nFC = X is not a user\nE = X tests positive for the drug\nEC = X tests negative for the drug\n\nStep 2: Extract probabilities from problem definition\nl p(F) = 0.005\nl p(FC) = 0.995\nl p(E|F) = 0.99\nl p(E|FC) = 0.01\n\nDrug screening (continued)\np F E =\n\np E F p F\nP E F p F + p E F! p F!\n\nStep 3: Plug in to Bayes' Theorem\n\nRecall:\n\u2022 p(F) = 0.005\n\u2022 p(FC) = 0.995\n\u2022 p(E|F) = 0.99\n\u2022 p(E|FC) = 0.01\n\nl p(F|E) = (0.99 \u00d7 0.005)/(0.99 \u00d7 0.005 + 0.01 \u00d7 0.995)\nl\n= 0.3322\n\nConclusion: If an employee tests positive for the drug, there is\nonly a 33% chance that they are actually a user!\n\nIn-class exercises\nSuppose that 1 person in 100,000 has a particular rare disease. A\ndiagnostic test is correct 99% of the time when given to someone\nwith the disease, and is correct 99.5% of the time when given to\nsomeone without the disease.\nProblem 1: Calculate the probability that someone who tests\npositive for the disease actually has it.\nProblem 2: Calculate the probability that someone who tests\nnegative for the disease does not have the disease.\n\nApplication: Spam filtering\nDefinition: Spam is unsolicited bulk email\nI didn't ask for it, I probably\ndon't want it\n\nSent to lots of people...\n\nIn recent years, spam has become increasingly\nproblematic. For example, in 2015, spam accounted\nfor ~50% of all email messages sent.\nTo combat this problem, people have developed spam\nfilters based on Bayes' theorem!\n\nHow does a Bayesian spam filter work?\nEssentially, these filters determine the probability that a message\nis spam, given that it contains certain keywords.\np E F p F\np F E =\nP E F p F + p E F' p F'\nMessage is spam\n\nMessage contains\nquestionable keyword\n\nIn the above equation:\nl p(E|F) = Probability that our keyword occurs in spam messages\nl p(E|FC) = Probability that our keyword occurs in legitimate messages\nl p(F) = Probability that an arbitrary message is spam\nl p(FC) = Probability that an arbitrary message is legitimate\n\nQuestion: How do we derive these parameters?\n\nWe can learn these parameters by examining\nhistorical email traces\nImagine that we have a corpus of email messages...\nWe can ask a few intelligent questions to learn the parameters of our\nBayesian filter:\nl How many of these messages do we consider spam?\nl In the spam messages, how often does our keyword appear?\nl In the good messages, how often does our keyword appear?\n\np(F)\np(E|F)\np(E|FC)\n\nAside: This is what happens when you click the \"mark as spam\" button\nin your email client!\n\nGiven this information, we can apply Bayes' theorem!\n\nFiltering spam using a single keyword\nSuppose that the keyword \"Rolex\" occurs in 250 of 2000 known spam\nmessages, and in 5 of 1000 known good messages. Estimate the\nprobability that an incoming message containing the word \"Rolex\" is\nspam, assuming that it is equally likely that an incoming message is\nspam or not spam. If our threshold for classifying a message as spam\nis 0.9, will we reject this message?\nStep 1: Define events\nl F = message is spam\nl FC = message is good\nl E = message contains the keyword \"Rolex\"\nl EC = message does not contain the keyword \"Rolex\"\n\nStep 2: Gather probabilities from the problem statement\nl p(F) = p(FC)= 0.5\nl p(E|F) = 250/2000 = 0.125\nl p(E|FC) = 5/1000 = 0.005\n\nSpam Rolexes (continued)\nRecall:\n\u2022 p(F) = p(FC) = 0.5\n\u2022 p(E|F) = 0.125\n\u2022 p(E|FC) = 0.005\n\nStep 3: Plug in to Bayes' Theorem\nl p(F|E) = (0.125 \u00d7 0.5)/(0.125 \u00d7 0.5 + 0.005 \u00d7 0.5)\nl\n= 0.125/(0.125 + 0.005)\nl\n\u2248 0.962\n\nConclusion: Since the probability that our message is spam given\nthat it contains the string \"Rolex\" is approximately 0.962 >\n0.9, we will discard the message.\n\nProblems with this simple filter\nHow would you choose a single keyword/phrase to use?\nl \"All natural\"\nl \"Nigeria\"\nl \"Click here\"\nl ...\n\nUsers get upset if false positives occur, i.e., if legitimate\nmessages are incorrectly classified as spam\nl When was the last time you checked your spam folder?\n\nHow can we fix this?\nl Choose keywords so p(spam | keyword) is very high or very low\nl Filter based on multiple keywords\n\nSpecifically, we want to develop a Bayesian filter that tells us\np(F | E1 \u2229 E2)\nFirst, some assumptions\n1. Events E1 and E2 are independent\n2. The events E1|F and E2|F are independent\n3. p(F) = p(FC) = 0.5\n\nBy Bayes' theorem\n\nNow, let's derive formula for this p(F | E1 \u2229 E2)\np F E( \u2229 E) =\n=\n=\n\np E(\n\np E( \u2229 E)\n\np E( \u2229 E) \u2223 F p F\nF p F + p E( \u2229 E) F ' p F '\n\np(E( \u2229 E) \u2223 F)\np E( \u2229 E) F + p E( \u2229 E) F '\n\nAssumption 3\nAssumptions 1 and 2\n\np E( F p(E) \u2223 F)\nF p E) F + p E( F ' p E) F '\n\nSpam filtering on two keywords\nSuppose that we train a Bayesian spam filter on a set of 2000 spam\nmessages and 1000 messages that are not spam. The word \"stock\"\nappears in 400 spam messages and 60 good messages, and the word\n\"undervalued\" appears in 200 spam messages and 25 good messages.\nEstimate the probability that a message containing the words \"stock\"\nand \"undervalued\" is spam. Will we reject this message if our spam\nthreshold is set at 0.9?\nStep 1: Set up events\nl F = message is spam, FC = message is good\nl E1 = message contains the word \"stock\"\nl E2 = message contains the word \"undervalued\"\n\nStep 2: Identify probabilities\nl P(E1|F) = 400/2000 = 0.2\nl p(E1|FC) = 60/1000 = 0.06\nl p(E2|F) = 200/2000 = 0.1\nl p(E2|FC) = 25/1000 = 0.025\n\nTwo keywords (continued)\np F E! \u2229 E\" =\n\np E!\n\np E! F p(E\" \u2223 F)\nF p E\" F + p E! F # p E\" F #\n\nStep 3: Plug in to Bayes' Theorem\n\nRecall:\n\u2022 p(E1|F) = 0.2\n\u2022 p(E1|FC) = 0.06\n\u2022 p(E2|F) = 0.1\n\u2022 p(E2|FC) = 0.025\n\nl p(F|E1 \u2229 E2) = (0.2 \u00d7 0.1)/(0.2 \u00d7 0.1 + 0.06 \u00d7 0.025)\nl\n= 0.02/(0.02 + 0.0015)\nl\n\u2248 0.9302\n\nConclusion: Since the probability that our message is spam\ngiven that it contains the strings \"stock\" and\n\"undervalued\" is \u2248 0.9302 > 0.9, we will reject this\nmessage.\n\nIn-class exercises\nProblem 3: A business records incoming emails for 1 week and\ncollects 1,000 spam messages and 400 non-spam messages. The\nword \"opportunity\" appears in 175 spam messages and 20 nonspam messages. Assuming this week's emails were typical\n(including the proportion of spam), should an incoming message\nbe labeled as spam if it contains the word \"opportunity\" and the\nthreshold for rejecting is 0.9?\nProblem 4: Suppose that a Bayesian spam filter is trained on a\nset of 10,000 spam messages and 5,000 messages that are not\nspam. The word \"enhancement\" appears in 1,500 spam messages\nand 20 non-spam messages, while the word \"herbal\" appears in\n800 spam messages and 200 non-spam messages. Estimate the\nprobability that a received message containing both the words\n\"enhancement\" and \"herbal\" is spam. Here, you may assume that\n50% of emails are spam.\n\nFinal Thoughts\nn Conditional probability is very useful\nn Bayes' theorem\nl Helps us assess conditional probabilities\nl Has a range of important applications\n\nn Next time:\nl Expected values and variance (Section 7.4)\n\n", "label": [[280, 303, "Concept"], [990, 1013, "Concept"], [1492, 1515, "Concept"], [4738, 4761, "Concept"], [5051, 5074, "Concept"], [14340, 14363, "Concept"], [109, 123, "Concept"], [188, 202, "Concept"], [733, 747, "Concept"], [748, 762, "Concept"], [863, 877, "Concept"], [1645, 1659, "Concept"], [1683, 1697, "Concept"], [2077, 2091, "Concept"], [4846, 4860, "Concept"], [4869, 4883, "Concept"], [5874, 5888, "Concept"], [7053, 7067, "Concept"], [8048, 8062, "Concept"], [9125, 9139, "Concept"], [10242, 10256, "Concept"], [11071, 11085, "Concept"], [11967, 11981, "Concept"], [13102, 13116, "Concept"], [14381, 14395, "Concept"], [2200, 2219, "Concept"], [6019, 6037, "Concept"], [8792, 8796, "Concept"], [8819, 8823, "Concept"], [8941, 8945, "Concept"], [9005, 9009, "Concept"], [9103, 9107, "Concept"], [9162, 9166, "Concept"], [9251, 9255, "Concept"], [9351, 9355, "Concept"], [9469, 9473, "Concept"], [9604, 9608, "Concept"], [9967, 9971, "Concept"], [9982, 9986, "Concept"], [10169, 10173, "Concept"], [10269, 10273, "Concept"], [10358, 10362, "Concept"], [10492, 10496, "Concept"], [10561, 10565, "Concept"], [10573, 10577, "Concept"], [10625, 10629, "Concept"], [10706, 10710, "Concept"], [10962, 10966, "Concept"], [11230, 11234, "Concept"], [11574, 11578, "Concept"], [11621, 11625, "Concept"], [11679, 11683, "Concept"], [12241, 12245, "Concept"], [12305, 12309, "Concept"], [12334, 12338, "Concept"], [12379, 12383, "Concept"], [12417, 12421, "Concept"], [12495, 12499, "Concept"], [12621, 12625, "Concept"], [12662, 12666, "Concept"], [12731, 12735, "Concept"], [13343, 13347, "Concept"], [13553, 13557, "Concept"], [13579, 13583, "Concept"], [13632, 13636, "Concept"], [13741, 13745, "Concept"], [13789, 13793, "Concept"], [13907, 13911, "Concept"], [13949, 13953, "Concept"], [13995, 13999, "Concept"], [14041, 14045, "Concept"], [14066, 14070, "Concept"], [14120, 14124, "Concept"], [14146, 14150, "Concept"], [14266, 14270, "Concept"], [14316, 14320, "Concept"], [9162, 9173, "Concept"], [12305, 12316, "Concept"], [13907, 13918, "Concept"], [9153, 9173, "Concept"], [12296, 12316, "Concept"], [13898, 13918, "Concept"], [9387, 9394, "Concept"], [9451, 9458, "Concept"], [9516, 9523, "Concept"], [10016, 10023, "Concept"], [10075, 10082, "Concept"], [10289, 10296, "Concept"], [10314, 10321, "Concept"], [10761, 10768, "Concept"], [10813, 10820, "Concept"], [11400, 11407, "Concept"], [11686, 11693, "Concept"], [1848, 1862, "Concept"], [1848, 1867, "Concept"], [7314, 7328, "Concept"], [7958, 7972, "Concept"]]}