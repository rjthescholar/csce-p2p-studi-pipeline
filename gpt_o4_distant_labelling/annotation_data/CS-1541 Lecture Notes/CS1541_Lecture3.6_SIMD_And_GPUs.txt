SIMD and GPUs
CS 1541
Wonsun Ahn

SIMD Architectures

2

ISA not optimized for data parallel workloads
‚óè This loop does multiply accumulate (MAC):
for (int i = 0; i < 64; i++) {
y[i] = a * x[i] + y[i]
}
o A common operation in digital signal processing (DAXPY)
‚óè Note how we apply the same MAC operation on each data item
o This is how many data parallel workloads look like
‚óè A conventional ISA (likes MIPS) is not optimal for encoding this
o Results in wasted work and suboptimal performance
o Let‚Äôs look at the actual MIPS translation

3

MIPS code for ùíö ùíä = ùíÇ ‚àó ùíô ùíä + ùíö ùíä
l.d
$f0,0($sp)
;$f0 = a
addi $s2,$s0,512
;64 elements (64*8=512 bytes)
loop: l.d
$f2,0($s0)
;$f2 = x(i)
mul.d $f2,$f2,$f0
;$f2 = a * x(i)
l.d
$f4,0($s1)
;$f4 = y(i)
add.d $f4,$f4,$f2
;$f4 = a * x(i) + y(i)
s.d
$f4,0($s1)
;y(i) = $f4
addi $s0,$s0,8
;increment index to x
addi $s1,$s1,8
;increment index to y
subu $t0,$s2,$s0
;evaluate i < 64 loop condition
bne
$t0,$zero,loop ;loop if not done
‚óè Blue instructions don‚Äôt do actual computation. There for indexing and loop control.
o Is there a way to avoid? Loop unrolling yes. But that causes code bloat!
‚óè Red instructions do computation. But why decode them over and over again?
o Is there a way to fetch and decode once and apply to all data items?
4

SIMD (Single Instruction Multiple Data)
‚óè SIMD (Single Instruction Multiple Data)
o An architecture for applying one instruction on multiple data items
o ISA includes vector instructions for doing just that
‚ñ™ Along with vector registers to hold multiple data items

‚óè Using MIPS vector instruction extensions:
l.d
$f0,0($sp)
lv
$v1,0($s0)
mulvs.d $v2,$v1,$f0
lv
$v3,0($s1)
addv.d $v4,$v2,$v3
sv
$v4,0($s1)

;$f0 = scalar a
;$v1 = vector x (64 values)
;$v2 = a * vector x
;$v3 = vector y (64 values)
;$v4 = a * vector x + vector y
;vector y = $v4

o Note: no indexing and loop control overhead
o Note: each instruction is fetched and decoded only once

5

SIMD Processor Design
‚óè How would you design a processor for the vector instructions?
data

1. One processing element (PE)
o Fetch and decode instruction once
o PE applies op on each data item
‚ñ™ Item may be in vector register
program
‚ñ™ Item may be in data memory
2. Multiple PEs in parallel
program
o Fetch and decode instruction once
o PEs apply op in parallel
‚ñ™ In synchronous lockstep
PE
‚Üí The more PEs, the faster!
data

control

PE

control

PE

PE

PE

data

data

data
6

Example: Adding Two Vectors
‚óè Instead of having a single FP adder work on each item (a)
‚óè Have four FP adders work on items in parallel (b)
‚óè Each pipelined FP unit is in charge of pre-designated items in vector
o For full parallelization, put as many FP units as there are items

7

Vector Load-Store Unit
‚óè Striding lets you load/store non-contiguous data from memory at
regular offsets. (e.g. the first member of each struct in an array)

0

1

2

3

4

5

0

4

8

C

6

7

8

9

A

B

C

D

E

F

‚óè Gather-scatter lets you put pointers in a vector, then load/store
from arbitrary memory addresses. (gather = load, scatter = store)

0

1

2

3

4

5

2

E

7

4

6

7

8

9

A

B

C

D

E

F
8

How does Gather-Scatter work?

9

When does Gather-Scatter make sense?
‚óè Often data for scientific or AI applications are sparse.
o Time-sampled points where a sensor measurement changes
o In social networking, connections in a N x N friendship matrix
o In neural networks, weights in a filter layer that are non-zero
‚óè To save memory, sparse data is stored in sparse format:

How a sparse filter layer is stored in a
Convolutional Neural Network (CNN)

10

When does Gather-Scatter make sense?
‚óè Convolution works by applying filter like a shifting window:
row column weight

Image

Convolved Feature

0

0

1

0 1 2 3

0

2

1

1 1 0 1

1

1

1

2 0 1 0

2

0

1

3 1 0 1

2

2

1

Filter (sparse matrix format)

‚óè When applying filter on image, a gather needs to take place
1. Gather values in image in corresponding rows and columns
2. Create an image vector out of those gathered values
3. Do a dot (‚óè) product between image vector and filter vector
11

SIMD instructions in real processors
‚óè x86 vector extensions
o Historically: MMX, SSE, AVX, AVX-2
o Current: AVX-512 (512-bit vector instructions)
‚óè ARM vector extensions
o Historically: VFP (Vector Floating Point)
o Current: Neon (128-bit vector instructions)
‚óè Vector instructions have progressively become wider historically
o Due to increase of data parallel applications
o Due to their power efficiency
‚ñ™ Compared to fetching, decoding, scheduling multiple instructions
‚ñ™ Good way to increase FLOPS while staying within TDP limit
‚óè Enter GPUs for general computing (circa 2001)
13

GPUs:
Graphical Processing Units

14

History of GPUs
‚Ä¢ VGA (Video graphic array) has been around since the early 90‚Äôs
‚Ä¢ A display generator connected to some (video) RAM
‚Ä¢ By 2000, VGA controllers were handling almost all graphics computation
‚Ä¢ Programmable through OpenGL, Direct 3D API
‚Ä¢ APIs allowed accelerated vertex/pixel processing:
‚Ä¢ Shading
‚Ä¢ Texture mapping
‚Ä¢ Rasterization
‚Ä¢ Gained moniker Graphical Processing Unit
‚Ä¢ 2007: First general purpose use of GPUs
‚Ä¢ 2007: Release of CUDA language
‚Ä¢ 2011: Release of OpenCL language
15

GPU is Really a SIMD Processor
GPU

Streaming Multi-processor (SM)

Streaming
Processor (SP)

IF/ID

IF/ID

IF/ID

L1 cache

L1 cache

L1 cache

Shared
memory

Shared
memory

Shared
memory

CPU
L2 cache

CPU (Host)
memory

L2 cache
PCIe bus

Global (GPU) memory

‚óè Logically, a GPU is composed of SMs (Streaming Multi-processors)
o An SM is a vector unit that can process multiple pixels (or data items)
‚óè Each SM is composed of SPs which work on each pixel or data item
16

CPU-GPU architecture
GPU

Streaming Multi-processor (SM)

Streaming
Processor (SP)

IF/ID

IF/ID

IF/ID

L1 cache

L1 cache

L1 cache

Shared
memory

Shared
memory

Shared
memory

CPU
L2 cache

CPU (Host)
memory

L2 cache
PCIe bus

Global (GPU) memory

‚óè Dedicated GPU memory separate from system memory
‚óè Code and data must be transferred to GPU memory for it to work on it
o Through PCI-Express bus connecting GPU to CPU
17

Modern GPU architecture
SM=streaming
multiprocessor

TPC = Texture
Processing
TPC = texture Cluster
processing cluster

ROP = raster operations pipeline

SFU = special
function unit

GPU Programming Model
Copy data from CPU
memory to GPU memory

CPU

IF/ID

CPU
memory

Launch the kernel

CPU

CPU

CPU
memory

IF/ID

Global (GPU) memory

IF/ID

CPU
memory

Copy data from GPU
memory to CPU memory

IF/ID

IF/ID

IF/ID

Global (GPU) memory

IF/ID

IF/ID

IF/ID

Global (GPU) memory
19

GPU Programming Model
CPU program
(serial code)

cudaMemcpy ( ‚Ä¶ )

Copy data from CPU
memory to GPU memory

Function <<<nb,nt >>>Launch kernel on GPU

cudaMemcpy ( ‚Ä¶ )
_global_ Function ( ‚Ä¶ )

Copy results from GPU
memory to CPU memory
Implementation of GPU kernel
kernel: Function executed on the GPU

20

GPU Programming Model: Copying Data
/* malloc in GPU global memory */
cudaMalloc (void **pointer, size_t nbytes);
/* free malloced GPU global memory */
cudaFree(void **pointer) ;
/* initialize GPU global memory with value */
cudaMemset (void *pointer, int value, size_t count);
/* copy to and from between CPU and GPU memory */
cudaMemcpy(void *dest, void *src, size_t nbytes, enum cudaMemcopyKind dir);
enum cudaMemcpyKind
‚Ä¢ cudaMemcpyHostToDevice
‚Ä¢ cudaMemcpyDeviceToHost
‚Ä¢ cudaMemcpyDeviceToDevice

CPU
memory

PCIe bus

GPU Global
memory

21

Example: Copying array a to array b using the GPU

22

GPU Programming Model: Launching the Kernel
CPU program
(serial code)

cudaMemcpy ( ‚Ä¶ )

Copy data from CPU
memory to GPU memory

Function <<<nb,nt >>>Launch a kernel with nb
blocks, each with nt threads

cudaMemcpy ( ‚Ä¶ )
_global_ Function ( ‚Ä¶ )

Copy results from GPU
memory to CPU memory
Implementation of kernel
(the function run by each GPU thread)

23

The Execution Model
IF/ID

L1 cache

Shared
memory

‚Ä¢ The thread blocks are dispatched to SMs
‚Ä¢ The number of blocks dispatched to an
SM depends on the SM‚Äôs resources
(registers, shared memory, ‚Ä¶).
Blocks not dispatched initially are dispatched
when an SM frees up after finishing a block
‚Ä¢ When a block is dispatched to an
SM, each of its threads executes on
an SP in the SM.

IF/ID

L1 cache
Shared
memory
24

A thread block is executed in warps
‚óè Each block (up to 1K threads) is divided into groups of 32
threads (called warps) ‚Äì empty threads are used as fillers.
‚óè A warp executes as a SIMD vector instruction on the SM.
‚óè Depending on the number of SPs per SM:
0 1

30 31

o If 32 SP per SM ‚Üí 1 thread of a warp executes on 1 SP
(32 lanes of execution, one thread per lane)
0 1

30 31

01

30 31

o If 16 SP per SM ‚Üí 2 threads are time multiplexed on 1 SP (16
lanes of execution, 2 threads per lane)
0

15

0123

31

0

o If 8 SP per SM ‚Üí 4 threads are time multiplexed on 1 SP
(8 lanes of execution, 4 threads per lane)

7
25

A SM is composed of one or more warp schedulers
‚óè In this SM, there are 4 warp schedulers.

‚óè Warps in thread block are divided
statically among warp schedulers.
‚óè E.g. for 4 schedulers:
o Scheduler 1: Warp 0, Warp 4, ‚Ä¶
o Scheduler 2: Warp 1, Warp 5, ‚Ä¶
o Scheduler 3: Warp 2, Warp 6, ‚Ä¶
o Scheduler 4: Warp 3, Warp 7, ‚Ä¶
‚óè Round robin assignment to ensure
equal distribution of warps
26

Warp scheduling enables latency hiding
‚óè Warp scheduler can hide bubbles (just like a superscalar scheduler)
o But without an instruction queue and out-of-order execution
‚óè How?
o In-order execution.
o Switch to different warp
when a bubble is
about to form.
‚óè Warp can come from any
thread block in SM
o More thread blocks can
lead to higher utilization.
27

All threads execute the same code
‚óè Launched using Kernel <<<1, 64>>> : 1 block with 64 threads
threadIdx.x 0

1

2

3

60

61

62

63

int i = threadIdx.x;
B[i] = A[63-i];
C[i] = B[i] + A[i]

A[0,‚Ä¶,63]
B[0,‚Ä¶,63]
C[0,‚Ä¶,63]
GPU memory

‚óè Each thread in a thread block has a unique ‚Äúthread index‚Äù ‚Üí threadIdx.x
‚óè The same sequence of instructions can apply to different data items.
28

Blocks of Threads
‚óè Launched using Kernel <<<2, 32>>> : 2 blocks of 32 threads
blockIdx.x = 1

blockIdx.x = 0

threadIdx.x 0

1

30

31

0

1

30

31

int i = 32 * blockIdx.x + threadIdx.x;
B[i] = A[63-i];
C[i] = B[i] + A[i]

A[0,‚Ä¶,63]
B[0,‚Ä¶,63]
C[0,‚Ä¶,63]
GPU memory

‚óè Each thread block has a unique ‚Äúblock index‚Äù ‚Üí blockIdx.x
‚óè Each thread has a unique threadIdx.x within its own block
‚óè Can compute a global index from the blockIdx.x and threadIdx.x
29

Two-dimensions grids and blocks
‚óè Launched using Kernel <<<(2, 2), (4, 8)>>> : 2X2 blocks of 4X8 threads
blockIdx.x = 0 blockIdx.x = 1
blockIdx.y = 0 blockIdx.y = 0
0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7

0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7

1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7

1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7

2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7

2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7

3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7

3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7

0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7

0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7

1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7

1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7

2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7

2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7

3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7

3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7

x

0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7
1,0 1,1 1,2 1,3 1,4 1,5 1,6 1,7
2,0 2,1 2,2 2,3 2,4 2,5 2,6 2,7
3,0 3,1 3,2 3,3 3,4 3,5 3,6 3,7

y

blockIdx.x = 0 blockIdx.x = 1
blockIdx.y = 1 blockIdx.y = 1

‚óè Each block has two indices (blockIdx.x, blockIdx.y)
‚óè Each thread in a thread block has two indices (threadIdx.x, threadIdx.y)
30

Example: Computing the global index
threadIdx.x
void main ()
{ cudaMalloc (int* &a, 20*sizeof(int));
blockIdx.x
blockIdx.x
blockIdx.x
blockIdx.x
cudaMalloc (int* &b, 20*sizeof(int));
=0
=1
=2
=3
cudaMalloc (int* &c, 20*sizeof(int));
0 1 2 3 4
0 1 2 3 4
0 1 2 3 4
0 1 2 3 4
‚Ä¶
kernel<<<4,5>>(a, b, c) ;
‚Ä¶
}
NOTE: Each block will consist of one warp ‚Äì
_global_ void kernel(int *a, *b, *c)
{ int i = blockIdx.x * blockDim.x + threadIdx.x ; only 5 threads in warp will do useful work.
(Other 27 threads will execute no-ops.)
a[i] = i ;
b[i] = blockIdx.x;
c[i] = threadIdx.x;
}

a[]
Global
Memory b[]
c[]

0

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19

0

0

0

0

0

1

1

1

1

1

2

2

2

2

2

3

3

3

3

3

0

1

2

3

4

0

1

2

3

4

0

1

2

3

4

0

1

2

3

4
31

Example: Computing ùíö ùíä = ùíÇ ‚àó ùíô ùíä + ùíö ùíä
C program (on CPU)

CUDA program (on CPU+GPU)

void saxpy_serial(int n, float a, float
*x, float *y)
{
for(int i = 0; i<n; i++)
y[i] = a * x[i] + y[i];
}

_global_ void saxpy_gpu(int n, float a, float *x,
float *y)
{
int i = blockIdx.x*blockDim.x +
threadIdx.x;
if (i < n ) y[i] = a * x[i] + y[i];
}

void main ()
{
‚Ä¶
saxpy_serial(n, 2.0, x, y);
‚Ä¶
}

void main ()
{‚Ä¶
// cudaMalloc arrays X and Y
// cudaMemcpy data to X and Y
int NB = (n + 255) / 256;
saxpy_gpu<<<NB, 256>>>(n, 2.0, X, Y);
// cudaMemcpy data from Y
}
32

Example: Computing ùíö ùíä = ùíÇ ‚àó ùíô ùíä + ùíö ùíä
‚óè What happens when n = 1?
_global_void saxpy_gpu(int n, float a, float *X, float *Y)
{
int i = blockIdx.x*blockDim.x + threadIdx.x;
if (i < n ) Y[i] = a * X[i] + Y[i];
}
‚Ä¶..
saxpy_gpu<<<1, 256>>>(1, 2.0, X, Y); /* X and Y are both sized 1! */

‚óè ‚Äúif (i < n)‚Äù condition prevents writing beyond bounds of array.
‚óè But that requires some threads within a warp not performing the write.
o But a warp is a single vector instruction. How can you branch?
o ‚Äúif (i < n)‚Äù creates a predicate ‚Äúmask‚Äù vector to use for the write
o Only thread 0 has predicate turned on, rest has predicate turned off
33

GPUs Use Predication for Branches

‚óè Each thread computes own predicate for condition threadIdx.x < 4
‚óè Taken together, 32 threads of a warp create a 32-bit predicate mask
‚óè Mask is applied to warps for A, B, X, and Y.
‚óè Just like for VLIW processors, this can lead to low utilization.
34

GPU Performance

35

Lesson 1:
Parallelism is Important

36

Thread Level Parallelism
‚óè Superscalars and VLIWs are useful only if‚Ä¶
o Program exhibits ILP (Instruction Level Parallelism) in the code
‚óè GPUs are useful only if‚Ä¶
o Program has TLP (Thread Level Parallelism) in the code
o TLP can be expressed as the number of threads in the code
‚óè How that TLP is laid out in the kernel is also important
o How many threads are in a thread block
‚ñ™ If less than threads in warp, some SPs may get unused
o How many thread blocks are in the grid
‚ñ™ If less than number of SMs, some SMs may get unused
‚Üí If not careful, your GPU may get underutilized
37

Example: Kernels with Bad Layout
‚óè Suppose there are 4 SMs in GPU with 32 SPs in each SM.
o Case 1, 2 below have enough TLP (1024 threads) but bad layout.
o Utilized threads are marked in red. Rest are unused.
‚óè Case 1: Not enough threads
kernel<<<1024, 1>>(‚Ä¶) ;
‚óè Case 2: Not enough blocks
kernel<<<1, 1024>>(‚Ä¶) ;
‚óè Balanced threads and blocks
kernel<<<32, 32>>(‚Ä¶) ;
kernel<<<16, 64>>(‚Ä¶) ;
kernel<<<4, 256>>(‚Ä¶) ;

SM 0

SM 1

SM 2

SM 3

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

SM 0

SM 1

SM 2

SM 3

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

SM 0

SM 1

SM 2

SM 3

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

0 1 2 ‚Ä¶ 31

38

Lesson 2:
Bandwidth is Important

39

Example: Computing ùíö ùíä = ùë® ùíä, ùíã ‚àó ùíô ùíã
C program (on CPU)
void mv_cpu(float* y, float* A,
float* x, int n) {
for (int i=0; i<n; i++)
for (int j=0; j<n; j++)
y[i] += A[i*n + j] * x[j];
}

void main ()
{
‚Ä¶
mv_cpu(y, A, x, n);
‚Ä¶
}

CUDA program (on CPU+GPU)
void mv_gpu(float* y, float* A, float* x, int n) {
int i = blockIdx.x * blockDim.x + threadIdx.x;
if (i < n) {
for (int j = 0; j < n; j++)
y[i] += A[i*n + j] * x[j];
}
}
void main ()
{
‚Ä¶
int nblocks = (n + block_size - 1) / block_size;
mv_gpu <<<nblocks, block_size>>> (y, A, x, n);
‚Ä¶
}
40

Performance Results for ùíö ùíä = ùë® ùíä, ùíã ‚àó ùíô ùíã
‚óè Guess what? CPU is faster than GPU!

But even comparing pure compute
time, CPU is still faster than GPU.
What the‚Ä¶?

* Run on netlab-1.cs.pitt.edu with n=8192:
- Intel Core i7-4770 CPU
- NVIDIA GF119-825-A1 GPU

A lot of time is spent copying back and
forth between CPU and GPU memories.

41

Performance Results for ùíö ùíä = ùë® ùíä, ùíã ‚àó ùíô ùíã
‚óè Was it because the GPU was wimpy and can‚Äôt do enough FLOPS?

‚óè NVIDIA GF119-825-A1 is a Fermi GPU Capability 2.1
o Clock rate: 1046 MHz (X 2 for warp execution)
o Number of SMs: 1
o Number of SPs per SM: 48
o Max FLOPS = 1046 MHz * 2 * 1 * 48 = 100.4 GFLOPS
‚óè What was the FLOPS achieved?
o y[i] += A[i*n + j] * x[j] = 2 FP ops each iteration for n * n iterations
o n = 8192, so FP ops = 8192 * 8192 * 2 = 134 M
o Time = 0.27 seconds (shortest at 32 thread block size)
o FLOPS = 134 M / 0.27 = 496 MFLOPS
o Not even close to the limit!
42

Performance Results for ùíö ùíä = ùë® ùíä, ùíã ‚àó ùíô ùíã
‚óè Could it be that the GPU didn‚Äôt have enough memory bandwidth?

‚óè NVIDIA GF119-825-A1 is a Fermi GPU Capability 2.1
o Memory Type: DDR3
o Memory Bandwidth: 14.00 GB/s
‚óè GPUs also have Performance Monitoring Units (PMUs)
o NVIDIA Profiler (nvprof) provides an easy way to read them:
https://docs.nvidia.com/cuda/profiler-users-guide/index.html
o Let‚Äôs use the PMU to profile the following:
‚ñ™ DRAM Transfer Rate (GB/s)
‚ñ™ L1 Hit Rate (%)
‚ñ™ L2 Hit Rate (%)
43

Memory Wall Hits Again

44

Memory Wall Hits Again

DRAM bandwidth not saturated:
Benefits from more parallelism
due to larger thread block sizes

DRAM bandwidth saturated:
Larger thread blocks ‚Üí increased working set size ‚Üí
higher cache miss rates ‚Üí worse bandwidth problem
DRAM
Bandwidth
Limit (14 GB/s)

45

Is there a way we can reach max FLOPS?
‚óè Let‚Äôs take a look at the GPU design metrics again:
o Max FLOPS = 100.4 GFLOPS
o Memory Bandwidth: 14.00 GB/s
‚óè To sustain max FLOPS, you need to do a lot of work per byte
o 100.4 GFLOPS / 14.00 GB/s = 7.17 FP ops / byte
o Or, about 28 FP ops / float (4 bytes) fetched from memory
o Otherwise, the memory bandwidth cannot sustain the FLOPS
‚óè All GPUs have this problem with memory bandwidth:
o It‚Äôs easy to put in more SMs using transistors for Moore‚Äôs Law
o Your memory bandwidth is limited due to your DDR interface

46

Arithmetic Intensity: A property of the program
‚óè How many FP ops / float for our mat-vec multiplication?
o y[i] += A[i*n + j] * x[j] each iteration with n * n iterations
o FP ops = 2 * n * n (one multiply and one add)
o Float accesses = n * n + 2n (1 matrix and 2 vector accesses)
‚ñ™ That‚Äôs counting only cold misses but could be even more
o So approx. 2 FP ops / float (a far cry from 28 FP ops / float)
o This metric is called arithmetic intensity

‚óè Arithmetic intensity is a property of the program needed by GPUs
o Just like TLP (thread-level-parallelism) is needed by GPUs
o Matrix-vector multiplication has low intensity
‚Üí Fundamentally not suited for fast GPU computation

47

Arithmetic Intensity: A property of the program

* Courtesy of Lawrence Berkeley National Laboratory:
https://crd.lbl.gov/departments/computer-science/par/research/roofline/introduction/

‚óè BLAS: Basic Linear Algebra Subprograms
o BLAS 1: Vector operations only (e.g. saxpy) ‚Üí Bad intensity
o BLAS 2: General Matrix-Vector Multiplication (GeMV) ‚Üí Bad intensity
o BLAS 3: General Matrix Multiplication (GeMM) ‚Üí Good intensity
48

Matrix-Matrix Multiply: Good Arithmetic Intensity
‚óè Matrix-multiplication:
for (int i=0; i<n; i++)
for (int j=0; j<n; j++)
for (int k=0; k<n; k++)
C[i*n + j] += A[i*n + k] * B[k*n + j];
‚óè What‚Äôs the arithmetic intensity for this program?
o FP ops = 2 * n * n * n (one multiply and one add)
o Float accesses = 3 * n * n (3 matrix accesses)
‚ñ™ If we only have cold misses and no capacity misses
o Arithmetic intensity = 2 * n / 3 = 0.66 * n = O(n)
o Implication: The larger the matrix size, the better suited for GPUs!
‚ñ™ Important result for deep learning and other apps
49

Example: Computing ùë™ ùíä, ùíã = ùë® ùíä, ùíå ‚àó ùë© ùíå, ùíã
C program (on CPU)
void mm_cpu(float* C, float* A, float* B,
int n) {
for (int i=0; i<n; i++)
for (int j=0; j<n; j++)
for (int k=0; k<n; k++)
C[i*n + j] += A[i*n + k] * B[k*n + j];
}

void main ()
{
mm_cpu(C, A, B, n);
}

CUDA program (on CPU+GPU)
void mm_gpu(float* C, float* A, float* B, int n) {
float Cvalue = 0;
int i = blockIdx.y * blockDim.y + threadIdx.y;
int j = blockIdx.x * blockDim.x + threadIdx.x;
for (int k = 0; k < n; ++k)
Cvalue += A[i * n + k] * B[k * n + j];
C[i * n + j] = Cvalue;
}
void main ()
{
dim3 dimBlock(block_size, block_size);
dim3 dimGrid(n / dimBlock.x, n / dimBlock.y);
mm_gpu <<<dimGrid, dimBlock>>> (C, A, B, n);
}
50

Performance Results for ùë™ ùíä, ùíã = ùë® ùíä, ùíå ‚àó ùë© ùíå, ùíã

Now GPU is much faster
than CPU given enough TLP

What‚Äôs this? This version of
matrix-multiply reduces
cache capacity misses using
shared memory in the GPU.

51

Capacity Misses can Reduce Arithmetic Intensity
for (int i=0; i<n; i++)
for (int j=0; j<n; j++)
for (int k=0; k<n; k++)
C[i*n + j] += A[i*n + k] * B[k*n + j];

‚óè The ideal arithmetic intensity for this program was:
o FP ops = 2 * n * n * n (one multiply and one add)
o Float accesses = 3 * n * n (3 matrix accesses)
o Arithmetic intensity = 2 * n / 3 = 0.66 * n = O(n)
‚óè Only if we have no capacity misses. What if we did have them?
o For B[k*n + j], reuse distance is the entire matrix of B
o If B[k*n + j] always misses, memory accesses is in the order of n3!
52

So what is Shared Memory?
IF/ID

IF/ID

IF/ID

L1 cache

L1 cache

L1 cache

Shared
memory

Shared
memory

Shared
memory

L2 cache

Global (GPU) memory

‚óè Shared Memory: memory shared among threads in a thread block
o Variables declared with __shared__ modifier live in shared memory
o Is same as L1 cache in terms of latency and bandwidth!
o Storing frequently used data in shared memory can save on bandwidth
53

Loop Tiling with Shared Memory
‚óè Store a ‚Äútile‚Äù within matrix in shared memory while operating on it
o Can reduce accesses to DRAM memory
‚óè Code in: https://docs.nvidia.com/cuda/cuda-c-best-practicesguide/index.html#shared-memory-in-matrix-multiplication-c-ab
__shared__ float aTile[TILE_DIM][TILE_DIM],
bTile[TILE_DIM][TILE_DIM];
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
float sum = 0.0f;
aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];
bTile[threadIdx.y][threadIdx.x] = b[threadIdx.y*N+col];
__syncthreads();
for (int i = 0; i < TILE_DIM; i++)
sum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];
c[row*N+col] = sum;
‚óè Assumption: TILE_DIM = w. What if w > TILE_DIM?

54

Loop Tiling with Shared Memory
‚óè TILE_DIM is limited by amount of shared memory. What if w > TILE_DIM?
o Now must load A and B tiles w / TILE_DIM times per thread block
‚óè Now code will look like:
N
__shared__ float aTile[TILE_DIM][TILE_DIM],
B
bTile[TILE_DIM][TILE_DIM];
float sum = 0.0f;
for (int t = 0; t < w / TILE_DIM; t++) {
‚Ä¶
aTile[threadIdx.y][threadIdx.x] = ‚Ä¶;
A
C
bTile[threadIdx.y][threadIdx.x] = ‚Ä¶;
__syncthreads();
N
for (int i = 0; i < TILE_DIM; i++)
sum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];
w
}
c[row*N+col] = sum;
55

w

